,paper_text
3081,"Localizing Bugs in Program Executions
with Graphical Models

Valentin Dallmeier
Saarland University
Saarbruecken, Germany
dallmeier@cs.uni-saarland.de

Laura Dietz
Max-Planck Institute for Computer Science
Saarbruecken, Germany
dietz@mpi-inf.mpg.de
Andreas Zeller
Saarland University
Saarbruecken, Germany
zeller@cs.uni-saarland.de

Tobias Scheffer
Potsdam University
Potsdam, Germany
scheffer@cs.uni-potsdam.de

Abstract
We devise a graphical model that supports the process of debugging software by
guiding developers to code that is likely to contain defects. The model is trained
using execution traces of passing test runs; it reflects the distribution over transitional patterns of code positions. Given a failing test case, the model determines the least likely transitional pattern in the execution trace. The model is
designed such that Bayesian inference has a closed-form solution. We evaluate
the Bernoulli graph model on data of the software projects AspectJ and Rhino.

1

Introduction

In today?s software projects, two types of source code are developed: product and test code. Product
code, also referred to as the program, contains all functionality and will be shipped to the customer.
The program and its subroutines are supposed to behave according to a specification. The example
program in Figure 1 (left), is supposed to always return the value 10. It contains a defect in line
number 20, which lets it return a wrong value if the input variable equals five.
In addition to product code, developers write test code that consists of small test programs, each
testing a single procedure or module for compliance with the specification. For instance, Figure 1
(right) shows three test cases, the second of which reveals the defect. Development environments
provide support for running test cases automatically and would report failure of the second test case.
Localizing defects in complex programs is a difficult problem because the failure of a test case
confirms only the existence of a defect, not its location.
When a program is executed, its trace through the source code can be recorded. An executed line of
source code is identified by a code position s ? S. The stream of code positions forms the trace t
of a test case execution. The data that our model analyses consists of a set T of passing test cases
t. In addition to the passing tests we are given a single trace t? of a failing test case. The passing
test traces and the trace of the failing case refer to the same code revision; hence, the semantics of
each code position remain constant. For the failing test case, the developer is to be provided with a
ranking of code positions according to their likelihood of being defective.
The semantics of code positions may change across revisions, and modifications of code may impact
the distribution of execution patterns in the modified as well as other locations of the code. We focus
on the problem of localizing defects within a current code revision. After each defect is localized,
the code is typically revised and the semantics of code positions changes. Hence, in this setting, we
1

10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25

/**
* A procedure containing a defect.
*
* @param param an arbitrary parameter.
* @return 10
*/
public static int defect (int param) {
int i = 0;
while (i < 10) {
if (param == 5) {
return 100;
}
i++;
}
return i;
}

public static class TestDefect extends TestCase {
public void testParam1() {
assertEquals(10, defect(1));
}
/** Failing test case. */
public void testParam5() {
assertEquals(10, defect(5));
}

}

public void testParam10() {
assertEquals(10, defect(10));
}

Figure 1: Example with product code (left) and test code (right).
cannot assume that any negative training data?that is, previous failing test cases of the same code
revision?are available. For that reason, discriminative models do not lend themselves to our task.
Instead of representing the results as a ranked list of positions, we envision a tight integration in
development environments. For instance, on failure of a test case, the developer could navigate
between predicted locations of the defect, starting with top ranked positions.
So far, Tarantula [1] is the standard reference model for localizing defects in execution traces. The
authors propose an interface widget for test case results in which a pixel represents a code position.
The hue value of the pixel is determined by the number of failing and passing traces that execute this
position and correlates with the likelihood that s is faulty [1]. Another approach [2] includes return
values and flags for executed code blocks and builds on sensitivity and increase of failure probability.
This approach was continued in project Holmes [3] to include information about executed control
flow paths. Andrzejewski et al. [4] extend latent Dirichlet allocation (LDA) [5] to find bug patterns in
recorded execution events. Their probabilistic model captures low-signal bug patterns by explaining
passing executions from a set of usage topics and failing executions from a mix of usage and bug
topics. Since a vast amount of data is to be processed, our approach is designed to not require
estimating latent variables during prediction as is necessary with LDA-based approaches [4].
Outline. Section 2 presents the Bernoulli graph model, a graphical, generative model that explains
program executions. This section?s main result is the closed-form solution for Bayesian inference of
the likelihood of a transitional pattern in a test trace given example execution traces. Furthermore,
we discuss how to learn hyperparameters and smoothing coefficients from other revisions, despite
the fragile semantics of code positions. In Section 3, reference methods and simpler probabilistic
models are detailed. Section 4 reports on the prediction performance of the studied models for the
AspectJ and Rhino development projects. Section 5 concludes.

2

Bernoulli Graph Model

The Bernoulli graph model is a probabilistic model that generates program execution graphs. In
contrast to an execution trace, the graph is a representation of an execution that abstracts from the
number of iterations over code fragments. The model allows for Bayesian inference of the likelihood
of a transition between code positions within an execution, given previously seen executions.
The n-gram execution graph Gt = (Vt , Et , Lt ) of an execution t connects vertices Vt by edges
Et ? Vt ? Vt . Labeling function Lt : Vt ? S (n?1) injectively maps vertices to n ? 1-grams of
code positions, where S is the alphabet of code positions.
In the bigram execution graph, each vertex v represents a code position Lt (v); each arc (u, v)
indicates that code position Lt (v) has been executed directly after code position Lt (u) at least once
during the program execution. In n-gram execution graphs, each vertex v represents a fragment
Lt (v) = s1 . . . sn?1 of consecutively executed statements. Vertices u and v can only be connected
by an arc if the fragments are overlapping in all but the first code position of u and the last code
position of v; that is, Lt (u) = s1 . . . sn?1 and Lt (v) = s2 . . . sn . Such vertices u and v are
2

e 17

ee
18 17

17 18

0 ~ y22 18 17
18 19

1 ~ y22 18 19

0 ~ y22 18 18

22 18

0~
y22 18 20
19 22

18 18
1 ~ y22 18 24

18 24

0 ~ y22 18 23

18 20 ... 18 23

17 | 18 | 19 | 22 | 18| 19 | 22 |?. | 22| 18 | 24

Figure 2: Expanding vertex ?22 18? in the generation of a tri-gram execution graph corresponding
to the trace at the bottom. Graph before expansion is drawn in black, new parts are drawn in solid
red.
connected by an arc if code positions s1 . . . sn are executed consecutively at least once during the
execution. For the example program in Figure 1 the tri-gram execution graph is given in Figure 2.
Generative process. The Bernoulli graph model generates one graph Gm,t = (Vm,t , Em,t , Lm,t )
per execution t and procedure m. The model starts the graph generation with an initial vertex
representing a fragment of virtual code positions ?.
In each step, it expands a vertex u labeled Lm,t (u) = s1 . . . sn?1 that has not yet been expanded;
e.g., vertex ?22 18? in Figure 2. Expansion proceeds by tossing a coin with parameter ?m,s1 ...sn
for each appended code position sn ? S. If the coin toss outcome is positive, an edge to vertex v
labeled Lm,t (v) = s2 . . . sn is introduced. If Vm,t does not yet include a vertex v with this labeling,
it is added at this point. Each vertex is expanded only once. The process terminates if no vertex is
left that has been introduced but not yet expanded. Parameters ?m,s1 ...sn are governed by a Beta
distribution with fixed hyperparameters ?? and ?? . In the following we focus on the generation
of edges, treating the vertices as observed. Figure 3a) shows a factor graph representation of the
generative process and Algorithm 1 defines the generative process in detail.
Inference. Given a collection Gm of previously seen execution graphs for method m and a
new execution Gm = (Vm , Em , Lm ), Bayesian inference determines the likelihood p((u, v) ?
Em |Vm , Gm , ?? , ?? ) of each of the edges (u, v), thus indicating unlikely transitions in the new
execution of m represented by execution graph Gm . Since we employ independent models for all
Algorithm 1 Generative process of the Bernoulli graph model.
for all procedures m do
n
for all s1 ...sn ? (Sm ) do
draw ?m,s1 ...sn ? Beta(?? , ?? ).
for all executions t do
create a new graph Gm,t .
add a vertex u labeled ??...?.
initialize queue Q = {u}.
while queue Q is not empty do
dequeue u ? Q, with L(u) = s1 . . . sn?1 .
for all sn ? Sm do
let v be a vertex with L(v) = s2 . . . sn .
draw b ? Bernoulli(?m,s1 ...sn ).
if b = 1 then
if v ?
/ Vm,t then
add v to Vm,t .
enqueue v ? Q.
add arc (u, v) to Em,t .

3

??

??

false

??

??

Beta

Beta
Edge
coin

u V

(u,v)

b

for each graph G
for each code position s
for each vertex u
for each procedure m

a) Bernoulli graph

f

?

for each fragment f
for each procedure m

Bernoulli

E

Code Pos.
distr

?

?

true false
Bern Equals

Symm. Dirichlet

Procedure
distr

Fragment
coin

?

??

??

Symm.
Dirichlet

Multinomial
Procedure

m

t

b

for each trace t

Fragment
n

for each fragment f S
for each procedure m

b) Bernoulli fragment

f=si-1,...

m,f
Multi

Code pos.

s

for each code position in t
for each trace t

c) Multinomial n-gram

Figure 3: Generative models in directed factor graph notation with dashed rectangles indicating
gates [6].
methods m, inference can be carried out for each method separately. Since vertices Vm are observed, coin parameters ? are d-separated from each other (cf. Figure 3a). We yield independent
Beta-Bernoulli models conditioned on the presence of start vertices u. Thus, predictive distributions
for presence of edges in future graphs can be derived in closed form (Equation 1) where #Gu denotes
the number of training graphs containing vertices labeled L(u) and #G(u,v) denotes the number of
training graphs containing edges between vertices labeled L(u) and L(v). See the appendix for a
detailed derivation of Equation 1.
#G(u,v) + ??
p((u, v) ? Em |Vm , Gm , ?? , ?? ) = G
.
(1)
#u + ?? + ??
By definition, an execution graph G for an execution contains a vertex if its label is a substring of
the execution?s trace t. Likewise, an edge is contained if an aggregation of the vertex labels is a
substring of t. It follows1 that the predictive distribution can be reformulated as in Equation 2 to
predict the probability of seeing the code position s? = sn after a fragment of preceding statements
f? = s1 . . . sn?1 using the trace representation of an execution. Thus, it is not neccessary to represent
execution graphs G explicitly.
#{t ? T |f?s? ? t} + ??
p(?
s|f?, T, ?? , ?? ) =
(2)
#{t ? T |f? ? t} + ?? + ??
Estimating interpolation coefficients and hyperparameters. For given hyperparameters and
fixed context length n, Equation 2 predicts the likelihood for s?i following a fragment f? =
s?i?1 . . . s?i?n+1 . To avoid sparsity issues while maintaining good expressiveness, we smooth various
context lengths up to N by interpolation.
N
X
p(?
si |?
si?1 . . . s?i?N +1 , T, ?? , ?? , ?) =
p(n|?) ? p(?
si |?
si?1 . . . s?i?n+1 , T, ?? , ?? )
n=1

We can learn from different revisions by integrating multiple Bernoulli graphs models in a generative
process, in which coin parameters are not shared across revisions and context lengths n. This process
generates a stream of statements with defect flags. We learn hyperparameters ?? and ?? jointly with
? using an automatically derived Gibbs sampling algorithm [7].
Predicting defective code positions. Having learned point estimates for ?
? ? , ??? , and ?? from other
revisions in a leave-one-out fashion, statements s? are scored by the complementary event of being
normal for any preceding fragment f?.


?
score(?
s) = max
1 ? p(?
s|f?, T, ?
? ? , ??? , ?)
(3)
f? preceding s?

The maximum is justified because an erroneous code line may show its defective behavior only
in combination with some preceding code fragments, and even a single erroneous combination is
enough to lead to defective behavior of the software.
1

For a set A we denote its cardinality by #A rather than |A| to avoid confusion with conditioned signs.

4

3

Reference Methods

The Tarantula model is a popular scoring heuristic for defect localization in software engineering.
We will prove a connection between Tarantula and the unigram variant of a Bernoulli graph model.
Furthermore, we will discuss other reference models which we will consider in the experiments.
3.1

Tarantula

Tarantula [1] scores the likelihood of a code position s being defective according to the proportions
of failing F and passing traces T that execute this position (Equation 4).
scoreT arantula (?
s) =

#{t??F |?
s?t?}
#{t??F }
#{t??F |?
s?t?}
|?
s?t}
+ #{t?T
#{t??F }
#{t?T }

(4)

For the case that only one test case fails, we can show an interesting relationship between Tarantula,
the unigram Bernoulli graph model, and multivariate Bernoulli models (referred to in [8]). In the
unigram case, the Bernoulli graph model generates a graph in which all statements in an execution
are directly linked to an empty start vertex. In this case, the Bernoulli graph model is equal to a
multi-variate Bernoulli model generating a set of statements for each execution.
Using an improper prior ?? = ?? = 0, the unigram Bernoulli graph model scores a statement by
|?
s?t}
#{t?T |?
s?t}
scoreGraph (?
s) = 1 ? #{t?T
#{t?T } . Letting g(s) =
#{t?T } , the rank order of any two code
1
1
positions s1 , s2 is determined by 1 ? g(s1 ) > 1 ? g(s2 ) or equivalently 1+g(s
> 1+g(s
which is
1)
2)
Tarantula?s ranking criterion if #F is 1.
3.2

Bernoulli Fragment Model

Inspired by this equivalence, we study a naive n-gram extension to multi-variate Bernoulli models
which we call Bernoulli fragment model. Instead of generating a set of statements, the Bernoulli
model may generate a set of fragments for each execution.
Given a fixed order n, the Bernoulli fragment model draws a coin parameter for each possible
fragment f = s1 . . . sn over the alphabet Sm . For each execution the fragment set is generated by
tossing a fragment?s coin and including all fragments with outcome b = 1 (cf. Figure 3b). The
#{t?T |f??t}+?
probability of an unseen fragment f? is given by p(f?|T, ?? , ?? ) = #{t?T }+?? +??? .
The model deviates from reality in that it may generate fragments that may not be aggregateable
into a consistent sequence of code positions. Thus, non-zero probability mass is given to impossible
events, which is a potential source of inaccuracy.
3.3

Multinomial Models

The multinomial model is popular in the text domain?e.g., [8]. In contrast to the Bernoulli graph
model, the multinomial model takes the number of occurrences of a pattern within an execution into
account. It consists of a hierarchical process in which first a procedure m is drawn from multinomial
distribution ?, then a code position s is drawn from the multinomial distribution ?m ranging over all
code positions Sm in the procedure.
The n-gram model is a well-known extension of the unigram multinomial model, where the distributions ? are conditioned on the preceding fragment of code positions f = s1 . . . sn?1 to draw
a follow-up statement sn ? ?m,f . Using fixed symmetric Dirichlet distributions with parameter
?? and ?? as priors for the multinomial distributions, the probability for unseen code positions s?
following on fragment f? is given in Equation 5. Shorthand #Ts?m denotes how often statements in
prodecure m are executed (summing over all traces t ? T in the training set); and #Tm,s1 ...sn denotes
the number times statements s1 . . . sn are executed subsequently by procedure m.
#Tm,
+ ??
#Ts?m
? f?s?
? + ??
p(?
s, m|
? f?, T, ?? , ?? ) ? P
?
T
T
#
# ? f? + ?? #Sm
0 + ?? #M
0
?
| m ?M s?m
{z
} | m,
{z
}
?(m)
?

5

?m,
s)
? f?(?

(5)

3.4

Holmes

Chilimbi et al. [3] propose an approach that relies on a stream of sampled boolean predicates P ,
each corresponding to an executed control flow branch starting at code position s. The approach
evaluates whether P being true increases the probability of failure in contrast to reaching the code
position by chance. Each code position is scored according to the importance of its predicate P
which is the harmonic mean of sensitivity and increase in failure probability. Shorthands Fe (P ) and
Se (P ) refer to the failing/passing traces that executed the path P , where Fo (P ) and So (P ) refer to
failing/passing traces that executed the start point of P .
2

Importance(P ) =
log #F
log Fe (P )

+



Fe (P )
Se (P )+Fe (P )

?

Fo (P )
So (P )+Fo (P )

?1

This scoring procedure is not applicable to cases where a path is executed in only one failing trace, as
a division by zero occurs in the first term when Fe (P ) = 1. This issue renders Holmes inapplicable
to our case study where typically only one test case fails.
3.5

Delta LDA

Andrzejewski et al. [4] use a variant of latent Dirichlet Allocation (LDA) [5] to identify topics
of co-occurring statements. Most topics may be used to explain passing and failing traces, where
some topics are reserved to explain statements in the failing traces only. This is obtained by running
LDA with different Dirichlet priors on passing and failing traces. After inference, the topic specific
statement distributions ? = p(s|z) are converted to p(z|s) via Bayes? rule. Then statements j are
ranked according to the confidence Sij = p(z = i|s = j) ? maxk6=i p(z = k|s = j) of being rather
about a bug topic i than any other topic k.

4

Experimental Evaluation

In this section we study empirically how accurately the Bernoulli graph model and the reference
models discussed in Section 3 localize defects that occurred in two large-scale development projects.
We find that data used for previous studies is not appropriate for our investigation. The SIR repository [9] provides traces of small programs into which defects have been injected. However, as
pointed out in [10], there is no strong argument as to why results obtained on specifically designed
programs with artificial defects should necessarily transfer to realistic software development projects
with actual defects. The Cooperative Bug Isolation project [11], on the other hand, collects execution
data from real applications, but records only a random sample of 1% of the executed code positions;
complete execution traces cannot be reconstructed. Therefore, we use the development history of
two large-scale open source development projects, AspectJ and Rhino, as gathered in [12].
Data set. From Rhino?s and AspectJ?s bug database, we select defects which are reproducable by
a test case and identify corresponding revisions in the source code repository. For such revisions,
the test code contains a test case that fails in one revision, but passes in the following revision. We
use the code positions that were modified between the two revisions as ground truth for the defective
code positions D. For AspectJ, these are one or two lines of code; the Rhino project contains larger
code changes. For each such revision, traces T of passing test cases are recorded on a line number
basis. In the same manner, the failing trace t (in which the defective code is to be identified) is
recorded.
The AspectJ data set consists of 41 defective revisions and a total of 45 failing traces. Each failing
trace has a length of up to 2,000,000 executed statements covering approx. 10,000 different code
positions (of the 75,000 lines in the project), spread across 300 to 600 files and 1,000 to 4,000
procedures. For each revision, we recorded 100 randomly selected valid test cases (drawn out of
approx. 1000).
Rhino consists of 15 defective revisions with one failing trace per bug. Failing traces
have an average length of 3,500,000 executed statements, covering approx. 2,000 of 38,000
6

AspectJ: h = 0

AspectJ: h = 1

Recall

0.4
0.3

Recall

n-gram Bernoulli Graph
n-gram Bernoulli Fragment
n-gram Multinomial
Unigram Multinomial
Tarantula
Delta LDA

0.5

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.2

0.1

0.1

0.1

0

0
0

0.2

0.4
0.6
Top k%

0.8

1

0
0

0.2

Rhino: h = 0

0.4
0.6
Top k%

0.8

1

0

0.5

0.5

0.5

0.4

0.4

0.4
Recall

0.6

0.3

0.3
0.2

0.2

0.1

0.1

0.1

0

0
0.4
0.6
Top k%

0.8

1

0.8

1

0.8

1

0.3

0.2

0.2

0.4
0.6
Top k%
Rhino: h = 10

0.6

0

0.2

Rhino: h = 1

0.6

Recall

Recall

AspectJ: h = 10

0.6

Recall

0.6

0
0

0.2

0.4
0.6
Top k%

0.8

1

0

0.2

0.4
0.6
Top k%

Figure 4: Recall of defective code positions within the 1% highest scored statements for AspectJ
(top) and Rhino (bottom), for windows of h = 0, h = 1, and h = 10 code lines.
code positions, spread across 70 files and 650 procedures. We randomly selected 100 of
the 1500 valid traces for each revision as training data. Both data sets are available at
http://www.mpi-inf.mpg.de/~dietz/debugging.html.
Evaluation criterion. Following the evaluation in [1], we evaluate how well the models are able
to guide the user into the vicinity of a defective code position. The models return a ranked list of
code positions. Envisioning that the developer can navigate from the ranking into the source code
to inspect a code line within its context, we evaluate the rank k at which a line of code occurs that
lies within a window of ?h lines of code of a defective line. We plot relative ranks; that is, absolute
ranks divided by the number of covered code lines, corresponding to the fraction of code that the
developer has to walk through in order to find the defect. We examine the recall@k%, that is the
fraction of successfully localized defects over the fraction of code the user has to inspect. We expect
a typical developer to inspect the top 0.25% of the ranking, corresponding to approximately 25 ranks
for AspectJ.
Neither the AUC nor the Normalized Discounted Cummulative Gain (NDCG) appropriately measure
performance in our application. AUC does not allow for a cut-off rank; NDCG will inappropriately
reward cases in which many statements in a defect?s vicinity are ranked highly.
Reference methods. In order to study the helpfulness of each generative model, we evaluate
smoothed models with maximum length N = 5 for each the multinomial, Bernoulli fragment and
Bernoulli graph model. We compare those to the unigram multinomial model and Tarantula. Tuning
and prediction of reference methods follow in accordance to Section 2. In addition, we compare to
the latent variable model Delta LDA with nine usage and one bug topics, ? = 0.5, ? = 0.1, and 50
sampling iterations.
Results. The results are presented in Figure 4. The Bernoulli graph model is always ahead of the
reference methods that have a closed form solution in the top 0.25% and top 0.5% of the ranking.
This improvement is significant with level 0.05 in comparison to Tarantula for h = 1 and h = 10. It
is significantly better than the n-gram multinomial model for h = 1. Although increasing h makes
the prediction problem generally easier, only Bernoulli graph and the multinomial n-gram model
play to their strength.
A comparison by Area under the Curve in top 0.25% and top 0.5% indicates that the Bernoulli
graph is more than twice as effective as Tarantula for the data sets for h = 1 and h = 10. Using the
7

Bernoulli graph model, a developer finds nearly every second bug in the top 1% in both data sets,
where ranking a failing trace takes between 10 and 20 seconds.
According to a pair-t-test with 0.05-level, Bernoulli graph?s prediction performance is significantly
better than Delta LDA for the Rhino data set. No significant diffference is found for the AspectJ
data set, but Delta LDA takes much longer to compute (approx. one hour versus 20 seconds) since
parameters can not be obtained in closed form but require iterative sampling.
Analysis. Most revisions in our data sets had bugs that were equally difficult for most of the
models. From revisions where one model drastically outperformed the others we identified different
categories of suspicious code areas. In some cases, the defective procedures were executed in very
few or no passing trace; we refer such code as being insufficiently covered. Another category refers
to defective code lines in the vicinity of branching points such as if-statements. If code before the
branch point is executed in many passing traces, but code in one of the branches only rarely, we call
this a suspicious branch point.
The Bernoulli fragment model treats both kinds of suspicious code areas in a similar way. They
have a different effect on the predictive Beta-posteriors in the Bernoulli graph model: insufficient
coverage decreases the confidence; suspicious branch points will decrease the mean. The Betapriors ?? and ?? play a crucial role in weighting these two types of potential bugs in the ranking
and encode prior beliefs on expecting one or the other. Our hyperparameter estimation procedure
usually selects ?? = 1.25 and ?? = 1.03 for all context lengths.
Revisions in which Bernoulli fragment outperformed Bernoulli graph contained defects in insufficiently covered areas. Presumably, Bernoulli graph identified many suspicious branching points, and
assigned them a higher score. Revisions in which Bernoulli graph outperformed Bernoulli fragment
contained bugs at suspicious branching points.
In contrast to the Bernoulli-style models, the multinomial models take the number of occurrences of
a code position within a trace into account. Presumably, multiple occurrences of code lines within a
trace do not indicate their defectiveness.

5

Conclusions

We introduced the Bernoulli graph model, a generative model that implements a distribution over
program executions. The Bernoulli graph model generates n-gram execution graphs. Compared
to execution traces, execution graphs abstract from the number of iterations that sequences of code
positions have been executed for. The model allows for Bayesian inference of the likelihood of
transitional patterns in a new trace, given execution traces of passing test cases. We evaluated the
model and several less complex reference methods with respect to their ability to localize defects
that occurred in the development history of AspectJ and Rhino. Our evaluation does not rely on
artificially injected defects.
We find that the Bernoulli graph model outperforms Delta LDA on Rhino and performs as good
as Delta LDA on the AspectJ project, but in substantially less time. Delta LDA is based on a
multinomial unigram model, which performs worst in our study. This gives raise to the conjecture
that Delta LDA might benefit from replacing the multinomial model with a Bernoulli graph model.
this conjecture would need to be studied empirically.
The Bernoulli graph model outperforms the reference models with closed-form solution with respect
to giving a high rank to code positions that lie in close vicinity of the actual defect. In order to find
every second defect in the release history of Rhino, the Bernoulli graph model walks the developer
through approximately 0.5% of the code positions and 1% in the AspectJ project.
Acknowledgements
Laura Dietz is supported by a scholarship of Microsoft Research Cambridge. Andreas Zeller and
Tobias Scheffer are supported by a Jazz Faculty Grant.

8

References
[1] James A. Jones and Mary J. Harrold. Empirical evaluation of the tarantula automatic faultlocalization technique. In Proceedings of the International Conference on Automated Software
Engineering, 2005.
[2] Ben Liblit, Mayur Naik, Alice X. Zheng, Alex Aiken, and Michael I. Jordan. Scalable statistical bug isolation. In Proceedings of the Conference on Programming Language Design and
Implementation, 2005.
[3] Trishul Chilimbi, Ben Liblit, Krishna Mehra, Aditya Nori, and Kapil Vaswani. Holmes: Effective statistical debugging via efficient path profiling. In Proceedings of the International
Conference on Software Engineering, 2009.
[4] David Andrzejewski, Anne Mulhern, Ben Liblit, and Xiaojin Zhu. Statistical debugging using
latent topic models. In Proceedings of the European Conference on Machine Learning, 2007.
[5] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet allocation. Journal of
Machine Learning Research, 3:993?1022, 2003.
[6] Tom Minka and John Winn. Gates. In Advances in Neural Information Processing Systems,
2008.
[7] Hal Daume III. Hbc: Hierarchical Bayes Compiler. http://hal3.name/HBC, 2007.
[8] Andrew McCallum and Kamal Nigam. A comparison of event models for Naive Bayes text
classification. In Proceedings of the AAAI Workshop on Learning for Text Categorization,
1998.
[9] Hyunsook Do, Sebastian Elbaum, and Gregg Rothermel. Supporting controlled experimentation with testing techniques: An infrastructure and its potential impact. Empirical Software
Engineering, 10(4):405?435, October 2005.
[10] Lionel C. Briand. A critical analysis of empirical research in software testing. In Proceedings
of the Symposium on Empirical Software Engineering and Measurement, 2007.
[11] Ben Liblit, Mayur Naik, Alice X. Zheng, Alex Aiken, and Michael I. Jordan. Public deployment of cooperative bug isolation. In Proceedings of the Workshop on Remote Analysis and
Measurement of Software Systems, 2004.
[12] Valentin Dallmeier and Thomas Zimmermann. Extraction of bug localization benchmarks from
history. In Proceedings of the International Conference on Automated Software Engineering,
2007.

9

"
6184,"Combinatorial Energy Learning for Image
Segmentation

Jeremy Maitin-Shepard
UC Berkeley Google
jbms@google.com
Peter Li
Google
phli@google.com

Viren Jain
Google
viren@google.com

Michal Januszewski
Google
mjanusz@google.com

Pieter Abbeel
UC Berkeley
pabbeel@cs.berkeley.edu

Abstract
We introduce a new machine learning approach for image segmentation that uses a
neural network to model the conditional energy of a segmentation given an image.
Our approach, combinatorial energy learning for image segmentation (CELIS)
places a particular emphasis on modeling the inherent combinatorial nature of
dense image segmentation problems. We propose efficient algorithms for learning
deep neural networks to model the energy function, and for local optimization of
this energy in the space of supervoxel agglomerations. We extensively evaluate
our method on a publicly available 3-D microscopy dataset with 25 billion voxels
of ground truth data. On an 11 billion voxel test set, we find that our method
improves volumetric reconstruction accuracy by more than 20% as compared to
two state-of-the-art baseline methods: graph-based segmentation of the output
of a 3-D convolutional neural network trained to predict boundaries, as well as a
random forest classifier trained to agglomerate supervoxels that were generated by
a 3-D convolutional neural network.

1

Introduction

Mapping neuroanatomy, in the pursuit of linking hypothesized computational models consistent
with observed functions to the actual physical structures, is a long-standing fundamental problem
in neuroscience. One primary interest is in mapping the network structure of neural circuits by
identifying the morphology of each neuron and the locations of synaptic connections between
neurons, a field called connectomics. Currently, the most promising approach for obtaining such
maps of neural circuit structure is volume electron microscopy of a stained and fixed block of
tissue. [4, 16, 17, 10] This technique was first used successfully decades ago in mapping the structure
of the complete nervous system of the 302-neuron Caenorhabditis elegans; due to the need to
manually cut, image, align, and trace all neuronal processes in about 8000 50 nm serial sections, even
this small circuit required over 10 years of labor, much of it spent on image analysis. [31] At the time,
scaling this approach to larger circuits was not practical.
Recent advances in volume electron microscopy [11, 20, 15] make feasible the imaging of large
circuits, potentially containing hundreds of thousands of neurons, at sufficient resolution to discern
even the smallest neuronal processes. [4, 16, 17, 10] The high image quality and near-isotropic
resolution achievable with these methods enables the resultant data to be treated as a true 3-D volume,
which significantly aids reconstruction of processes that do not run parallel to the sectioning axis, and
is potentially more amenable to automated image processing.
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

E s3
Fully-connected layer
Convolutional
neural network

E(S; I)

Image (I)

E s2

all voxel positions x

Boundary
classification

E s1

Global
energy

P

Shape descriptors

Candidate
segmentation
(S)
Initial oversegmentation

Agglomeration

Local energy

Figure 1: Illustration of computation of global energy for a single candidate segmentation S. The
local energy Es (x; S; I) ? [0, 1], computed by a deep neural network, is summed over all shape
descriptor types s and voxel positions x.
Image analysis remains a key challenge, however. The primary bottleneck is in segmenting the
full volume, which is filled almost entirely by heavily intertwined neuronal processes, into the
volumes occupied by each individual neuron. While the cell boundaries shown by the stain provide
a strong visual cue in most cases, neurons can extend for tens of centimeters in path length while
in some places becoming as narrow as 40 nm; a single mistake anywhere along the path can render
connectivity information for the neuron largely inaccurate. Existing automated and semi-automated
segmentation methods do not sufficiently reduce the amount of human labor required: a recent
reconstruction of 950 neurons in the mouse retina required over 20000 hours of human labor, even
with an efficient method of tracing just a skeleton of each neuron [18]; a recent reconstruction of
379 neurons in the Drosophila medulla column (part of the visual pathway) required 12940 hours of
manual proof-reading/correction of an automated segmentation [26].
Related work: Algorithmic approaches to image segmentation are often formulated as variations on
the following pipeline: a boundary detection step establishes local hypotheses of object boundaries, a
region formation step integrates boundary evidence into local regions (i.e. superpixels or supervoxels),
and a region agglomeration step merges adjacent regions based on image and object features. [1, 19,
30, 2] Although extensive integration of machine learning into such pipelines has begun to yield
promising segmentation results [3, 14, 22], we argue that such pipelines, as previously formulated,
fundamentally neglect two potentially important aspects of achieving accurate segmentation: (i) the
combinatorial nature of reasoning about dense image segmentation structure,1 and (ii) the fundamental
importance of shape as a criterion for segmentation quality.
Contributions: We propose a method that attempts to overcome these deficiencies. In particular,
we propose an energy-based model that scores segmentation quality using a deep neural network
that flexibly integrates shape and image information: Combinatorial Energy Learning for Image
Segmentation (CELIS). In pursuit of such a model this paper makes several specific contributions:
a novel connectivity region data structure for efficiently computing the energy of configurations of
3-D objects; a binary shape descriptor for efficient representation of 3-D shape configurations; a
neural network architecture that splices the intermediate unit output from a trained convolutional
network as input to a deep fully-connected neural network architecture that scores a segmentation
and 3-D image; a training procedure that uses pairwise object relations within a segmentation to
learn the energy-based model. an experimental evaluation of the proposed and baseline automated
reconstruction methods on a massive and (to our knowledge) unprecedented scale that reflects the
true size of connectomic datasets required for biological analysis (many billions of voxels).

2

Conditional energy modeling of segmentations given images

We define a global, translation-invariant energy model for predicting the cost of a complete segmentation S given a corresponding image I. This cost can be seen as analogous to the negative
1

While prior work [30, 14, 2] has recognized the importance of combinatorial reasoning, the previously
proposed global optimization methods allow local decisions to interact only in a very limited way.

2

log-likelihood of the segmentation given the image, but we do not actually treat it probabilistically.
Our goal is to define a model such that the true segmentation corresponding to a given image can be
found by minimizing the cost; the energy can reflect both a prior over object configurations alone, as
well as compatibility between object configurations and the image.
As shown in Fig. 1, we define the global energy E(S; I) as the sum over local energy models (defined
by a deep neural network) Es (x; S; I) at several different scales s computed in sliding-window
fashion centered at every position x within the volume:
XX
E(S; I) :=
Es (x; S; I),
s

x

?s (rs (x; S); ?(x; I)) .
Es (x; S; I) := E
The local energy Es (x; S; I) depends on the local image context centered at position x by way
of a vector representation ?(x; I) computed by a deep convolutional neural network, and on the
local shape/object configuration at scale s by way of a novel local binary shape descriptor rs (x; S),
defined in Section 3.
To find (locally) minimal-cost segmentations under this model, we use local search over the space of
agglomerations starting from some initial supervoxel segmentation. Using a simple greedy policy,
at each step we consider all possible agglomeration actions, i.e. merges between any two adjacent
segments, and pick the action that results in the lowest energy.
Na?vely, computing the energy for just a single segmentation requires computing shape descriptors
and then evaluating the energy model at every voxel position with the volume; a small volume may
have tens or hundreds of millions of voxels. At each stage of the agglomeration, there may be
thousands, or tens of thousands, of potential next agglomeration steps, each of which results in a
unique segmentation. In order to choose the best next step, we must know the energy of all of these
potential next segmentations. The computational cost to perform these computations directly would
be tremendous, but in the supplement, we prove a collection of theorems that allow for an efficient
implementation that computes these energy terms incrementally.

3

Representing 3-D Shape Configurations with Local Binary Descriptors

We propose a binary shape descriptor based on subsampled pairwise connectivity information: given
a specification s of k pairs of position offsets {a1 , b1 }, . . . , {ak , bk } relative to the center of some
fixed-size bounding box of size Bs , the corresponding k-bit binary shape descriptor r(U ) for a
particular segmentation U of that bounding box is defined by

1 if ai is connected to bi in U ;
ri (U ) :=
for i ? [1, k].
0 otherwise.

As shown in Fig. 2a, each bit of the descriptor specifies whether a particular pair of positions are
part of the same segment, which can be determined in constant time by the use of a suitable data
structure. In the limit case, if we use the list of all n2 pairs of positions within an n-voxel bounding
box, no information is lost and the Hamming distance between two descriptors is precisely
equal to

the Rand index. [23] In general we can sample a subset of only k pairs out of the n2 possible; if we
sample uniformly at random, we retain the property that the expected Hamming distance between
two descriptors is equal to the Rand index. We found that picking k = 512 bits provides a reasonable
trade-off between fidelity and representation size. While the pairs may be randomly sampled initially,
naturally to obtain consistent results when learning models based on these descriptors we must use
the same fixed list of positions for defining the descriptor at both training and test time. 2

Note that this descriptor serves in general as a type of sketch of a full segmentation of a given
bounding box. By restricting one of the two positions of each pair to be the center position of the
bounding box, we instead obtain a sketch of just the single segment containing the center position.
We refer to the descriptor in this case as center-based, and to the general case as pairwise, as shown
in Fig. 2b. We will use these shape descriptors to represent only local sub-regions of a segmentation.
To represent shape information throughout a large volume, we compute shape descriptors densely at
all positions in a sliding window fashion, as shown in Fig. 2c.
2

The BRIEF descriptor [5] is similarly defined as a binary descriptor based on a subset of the pairs of points
within a patch, but each bit is based on the intensity difference, rather than connectivity, between each pair.

3

r = 1...

r = 100000000110 . . .

r = 10000000011000000110100000101001

(a) Sequence showing computation of a shape descriptor.

r = 00001000001011100111100100001000

r = 00000000000101110000010000110010

r = 10001001101100010100000010000111

(b) Shape descriptors are computed at multiple scales. Pairwise descriptors (shown left and center) consider
arbitrary pairwise connectivity, while center-based shape descriptors (shown right) restrict one position of each
pair to be the center point.

r = 10000001110010100110100001011001

r = 11000011110011100100100011011011

r = 10000011100111100100110011011111

(c) Shape descriptors are computed densely at every position within the volume.

Figure 2: Illustration of shape descriptors. The connected components of the bounding box U for
which the descriptor is computed are shown in distinct colors. The pairwise connectivity relationships
that define the descriptor are indicated by dashed lines; connected pairs are shown in white, while
disconnected pairs are shown in black. Connectivity is determined based on the connected components
of the underlying segmentation, not the geometry of the line itself. While this illustration is 2-D, in
our experiments shape descriptors are computed fully in 3-D.

Connectivity Regions
As defined, a single shape descriptor represents the segmentation within its fixed-size bounding box;
by shifting the position of the bounding box we can obtain descriptors corresponding to different
local regions of some larger segmentation. The size of the bounding box determines the scale of the
local representation. This raises the question of how connectivity should be defined within these local
regions. Two voxels may be connected only by a long path well outside the descriptor bounding box.
As we would like the shape descriptors to be consistent with the local topology, such pairs should
be considered disconnected. Shape descriptors are, therefore, defined with respect to connectivity
within some larger connectivity region, which necessarily contains one or more descriptor bounding
boxes but may in general be significantly smaller than the full segmentation; conceptually, the shape
descriptor bounding box slides around to all possible positions contained within the connectivity
region. (This sliding necessarily results in some minor inconsistency in context between different
positions, but reduces computational and memory costs.) To obtain shape descriptors at all positions,
we simply tile the space with overlapping rectangular connectivity regions of appropriate uniform
size and stride, as shown in the supplement. The connectivity region size determines the degree
of locality of the connectivity information captured by the shape descriptor (independent of the
descriptor bounding box size). It also affects computational costs, as described in the supplement.
4

4

Energy model learning

?s (r; v) for each shape descriptor type/scale s by a learned neural
We define the local energy model E
network model that computes a real-valued score in [0, 1] from a shape descriptor r and image feature
vector v.
To simplify the presentation, we define the following notation for the forward discrete derivative of f
with respect to S: ?eS f (S) := f (S + e) ? f (S).

Based on this notation, we have the discrete derivative of the energy function ?eS E(S; I) =
E(S + e; I) ? E(S; I), where S + e denotes the result of merging the two supervoxels corresponding to e in the existing segmentation S. To agglomerate, our greedy policy simply chooses at
step t the action e that minimizes ?eS t E(S t ; I), where S t denotes the current segmentation at step t.

As in prior work [22], we treat this as a classification problem, with the goal of matching the sign of
?eS t E(S t ; I) to ?eS t error(S t , S ? ), the corresponding change in segmentation error with respect to a
ground truth segmentation S ? , measured using Variation of Information [21].
4.1

Local training procedure

Because the ?eS t E(S t ; I) term is simply the sum of the change in energies from each position
?s (r; v)
and descriptor type s, as a heuristic we optimize the parameters of the energy model E
independently for each shape descriptor type/scale s. We seek to minimize the expectation

?s (rs (xi ; Si + e); ?(xi ; I)))+
Ei `(?eSii error(Si , S ? ), E

?s (rs (x; Si ); ?(xi ; I))) ,
`(??eSii error(Si , S ? ), E

where i indexes over training examples that correspond to a particular sampled position xi and a
merge action ei applied to a segmentation Si . `(y, a) denotes a binary classification loss function,
where a ? [0, 1] is the predicted probability that the true label y is positive, weighted by |y|. Note that
if ?eSii error(Si , S ? ) < 0, then action e improved the score and therefore we want a low predicted
score for the post-merge descriptor rs (xi ; Si + e) and a high predicted score for the pre-merge
descriptor rs (xi ; Si ); if ?eSii error(Si , S ? ) > 0 the opposite applies. We tested the standard log loss
`(y, a) := |y| ? [1y>0 log(a) + 1y<0 log(1 ? a)], as well as the signed linear loss `(y, a) := y ? a,
which more closely matches how the Es (x; Si ; I) terms contribute to the overall ?eS E(S; I) scores.
Stochastic gradient descent (SGD) is used to perform the optimization.
We obtain training examples by agglomerating using the expert policy that greedily optimizes
error(S t , S ? ). At each segmentation state S t during an agglomeration step (including the initial state),
for each possible agglomeration action e, and each position x within the volume, we compute the shape
descriptor pair rs (x; S t ) and rs (x; S t +e) reflecting the pre-merge and post-merge states, respectively.
If rs (x; S t ) 6= rs (x; S t + e), we emit a training example corresponding to this descriptor pair. We
thereby obtain a conceptual stream of examples he, ?eS t error(S t , S ? ), ?(x; I), rs (x; S t ), rs (x; S t +
e)i.
This stream of examples may contain billions of examples (and many highly correlated), far more
than required to learn the parameters of Es . To reduce resource requirements, we use priority
sampling [12], based on |?eS error(S, S ? )|, to obtain a fixed number of weighted samples without
replacement for each descriptor type s. We equalize the total weight of true merge examples
(?eS error(S, S ? ) < 0) and false merge examples (?eS error(S, S ? ) > 0) in order to avoid learning
degenerate models.3

5

Experiments

We tested our approach on a large, publicly available electron microscopy dataset, called Janelia FIB25, of a portion of the Drosophila melangaster optic lobe. The dataset was collected at 8 ? 8 ? 8 nm
3

For example, if most of the weight is on false merge examples, as would often occur without balancing, the
model can simply learn to assign a score that increases with the number of 1 bits in the shape descriptor.

5

3.5

Split error (H(p|t))

3.0
2.5
2.0
1.5

??

1.0
0.5
0.0
0.0

0.5

1.0

1.5

2.0

2.5

3.0

VI

Rand F1

CELIS (this paper)
3d-CNN+GALA
3d-CNN+Watershed
7colseg1

1.672
2.069
2.143
2.981

0.691
0.597
0.629
0.099

Oracle

0.428

0.901

3.5

Merge error (H(t|p))

Figure 3: Segmentation accuracy on 11-gigavoxel FIB-25 test set. Left: Pareto frontiers of
information-theoretic split/merge error, as used previously to evaluate segmentation accuracy. [22]
Right: Comparison of Variation of Information (lower is better) and Rand F1 score (higher is better).
For CELIS, 3d-CNN+GALA, and 3d-CNN+watershed, the hyperparameters were optimized for each
metric on the training set.
resolution using Focused Ion Beam Scanning Electron Microscopy (FIB-SEM); a labor-intensive
semi-automated approach was used to segment all of the larger neuronal processes within a ? 20,000
cubic micron volume (comprising about 25 billion voxels). [27] To our knowledge, this challenging
dataset is the largest publicly available electron microscopy dataset of neuropil with a corresponding
?ground truth? segmentation.
For our experiments, we split the dataset into separate training and testing portions along the z axis:
the training portion comprises z-sections 2005?5005, and the testing portion comprises z-sections
5005?8000 (about 11 billion voxels).
5.1

Boundary classification and oversegmentation

To obtain image features and an oversegmentation to use as input for agglomeration, we trained
convolutional neural networks to predict, based on a 35 ? 35 ? 9 voxel image context region, whether
the center voxel is part of the same neurite as the adjacent voxel in each of the x, y, and z directions, as
in prior work. [29] We optimized the parameters of the network using stochastic gradient descent with
log loss. We trained several different networks, varying as hyperparameters the amount of dilation of
boundaries in the training data (in order to increase extracellular space) from 0 to 8 voxels and whether
components smaller than 10000 voxels were excluded. See the supplementary information for a
description of the network architecture. Using these connection affinities, we applied a watershed
algorithm [33, 34] to obtain an (approximate) oversegmentation. We used parameters Tl = 0.95,
Th = 0.95, Te = 0.5, and Ts = 1000 voxels.
5.2

Energy model architecture

We used five types of 512-dimensional shape descriptors: three pairwise descriptor types with 93 ,
173 , and 333 bounding boxes, and two center-based descriptor types with 173 and 333 bounding
boxes, respectively. The connectivity positions within the bounding boxes for each descriptor type
were sampled uniformly at random.
We used the 512-dimensional fully-connected penultimate layer output of the low-level classification
convolutional neural network as the image feature vector ?(x; I). For each shape descriptor type s,
?s (r; v): we concatenated the shape
we used the following architecture for the local energy model E
descriptor vector and the image feature vector to obtain a 1024-dimensional input vector. We used
two 2048-dimensional fully-connected rectified linear hidden layers, followed by a logistic output
unit, and applied dropout (with p = 0.5) after the last hidden layer. While this effectively computes a
6

score from a raw image patch and a shape descriptor, by segregating expensive convolutional image
processing that does not depend on the shape descriptor, this architecture allows us to benefit from
pre-training and precomputation of the intermediate image feature vector ?(x; I) for each position x.
Training for both the energy models and the boundary classifier was performed using asynchronous
SGD using a distributed architecture. [9]

5.3

Evaluation

We compared our method to the state-of-the-art agglomeration method GALA [22], which trains
a random forest classifier to predict merge decisions using image features derived from boundary
probabilities. 4 To obtain such probabilities from our low-level convolutional neural network classifier,
which predicts edge affinities between adjacent voxels rather than per-voxel predictions, we compute
for each voxel the minimum connection probability to any voxel in its 6-connectivity neighborhood,
and treat this as the probability/score of it being cell interior.
For comparison, we also evaluated a watershed procedure applied to the CNN affinity graph output,
under varying parameter choices, to measure the accuracy of the deep CNN boundary classification
without the use of an agglomeration procedure. Finally, we evaluated the accuracy of the publicly
released automated segmentation of FIB-25 (referred to as 7colseg1) [13] that was the basis of
the proofreading process used to obtain the ground truth; it was produced by applying watershed
segmentation and a variant of GALA agglomeration to the predictions made by an Ilastik [25]-trained
voxel classifier.
We tested both GALA and CELIS using the same initial oversegmentations for the training and test
regions. To compare the accuracy of the reconstructions, we computed two measures of segmentation
consistency relative to the ground truth: Variation of Information [21] and Rand F1 score, defined as
the F1 classification score over connectivity between all voxel pairs within the volumes; these are the
primary metrics used in prior work. [28, 8, 22] The former has the advantage of weighing segments
linearly in their size rather than quadratically.
Because any agglomeration method is ultimately limited by the quality of the initial oversegmentation,
we also computed the accuracy of an oracle agglomeration policy that greedily optimizes the
error metric directly. (Computing the true globally-optimal agglomeration under either metric is
intractable.) This serves as an (approximate) upper bound that is useful for separating the error due to
agglomeration from the error due to the initial oversegmentation.

6

Results

Figure 3 shows the Pareto optimal trade-offs between test set split and merge error of each method
obtained by varying the choice of hyperparameters and agglomeration thresholds, as well as the
Variation of Information and Rand F1 scores obtained from the training set-optimal hyperparameters.
CELIS consistently outperforms all other methods by a significant margin under both metrics. The
large gap between the Oracle results and the best automated reconstruction indicates, however, that
there is still large room for improvement in agglomeration.
While the evaluations are done on a single dataset, it is a single very large dataset; to verify that
the improvement due to CELIS is broad and general (rather than localized to a very specific part of
the image volume), we also evaluated accuracy independently on 18 non-overlapping 5003 -voxel
subvolumes evenly spaced within the test region. On all subvolumes CELIS outperformed the best
existing method under both metrics, with a median reduction in Variation of Information error of 19%
and in Rand F1 error of 22%. This suggests that CELIS is improving accuracy in many parts of the
volume that span significant variations in shape and image characteristics.

4
GALA also supports multi-channel image features, potentially representing predicted probabilities of
additional classes, such as mitochondria, but we did not make use of this functionality as we did not have training
data for additional classes.

7

7

Discussion

We have introduced CELIS, a framework for modeling image segmentations using a learned energy
function that specifically exploits the combinatorial nature of dense segmentation. We have described
how this approach can be used to model the conditional energy of a segmentation given an image, and
how the resulting model can be used to guide supervoxel agglomeration decisions. In our experiments
on a challenging 3d microscopy reconstruction problem, CELIS improved volumetric reconstruction
accuracy by 20% over the best existing method, and offered a strictly better trade-off between split
and merge errors, by a wide margin, compared to existing methods.
The experimental results are unique in the scale of the evaluations: the 11-gigavoxel test region is 2?4
orders of magnitude larger than used for evaluation in prior work, and we believe this large scale of
evaluation to be critically important; we have found evaluations on smaller volumes, containing only
short neurite fragments, to be unreliable at predicting accuracy on larger volumes (where propagation
of merge errors is a major challenge). While more computationally expensive than many prior
methods, CELIS is nonetheless practical: we have successfully run CELIS on volumes approaching
? 1 teravoxel in a matter of hours, albeit using many thousands of CPU cores.

In addition to advancing the state of the art in learning-based image segmentation, this work also has
significant implications for the application area we have studied, connectomic reconstruction. The
FIB-25 dataset reflects state-of-the-art techniques in sample preparation and imaging for large-scale
neuron reconstruction, and in particular is highly representative of much larger datasets actively
being collected (e.g. of a full adult fly brain). We expect, therefore, that the significant improvements
in automated reconstruction accuracy made by CELIS on this dataset will directly translate to a
corresponding decrease in human proof-reading effort required to reconstruct a given volume of tissue,
and a corresponding increase in the total size of neural circuit that may reasonably be reconstructed.
Future work in several specific areas seems particularly fruitful:
? End-to-end training of the CELIS energy modeling pipeline, including the CNN model
for computing the image feature representation and the aggregation of local energies at
each position and scale. Because the existing pipeline is fully differentiable, it is directly
amenable to end-to-end training.
? Integration of the CELIS energy model with discriminative training of a neural networkbased agglomeration policy. Such a policy could depend on the distribution of local energy
changes, rather than just the sum, as well as other per-object and per-action features proposed
in prior work. [22, 3]
? Use of a CELIS energy model for fixing undersegmentation errors. While the energy
minimization procedure proposed in this paper is based on a greedy local search limited to
performing merges, the CELIS energy model is capable of evaluating arbitrary changes to
the segmentation. Evaluation of candidate splits (based on a hierarchical initial segmentation
or other heuristic criteria) would allow for the use of a potentially more robust simulated
annealing energy minimization procedure capable of both splits and merges.
Several recent works [24, 32, 7, 6] have integrated deep neural networks into pairwise-potential
conditional random field models. Similar to CELIS, these approaches combine deep learning with
structured prediction, but differ from CELIS in several key ways:
? Through a restriction to models that can be factored into pairwise potentials, these approaches are able to use mean field and pseudomarginal approximations to perform efficient
approximate inference. The CELIS energy model, in contrast, sacrifices factorization for the
richer combinatorial modeling provided by the proposed 3-D shape descriptors.
? More generally, these prior CRF methods are focused on refining predictions (e.g. improving
boundary localization/detail for semantic segmentation) made by a feed-forward neural
network that are correct at a high level. In contrast, CELIS is designed to correct fundamental
inaccuracy of the feed-forward convolutional neural network in critical cases of ambiguity,
which is reflected in the much greater complexity of the structured model.
Acknowledgments

This material is based upon work supported by the National Science Foundation under Grant No.
1118055.
8

References
[1] B. Andres, U. K?the, M. Helmstaedter, W. Denk, and F. Hamprecht. Segmentation of SBFSEM volume data of neural tissue by hierarchical classification. Pattern recognition, pages 142?152, 2008. 2
[2] Bjoern Andres, Thorben Kroeger, Kevin L Briggman, Winfried Denk, Natalya Korogod, Graham Knott, Ullrich Koethe, and Fred A
Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Computer Vision?ECCV 2012, pages 778?791. Springer,
2012. 2
[3] John A Bogovic, Gary B Huang, and Viren Jain. Learned versus hand-designed feature representations for 3d agglomeration.
arXiv:1312.6159, 2013. 2, 8
[4] Kevin L Briggman and Winfried Denk. Towards neural circuit reconstruction with volume electron microscopy techniques. Current
Opinion in Neurobiology, 16(5):562 ? 570, 2006. Neuronal and glial cell biology / New technologies. 1
[5] Michael Calonder, Vincent Lepetit, Christoph Strecha, and Pascal Fua. Brief: Binary robust independent elementary features. In
European conference on computer vision, pages 778?792. Springer, 2010. 3
[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. Semantic image segmentation with deep
convolutional nets and fully connected crfs. CoRR, abs/1412.7062, 2014. 8
[7] Liang-Chieh Chen, Alexander G Schwing, Alan L Yuille, and Raquel Urtasun. Learning deep structured models. In Proc. ICML, 2015.
8
[8] Dan Claudiu Ciresan, Alessandro Giusti, Luca Maria Gambardella, and J?rgen Schmidhuber. Deep neural networks segment neuronal
membranes in electron microscopy images. In NIPS, pages 2852?2860, 2012. 7
[9] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc?Aurelio Ranzato, Andrew Senior, Paul Tucker,
Ke Yang, Quoc V. Le, and Andrew Y. Ng. Large scale distributed deep networks. In F. Pereira, C.J.C. Burges, L. Bottou, and K.Q.
Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1223?1231. Curran Associates, Inc., 2012. 7
[10] Winfried Denk, Kevin L Briggman, and Moritz Helmstaedter. Structural neurobiology: missing link to a mechanistic understanding of
neural computation. Nature Reviews Neuroscience, 13(5):351?358, 2012. 1
[11] Winfried Denk and Heinz Horstmann. Serial block-face scanning electron microscopy to reconstruct three-dimensional tissue nanostructure. PLoS Biol, 2(11):e329, 10 2004. 1
[12] Nick Duffield, Carsten Lund, and Mikkel Thorup. Priority sampling for estimation of arbitrary subset sums. Journal of the ACM (JACM),
54(6):32, 2007. 5
[13] Janelia FlyEM. https://www.janelia.org/project-team/flyem/data-and-software-release. Accessed: 2016-05-19. 7
[14] Jan Funke, Bjoern Andres, Fred A Hamprecht, Albert Cardona, and Matthew Cook. Efficient automatic 3d-reconstruction of branching
neurons from em data. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 1004?1011. IEEE, 2012.
2
[15] KJ Hayworth, N Kasthuri, R Schalek, and JW Lichtman. Automating the collection of ultrathin serial sections for large volume tem
reconstructions. Microscopy and Microanalysis, 12(Supplement,S02):86?87, 2006. 1
[16] Moritz Helmstaedter, Kevin L Briggman, and Winfried Denk. 3d structural imaging of the brain with photons and electrons. Current
Opinion in Neurobiology, 18(6):633 ? 641, 2008. 1
[17] Moritz Helmstaedter, Kevin L Briggman, and Winfried Denk. High-accuracy neurite reconstruction for high-throughput neuroanatomy.
Nature neuroscience, 14(8):1081?1088, 2011. 1
[18] Moritz Helmstaedter, Kevin L Briggman, Srinivas C Turaga, Viren Jain, H Sebastian Seung, and Winfried Denk. Connectomic reconstruction of the inner plexiform layer in the mouse retina. Nature, 500(7461):168?174, 2013. 2
[19] Viren Jain, Srinivas C Turaga, Kevin L Briggman, Moritz N Helmstaedter, Winfried Denk, and H Sebastian Seung. Learning to agglomerate superpixel hierarchies. Advances in Neural Information Processing Systems, 2(5), 2011. 2
[20] Graham Knott, Herschel Marchman, David Wall, and Ben Lich. Serial section scanning electron microscopy of adult brain tissue using
focused ion beam milling. The Journal of Neuroscience, 28(12):2959?2964, 2008. 1
[21] Marina Meil?a. Comparing clusterings?an information based distance. Journal of Multivariate Analysis, 98(5):873?895, 2007. 5, 7
[22] Juan Nunez-Iglesias, Ryan Kennedy, Toufiq Parag, Jianbo Shi, and Dmitri B Chklovskii. Machine learning of hierarchical clustering to
segment 2d and 3d images. PloS one, 8(8):e71715, 2013. 2, 5, 6, 7, 8
[23] William M. Rand. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association,
66(336):846?850, 1971. 3
[24] Alexander G Schwing and Raquel Urtasun. Fully connected deep structured networks. arXiv preprint arXiv:1503.02351, 2015. 8
[25] Christoph Sommer, Christoph Straehle, Ullrich Kothe, and Fred A Hamprecht. ilastik: Interactive learning and segmentation toolkit. In
Biomedical Imaging: From Nano to Macro, 2011 IEEE International Symposium on, pages 230?233. IEEE, 2011. 7
[26] Shin-ya Takemura, Arjun Bharioke, Zhiyuan Lu, Aljoscha Nern, Shiv Vitaladevuni, Patricia K Rivlin, William T Katz, Donald J Olbris,
Stephen M Plaza, Philip Winston, et al. A visual motion detection circuit suggested by drosophila connectomics. Nature, 500(7461):175?
181, 2013. 2
[27] Shin-ya Takemura, C Shan Xu, Zhiyuan Lu, Patricia K Rivlin, Toufiq Parag, Donald J Olbris, Stephen Plaza, Ting Zhao, William T Katz,
Lowell Umayam, et al. Synaptic circuits and their variations within different columns in the visual system of drosophila. Proceedings of
the National Academy of Sciences, 112(44):13711?13716, 2015. 6
[28] Srinivas Turaga, Kevin Briggman, Moritz Helmstaedter, Winfried Denk, and Sebastian Seung. Maximin affinity learning of image
segmentation. In Advances in Neural Information Processing Systems 22, pages 1865?1873. MIT Press, Cambridge, MA, 2009. 7
[29] Srinivas C. Turaga, Joseph F. Murray, Viren Jain, Fabian Roth, Moritz Helmstaedter, Kevin Briggman, Winfried Denk, and H. Sebastian
Seung. Convolutional networks can learn to generate affinity graphs for image segmentation. Neural Comput., 22(2):511?538, 2010. 6
[30] Amelio Vazquez-Reina, Michael Gelbart, Daniel Huang, Jeff Lichtman, Eric Miller, and Hanspeter Pfister. Segmentation fusion for
connectomics. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 177?184. IEEE, 2011. 2
[31] J. G. White, E. Southgate, J. N. Thomson, and S. Brenner. The Structure of the Nervous System of the Nematode Caenorhabditis elegans.
Philosophical Transactions of the Royal Society of London. B, Biological Sciences, 314(1165):1?340, 1986. 1
[32] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip HS
Torr. Conditional random fields as recurrent neural networks. In Proceedings of the IEEE International Conference on Computer Vision,
pages 1529?1537, 2015. 8
[33] Aleksandar Zlateski. A design and implementation of an efficient, parallel watershed algorithm for affinity graphs. PhD thesis, Massachusetts Institute of Technology, 2011. 6
[34] Aleksandar Zlateski and H. Sebastian Seung. Image segmentation by size-dependent single linkage clustering of a watershed basin graph.
CoRR, 2015. 6

9

"
4615,"A multi-agent control framework for co-adaptation in
brain-computer interfaces

Josh Merel1 , ? Roy Fox2 , Tony Jebara3, Liam Paninski4
Department of Neurobiology and Behavior, 3 Department of Computer Science,
4
Department of Statistics, Columbia University, New York, NY 10027
2
School of Computer Science and Engineering, Hebrew University, Jerusalem 91904, Israel
jsm2183@columbia.edu, royf@cs.huji.ac.il,
jebara@cs.columbia.edu, liam@stat.columbia.edu
?

1

Abstract
In a closed-loop brain-computer interface (BCI), adaptive decoders are used to
learn parameters suited to decoding the user?s neural response. Feedback to the
user provides information which permits the neural tuning to also adapt. We
present an approach to model this process of co-adaptation between the encoding model of the neural signal and the decoding algorithm as a multi-agent formulation of the linear quadratic Gaussian (LQG) control problem. In simulation
we characterize how decoding performance improves as the neural encoding and
adaptive decoder optimize, qualitatively resembling experimentally demonstrated
closed-loop improvement. We then propose a novel, modified decoder update rule
which is aware of the fact that the encoder is also changing and show it can improve simulated co-adaptation dynamics. Our modeling approach offers promise
for gaining insights into co-adaptation as well as improving user learning of BCI
control in practical settings.

1 Introduction
Neural signals from electrodes implanted in cortex [1], electrocorticography (ECoG) [2], and electroencephalography (EEG) [3] all have been used to decode motor intentions and control motor
prostheses. Standard approaches involve using statistical models to decode neural activity to control
some actuator (e.g. a cursor on a screen [4], a robotic manipulator [5], or a virtual manipulator [6]).
Performance of offline decoders is typically different from the performance of online, closed-loop
decoders where the user gets immediate feedback and neural tuning changes are known to occur
[7, 8]. In order to understand how decoding will be performed in closed-loop, it is necessary to
model how the decoding algorithm updates and neural encoding updates interact in a coordinated
learning process, termed co-adaptation.
There have been a number of recent efforts to learn improved adaptive decoders specifically tailored
for the closed loop setting [9, 10], including an approach relying on stochastic optimal control theory
[11]. In other contexts, emphasis has been placed on training users to improve closed-loop control
[12]. Some efforts towards modeling the co-adaptation process have sought to model properties
of different decoders when used in closed-loop [13, 14, 15], with emphasis on ensuring the stability of the decoder and tuning the adaptation rate. One recent simulation study also demonstrated
how modulating task difficulty can improve the rate of co-adaptation when feedback noise limits
performance [16]. However, despite speculation that exploiting co-adaptation will be integral to
state-of-the-art BCI [17], general models of co-adaptation and methods which exploit those models
to improve co-adaptation dynamics are lacking.
?

These authors contributed equally.

1

We propose that we should be able to leverage our knowledge of how the encoder changes in order
to better update the decoder. In the current work, we present a simple model of the closed-loop coadaptation process and show how we can use this model to improve decoder learning on simulated
experiments. Our model is a novel control setting which uses a split Linear Quadratic Gaussian
(LQG) system. Optimal decoding is performed by Linear Quadratic Estimation (LQE), effectively
the Kalman filter model. Encoding model updates are performed by the Linear Quadratic Regulator
(LQR), the dual control problem of the Kalman filter. The system is split insofar as each agent has
different information available and each performs optimal updates given the state of the other side
of the system. We take advantage of this model from the decoder side by anticipating changes in
the encoder and pre-emptively updating the decoder to match the estimate of the further optimized
encoding model. We demonstrate that this approach can improve the co-adaptation process.

2 Model framework
2.1 Task model
For concreteness, we consider a motor-cortical neuroprosthesis setting. We assume a naive user,
placed into a BCI control setting, and propose a training scheme which permits the user and decoder
to adapt. We provide a visual target cue at a 3D location and the user controls the BCI via neural signals which, in a natural setting, relate to hand kinematics. The target position is moved each timestep
to form a trajectory through the 3D space reachable by the user?s hand. The BCI user receives visual
feedback via the displayed location of their decoded hand position. The user?s objective is to control
their cursor to be as close to the continuously moving target cursor as possible. A key feature of this
scheme is that we know the ?intention? of the user, assuming it corresponds to the target.
The complete graphical model of this system is provided in figure 1. xt in our simulations is a three
dimensional position vector (Cartesian Coordinates) corresponding to the intended hand position.
This variable could be replaced or augmented by other variables of interest (e.g. velocity). We
randomly evolve the target signal using a linear-Gaussian drift model (eq. (1)). The neural encoding
model is linear-Gaussian in response to intended position xt and feedback x
?t?1 (eq. (2)), giving
a vector of neural responses ut (e.g. local field potential or smoothed firing rates of neural units).
Since we do not observe the whole brain region, we must subsample the number of neural units
from which we collect information. The transformation C is conceptually equivalent to electrode
sampling and yt is the observable neural response vector via the electrodes (eq. (3)). Lastly, x?t is
the decoded hand position estimate, which also serves as visual feedback (eq. (4)).
xt = P xt?1 + ?t ;
?t ? N (0, Q)
(1)
ut = Axt + B x
?t?1 + ?t ;
?t ? N (0, R)
(2)
yt = Cut + ?t ;
?t ? N (0, S)
(3)
x?t = F yt + G?
xt?1 .
(4)

P

xt

xt+1

A

A
ut+1

ut
C
B

x
?t?1

C
yt

G

B

F
x
?t

yt+1

G

F
x
?t+1

During training, the decoding system is allowed access to the target position, interpreted as the real intention xt . The decoded x
?t is only used as feedback,
to inform the user of the gradually learned dynamics
of the decoder. After training, the system is tested
on a task with the same parameters of the trajectory
dynamics, but with the actual intention only known
to the user, and hidden from the decoder. A natural
objective is to minimize tracking error, measured as
accumulated mean squared error between the target
and neurally decoded pose over time.

Figure 1: Graphical model relating target signal (xt ), neural response (ut ), electrode ob- For contemporary BCI applications, the Kalman filservation of neural response (yt ), and de- ter is a reasonable baseline decoder, so we do not
consider even simpler models. However, for other
coded feedback signal (?
xt ).
applications one might wish to consider a model in
which the state at each timestep is encoded independently. It is possible to find a closed form for the optimal encoder and decoder that minimizes the
error in this case [18, 19].
2

Sections 2.2 and 2.3 describe the model presented in figure 1 as seen from the distinct viewpoints
of the two agents involved ? the encoder and the decoder. The encoder observes xt and x
?t?1 , and
selects A and B to generate a control signal ut . The decoder observes yt , and selects F and G
to estimate the intention as x
?t . We assume that both agents are free to performed unconstrained
optimization on their parameters.
2.2 Encoding model and optimal decoder
Our encoding model is quite simple, with neural units responding in a linear-Gaussian fashion to
intended position xt and feedback x
?t?1 (eq. (2)). This is a standard model of neural responses for
BCI. The matrices A and B effectively correspond to the tuning response functions of the neural
units, and we will allow these parameters to be adjusted under the control of the user. The matrix
C corresponds to the observation of the neural units by the electrodes, so we treat it as fixed (in our
case C will down-sample the neurons). For this paper, we assume noise covariances are fixed and
known, but this can be generalized. Given the encoder, the decoder will estimate the intention xt ,
which follows a hidden Markov chain (eq. (1)). The observations available to the decoder are the
electrode samples yt (eq. (2) and (3))
yt = CAxt + CB x
?t?1 + ??t ;

??t ? N (0, RC )

(5)

T

RC = CRC + S.

(6)

Given all the electrode samples up to time t, the problem of finding the most likely hidden intention
is a Linear-Quadratic Estimation problem (figure 2), and its standard solution is the Kalman filter,
and this decoder is widely in similar contexts. To choose appropriate Kalman gain F and mean
dynamics G, the decoding system needs a good model of the dynamics of the underlying intention
process (P , Q of eq.(1)) and the electrode observations (CA, CB, and RC of eqs. (5) & (6)).
We can assume that P and Q are known since the decoding algorithm is controlled by the same
experimenter who specifies the intention process for the training phase. We discuss the estimation
of the observation model in section 4.
P

xt
CA

xt+1
CA

x
?t?1

A

F
x
?t

ut+1

ut
B

CB
G

xt+1

A
yt+1

yt
CB

P

xt

G

F
x
?t+1

x
?t?1

Figure 2: Decoder?s point of view ? target
signal (xt ) directly generates observed responses (yt ), with the encoding model collapsed to omit the full signal (ut ). Decoded feedback signal (?
xt ) is generated by
the steady state Kalman filter.

B
G

FC
x
?t

G

FC
x
?t+1

Figure 3: Encoder?s point of view ? target signal (xt ) and decoded feedback signal (?
xt?1 )
generate neural response (ut ). Model of decoder collapses over responses (yt ) which are
unseen by the encoder side.

Given an encoding model, and assuming a very long horizon 1 , there exist standard methods to
optimize the stationary value of the decoder parameters [20]. The stationary covariance ? of xt
given x
?t?1 is the unique positive-definite fixed point of the Riccati equation
? = P ?P T ? P ?(CA)T (RC + (CA)?(CA)T )?1 (CA)?P T + Q.

(7)

The Kalman gain is then
F = ?(CA)T ((CA)?(CA)T + RC )?1

(8)

G = P ? F (CA)P ? F (CB).

(9)

with mean dynamics
1
Our task is control of the BCI for arbitrarily long duration, so it makes sense to look for the stationary
decoder. Similarly the BCI user will look for a stationary encoder. We could also handle the finite horizon case
(see section 2.3 for further discussion).

3

We estimate x
?t using eq. (4), and this is the most likely value, as well as the expected value,
of xt given the electrode observations y1 , . . . , yt . Using this estimate as the decoded intention is
equivalent to minimizing the expectation of a quadratic cost
X
1
clqe =
?t k2 .
(10)
2 kxt ? x
t

2.3 Model of co-adaptation
At the same time as the decoder-side agent optimizes the decoder parameters F and G, the encoderside agent can optimize the encoder parameters A and B. We formulate encoder updates for the BCI
application as a standard LQR problem. This framework requires that the encoder-side agent has an
intention model (same as eq. (1)) and a model of the decoder. The decoder model combines eq. (3)
and (4) into
x?t = F Cut + G?
xt?1 + F ?t .
(11)
This model is depicted in figure 3. We assume that the encoder has access to a perfect estimate of the
intention-model parameters P and Q (task knowledge). We also assume that the encoder is free to
change its parameters A and B arbitrarily given the decoder-side parameters (which it can estimate
as discussed in section 4).
As a model of real neural activity, there must be some cost to increasing the power of the neural
signal. Without such a cost, the solutions diverge. We add an additional cost term (a regularizer),
which is quadratic in the magnitude of the neural response ut , and penalizes a large neural signal
X
1
? t.
clqr =
kxt ? x
?t k2 + 1 uT Ru
(12)
2 t

2

t

Since the decoder has no direct influence on this additional term, it can be viewed as optimizing for
this target cost function as well. The LQR problem is solved similarly to eq. (7), by assuming a very
long horizon and optimizing the stationary value of the encoder parameters [20].
We next formulate our objective function in terms of standard LQR parameters. The control depends
on the joint process of the intention and the feedback (xt , x?t?1 ), but the cost is defined between xt
and x
?t . To compute the expected cost given xt , x
?t?1 and ut , we use eq. (11) to get
E k?
xt ? xt k2 = kF Cut + G?
xt?1 ? xt k2 + const
(13)
T
T
T
= (G?
xt?1 ? xt ) (G?
xt?1 ? xt ) + (F Cut ) (F Cut ) + 2(G?
xt?1 ? xt ) (F Cut ) + const.
Equation 13 provides the error portion of the quadratic objective of the LQR problem. The standard
solution
 for the stationary case involves computing the Hessian V of the cost-to-go in joint state

xt
as the unique positive-definite fixed point of the Riccati equation
x
?t?1
? + P? T V D)(
? R
? + S? + D
? T V D)
? ?1 (N
?T + D
? T V P? ) + Q.
?
V = P? T V P? + (N
(14)
? is the controllability of this
Here P? is the process dynamics for the joint state of xt and x
?t?1 and D
? S? and N
? are the cost parameters which can be determined by inspection of eq. (13).
dynamics. Q,
? is the Hessian of the neural response cost term which is chosen in simulations so that the resulting
R
increase in neural signal strength is reasonable.








?F C
?GT
P 0
T
?
?
?= I
? = 0 , Q
.
,
S
=
(F
C)
(F
C),
N
=
, D
P? =
FC
0 G
GT (F C)
?G GT G
In our formulation, the encoding model (A, B) is equivalent to the feedback gain
?TV D
? +R
? + S)
? ?1 (N
?T + D
? T V P? ).
[A B] = ?(D

(15)

This is the optimal stationary control, and is generally not optimal for shorter planning horizons. In
the co-adaptation setting, the encoding model (At , Bt ) regularly changes to adapt to the changing
decoder. This means that (At , Bt ) is only used for one timestep (or a few) before it is updated. The
effective planning horizon is thus shortened from its ideal infinity, and now depends on the rate and
magnitude of the perturbations introduced in the encoding model. Eq. (14) can be solved for this
finite horizon, but here for simplicity we assume the encoder updates introduce small or infrequent
enough changes to keep the planning horizon very long, and the stationary control close to optimal.
4

1

13000

0.95
12000

0.9

11000
10000

?

error (summed over x,y,z)

14000

0.85

9000

0.8

8000
7000

0.75

6000

2

4

6

8

10

12

14

16

18

0.7
1

20

update iteration index

2

3

4

5

6

7

8

9

10

encoder update iteration index

(a)

(b)

Figure 4: (a) Each curve plots single trial changes in decoding mean squared error (MSE) over
whole timeseries as a function of the number of update half-iterations. The encoder is updated in
even steps, the decoder in odd ones. Distinct curves are for multiple, random initializations of the
encoder. (b) Plots the corresponding changes in encoder parameter updates - y-axis, ?, is correlation
between the vectorized encoder parameters after each update with the final values.

3 Perfect estimation setting
We can consider co-adaptation in a hypothetical setting where each agent has instant access to a
perfect estimate of the other?s parameters as soon as they change. To keep this setting comparable
to the setting of section 4, where parameter estimation is needed, we only allow each agent access to
those variables that it could, in principle, estimate. We assume both agents know the parameters P
and Q of the intention dynamics, that the encoder knows F C and G of eq. (11), and that the decoder
knows CA, CB and RC of eq. (5) and (6). These are the same parameters needed by each agent for
its own re-optimization. This process of parameter updates is performed by alternating between the
encoder update equations (7)-(9) and the decoder update equations (14)-(15). Since the agents take
turns minimizing the expected infinite-horizon objectives of eq. (12) given the other, this cost will
tend to decrease, approximately converging.
Note that neither of these steps depends explicitly on the observed values of the neural signal ut
or the decoded output x
?t . In other words, co-adaptation can be simulated without ever actually
generating the stochastic process of intention, encoding and decoding. However, this process and
the signal-feedback loop become crucial when estimation is involved, as in section 4. Then each
agent?s update indirectly depends on its observations through its estimated model of the other agent.
To examine the dynamics in this idealized setting, we hold fixed the target trajectory x1...T as well
as the realization of the noise terms. We initialize the simulation with a random encoding model and
observe empirically that, as the encoder and the decoder are updated alternatingly, the error rapidly
reduces to a plateau. As the improvement saturates, the joint encoder-decoder pair approximates
a locally optimal solution to the co-adaptation problem. Figure 4(a) plots the error as a function
of the number of model update iterations ? the different curves correspond to distinct, random initializations of the encoder parameters A, B with everything else held fixed. We emphasize that
for a fixed encoder, the first decoder update would yield the infinite-horizon optimal update if the
encoder could not adapt, and the error can be interpreted relative to this initial optimal decoding
(see supplementary fig1(a) for depiction of initial error and improvement by encoder adaptation in
supplementary fig1(b)). This method obtains optimized encoder-decoder pairs with moderate sensitivity to the initial parameters of the encoding model. Interpreted in the context of BCI, this suggests
that the initial tuning of the observed neurons may affect the local optima attainable for BCI performance due to standard co-adaptation. We may also be able to optimize the final error by cleverly
choosing updates to decoder parameters in a fashion which shifts which optimum is reached. Figure
4(b) displays the corresponding approximate convergence of the encoder parameters - as the error
decreases, the encoder parameters settle to a stable set (the actual final values across initializations
vary).
Parameters free from the standpoint of the simulation are the neural noise covariance RC and the
? of the neural signal cost. We set these to reasonable values - the noise to a moderate
Hessian R
5

level and the cost sufficiently high as to prevent an exceedingly large neural signal which would
swamp the noise and yield arbitrarily low error (see supplement). In an experimental setting, these
parameters would be set by the physical system and they would need to be estimated beforehand.

4 Partially observable setting with estimation
More realistic than the model of co-adaptation where the decoder-side and encoder-side agents automatically know each other?s parameters, is one where the rate of updating is limited by the partial
knowledge each agent has about the other. In each timestep, each agent will update its estimate of
the other agent?s parameters, and then use the current estimates to re-optimize its own parameters.
In this work we use a recursive least squares (RLS) which is presented in the supplement section 3
for this estimation. RLS has a forgetting factor ? which regulates how quickly the routine expects
the parameters it estimates to change. This co-adaptation process is detailed in procedure 1. We
elect to use the same estimation routine for each agent and assume that the user performs idealobserver style optimal estimation. In general, if more knowledge is available about how a real BCI
user updates their estimates of the decoder parameters, such a model could easily be used. We could
also explore in simulation how various suboptimal estimation models employed by the user affect
co-adaptation.
As noted previously, we will assume the noise model is fixed and that the decoder side knows the
neural signal noise covariance RC (eq. (6)). The encoder-side will use a scaled identity matrix as the
estimate of the electrodes-decoder noise model. To jointly estimate the decoder parameters and the
noise model, an EM-based scheme would be a natural approach (such estimation of the BCI user?s
internal model of the decoder has been treated explicitly in [21]).
Procedure 1 standard co-adaptation
for t = 1 to lengthT raining do
Encoder-side
Get xt and x
?t?1
d
b (RLS)
Update encoder-side estimate of decoder F
C, G
d
b (LQR)
Update optimal encoder A, B using current decoder estimate F
C, G
Encode current intention using A, B and send signal yt
Decoder-side
Get xt and yt
d CB
d (RLS)
Update decoder-side estimate of encoder CA,
d CB
d (LQE)
Update optimal decoder F, G using current encoder estimate CA,
Decode current signal using F, G and display as feedback x
?t
end for
Standard co-adaptation yields improvements in decoding performance over time as the encoder and
decoder agents estimate each others? parameters and update based on those estimates. Appropriately,
that model will improve the encoder-decoder pair over time, as in the blue curves of figure 5 below.

5 Encoder-aware decoder updates
In this section, we present an approach to model the encoder updates from the decoder side. We
will use this to ?take an extra step? towards optimizing the decoder for what the anticipated future
encoder ought to look like.
In the most general case, the encoder can update At and Bt in an unconstrained fashion at each
timestep t. From the decoder side, we do not know C and therefore we cannot know F C, an
estimate of which is needed by the user to update the encoder. However, the decoder sets F and can
predict updates to [CA CB] directly, instead of to [A B] as the actual encoder does (equation
15). We emphasize that this update is not actually how the user will update the encoder, rather it
captures how the encoder ought to change the signals observed by the decoder (from the decoder?s
perspective).
6

Figure 5: In each subplot, the blue line corresponds to decreasing error as a function of simulated
time from standard co-adaptation (procedure 1). The green line corresponds to the improved onestep-ahead co-adaptation (procedure 2). Plots from left to right have decreasing RLS forgetting
factor used by the encoder-side to estimate the decoder parameters. Curves depict the median error
across 20 simulations with confidence intervals of 25% and 75% quantiles. Error at each timestep is
appropriately cross-validated, it corresponds to taking the encoder-decoder pair of that timestep and
computing error on ?test? data.
We can find the update [CApred
presented in section 2.3, eq. (15)
[CApred

CBpred ] by solving a modified version of the LQR problem

? ?T V D
?? + R
? ? + S?? )?1 (N
? ?T + D
? ?T V P? ),
CBpred ] = ?(D

with terms defined similarly to section 2.3, except
 
0
?
?
, S?? = F T F,
D =
F

?? =
N




?F
.
GT F

(16)

(17)

We also note that the quadratic penalty used in this approximation been transformed from a cost
? ? serves as a
on the responses of all of the neural units to a cost only on the observed ones. R
regularization parameter which now must be tuned so the decoder-side estimate of the encoding
? ? = ?I for some constant coarsely tuned ?, though
update is reasonable. For simplicity we let R
in general this cost need not be a scaled identity matrix. Equations 16 & 17 only use information
available at the decoder side, with terms dependent on F C having been replaced by terms dependent
instead on F . These predictions will be used only to engineer decoder update schemes that can be
used to improve co-adaptation (as in procedure 2).
Procedure 2 r-step-ahead co-adaptation
for t = 1 to lengthT raining do
Encoder-side
As in section 5
Decoder-side
Get xt and yt
d CB
d (RLS)
Update decoder-side estimate of encoder CA,
d CB
d (LQE)
Update optimal decoder F, G using current encoder estimate CA,
for r = 1 to numStepsAhead do
Anticipate encoder update CApred , CBpred to updated decoder F, G (modified LQR)
Update r-step-ahead optimal decoder F, G using CApred , CBpred (LQE)
end for
Decode current signal using r-step-ahead F, G and display as feedback x
bt
end for
The ability to compute decoder-side approximate encoder updates opens the opportunity to improve
encoder-decoder update dynamics by anticipating encoder-side adaptation to guide the process towards faster convergence, and possibly to better solutions. For the current estimate of the encoder,
we update the optimal decoder, anticipate the encoder update by the method of section above, and
then update the decoder in response to the anticipated encoder update. This procedure allows rstep-ahead updating as presented in procedure 2. Figure 5 demonstrates how the one-step-ahead
7

scheme can improve the co-adaptation dynamics. It is not a priori obvious that this method would
help - the decoder-side estimate of the encoder update is not identical to the actual update. An
encoder-side agent more permissive of rapid changes in the decoder may better handle r-step-ahead
co-adaptation. We have also tried r-step-ahead updates for r > 1. However, this did not outperform
the one-step-ahead method, and in some cases yields a decline relative to standard co-adaptation.
These simulations are susceptible to the setting of the forgetting factor used by each agent in the
RLS estimation, the initial uncertainty of the parameters, and the quadratic cost used in the one? ? . The encoder-side RLS parameters in a real setting will be determined
step-ahead approximation R
?
?
by the BCI user and R should be tuned (as a regularization parameter).
The encoder-side forgetting factor would correspond roughly to the plasticity of the BCI user with
respect to the task. A high forgetting factor permits the user to tolerate very large changes in the
decoder, and a low forgetting factor corresponds to the user assuming more decoder stability. From
left to right in the subplots of figure 5, encoder-side forgetting factor decreases - the regime where
augmenting co-adaptation may offer the most benefit corresponds to a user that is most uncertain
about the decoder and willing to tolerate decoder changes. Whether or not co-adaptation gains
are possible in our model depend upon parameters of the system. Nevertheless, for appropriately
selected parameters, attempting to augment the co-adaptation should not hurt performance even
if the user were outside of the regime where the most benefit is possible. A real user will likely
perform their half of co-adaptation sub-optimally relative to our idealized BCI user and the structure
of such suboptimalities will likely increase the opportunity for co-adaptation to be augmented. The
timescale of these simulation results are unspecified, but would correspond to the timescale on which
the biological neural encoding can change. This varies by task and implicated brain-region, ranging
from a few training sessions [22, 23] to days [24].

6 Conclusion
Our work represents a step in the direction of exploiting co-adaptation to jointly optimize the neural
encoding and the decoder parameters, rather than simply optimizing the decoder parameters without
taking the encoder parameter adaptation into account. We model the process of co-adaptation that
occurs in closed-loop BCI use between the user and decoding algorithm. Moreover, the results using
our modified decoding update demonstrate a proof of concept that reliable improvement can be
obtained relative to naive adaptive decoders by encoder-aware updates to the decoder in a simulated
system. It is still open how well methods based on this approach will extend to experimental data.
BCI is a two-agent system, and we may view co-adaptation as we have formulated it within multiagent control theory. As both agents adapt to reduce the error of the decoded intention given their
respective estimates of the other agent, a fixed point of this co-adaptation process is a Nash equilibrium. This equilibrium is only known to be unique in the case where the intention at each timestep is
independent [25]. In our more general setting, there may be more than one encoder-decoder pair for
which each is optimal given the other. Moreover, there may exist non-linear encoders with which
non-linear decoders can be in equilibrium. These connections will be explored in future work.
Obviously our model of the neural encoding and the process by which the neural encoding model
is updated are idealizations. Future experimental work will determine how well our co-adaptive
model can be applied to the real neuroprosthetic context. For rapid, low-cost experiments it might
be best to begin with a human, closed-loop experiments intended to simulate a BCI [26]. As the
Kalman filter is a standard decoder, it will be useful to begin experimental investigations with this
choice (as analyzed in this work). More complicated decoding schemes also appear to improve
decoding performance [23] by better accounting for the non-linearities in the real neural encoding,
and such methods scale to BCI contexts with many output degrees of freedom [27]. An important
extension of the co-adaptation model presented in this work is to non-linear encoding and decoding
schemes. Even in more complicated, realistic settings, we hope the framework presented here will
offer similar practical benefits for improving BCI control.
Acknowledgments
This project is supported in part by the Gatsby Charitable Foundation. Liam Paninski receives
support from a NSF CAREER award.
8

References
[1] M. D. Serruya, N. G. Hatsopoulos, L. Paninski, M. R. Fellows, and J. P. Donoghue, ?Instant neural control
of a movement signal.,? Nature, vol. 416, no. 6877, pp. 141?142, 2002.
[2] K. J. Miller et al., ?Cortical activity during motor execution, motor imagery, and imagery-based online
feedback.,? PNAS, vol. 107, no. 9, pp. 4430?4435, 2010.
[3] D. J. McFarland, W. A. Sarnacki, and J. R. Wolpaw, ?Electroencephalographic (eeg) control of threedimensional movement.,? Journal of Neural Engineering, vol. 7, no. 3, p. 036007, 2010.
[4] V. Gilja et al., ?A high-performance neural prosthesis enabled by control algorithm design.,? Nat Neurosci,
2012.
[5] L. R. Hochberg et al., ?Reach and grasp by people with tetraplegia using a neurally controlled robotic
arm,? Nature, vol. 485, no. 7398, pp. 372?375, 2012.
[6] D. Putrino et al., ?Development of a closed-loop feedback system for real-time control of a highdimensional brain machine interface,? Conf Proc IEEE EMBS, vol. 2012, pp. 4567?4570, 2012.
[7] S. Koyama et al., ?Comparison of brain-computer interface decoding algorithms in open-loop and closedloop control.,? Journal of Computational Neuroscience, vol. 29, no. 1-2, pp. 73?87, 2010.
[8] J. M. Carmena et al., ?Learning to control a brainmachine interface for reaching and grasping by primates,? PLoS Biology, vol. 1, no. 2, p. E42, 2003.
[9] V. Gilja et al., ?A brain machine interface control algorithm designed from a feedback control perspective.,? Conf Proc IEEE Eng Med Biol Soc, vol. 2012, pp. 1318?22, 2012.
[10] Z. Li, J. E. ODoherty, M. A. Lebedev, and M. A. L. Nicolelis, ?Adaptive decoding for brain-machine
interfaces through bayesian parameter updates.,? Neural Comput., vol. 23, no. 12, pp. 3162?204, 2011.
[11] K. Kowalski, B. He, and L. Srinivasan, ?Dynamic analysis of naive adaptive brain-machine interfaces,?
Neural Comput., vol. 25, no. 9, pp. 2373?2420, 2013.
[12] C. Vidaurre, C. Sannelli, K.-R. Muller, and B. Blankertz, ?Machine-learning based co-adaptive calibration
for brain-computer interfaces,? Neural Computation, vol. 816, no. 3, pp. 791?816, 2011.
[13] M. Lagang and L. Srinivasan, ?Stochastic optimal control as a theory of brain-machine interface operation,? Neural Comput., vol. 25, pp. 374?417, Feb. 2013.
[14] R. Heliot, K. Ganguly, J. Jimenez, and J. M. Carmena, ?Learning in closed-loop brain-machine interfaces: Modeling and experimental validation,? Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE
Transactions on, vol. 40, no. 5, pp. 1387?1397, 2010.
[15] S. Dangi, A. L. Orsborn, H. G. Moorman, and J. M. Carmena, ?Design and Analysis of Closed-Loop Decoder Adaptation Algorithms for Brain-Machine Interfaces,? Neural Computation, pp. 1?39, Apr. 2013.
[16] Y. Zhang, A. B. Schwartz, S. M. Chase, and R. E. Kass, ?Bayesian learning in assisted brain-computer
interface tasks.,? Conf Proc IEEE Eng Med Biol Soc, vol. 2012, pp. 2740?3, 2012.
[17] S. Waldert et al., ?A review on directional information in neural signals for brain-machine interfaces.,?
Journal Of Physiology Paris, vol. 103, no. 3-5, pp. 244?254, 2009.
[18] G. P. Papavassilopoulos, ?Solution of some stochastic quadratic Nash and leader-follower games,? SIAM
J. Control Optim., vol. 19, pp. 651?666, Sept. 1981.
[19] E. Doi and M. S. Lewicki, ?Characterization of minimum error linear coding with sensory and neural
noise.,? Neural Computation, vol. 23, no. 10, pp. 2498?2510, 2011.
[20] M. Athans, ?The discrete time linear-quadratic-Gaussian stochastic control problem,? Annals of Economic
and Social Measurement, vol. 1, pp. 446?488, September 1972.
[21] M. D. Golub, S. M. Chase, and B. M. Yu, ?Learning an internal dynamics model from control demonstration.,? 30th International Conference on Machine Learning, 2013.
[22] R. Shadmehr, M. A. Smith, and J. W. Krakauer, ?Error correction, sensory prediction, and adaptation in
motor control.,? Annual Review of Neuroscience, vol. 33, no. March, pp. 89?108, 2010.
[23] L. Shpigelman, H. Lalazar, and E. Vaadia, ?Kernel-arma for hand tracking and brain-machine interfacing
during 3d motor control,? in NIPS, pp. 1489?1496, 2008.
[24] A. C. Koralek, X. Jin, J. D. Long II, R. M. Costa, and J. M. Carmena, ?Corticostriatal plasticity is necessary for learning intentional neuroprosthetic skills.,? Nature, vol. 483, no. 7389, pp. 331?335, 2012.
[25] T. Basar, ?On the uniqueness of the Nash solution in linear-quadratic differential games,? International
Journal of Game Theory, vol. 5, no. 2-3, pp. 65?90, 1976.
[26] J. P. Cunningham et al., ?A closed-loop human simulator for investigating the role of feedback control in
brain-machine interfaces.,? Journal of Neurophysiology, vol. 105, no. 4, pp. 1932?1949, 2010.
[27] Y. T. Wong et al., ?Decoding arm and hand movements across layers of the macaque frontal cortices.,?
Conf Proc IEEE Eng Med Biol Soc, vol. 2012, pp. 1757?60, 2012.

9

"
3936,"Distributed Non-Stochastic Experts

Varun Kanade?
UC Berkeley
vkanade@eecs.berkeley.edu

Zhenming Liu?
Princeton University
zhenming@cs.princeton.edu

Bo?zidar Radunovi?c
Microsoft Research
bozidar@microsoft.com

Abstract
We consider the online distributed non-stochastic experts problem, where the distributed system consists of one coordinator node that is connected to k sites, and
the sites are required to communicate with each other via the coordinator. At each
time-step t, one of the k site nodes has to pick an expert from the set {1, . . . , n},
and the same site receives information about payoffs of all experts for that round.
The goal of the distributed system is to minimize regret at time horizon T , while
simultaneously keeping communication to a minimum. The two extreme solutions
to this problem are: (i) Full communication:
p This essentially simulates the nondistributed setting to obtain the optimal O( log(n)T ) regret bound at the cost of
T communication.
p (ii) No communication: Each site runs an independent copy ?
the regret is O( log(n)kT ) and the communication is 0. This paper shows
the
?
difficulty of simultaneously achieving regret asymptotically better than kT and
communication better than T . We give a novel algorithm
that for an oblivious
?
adversary achieves a non-trivial trade-off: regret O( k 5(1+)/6 T ) and communication O(T /k  ), for any value of  ? (0, 1/5). We also consider a variant of the
model, where the coordinator picks the expert. In this model, we show that the
label-efficient forecaster of Cesa-Bianchi et al. (2005) already gives us strategy
that is near optimal in regret vs communication trade-off.

1

Introduction

In this paper, we consider the well-studied non-stochastic expert problem in a distributed setting.
In the standard (non-distributed) setting, there are a total of n experts available for the decisionmaker to consult, and at each round t = 1, . . . , T , she must choose to follow the advice of one of
the experts, say at , from the set [n] = {1, . . . , n}. At the end of the round, she observes a payoff
vector pt ? [0, 1]n , where pt [a] denotes the payoff that would have been received by following
the advice of expert a. The payoff received by the decision-maker is pt [at ]. In the non-stochastic
setting, an adversary decides the payoff vectors at any time step. At the end of the T rounds, the
regret of the decision maker is the difference in the payoff that she would have received using the
single best expert at all times in hindsight, and the payoff that she actually received, i.e. R =
PT
PT
maxa?[n] t=1 pt [a] ? t=1 pt [at ]. The goal here is to minimize her regret; this general problem
?

This work was performed while the author was at Harvard University supported in part by grant NSF-CCF09-64401
?
This work was performed while the author was at Harvard University supported in part by grants NSF-IIS0964473 and NSF-CCF-0915922.

1

in the non-stochastic setting captures several applications of interest, such as experiment design,
online ad-selection, portfolio optimization, etc. (See [1, 2, 3, 4, 5] and references therein.)
Tight bounds on regret for the non-stochastic expert problem are obtained by the so-called follow
the regularized leader approaches; at time t, the decision-maker chooses a distribution, xt , over the
Pt?1
n experts. Here xt minimizes the quantity s=1 pt ? x + r(x), where r is a regularizer. Common
regularizers are the entropy function, which results in Hedge [1] or the exponentially weighted
forecaster (see chap. 2 in [2]), or as we consider in this paper r(x) = ?? ? x, where ?? ?R [0, ?]n is a
random vector, which gives the follow the perturbed leader (FPL) algorithm [6].
We consider the setting when the decision maker is a distributed system, where several different
nodes may select experts and/or observe payoffs at different time-steps. Such settings are common,
e.g. internet search companies, such as Google or Bing, may use several nodes to answer search
queries and the performance is revealed by user clicks. From the point of view of making better predictions, it is useful to pool all available data. However, this may involve significant communication
which may be quite costly. Thus, the question of interest is studying the trade-off between cost of
communication and cost of inaccuracy (because of not pooling together all data).

2

Models and Summary of Results

We consider a distributed computation model consisting of one central coordinator node connected
to k site nodes. The site nodes must communicate with each other using the coordinator node. At
each time step, the distributed system receives a query1 , which indicates that it must choose an
expert to follow. At the end of the round, the distributed system observes the payoff vector. We consider two different models described in detail below: the site prediction model where one of the k
sites receives a query at any given time-step, and the coordinator prediction model where the query
is always received at the coordinator node. In both these models, the payoff vector, pt , is always
observed at one of the k site nodes. Thus, some communication is required to share the information
about the payoff vectors among nodes. As we shall see, these two models yield different algorithms
and performance bounds. All missing proofs are provided in the long version [7]
Goal: The algorithm implemented on the distributed system may use randomness, both to decide
which expert to pick and to decide when to communicate with other nodes. We focus on simultaneously minimizing the expected regret and the expected communication used by the (distributed)
algorithm. Recall, that the expected regret is:
""
E[R] = E max

a?[n]

T
X

t

p [a] ?

t=1

T
X

#
t

t

p [a ],

(1)

t=1

where the expectation is over the random choices made by the algorithm. The expected communication is simply the expected number (over the random choices) of messages sent in the system.
As we show in this paper, this is a challenging problem and to keep the analysis simple we focus on
bounds in terms of the number of sites k and the time horizon T , which are often the most important
scaling parameters. In particular, our algorithms are variants of follow the perturbed leader (FPL)
and hence our bounds are not optimal in terms of the number of experts n. We believe that the
dependence on the number of experts in our algorithms (upper bounds) can be strengthened using
a different regularizer. Also, all our lower bounds are shown in terms of T and k, for n = 2. For
larger n, using techniques similar to Thm. 3.6 in [2] should give the appropriate dependence on n.
Adversaries: In the non-stochastic setting, we assume that an adversary may decide the payoff vectors, pt , at each time-step and also the site, st , that receives the payoff vector (and also the query in
the site-prediction model). An oblivious adversary cannot see any of the actions of the distributed
system, i.e. selection of expert, communication patterns or any random bits used. However, the
oblivious adversary may know the description of the algorithm. In addition to knowing the description of the algorithm, an adaptive adversary is stronger and can record all of the past actions of the
algorithm, and use these arbitrarily to decide the future payoff vectors and site allocations.
Communication: We do not explicitly account for message sizes, since we are primarily concerned
with scaling in terms of T and k. We require that message size not depend k or T , but only on the
1
We do not use the word query in the sense of explicitly giving some information or context, but merely as
indication of occurrence of an event that forces some site or coordinator to choose an expert

2

number of experts n. In other words, we assume that n is substantially smaller than T and k. All the
messages used in our algorithms contain at most n real numbers. As is standard in the distributed
systems literature, we assume that communication delay is 0, i.e. the updates sent by any node are
received by the recipients before any future query arrives. All our results still hold under the weaker
assumption that the number of queries received by the distributed system in the duration required to
complete a broadcast is negligible compared to k. 2
We now describe the two models in greater detail, state our main results and discuss related work:
1. S ITE P REDICTION M ODEL: At each time step t = 1, . . . , T , one of the k sites, say st , receives
a query and has to pick an expert, at , from the set, [n] = {1, . . . , n}. The payoff vector pt ? [0, 1]n ,
where pt [i] is the payoff of the ith expert is revealed only to the site st and the decision-maker
(distributed system) receives payoff pt [at ], corresponding to the expert actually chosen. The site
prediction model is commonly studied in distributed machine learning settings (see [8, 9, 10]). The
payoff vectors p1 , . . . , pT and also the choice of sites that receive the query, s1 , . . . , sT , are decided
by an adversary. There are two very simple algorithms in this model:
(i) Full communication: The coordinator always maintains the current cumulative payoff vector,
Pt?1 ?
Pt?1 ?
t
? =1 p . At time step t, s receives the current cumulative payoff vector
? =1 p from the
coordinator, chooses an expert at ? [n] using FPL, receives payoff vector pt and sends pt to the
coordinator, which updates its cumulative payoff vector. Note that the total communication
? is 2T
and the system simulates (non-distributed) FPL to achieve (optimal) regret guarantee O( nT ).
(ii) No communication: Each site maintains cumulative payoff vectors corresponding to the queries
received by them, thus implementing k independent versions of FPL. Suppose that the ?
ith site
?
Pk
Pk
receives a total of Ti queries ( i=1 Ti = T ), the regret is bounded by i=1 O( nTi ) = O( nkT )
and the total communication is 0. This upper bound is actually tight in the event that there is 0
communication (see the accompanying long version [7]).
?
Simultaneously achieving regret that is asymptotically lower than knT using communication
asymptotically lower than T turns out to be a significantly challenging question. Our main positive
result is the first distributed expert algorithm in the oblivious adversarial (non-stochastic) setting,
using sub-linear communication. Finding such an algorithm in the case of an adaptive adversary is
an interesting open problem.
Theorem 1. When T ? 2k 2.3 , there exists an algorithm ?
for the distributed experts problem that
against an oblivious adversary achieves regret O(log(n) k 5(1+)/6 T ) and uses communication
O(T /k  ), giving non-trivial guarantees in the range  ? (0, 1/5).
2. C OORDINATOR P REDICTION M ODEL: At every time step, the query is received by the coordinator node, which chooses an expert at ? [n]. However, at the end of the round, one of the
site nodes, say st , observes the payoff vector pt . The payoff vectors pt and choice of sites st are
decided by an adversary. This model is also a natural one and is explored in the distributed systems
and streaming literature (see [11, 12, 13] and references therein).
?
The full communication protocol is equally applicable here getting optimal regret bound, O( nT ) at
the cost of substantial (essentially T ) communication. But here, we do not have any straightforward
algorithms that achieve non-trivial regret without using any communication. This model is closely
related to the label-efficient prediction problem (see Chapter 6.1-3 in [2]), where the decision-maker
has a limited budget and has to spend part of its budget to observe any payoff information. The
optimal strategy is to request payoff information randomly with probability C/T at each time-step,
if C is the communication budget. We refer to this algorithm as LEF (label-efficient forecaster) [14].
Theorem 2.p[14] (Informal) The LEF algorithms using FPL with communication budget C achieves
regret O(T n/C) against both an adaptive and an oblivious adversary.
One of the crucial differences between this model and that of the label-efficient setting is that when
communication does occur, the site can send cumulative payoff vectors comprising all previous
updates to the coordinator rather than just the latest one. The other difference is that, unlike in
the label-efficient case, the sites have the knowledge of their local regrets and can use it to decide
2
This is because in regularized leader like approaches, if the cumulative payoff vector changes by a small
amount the distribution over experts does not change much because of the regularization effect.

3

when to communicate. However, our lower bounds for natural types of algorithms show that these
advantages probably do not help to get better guarantees.
Lower Bound Results: In the case of an adaptive adversary, we have an unconditional (for any
type of algorithm) lower bound in both the models:
Theorem 3. Let n?= 2 be the number of experts. Then any (distributed) algorithm that achieves
expected regret o( kT ) must use communication (T /k)(1 ? o(1)).
The proof appears in [7]. Notice that in the coordinator prediction model, when C = T /k, this
lower bound is matched by the upper bound of LEF.
In the case of an oblivious adversary, our results are weaker, but we can show that certain natural
types of algorithms are not applicable directly in this setting. The so called regularized leader
algorithms, maintain a cumulative payoff vector, Pt , and use only this and a regularizer to select an
expert at time t. We consider two variants in the distributed setting:
? t , which is an (approximate)
(i) Distributed Counter Algorithms: Here the forecaster only uses P
t
version of the cumulative payoff vector P . But we make no assumptions on how the forecaster will
? t. P
? t can be maintained while using sub-linear communication by applying techniques from
use P
distributed systems literature [12]. (ii) Delayed Regularized Leader: Here the regularized leaders
don?t try to explicitly maintain an approximate version of the cumulative payoff vector. Instead,
they may use an arbitrary communication protocol, but make prediction using the cumulative payoff
vector (using any past payoff vectors that they could have received) and some regularizer.
We show in Section 3.2 that the distributed counter approach does not yield any non-trivial guarantee
in the site-prediction model even against an oblivious adversary. It is possible to show a similar lower
bound the in the coordinator prediction model, but is omitted since it follows easily from the idea in
the site-prediction model combined with an explicit communication lower bound given in [12].
Section 4 shows that the delayed regularized leader approach is ineffective even against an oblivious
adversary for coordinator prediction model, suggesting LEF algorithm is near optimal.
Related Work: Recently there has been significant interest in distributed online learning questions
(see for example [8, 9, 10]). However, these works have focused mainly on stochastic optimization problems. Thus, the techniques used, such as reducing variance through mini-batching, are not
applicable to our setting. Questions such as network structure [9] and network delays [10] are interesting in our setting as well, however, at present our work focuses on establishing some non-trivial
regret guarantees in the distributed online non-stochastic experts setting. Study of communication
as a resource in distributed learning is also considered in [15, 16, 17]; however, this body of work
seems only applicable to offline learning.
The other related work is that of distributed functional monitoring [11] and in particular distributed
counting[12, 13], and sketching [18]. Some of these techniques have been successfully applied
in offline machine learning problems [19]. However, we are the first to analyze the performancecommunication trade-off of an online learning algorithm in the standard distributed functional monitoring framework [11]. An application of a distributed counter to an online Bayesian regression was
proposed in Liu et al. [13]. Our lower bounds discussed below, show that approximate distributed
counter techniques do not directly yield non-trivial algorithms.

3
3.1

Site-prediction model
Upper Bounds

We describe our algorithm that simultaneously achieves non-trivial bounds on expected regret and
expected communication. We begin by making two assumptions that simplify the exposition. First,
we assume that there are only 2 experts. The generalization from 2 experts to n is easy, as discussed
in the Remark 1 at the end of this section. Second, we assume that there exists a global query
counter, that is available to all sites and the co-ordinator, which keeps track of the total number of
queries received across the k sites. We discuss this assumption in Remark 2 at the end of the section.
As is often the case in online algorithms, we assume that the time horizon T is known. Otherwise,
the standard doubling trick may be employed. The notation used in this Section is defined in Table 1.
4

Symbol
pt
`
b
Pi
Qi
M (v)
FPi (?)
FRia (?)
FRi (?)

Definition
Payoff vector at time-step t, pt ? [0, 1]2
The length of block into which inputs are divided
Number of input blocks b = T /`
Pi`
Cumulative payoff vector within block i, Pi = t=(i?1)`+1 pt
Pi?1
Cumulative payoff vector until end of block (i ? 1), Qi = j=1 Pj
For vector v ? R2 , M (v) = 1 if v1 > v2 ; M (v) = 2 otherwise
Random variable denoting the payoff obtained by playing FPL(?) on block i
Random variable denoting the regret with respect to action a of playing FPL(?) on block i
FRia (?) = Pi [a] ? FPi (?)
Random variable denoting the regret of playing FPL(?) on payoff vectors in block i
FRi (?) = maxa=1,2 Pi [a] ? FPi (?) = maxa=1,2 FRia (?)
Table 1: Notation used in Algorithm DFPL (Fig. 1) and in Section 3.1.

DFPL(T , `, ?)

?
set b = T /`; ? 0 = `; q = 2`3 T 2 /? 5
for i = 1 . . . , b
let Yi = Bernoulli(q)
if Yi = 1 then #step phase
play FPL(? 0 ) for time-steps (i ? 1)` + 1, . . . , i`
else #block phase
ai = M (Qi + r) where r ?R [0, ?]2
play ai for time-steps (i ? 1)` + 1, . . . , i`
P
i
t
P = i`
t=(i?1)`+1 p
i+1
i
i
Q
=Q +P

FPL(T, n = 2, ?)
for t = 1, . .P
.,T
?
2
at = M ( t?1
? =1 p + r) where r ?R [0, ?]
t
follow expert a at time-step t
observe payoff vector pt

(a)

(b)

Figure 1: (a) DFPL: Distributed Follow the Perturbed Leader, (b) FPL: Follow the Perturbed Leader with
parameter ? for 2 experts (M (?) is defined in Table 1, r is a random vector)

Algorithm Description: Our algorithm DFPL is described in Figure 1(a). We make use of FPL
algorithm, described in Figure 1(b), which takes as a parameter the amount of added noise ?. DFPL
algorithm treats the T time steps as b(= T /`) blocks, each of length `. At a high level, with
probability q on any given block the algorithm is in the step phase, running a copy of FPL (with
noise parameter ? 0 ) across all time steps of the block, synchronizing after each time step. Otherwise
it is in a block phase, running a copy of FPL (with noise parameter ?) across blocks with the same
expert being followed for the entire block and synchronizing after each block. This effectively makes
Pi , the cumulative payoff over block i, the payoff vector for the block FPL. The block FPL has on
average (1 ? q)T /` total time steps. We begin by stating a (slightly stronger) guarantee for FPL.
Lemma 1. Consider the case n = 2. Let p1 , . . . , pT ? [0, 1]2 be a sequence of payoff vectors such
that maxt |pt |? ? B and let the number of experts be 2. Then FPL(?) has the following guarantee
PT
t
t
on expected regret, E[R] ? B
t=1 |p [1] ? p [2]| + ?.
?
The proof is a simple modification to the proof of the standard analysis [6] and is given in [7]. The
rest of this section is devoted to the proof of Lemma 2
Lemma 2. Consider the case n = 2. If T > 2k 2.3 , Algorithm DFPL (Fig. 1) when ?
run with
parameters `, T , ? = `5/12 T 1/2 and b, ? 0 , q as defined in Fig 1, has expected regret O( `5/6 T )
and expected communication O(T k/`). In particular for ` = k 1+?for 0 <  < 1/5, the algorithm
simultaneously achieves regret that is asymptotically lower than kT and communication that is
asymptotically lower3 than T .
3

T

Note that here asymptotics is in terms
? of both parameters k and T . Getting communication of the form
f (k) for regret bound better than kT , seems to be a fairly difficult and interesting problem

1??

5

Since we are in the case of an oblivious adversary, we may assume that the payoff vectors p1 , . . . , pT
are fixed ahead of time. Without loss of generality let expert 1 (out of {1, 2}) be the one that has
greater payoff in hindsight. Recall that FRi1 (? 0 ) denotes the random variable that is the regret of
playing FPL(? 0 ) in a step phase on block i with respect to the first expert. In particular, this will
be negative if expert 2 is the best expert on block i, even though globally expert 1 is better. In fact,
this is exactly what our algorithm exploits: it gains on regret in the communication-expensive, step
phase while saving on communication in the block phase.

Pb
The regret can be written as R = i=1 Yi ? FRi1 (? 0 ) + (1 ? Yi )(Pi [1] ? Pi [ai ]) . Note that the
random variables Yi are independent of the random variables FRi1 (? 0 ) and the random variables ai .
As E[Yi ] = q, we can bound the expression for expected regret as follows:
E[R] ? q

b
X

E[FRi1 (? 0 )] + (1 ? q)

i=1

b
X

E[Pi [1] ? Pi [ai ]]

(2)

i=1

We first analyze the second term of the above equation. This is just the regret corresponding
to running FPL(?) at the block level, with T /` time steps. Using the fact that maxi |Pi |? ?
` maxt |pt |? ? `, Lemma 1 allows us to conclude that:
b
X

E[Pi [1] ? Pi [ai ]] ?

i=1

b
`X i
|P [1] ? Pi [2]| + ?
? i=1

(3)

?
Next, we also analyse the first term of the inequality
(2). We chose ? 0 = ` (see Fig. 1) and the
?
analysis of FPL guarantees that E[FRi (? 0 )] ? 2 `, where FRi (? 0 ) denotes the random variable
that is the actual regret of FPL(? 0 ), not the regret with respect to expert 1 (which is FRi1 (? 0 )). Now
i
i 0
i
0
0
either
1 (? ) (i.e. expert 1 was the better one on block i), in which case E[FR1 (? )] ?
? FR (? ) = FR
i 0
i
0
2 `; otherwise ?
FR (? ) = FR2 (? ) (i.e. expert 2 was the better one on block i), in which case
E[FRi1 (? 0 )] ? 2 ` + Pi [1] ? Pi [2]. Note that in this expression
Pi [1] ? Pi [2] is negative. Putting
?
i
0
i
everything together we can write that E[FR1 (? )] ? 2 ` ? (P [2] ? Pi [1])+ , where (x)+ = x if
x ? 0 and 0 otherwise. Thus, we get the main equation for regret.
b
b
X
?
`X i
(Pi [2] ? Pi [1])+ +
|P [1] ? Pi [2]| +?
E[R] ? 2qb ` ? q
?
i=1
i=1
|
{z
} |
{z
}
term 1

(4)

term 2

?
?
Note that the first (i.e. 2qb `) and last (i.e. ?) terms of inequality (4) are O( `5/6 T ) for the setting
of the parameters as in Lemma 2. The strategy is to show that when ?term 2? becomes large, then
?term 1? is also large in magnitude, but negative, compensating the effect of ?term 1?. We consider
a few cases:
Case 1: When the best expert is identified quickly and not changed thereafter. Let ? denote the
maximum index, i, such that Qi [1] ? Qi [2] ? ?. Note that after the block ? is processed, the
algorithm in the block phase will never follow expert 2.
Suppose that ? ? (?/`)2 . We note that the correct bound for ?term 2? is now actually
P?
(`/?) i=1 |Pi [1] ? Pi [2]| ? (`2 ?/?) ? ? since |Pi [1] ? Pi [2]| ? ` for all i.
Case 2 The best expert may not be identified quickly, furthermore |Pi [1] ? Pi [2]| is large often. In
this case, although ?term 2? may be large (when (Pi [1] ? Pi [2]) is large), this is compensated by
the negative regret in ?term 1? in expression (4). This is because if |Pi [1] ? Pi [2]| is large often,
but the best expert is not identified quickly, there must be enough blocks on which (Pi [2] ? Pi [1])
is positive and large.
Notice that ? ? (?/`)2 . Define ? = ? 2 /T and let S = {i ? ? | |Pi [1] ? Pi [2]| ? ?}.
P?
Let ? = |S|/?. We show that i=1 (Pi [2] ? Pi [1])+ ? (???)/2
P ? ?. To see this consider
S1 = {i ? S | Pi [1] > Pi [2]} and S2 = S \ S1 . First, observe that i?S |Pi [1] ? Pi [2]| ? ???.
P
P
Then, if i?S2 (Pi [2] ? Pi [1]) ? (???)/2, we are done. If not i?S1 (Pi [1] ? Pi [2]) ? (???)/2.
P?
P?
Now notice that i=1 Pi [1] ? Pi [2] ? ?, hence it must be the case that i=1 (Pi [2] ? Pi [1])+ ?
6

(???)/2 ? ?. Now for the value of q = 2`3 T 2 /? 5 and if ? ? ? 2 /(T `), the negative contribution of
?term 1? is at least q???/2 which greater than the maximum possible positive contribution of ?term
2? which is `2 ?/?. It is easy to see that these quantities are equal and hence the total contribution of
?term 1? and ?term 2? together is at most ?.
Case 3 When |Pi [1] ? Pi [2]| is ?small? most of the time. In this case the parameter ? is actually
well-tuned (which was not the case when |Pi [1] ? Pi [2]| ? `) and gives us a small overall regret.
(See Lemma 1.) We have ? < ? 2 /(T `). Note that ?` ? ? = ? 2 /T and that ? ? T /`. In this case
P?
?term 2? can be bounded easily as follows: ?` i=1 |Pi [1] ? Pi [2]| ? ?` (??` + (1 ? ?)??) ? 2?
The above three cases exhaust all possibilities and hence no matter what the nature of the payoff
sequence, the expected regret of DFPL is bounded by O(?) as required. The expected total communication is easily seen to be O(qT +T k/`) ? the q(T /`) blocks on which step FPL is used contribute
O(`) communication each, and the (1 ? q)(T /`) blocks where block FPL is used contributed O(k)
communication each.
Remark 1. Our algorithm can be generalized to n experts by recursively dividing the set of experts
in two and applying our algorithm to two meta-experts, to give the result of Theorem 1. Details are
provided in [7].
Remark 2. Instead of a global counter, it suffices for the co-ordinator to maintain an approximate
counter and notify all sites of beginning an end of blocks by broadcast. This only adds 2k communication per block. See [7] for more details.
3.2

Lower Bounds

In this section we give a lower bound on distributed counter algorithms in the site prediction model.
Distributed counters allow tight approximation guarantees,
i.e. for factor ? additive approximation,
?
the communication required
is
only
O(T
log(T
)
k/?)
[12].
We observe that the noise used by
?
FPL is quite large, O( T ), and so it is tempting to find a suitable ? and run FPL using approximate
cumulative payoffs. We consider the class of algorithms such that:
(i) Whenever each site receives a query, it has an (approximate) cumulative payoff of each expert to
additive accuracy ?. Furthermore, any communication is only used to maintain such a counter.
(ii) Any site only uses the (approximate) cumulative payoffs and any local information it may have
to choose an expert when queried.
However, our negative result shows that even with a highly accurate counter?? = O(k), the nonstochasticity of the payoff sequence may cause any such algorithm to have ?( kT ) regret. Furthermore, we show that any distributed algorithm that implements (approximate) counters to additive
error k/10 on all sites4 is at least ?(T ).
Theorem 4. At any time step t, suppose each site has an (approximate) cumulative payoff count,
? t [a], for every expert such that |Pt [a] ? P
? t [a]| ? ?. Then we have the following:
P
? t [a] and any local information at the
1. If ? ? k, any algorithm that uses the approximate counts P
?
site making the decision, cannot achieve expected regret asymptotically better than ?T .
2. Any protocol on the distributed system that guarantees that at each time step, each site has a
? = k/10 approximate cumulative payoff with probability ? 1/2, uses ?(T ) communication.

4

Coordinator-prediction model

In the co-ordinator prediction model, as mentioned earlier it is possible to use the label-efficient forecaster, LEF (Chap. 6 [2, 14]). Let C be an upper bound on the total amount of communication we are
allowed to use. The label-efficient predictor translates into the following simple protocol: Whenever
a site receives a payoff vector, it will forward that particular payoff to the coordinator with probability p ? C/T . The coordinator will always execute the exponentially weighted forecaster
p over the
sampled subset of payoffs to make new decisions.
Here, the expected regret is O(T log(n)/C).
?
In other words, if our regret needs to be O( T ), the communication needs to be linear in T .
4

The approximation guarantee is only required when a site receives a query and has to make a prediction.

7

We observe that in principle there is a possibility of better algorithms in this setting for mainly two
reasons: (i) when the sites send payoff vectors to the co-ordinator, they can send cumulative payoffs
rather than the latest ones, thus giving more information, and (ii) the sites may decided when to
communicate as a function of the payoff vectors instead of just randomly. However,
? we present a
lower-bound that shows that for a natural family of algorithms achieving regret O( T ) requires at
least ?(T 1? ) for every  > 0, even when k = 1. The type of algorithms we consider may have an
arbitrary communication protocol, but it satisfies the following: (i) Whenever a site communicates
with the coordinator, the site will report?
its local cumulative payoff vector. (ii) When the
? coordinator
makes a decision, it will execute, FPL( T ), (follow the perturbed leader with noise T ) using the
latest cumulative payoff vector. The proof of Theorem 5 appears in [7] and the results could be
generalized to other regularizers.
Theorem 5. Consider the distributed non-stochastic expert problem in ?coordinator prediction
model. Any algorithm of the kind described above that achieves regret O( T ) must use ?(T 1? )
communication against an oblivious adversary for every constant .

Simulations
400
Cumulative regret

4

x 10

No?communication
Mini?batch, p=4.64e?002
All?communication
HYZ, p=2.24e?001
DFPL, ?=0.00e+000
DFPL, ?=1.48e?001

500

300
200

Worst?case communication

5

100
0
?100
0

0.5

1
?

1.5

6
4
2
0
0

2
4

DFPL
Mini?batches
HYZ

8

500

x 10

(a)

1000
1500
Worst?case regret

2000

(b)

Figure 2: (a) - Cumulative regret for the MC sequences as a function of correlation ?, (b) - Worst-case
cumulative regret vs. communication cost for the MC and zig-zag sequences.

In this section, we describe some simulation results comparing the efficacy of our algorithm DFPL
with some other techniques. We compare DFPL against simple algorithms ? full communication
and no communication, and two other algorithms which we refer to as mini-batch and HYZ. In the
mini-batch algorithm, the coordinator requests randomly, with some probability p at any time step,
all cumulative payoff vectors at all sites. It then broadcasts the sum (across all of the sites) back to
the sites, so that all sites have the latest cumulative payoff vector. Whenever such a communication
does occur, the cost is 2k. We refer to this as mini-batch because it is similar in spirit to the minibatch algorithms used in the stochastic optimization problems. In the HYZ algorithm, we use the
distributed counter technique of Huang et al. [12] to maintain the (approximate) cumulative payoff
for each expert. Whenever a counter update occurs, the coordinator must broadcast to all nodes to
make sure they have the most current update.
We consider two types of synthetic sequences. The first is a zig-zag sequence, with ? being the
length of one increase/decrease. For the first ? time steps the payoff vector is always (1, 0) (expert
1 being better), then for the next 2? time steps, the payoff vector is (0, 1) (expert 2 is better), and
then again for the next 2? time-steps, payoff vector is (1, 0) and so on. The zig-zag sequence is also
the sequence used in the proof of the lower bound in Theorem 5. The second is a two-state Markov
1
. While in state 1, the payoff vector
chain (MC) with states 1, 2 and Pr[1 ? 2] = Pr[2 ? 1] = 2?
is (1, 0) and when in state 2 it is (0, 1).
In our simulations we use T = 20000 predictions, and k = 20 sites. Fig. 2 (a) shows the performance of the above algorithms for the MC sequences, the results are averaged across 100 runs,
over both the randomness of the MC and the algorithms. Fig. 2 (b) shows the worst-case cumulative communication vs the worst-case cumulative regret trade-off for three algorithms: DFPL,
mini-batch and HYZ, over all the described sequences. While in general it is hard to compare algorithms on non-stochastic inputs, our results confirm that for non-stochastic sequences inspired by
the lower-bounds in the paper, our algorithm DFPL outperforms other related techniques.
8

References
[1] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learnign and an
application to boosting. In EuroCOLT, 1995.
[2] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University
Press, 2006.
[3] T. Cover. Universal portfolios. Mathematical Finance, 1:1?19, 1991.
[4] E. Hazan and S. Kale. On stochastic and worst-case models for investing. In NIPS, 2009.
[5] E. Hazan. The convex optimization approach to regret minimization. Optimization for Machine
Learning, 2012.
[6] A. Kalai and S. Vempala. Efficient algorithms for online decision problems. Journal of Computer and System Sciences, 71:291?307, 2005.
[7] V. Kanade, Z. Liu, and B. Radunovi?c. Distributed non-stochastic experts. In arXiv, 2012.
[8] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction.
In ICML, 2011.
[9] J. Duchi, A. Agarwal, and M. Wainright. Distributed dual averaging in networks. In NIPS,
2010.
[10] A. Agarwal and J. Duchi. Distributed delayed stochastic optimization. In NIPS, 2011.
[11] G. Cormode, S. Muthukrishnan, and K. Yi. Algorithms for distributed functional monitoring.
ACM Transactions on Algorithms, 7, 2011.
[12] Z. Huang, K. Yi, and Q. Zhang. Randomized algorithms for tracking distributed count, frequencies and ranks. In PODS, 2012.
[13] Z. Liu, B. Radunovi?c, and M. Vojnovi?c. Continuous distributed counting for non-monotone
streams. In PODS, 2012.
[14] N. Cesa-Bianchi, G. Lugosi, and G. Stoltz. Minimizing regret with label efficient prediction.
In ISIT, 2005.
[15] M-F. Balcan, A. Blum, S. Fine, and Y. Mansour. Distributed learning, communication complexity and privacy. In COLT (to appear), 2012.
[16] H. Daum?e III, J. M. Phillips, A. Saha, and S. Venkatasubramanian. Protocols for learning
classifiers on distributed data. In AISTATS, 2012.
[17] H. Daum?e III, J. M. Phillips, A. Saha, and S. Venkatasubramanian. Efficients protocols for
distributed classification and optimization. In arXiv:1204.3523v1, 2012.
[18] G. Cormode, M. Garofalakis, P. Haas, and C. Jermaine. Synopses for Massive Data - Samples,
Histograms, Wavelets, Sketches. Foundations and Trends in Databases, 2012.
[19] K. Clarkson, E. Hazan, and D. Woodruff. Sublinear optimization for machine learning. In
FOCS, 2010.

9

"
1986,"Learning Rankings via Convex Hull Separation

Glenn Fung, R?omer Rosales, Balaji Krishnapuram
Computer Aided Diagnosis, Siemens Medical Solutions USA, Malvern, PA 19355
{glenn.fung, romer.rosales, balaji.krishnapuram}@siemens.com

Abstract
We propose efficient algorithms for learning ranking functions from order constraints between sets?i.e. classes?of training samples. Our algorithms may be used for maximizing the generalized Wilcoxon Mann
Whitney statistic that accounts for the partial ordering of the classes: special cases include maximizing the area under the ROC curve for binary
classification and its generalization for ordinal regression. Experiments
on public benchmarks indicate that: (a) the proposed algorithm is at least
as accurate as the current state-of-the-art; (b) computationally, it is several orders of magnitude faster and?unlike current methods?it is easily
able to handle even large datasets with over 20,000 samples.

1

Introduction

Many machine learning applications depend on accurately ordering the elements of a set
based on the known ordering of only some of its elements. In the literature, variants of this
problem have been referred to as ordinal regression, ranking, and learning of preference
relations. Formally, we want to find a function f : ?n ? ? such that, for a set of test
samples {xk ? ?n }, the output of the function f (xk ) can be sorted to obtain a ranking. In
order to learn such a function we are provided with training data, A, containing S sets (or
SS
mj
classes) of training samples: A = j=1 (Aj = {xji }i=1
), where the j-th set Aj contains
PS
mj samples, so that we have a total of m = j=1 mj samples in A. Further, we are also
provided with a directed order graph G = (S, E) each of whose vertices corresponds to a
class Aj , and the existence of a directed edge EP Q ?corresponding to AP ? AQ ?means
that all training samples xp ? AP should be ranked higher than any sample xq ? AQ : i.e.
? (xp? AP , xq? AQ ), f (xp ) ? f (xq ).
In general the number of constraints on the ranking function grows as O(m2 ) so that naive
solutions are computationally infeasible even for moderate sized training sets with a few
thousand samples. Hence, we propose a more stringent problem with a larger (infinite) set
of constraints, that is nevertheless much more tractably solved. In particular, we modify
the constraints to: ? (xp ? CH(AP ), xq ? CH(AQ )), f (xp ) ? f (xq ), where CH(Aj )
denotes the set of all points in the convex hull of Aj .
We show how this leads to: (a) a family of approximations to the original problem; and (b)
considerably more efficient solutions that still enforce all of the original inter-group order
constraints. Notice that, this formulation subsumes the standard ranking problem (e.g. [4])
as a special case when each set Aj is reduced to a singleton and the order graph is equal to

{v,w,x}

{v,w}

{v,w}

{w}

{v}

{v,w,x}

{x}

{v,w}

{x}

{y,z}

(a)

{y,z}

{x}

(b)

{y,z}

{x}

(c)

{z}

{y}

(d)

{y}

{z}

(e)

{z}

{y}

(f)

Figure 1: Various instances of the proposed ranking problem consistent with the training set
{v, w, x, y, z} satisfying v > w > x > y > z. Each problem instance is defined by an order
graph. (a-d) A succession of order graphs with an increasing number of constraints (e-f) Two order
graphs defining the same partial ordering but different problem instances.

a full graph. However, as illustrated in Figure 1, the formulation is more general and does
not require a total ordering of the sets of training samples Aj , i.e. it allows any order graph
G to be incorporated into the problem.
1.1

Generalized Wilcoxon-Mann-Whitney Statistics

A distinction is usually made between classification and ordinal regression methods on
one hand, and ranking on the other. In particular, the loss functions used for classification
and ordinal regression evaluate whether each test sample is correctly classified: in other
words, the loss functions that are used to evaluate these algorithms?e.g. the 0?1 loss for
binary classification?are computed for every sample individually, and then averaged over
the training or test set.
By contrast, bipartite ranking solutions are evaluated using the Wilcoxon-Mann-Whitney
(WMW) statistic which measures the (sample averaged) probability that any pair of samples is ordered correctly; intuitively, the WMW statistic may be interpreted as the area
under the ROC curve (AUC). We define a slight generalization of the WMW statistic that
accounts for our notion of class-ordering:

Pmi Pmj 
j
i
X k=1
l=1 ? f (xk ) < f (xl )
Pmi Pmj
W M W (f, A) =
.
k=1
l=1 1
E
ij

Hence, if a sample is individually misclassified because it falls on the wrong side of the
decision boundary between classes it incurs a penalty in ordinal regression, whereas, in
ranking, it may be possible that it is still correctly ordered with respect to every other test
sample, and thus it may incur no penalty in the WMW statistic.
1.2

Previous Work

Ordinal regression and methods for handling structured output classes: For a classic
description of generalized linear models for ordinal regression, see [11]. A non-parametric
Bayesian model for ordinal regression based on Gaussian processes (GP) was defined
[1]. Several recent machine learning papers consider structured output classes: e.g. [13]
presents SVM based algorithms for handling structured and interdependent output spaces,
and [5] discusses automatic document categorization into pre-defined hierarchies or taxonomies of topics.
Learning Rankings: The problem of learning rankings was first treated as a classification
problem on pairs of objects by Herbrich [4] and subsequently used on a web page ranking
task by Joachims [6]; a variety of authors have investigated this approach recently. The
major advantage of this approach is that it considers a more explicit notion of ordering?
However, the naive optimization strategy proposed there suffers from the O(m2 ) growth

in the number of constraints mentioned in the previous section. This computational burden renders these methods impractical even for medium sized datasets with a few thousand
samples. In other related work, boosting methods have been proposed for learning preferences [3], and a combinatorial structure called the ranking poset was used for conditional
modeling of partially ranked data[8], in the context of combining ranked sets of web pages
produced by various web-page search engines. Another, less related, approach is [2].
Relationship to the proposed work: Our algorithm penalizes wrong ordering of pairs of
training instances in order to learn ranking functions (similar to [4]), but in addition, it can
also utilize the notion of a structured class order graph. Nevertheless, using a formulation based on constraints over convex hulls of the training classes, our method avoids the
prohibitive computational complexity of the previous algorithms for ranking.
1.3

Notation and Background

In the following, vectors will be assumed to be column vectors unless transposed to a row
vector by a prime superscript ? . For a vector x in the n-dimensional real space ?n , the
cardinality of a set A will be denoted by #(A). The scalar (inner) product of two vectors x
and y in the n-dimensional real space ?n will be denoted by x? y and the 2-norm of x will
be denoted by kxk. For a matrix A ? ?m?n , Ai is the ith row of A which is a row vector in
?n , while A?j is the jth column of A. A column vector of ones of arbitrary dimension will
be denoted by e. For A ? ?m?n and B ? ?n?k , the kernel K(A, B) maps ?m?n ? ?n?k
into ?m?k . In particular, if x and y are column vectors in ?n then, K(x? , y) is a real
number, K(x? , A? ) is a row vector in ?m and K(A, A? ) is an m ? m matrix. The identity
matrix of arbitrary dimension will be denoted by I.

2

Convex Hull formulation

We are interested in learning a ranking function f : ?n ? ? given known ranking relationships between some training instances Ai , Aj ? A. Let the ranking relationships be
specified by a set E = {(i, j)|Ai ? Aj }
To begin with, let us consider the linearly separable binary ranking case which is equivalent
to the problem of classifying m points in the n-dimensional real space ?n , represented by
the m ? n matrix A, according to membership of each point x = Ai in the class A+ or A?
as specified by a given vector of labels d. In others words, for binary classifiers, we want a
linear ranking function fw (x) = w? x that satisfies the following constraints:
? (x+? A+ , x?? A? ), f (x? ) ? f (x+ ) ? f (x? )? f (x+ ) = w? x?? w? x+ ? ?1 ? 0.
(1)
Clearly, the number of constraints grows as O(m+ m? ), which is roughly quadratic in
the number of training samples (unless we have severe class imbalance). While easily
overcome?based on additional insights?in the separable problem, in the non-separable
case, the quadratic growth in the number of constraints poses huge computational burdens
on the optimization algorithm; indeed direct optimization with these constraints is infeasible even for moderate sized problems. We overcome this computational problem based on
three key insights that are explained below.
First, notice that (by negation) the feasibility constraints in (1) can also be defined as:
? (x+? A+ , x?? A? ), w? x??w? x+ ? ?1 ? ?(x+? A+ , x?? A? ), w? x??w? x+ > ?1.
In other words, a solution w is feasible iff there exist no pair of samples from the two
classes such that fw () orders them incorrectly.
Second, we will make the constraints in (1) more stringent: instead of requiring that equation (1) be satisfied for each possible pair (x+? A+ , x?? A? ) in the training set, we will

Figure 2: Example binary problem where points belonging to the A+ and A? sets are represented
by blue circles and red triangles respectively. Note that two elements xi and xj of the set A?
are not correctly ordered and hence generate positive values of the corresponding slack variables
yi and yj . Note that the point xk (hollow triangle) is in the convex hull of the set A? and hence
the corresponding yk error can be writen as a convex combination (yk = ?ki yi + ?kj yj ) of the two
nonzero errors corresponding to points of A?

require (1) to be satisfied for each pair (x+ ? CH(A+ ), x? ? CH(A? )), where CH(Ai )
denotes the convex hull of the set Ai [12]. Thus, our constraints become:


P
?
?
0 ? ?+ ? 1, P ?+ = 1
+
?
?(? , ? ) such that
, w? A? ??? w? A+ ?+ ? ?1.(2)
0 ? ?? ? 1, ?? = 1

Next, notice that all the linear inequality and equality constraints on (?+ , ?? ) may be
conveniently grouped together as B? ? b, where,
"" +
#
"" ?
#
 + 
 ? 
0 m+ ?1
0 m? ?1
b
?
?
+
1
1
b=
?=
b =
b =
b?
?+ m?1
?1
?1
(m? +2)?1
(m+ +2)?1
(3)
""
#
""
#
 ? 
0 ?Im+
?Im? 0
B
e?
e?
0
B+ = 0
B=
B? =
B + (m+4)?m
?
?
?e
0 (m? +2)?m
0
?e
(m+ +2)?m
(4)
Thus, our constraints on w can be written as:
?

?

?

?

?? s.t. B? ? b, w? A? ??? w? A+ ?+ ? ?1
? ?? s.t. B? ? b, w? A? ??? w? A+ ?+ > ?1
?

?

??

? ?u s.t. B u? w [A

+?

?

? A ] = 0, b u ? ?1, u ? 0,

(5)
(6)
(7)

Where the second equivalent form of the constraints was obtained by negation (as before),
and the third equivalent form results from our third key insight: the application of Farka?s
theorem of alternatives[9]. The resulting linear system of m equalities and m + 5 inequal2
ities in m + n + 4 variables can be used while minimizing any regularizer (such as kwk )
to obtain the linear ranking function that satisfies (1); notice, however, that we avoid the
O(m2 ) scaling in constraints.
2.1

The binary non-separable case
T
In the non-separable case, CH(A+ ) CH(A? ) 6= ? so the requirements have to be relaxed by introducing slack variables. To this end, we allow one slack variable yi ? 0
for each training sample xi , and consider the slack for any point inside the convex hull
CH(Aj ) to also be a convex combination of y (see Fig. 2). For example, this implies that

if only a subset of training samples have non-zero slacks yi> 0 (i.e. they are possibly misclassified), then the slacks of any points inside the convex hull also only depend on those
yi . Thus, our constraints now become:
?

?

?? s.t. B? ? b, w? A? ??? w? A+ ?+ ? ?1 + (?? y ?+ ?+ y + ), y +? 0, y ?? 0. (8)
Applying Farka?s theorem of alternatives, we get:

  ? 
A? w
y
?
(2) ? ?u s.t. B u ?
+
= 0, b? u ? ?1, u ? 0
(9)
?A+ w
y+

Replacing B from equation (4) and defining u? = [u?
?

B + u+ + A+ w + y +
?? ?

?

?

B u ?A w+y
b+ u + + b? u ?
2.2

=

?

?

u+ ] ? 0 we get the constraints:

0,

(10)

= 0,
? ?1, u ? 0

(11)
(12)

The general ranking problem

Now we can extend the idea presented in the previous section for any given arbitrary directed order graph G = (S, E), as stated in the introduction, each of whose vertices corresponds to a class Aj and the existence of a directed edge Eij means that all training samples
xi ? Ai should be ranked higher than any sample xj ? Aj , that is:
f (xj ) ? f (xi ) ? f (xj ) ? f (xi ) = w? xj ? w? xi ? ?1 ? 0
(13)
Analogously we obtain the following set of equations that enforced the ordering between
sets Ai and Aj :
?
B i uij + Ai w + y i = 0
(14)
?

Bj u
?ij ? Aj w + y j
i ij
b u + bj u
?ij ? ?1
uij , u
?ij ? 0

=

0

(15)
(16)
(17)

It can be shown that using the definitions of B i ,B j ,bi ,bj and the fact that uij , u?ij ? 0,
equations (14) can be rewritten in the following way:
? ij + Ai w + y i ? 0
(18)
ij
j
j
?? ? A w + y ? 0
(19)
? ij + ?? ij ? ?1
(20)
yi , yj ? 0
(21)
where ? ij = bi uij and ?? ij = bj u
?ij . Note that enforcing the constraints defined above
indeed implies the desired ordering, since we have:
Ai w + y i ? ?? ij ? ?? ij + 1 ? ?? ij ? Aj w ? y j
It is also important to note the connection with Support Vector Machines (SVM) formulation [10, 14] for the binary case. If we impose the extra constraints ?? ij = ? + 1 and
?? ij = ? ?1, then equations (18) imply the constraints included in the standard primal SVM
formulation. To obtain a more general formulation,we can ?kernelize? equations (14) by
making a transformation of the variable w as: w = A? v, where v can be interpreted as an
arbitrary variable in Rm ,This transformation can be motivated by duality theory [10], then
equations (14) become:
? ij + Ai A? v + y i ? 0
(22)
?? ij ? Aj A? v + y j ? 0
(23)
ij
ij
? + ?? ? ?1
(24)
yi , yj ? 0
(25)

If we now replace the linear kernels Ai A? and Ai A? by more general kernels K(Ai , A? )
and K(Aj , A? ) we obtain a ?kernelized? version of equations (14)
? ij
?
? + K(Ai , A? )v + y i ? 0 ?
?
? ij
?
?? ? K(Aj , A? )v + y j ? 0
Eij ?
(26)
ij
ij
? ?1 ?
?
? ?i +j ??
?
y ,y
? 0

Given a graph G = (V, E) representing the ordering of the training data and using equations (26) , we present next, a general mathematical programming formulation the ranking
problem:
min
??(y) + R(v)
{v,y i ,? ij | (i,j)?E}
(27)
s.t.
Eij ?(i, j) ? E
Where ? is a given loss function for the slack variables y i and R(v) represents a regularizer
on the normal to the hyperplane v. For an arbitrary kernel K(x, x? ) the number of variables
of formulation (27) is 2 ? m + 2#(E) and the number of linear equations(excluding the
nonnegativity constraints) is m#(E) + #(E) = #(E)(m + 1). for a linear kernel i.e.
K(x, x? ) = xx? the number of variables of formulation (27) becomes m + n + 2#(E)
and the number of linear equations remains the same. When using a linear kernel and
2
using ?(x) = R(x) = kxk2 , the optimization problem (27) becomes a linearly constrained
quadratic optimization problem for which a unique solution exists due to the convexity of
the objective function:
2

min

{w,y i ,? ij | (i,j)?E}

s.t.

? kyk2 + 21 w? w
Eij

?(i, j) ? E

(28)

Unlike other SVM-like methods for ranking that need a O(m2 ) number of slack variables
y our formulation only require one slack variable for example, only m slack variables
are used, giving our formulation computational advantage over ranking methods. Next,
we demonstrate the effectiveness of our algorithm by comparing it to two state-of-the-art
algorithms.

3

Experimental Evaluation

We test tested our approach in a set of nine publicly available datasets 1 shown in Tab. 1
(several large datasets are not reported since only the algorithm presented in this paper was
able to run them). These datasets have been frequently used as a benchmark for ordinal
regression methods (e.g. [1]). Here we use them for evaluating ranking performance. We
compare our method against SVM for ranking (e.g. [4, 6]) using the SVM-light package 2
and an efficient Gaussian process method (the informative vector machine) 3 [7].
These datasets were originally designed for regression, thus the continuous target values
for each dataset were discretized into five equal size bins. We use these bins to define
our ranking constraints: all the datapoints with target value falling in the same bin were
grouped together. Each dataset was divided into 10% for testing and 90% for training.
Thus, the input to all of the algorithms tested was, for each point in the training set: (1) a
vector in ?n (where n is different for each set) and (2) a value from 1 to 5 denoting the
rank of the group to which it belongs.
Performance is defined in terms of the Wilcoxon statistic. Since we do not employ information about the ranking of the elements within each group, order constraints within a group
1

Available at http:\\www.liacc.up.pt\?
ltorgo\Regression\DataSets.html
http:\\www.cs.cornell.edu\People\tj\svm light\
3
http:\\www.dcs.shef.ac.uk\ neil\ivm\

2

Table 1: Benchmark Datasets
Name

m

n

Name

m

n

1 Abalone
2 Airplane Comp.
3 Auto-MPG
4 CA Housing
5 Housing-Boston

4177
950
392
20640
506

9
10
8
9
14

6 Machine-CPU
7 Pyrimidines
8 Triazines
9 WI Breast Cancer

209
74
186
194

7
28
61
33

Accuracy

Run time

1
3

10

0.8

2

10

0.7

Run time (Log?scale)

Generalized Wilcoxon statistic (AUC)

0.9

0.6
0.5
0.4
0.3
SVM?light
IVM
Proposed (full?graph)
Proposed (chain?graph)

0.2
0.1
0

1

2

3

4
5
6
Dataset number

1

10

0

10

?1

10

?2

10

7

8

9

1

2

3

4
5
6
Dataset number

7

8

9

Figure 3: Experimental comparison of the ranking SVM, IVM and the proposed method on nine
benchmark datasets. Along with the mean values in 10 fold cross-validation, the entire range of variation is indicated in the error-bars. (a) The overall accuracy for all the three methods is comparable.
(b) The proposed method has a much lower run time than the other methods, even for the full graph
case for medium to large size datasets. NOTE: Both SVM-light and IVM ran out of memory and
crashed on dataset 4; on dataset 1, SVM-light failed to complete even one fold after more than 24
hours of run time, so its results could not be compiled in time for submission.

cannot be verified.
P Letting b(m) = m(m ? 1)/2, the total number of order constraints is
equal to b(m) ? i b(mi ), where mi is the number of instances in group i.

The results for all of the algorithms are shown in Fig.3. Our formulation was tested employing two order graphs, the full directed acyclic graph and the chain graph. The performance
for all datasets is generally comparable or significantly better for our algorithm (when using a chain order graph). Note that the performance for the full graph is consistently lower
than that for the chain graph. Thus, interestingly enforcing more order constraints does not
necessarily imply better performance. We suspect that this is due to the role that the slack
variables play in both formulations, since the number of slack variables remains the same
while the number of constraints increases. Adding more slack variables may positively
affect performance in the full graph, but this comes at a computational cost. An interesting
problem is to find the right compromise. A different but potentially related problem is that
of finding good order graph given a dataset. Note also that the chain graph is much more
stable regarding performance overall. Regarding run-time, our algorithm runs an order of
magnitude faster than current implementations of state-of-the-art methods, even approximate ones (like IVM).

4

Discussions and future work

We propose a general method for learning a ranking function from structured order constraints on sets of training samples. The proposed algorithm was illustrated on benchmark
ranking problems with two different constraint graphs: (a) a chain graph; and (b) a full

ordering graph. Although a chain graph was more accurate in the experiments shown in
Figure 3, with either type of graph structure, the proposed method is at least as accurate (in
terms of the WMW statistic for ordinal regression) as state-of-the-art algorithms such as
the ranking-SVM and Gaussian Processes for ordinal regression.
Besides being accurate, the computational requirements of our algorithm scale much more
favorably with the number of training samples as compared to other state-of-the-art methods. Indeed it was the only algorithm capable of handling several large datasets, while the
other methods either crashed due to lack of memory or ran for so long that they were not
practically feasible. While our experiments illustrate only specific order graphs, we stress
that the method is general enough to handle arbitrary constraint relationships.
While the proposed formulation reduces the computational complexity of enforcing order constraints, it is entirely independent of the regularizer that is minimized (under these
constraints) while learning the optimal ranking function. Though we have used a simple
2
margin regularization (via kwk in (28), and RKHS regularization in (27) in order to learn
in a supervised setting, we can just as easily easily use a graph-Laplacian based regularizer that exploits unlabeled data, in order to learn in semi-supervised settings. We plan to
explore this in future work.

References
[1] W. Chu and Z. Ghahramani, Gaussian processes for ordinal regression, Tech. report, University
College London, 2004.
[2] K. Crammer and Y. Singer, Pranking with ranking, Neural Info. Proc. Systems, 2002.
[3] Y. Freund, R. Iyer, and R. Schapire, An efficient boosting algorithm for combining preferences,
Journal of Machine Learning Research 4 (2003), 933?969.
[4] R. Herbrich, T. Graepel, and K. Obermayer, Large margin rank boundaries for ordinal regression, Advances in Large Margin Classifiers (2000), 115?132.
[5] T. Hofmann, L. Cai, and M. Ciaramita, Learning with taxonomies: Classifying documents and
words, (NIPS) Workshop on Syntax, Semantics, and Statistics, 2003.
[6] T. Joachims, Optimizing search engines using clickthrough data, Proc. ACM Conference on
Knowledge Discovery and Data Mining (KDD), 2002.
[7] N. Lawrence, M. Seeger, and R. Herbrich, Fast sparse gaussian process methods: The informative vector machine, Neural Info. Proc. Systems, 2002.
[8] G. Lebanon and J. Lafferty, Conditional models on the ranking poset, Neural Info. Proc. Systems, 2002.
[9] O. L. Mangasarian, Nonlinear programming, McGraw?Hill, New York, 1969, Reprint: SIAM
Classic in Applied Mathematics 10, 1994, Philadelphia.
[10]

, Generalized support vector machines, Advances in Large Margin Classifiers, 2000,
pp. 135?146.

[11] P. McCullagh and J. Nelder, Generalized linear models, Chapman & Hall, 1983.
[12] R. T. Rockafellar, Convex analysis, Princeton University Press, Princeton, New Jersey, 1970.
[13] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun, Support vector machine learning for
interdependent and structured output spaces, Int.Conf. on Machine Learning, 2004.
[14] V. N. Vapnik, The nature of statistical learning theory, second ed., Springer, New York, 2000.

"
3379,"Distributed Recursive Structure Processing

Geraldine Legendre
Yoshiro Miyata
Department of
Optoelectronic
Linguistics
Computing Systems Center
University of Colorado
Boulder, CO 80309-0430?

Paul Smolensky
Department of
Computer Science

Abstract
Harmonic grammar (Legendre, et al., 1990) is a connectionist theory of linguistic well-formed ness based on the assumption that the well-formedness
of a sentence can be measured by the harmony (negative energy) of the
corresponding connectionist state. Assuming a lower-level connectionist
network that obeys a few general connectionist principles but is otherwise
unspecified, we construct a higher-level network with an equivalent harmony function that captures the most linguistically relevant global aspects
of the lower level network. In this paper, we extend the tensor product
representation (Smolensky 1990) to fully recursive representations of recursively structured objects like sentences in the lower-level network. We
show theoretically and with an example the power of the new technique
for parallel distributed structure processing.

1

Introduction

A new technique is presented for representing recursive structures in connectionist
networks. It has been developed in the context of the framework of Harmonic
Grammar (Legendre et a1. 1990a, 1990b), a formalism for theories of linguistic
well-formedness which involves two basic levels: At the lower level, elements of the
problem domain are represented as distributed patterns of activity in a networkj At
the higher level, the elements in the domain are represented locally and connection
weights are interpreted as soft rules involving these elements. There are two aspects
that are central to the framework.
-The authors are listed in alphabetical order.

591

592

Legendre, Miyata, and Smolensky
First, the connectionist well-formedness measure harmony (or negative ""energy""),
which we use to model linguistic well-formed ness , has the properties that it is preserved between the lower and the higher levels and that it is maximized in the
network processing. Our previous work developed techniques for deriving harmonies
at the higher level from linguistic data, which allowed us to make contact with existing higher-level analyses of a given linguistic phenomenon.
This paper concentrates on the second aspect of the framework: how particular
linguistic structures such as sentences can be efficiently represented and processed
at the lower level. The next section describes a new method for representing tree
structures in a network which is an extension of the tensor product representation
proposed in (Smolensky 1990) that allows recursive tree structures to be represented
and various tree operations to be performed in parallel.

2

Recursive tensor product representations

A tensor product representation of a set of structures S assigns to each 8 E S a
vector built up by superposing role-sensitive representations of its constituents. A
role decomposition of S specifies the constituent structure of s by assigning to it
an unordered set of filler-role bindings. For example, if S is the set of strings from
the alphabet {a, b, chand 8
cba, then we might choose a role decomposition in
which the roles are absolute positions in the string (rl = first, r2 = second, ... )
and the constituents are the filler/role bindings {b/r2, a/rs, c/rl}. 1

=

In a tensor product representation a constituent - i.e., a filler/role binding - is
represented by the tensor (or generalized outer) product of vectors representing the
filler and role in isolation: fir is represented by the vector v = f?r, which is in fact
a second-rank tensor whose elements are conveniently labelled by two subscripts and
defined simply by vt.pp = ft.prp.
Where do the filler and role vectors f and r come from? In the most straightforward
case, each filler is a member of a simple set F (e.g. an alphabet) and each role is
a member of a simple set R and the designer of the representation simply specifies
vectors representing all the elements of F and R. In more complex cases, one or
both of the sets F and R might be sets of structures which in turn can be viewed as
having constituents, and which in turn can be represented using a tensor product
representation. This recursive construction of the tensor product representations
leads to tensor products of three or more vectors, creating tensors of rank three and
higher, with elements conveniently labelled by three or more subscripts.
The recursive structure of trees leads naturally to such a recursive construction of
a tensor product representation. (The following analysis builds on Section 3.7.2 of
(Smolensky 1990.? We consider binary trees (in which every node has at most two
children) since the techniques developed below generalize immediately to trees with
higher branching factor, and since the power of binary trees is well attested, e.g.,
by the success of Lisp, whose basic datastructure is the binary tree. Adopting the
conventions and notations of Lisp, we assume for simplicity that the terminal nodes
lThe other major kind of role decomposition considered in (Smolensky 1990) is contextual roles; under one such decomposition, one constituent of cba is ""b in the role 'preceded
by c and followed by a'''.

Distributed Recursive Structure Processing
of the tree (those with no children), and only the terminal nodes, are labelled by
symbols or atoms. The set of structures S we want to represent is the union of a set
of atoms and the set of binary trees with terminal nodes labelled by these atoms.
One way to view a binary tree, by analogy with how we viewed strings above, is as
having a large number of positions with various locations relative to the root: we
adopt positional roles rill labelled by binary strings (or bit vectors) such as Z = 0110
which is the position in a tree accessed by ""caddar
car(cdr(cdr(car)))"", that
is, the left child (0; car) of the right child (1; cdr) of the right child of the left child
of the root of the tree. Using this role decomposition, each constituent of a tree is
an atom (the filler) bound to some role rill specifying its location; so if a tree s has
a set of atoms {fi} at respective locations {zih then the vector representing s is
8
Ei fi?rXi'

=

=

A more recursive view of a binary tree sees it as having only two constituents: the
atoms or subtrees which are the left and right children of the root. In this fully
recursive role decomposition, fillers may either be atoms or trees: the set of possible
fillers F is the same as the original set of structures S.
The fully recursive role decomposition can be incorporated into the tensor product
framework by making the vector spaces and operations a little more complex than
in (Smolensky 1990). The goal is a representation obeying, Vs, p, q E S:
s

= cons(p, q) => 8 = p?rO + q?rl

(1)

=

Here, s
cons(p, q) is the tree with left subtree p and right subtree q, while
p and q are the vectors representing s, p and q. The only two roles in this
recursive decomposition are ro, rl: the left and right children of root. These roles
are represented by two vectors rO and rl'
8,

A fully recursive representation obeying Equation 1 can actually be constructed
from the positional representation, by assuming that the (many) positional role
vectors are constructed recursively from the (two) fully recursive role vectors according to:
rxO = rx?rO rxl rx?rl'
For example, rOllO = rO?rl ?rl ?rO' 2 Thus the vectors representing positions
at depth d in the tree are tensors of rank d (taking the root to be depth 0). As
an example, the tree s
cons(A, cons(B, e))
cons(p, q), where p = A and q =
cons(B, e), is represented by

=

=

8

A?rO
A?rO

=

+ B?rOl + C?rll = A?rO + B?rO?rl + C?rl ?rl
+ (B?rO + C?rl)?rl =p?rO + q?rl,

in accordance with Equation 1.
The complication in the vector spaces needed to accomplish this recursive analysis
is one, that allows us to add together the tensors of different ranks representing
different depths in the tree. All we need do is take the direct sum of the spaces of
tensors of different rank; in effect, concatenating into a long vector all the elements
'By adopting this definition of rXt we are essentially taking the recursive structure that
is implicit in the subscripts z labelling the positional role vectors, and mapping it into the
structure of the vectors themselves.

593

594

Legendre, Miyata, and Smolensky

=

of the tensors. For example, in S
cons(A, cons(B, C?, depth 0 is 0, since s isn't an
atom; depth 1 contains A, represented by the tensor S~~1
AI;'rOP1' and depth 2
contains Band C, represented by

S~~IP2

=

= Bl;' r Opl rl p2 + Cl;'rlpl rl p2 '

The tree

. t h en represented by t he sequence s = {S(O)
(1) S(2)
}
h
as a who Ie IS
1;"" SI;'P1 ' I;'P 1P 2""""
were
the tensor for depth 0, S~), and the tensors for depths d> 2, S~~l""'PI.' are all zero.
We let V denote the vector space of such sequences of tensors of rank 0, rank I,
... , up to some maximum depth D which may be infinite. Two elements of V are
added (or ""superimposed"") simply by adding together the tensors of corresponding
rank. This is our vector space for representing trees. a
The vector operation cons for building the representation of a tree from that of its
two subtrees is given by Equation 1. As an operation on V this can be written:

({P~), P~~I' P~J1P2""""}, {Q~), Q~~l' Q~~lP2''''}) 1-+
(0)
(1)
}
{Q(O)
(1)
}
{
O'PI;' rO P1 'PI;'P 1r OP2"""" + 0, I;' rl p1 ,QI;'Pl rl P2""""

cons :

(Here, 0 denotes the zero vector in the space representing atoms.) In terms of
matrices multiplying vectors in V, this can be written
cons(p, q)

= W consO p + W consl

q

(parallel to Equation 1) where the non-zero elements of the matrix W consO are

W cons 0 I;'P1P2,,,PI.PI.+l'I;'P 1 P2?.. PI. --rO PHI
and W consl is gotten by replacing ro with rl'
Taking the car or cdr of a tree - extracting the left or right child - in the recursive
decomposition is equivalent to unbinding either ""0 or 7'1. As shown in (Smolensky
1990, Section 3.1), if the role vectors are linearly independent, this unbinding can be
performed accurately, via a linear operation, specifically, a generalized inner product
(tensor contraction) of the vector representing the tree with an unbinding vector
Uo or ul' In general, the unbinding vectors are the dual basis to the role vectors;
equivalently, they are the vectors comprising the inverse matrix to the matrix of
all role vectors. If the role vectors are orthonormal (as in the simulation discussed
below), the unbinding vectors are the same as the role vectors. The car operation
can be written explicitly as an operation on V:
(O)
(1)
(2)
}
car: {S1;"" SI;'P' SI;'P1P1' . .. .-

{E p1 S~~l UO P1 ' E p2 S~~IP2 UOp2 ' E p, S~~lP2P' UOp,' .. -}
3In the connectionist implementation simulated below, there is one unit for each element
of each tensor in the sequence. In the simulation we report, seven atoms are represented
by (binary) vectors in a three-dimensional space, so cp = O,1,2j rO and rl are vectors in
a two-dimensional space, so p = 0,1. The number of units representing the portion of V
for depth d is thus 3 . 24 and the total number of units representing depths up to D is
3(2D+l - 1). In tensor product representations, exact representation of deeply embedded
structure does not come cheap.

Distributed Recursive Structure Processing
(Replacing uo by u1 gives cdr.) The operation car can be realized as a matrix
W car mapping V to V with non-zero elements:
W car CPP J P2""""PI..CPP 1 P2""""""PI.PHJ = uOPI.+J?
W cdr is the same matrix, with uo replaced by u 1.

4

One of the main points of developing this connectionist representation of trees
is to enable massively parallel processing. Whereas in the traditional sequential
implementation of Lisp, symbol processing consists of a long sequence of car, cdr,
and cons operations, here we can compose together the corresponding sequence of
W car, W cdr' W consO and W cons1 operations into a single matrix operation.
Adding some minimal nonlinearity allows us to compose more complex operations
incorporating the equivalent of conditional branching. We now illustrate this with
a simple linguistically motivated example.

3

An example

The symbol manipulation problem we consider is that of transforming a tree representation of a syntactic parse of an English sentence into a tree representation of
a predicate-calculus expression for the meaning of the sentence. We considered two
possible syntactic structures: simple active sentences of the form

~ and passive

form~. Each was to be transformed into a tree representing V(A,P), namely v~. Here, the agent & and patient.?. of the verb V are
sentences of the

both arbitrarily complex noun phrase trees. (Actually, the network could handle
arbitrarily complex V's as well.) Aux is a marker for passive (eg. is in is feared.)

The network was presented with an input tree of either type, represented as an
activation vector using the fully recursive tensor product representation developed in
the preceding section. The seven non-zero binary vectors oflength three coded seven
atoms; the role vectors used were technique described above. The desired output
was the same tensorial representation of the tree representing V(A, B). The filler
vectors for the verb and for the constituent words of the two noun phrases should
be unbound from their roles in the input tree and then bound to the appropriate
roles in the output tree.
Such transformation was performed, for an active sentence, by the operation
cons ( cadr( s), cons( car( s), cddr( s))) on the input tree s, and for a passive sentence,
by cons(cdadr(s), cons(cdddr(s), car(s))). These operations were implemented in
the network as two weight matrices, W a and W p' 5 connecting the input units to
the output units as shown in Figure 1. In additIon, the network had a circuit for
tNote that in the caSe when the {rO,rl} are orthonormal, and therefore uo = 1'0,
W car = W consO T i similarly, W cdr = W consl T .
&The two weight matrices were constructed from the four basic matrices as Wa W consO W car W cdr + W cons1 (WconsO W car + W cons1 W cdr W cdr) and Wp =
W consO W cdr W car W cdr + W consl (W consO W cdr W cdr W cdr + W cons1 W car).

595

596

Legendre, Miyata, and Smolensky
Output

= cons{V,cons{C,cons(A,B?)

Input = cons(cons(A,B),cons(cons(Aux,V),cons(by,C))
Figure 1: Recursive tensor product network processing a passive sentence

determining whether the input sentence was active or passive. In this example, it
simply computed, by a weight matrix, the caddr of the input tree (where a passive
sentence should have an Aux), and if it was the marker Aux, gated (with sigma-pi
connections) W p , and otherwise gated Wa.
Given this setting, the network was able to process arbitrary input sentences of
either type, up to a certain depth (4 in this example) limited by the size of the
network, properly and generated correct case role assignments. Figure 1 shows the
network processing a passive sentence ?A.B).?Aux.V).(by.C))) as in All connectionist, are feared by Minsky and generating (V.(C.(A.B?) as output.

4

Discussion

The formalism developed here for the recursive representation of trees generates
quite different representations depending on the choice of the two fundamental role
vectors rO and rl and the vectors for representing the atoms. At one extreme is
the trivial fully local representation in which one connectionist unit is dedicated
to each possible atom in each possible position: this is the special case in which
rO and rl are chosen to be the canonical basis vectors (1 0) and (0 I), and the
vectors representing the n atoms are also chosen to be the canonical basis vectors
of n-space. The example of the previous section illustrated the case of (a) linearly
dependent vectors for atoms and (b) orthonormal vectors for the roles that were
""distributed"" in that both elements of both vectors were non-zero. Property (a)
permits the representation of many more than n atoms with n-dimensional vectors,
and could be used to enrich the usual notions of symbolic computation by letting
""similar atoms"" be represented by vectors that are closer to each other than are
""dissimilar atoms."" Property (b) contributes no savings in units of the purely
local case, amounting to a literal rotation in role space. But it does allow us

Distributed Recursive Structure Processing
to demonstrate that fully distributed representations are as capable as fully local
ones at supporting massively parallel structure processing. This point has been
denied (often rather loudly) by advocates oflocal representations and by such critics
as (Fodor & Pylyshyn 1988) and (Fodor & McLaughlin 1990) who have claimed
that only connectionist implementations that preserve the concatenative structure
of language-like representations of symbolic structures could be capable of true
structure-sensitive processing.
The case illustrated in our example is distributed in the sense that all units corresponding to depth d in the tree are involved in the representation of all the atoms
at that depth. But different depths are kept separate in the formalism and in the
network. We can go further by allowing the role vectors to be linearly dependent,
sacrificing full accuracy and generality in structure processing for representation of
greater depth in fewer units. This case is the subject of current research, but space
limitations have prevented us from describing our preliminary results here.
Returning to Harmonic Grammar, the next question is, having developed a fully
recursive tensor product representation for lower-level representation of embedded
structures such as those ubiquitous in syntax, what are the implications for wellformedness as measured by the harmony function? A first approximation to the
natural language case is captured by context free grammars, in which the wellformedness of a subtree is independent of its level of embedding. It turns out that
such depth-independent well-formed ness is captured by a simple equation governing
the harmony function (or weight matrix). At the higher level where grammatical
""rules"" of Harmonic Grammar reside, this has the consequence that the numerical
constant appearing in each soft constraint that constitutes a ""rule"" applies at all
levels of embedding. This greatly constrains the parameters in the grammar.

References
[1] J. A. Fodor and B. P. McLaughlin. Connectionism and the problem of systematicity: Why smolensky's solution doesn't work. Cognition, 35:183-204, 1990.
[2] J. A. Fodor and Z. W. Pylyshyn. Connectionism and cognitive architecture: A
critical analysis. Cognition, 28:3-71, 1988.
[3] G. Legendre, Y. Miyata, and P. Smolensky. Harmonic grammar - a formal
multi-level connectionist theory of linguistic well-formedness: Theoretical foundations. In the Proceeding. of the twelveth meeting of the Cognitive Science
Society, 1990a.
[4] G. Legendre, Y. Miyata, and P. Smolensky. Harmonic grammar - a formal multilevel connectionist theory of linguistic well-formedness: An application. In the
Proceedings of the twelveth meeting of the Cognitive Science Society, 1990b.
[5] P. Smolensky. Tensor product variable binding and the representation of symbolic structures in connectionist networks. Artificial Intelligence, 46:159-216,
1990.

597

"
367,"Training Methods for Adaptive Boosting
of Neural Networks
Holger Schwenk
Dept.IRO
Universite de Montreal
2920 Chemin de la Tour,
Montreal, Qc, Canada, H3C 317
schwenk@iro.umontreal.ca

Yoshua Bengio
Dept.IRO
Universite de Montreal
and AT&T Laboratories, NJ
bengioy@iro.umontreal.ca

Abstract
""Boosting"" is a general method for improving the performance of any
learning algorithm that consistently generates classifiers which need to
perform only slightly better than random guessing. A recently proposed
and very promising boosting algorithm is AdaBoost [5]. It has been applied with great success to several benchmark machine learning problems
using rather simple learning algorithms [4], and decision trees [1, 2, 6].
In this paper we use AdaBoost to improve the performances of neural
networks. We compare training methods based on sampling the training
set and weighting the cost function. Our system achieves about 1.4%
error on a data base of online handwritten digits from more than 200
writers. Adaptive boosting of a multi-layer network achieved 1.5% error
on the UCI Letters and 8.1 % error on the UCI satellite data set.

1 Introduction
AdaBoost [4, 5] (for Adaptive Boosting) constructs a composite classifier by sequentially
training classifiers, while putting more and more emphasis on certain patterns. AdaBoost
has been applied to rather weak learning algorithms (with low capacity) [4] and to decision trees [1 , 2, 6], and not yet, until now, to the best of our knowledge, to artificial neural
networks. These experiments displayed rather intriguing generalization properties, such as
continued decrease in generalization error after training error reaches zero. Previous workers also disagree on the reasons for the impressive generalization performance displayed
by AdaBoost on a large array of tasks. One issue raised by Breiman [1] and the authors of
AdaBoost [4] is whether some of this effect is due to a reduction in variance similar to the
one obtained from the Bagging algorithm.
In this paper we explore the application of AdaBoost to Diabolo (auto-associative) networks and multi-layer neural networks (MLPs). In doing so, we also compare three dif-

H. Schwenk and Y. Bengio

648

ferent versions of AdaBoost: (R) training each classifier with a fixed training set obtained
by resampling with replacement from the original training set (as in [1]), (E) training by
resampling after each epoch a new training set from the original training set, and (W) training by directly weighting the cost fundion (here the squared error) of the neural network.
Note that the second version (E) is a better approximation of the weighted cost function
than the first one (R), in particular when many epochs are performed. If the variance reduction induced by averaging the hypotheses from very different models explains a good
part of the generalization performance of AdaBoost, then the weighted training version
(W) should perform worse then the resampling versions, and the fixed sample version (R)
should perform better then the continuously resampled version (E).

2 AdaBoost
AdaBoost combines the hypotheses generated by a set of classifiers trained one after the
other. The tth classifier is trained with more emphasis on certain patterns, using a cost function weighted by a probability distribution D t over the training data (Dt(i) is positive and
Li Dt(i) = 1). Some learning algorithms don't permit training with respect to a weighted
cost function. In this case sampling with replacement (using the probability distribution
D t ) can be used to approximate a weighted cost function. Examples with high probability
would then occur more often than those with low probability, while some examples may
not occur in the sample at all although their probability is not zero. This is particularly true
in the simple resampling version (labeled ""R"" earlier), and unlikely when a new training
set is resampled after each epoch (""E"" version). Neural networks can be trained directly
with respect to a distribution over the learning data by weighting the cost function (this is
the ""W"" version): the squared error on the i-th pattern is weighted by the probability D t (i).
The result of training the tth classifier is a hypothesis h t : X -+ Y where Y = {I, ... , k} is
the space of labels, and X is the space of input features. After the tth round the weighted
error ?t of the resulting classifier is calculated and the distribution Dt+l is computed from
D t , by increasing the probability of incorrectly labeled examples. The global decision f is
obtained by weighted voting. Figure I (left) summarizes the basic AdaBoost algorithm. It
converges (learns the training set) if each classifier yields a weighted error that is less than
50%, i.e., better than chance in the 2-c1ass case. There is also a multi-class version, called
pseudoloss-AdaBoost, that can be used when the classifier computes confidence scores for
each class. Due to lack of space, we give only the algorithm (see figure 1, right) and we
refer the reader to the references for more details [4, 5].
AdaBoost has very interesting theoretical properties, in particular it can be shown that the
error of the composite classifier on the training data decreases exponentially fast to zero [5]
as the number of combined classifiers is increased. More importantly, however, bounds
on the generalization error of such a system have been formulated [7]. These are based
on a notion of margin of classification, defined as the difference between the score of the
correct class and the strongest score of a wrong class. In the case in which there are just
two possible labels {-I, +1}, this is yf(x), where f is the composite classifier and y the
correct label. Obviously, the classification is correct if the margin is positive. We now
state the theorem bounding the generalization error of Adaboost [7] (and any classifier
obtained by a convex combination of a set of classifiers). Let H be a set of hypotheses
(from which the ht hare chosen), with VC-dimenstion d. Let f be any convex combination
of hypotheses from H. Let S be a sample of N examples chosen independently at random
according to a distribution D. Then with probability at least 1 - 8 over the random choice
of the training set S from D, the following bound is satisfied for all () > 0:

PD[yf(x) ~ 0] ~ Ps[yf(x) ~ ()]

+0

1
( jN

dlog 2 (N/d)
(}2

+ log(1/8)

)

(1)

Note that this bound is independent of the number of combined hypotheses and how they

Training Methods for Adaptive Boosting ofNeural Networks

649

sequence of N examples (Xl, YI), ... , (X N , YN )
with labels Yi E Y = {I, ... , k}
Init: Dl(i) = l/N for all i
loit: letB = {(i,y): i E{l, ... ,N},y i= yd
Dl (i. y) = l/IBI for all (i, y) E B

Input:

Repeat:
Repeat:
1. Train neural network with respect
I. Train neural network with respect
to distribution D t and obtain
to distribution D t and obtain
hypothesis ht : X ~ Y
hypothesis h t : X x Y ~ [0,1]
2. calculate the weighted error of h t : 2. calculate the pseudo-loss of h t :
_ ""
D (.)
abort loop
?t = ~ LDt(i, y)(l-ht(xi, Yd+ht(Xi' y))
?t ~
t '/,
if ?t > ~
i:ht(x,)#y,

(i,y)EB

=

3. set (3t
?t/(1 - ?t)
4. update distribution D t
D
(i) - Dt(i) a O,
t+l

-

Zt

3. set (3t = ?t/(1 - ?t)
4. update distribution D t
D
(i ) - Dt(i,y) a~((1+ht(x""y,)-ht(x""y?

/Jt

with c5i = (ht(Xi) = Yi)
and Zt a normalization constant

Output: final hypothesis:

f(x) = arg max
yEY

L
t:ht(x)=y

t+l

,Y -

Zt

/Jt

where Zt is a normalization constant

Output: final hypothesis:
1
log Ii""

/Jt

f(x) = arg max
yEY

L
t

(log ; ) ht(x, y)

/Jt

Figure I: AdaBoost algorithm (left), multi-class extension using confidence scores (right)

are chosen from H. The distribution of the margins however plays an important role. It can
be shown that the AdaBoost algorithm is especially well suited to the task of maximizing
the number of training examples with large margin [7].

3 The Diabolo Classifier
Normally, neural networks used for classification are trained to map an input vector to an
output vector that encodes directly the classes, usually by the so called ""I-out-of-N encoding"". An alternative approach with interesting properties is to use auto-associative neural
networks, also called autoencoders or Diabolo networks, to learn a model of each class.
In the simplest case, each autoencoder network is trained only with examples of the corresponding class, i.e., it learns to reconstruct all examples of one class at its output. The
distance between the input vector and the reconstructed output vector expresses the likelihood that a particular example is part of the corresponding class. Therefore classification
is done by choosing the best fitting model. Figure 2 summarizes the basic architecture.
It shows also typical classification behavior for an online character recognition task. The
input and output vectors are (x, y)-coordinate sequences of a character. The visual representation in the figure is obtained by connecting these points. In this example the"" I"" is
correctly classified since the network for this class has the smallest reconstruction error.
The Diabolo classifier uses a distributed representation of the models which is much more
compact than the enumeration of references often used by distance-based classifiers like
nearest-neighbor or RBF networks. Furthermore, one has to calculate only one distance
measure for each class to recognize. This allows to incorporate knowledge by a domain
specific distance measure at a very low computational cost. In previous work [8], we have
shown that the well-known tangent-distance [11] can be used in the objective function of the
autoencoders. This Diabolo classifier has achieved state-of-the-art results in handwritten
OCR [8,9]. Recently, we have also extended the idea of a transformation invariant distance

H. Schwenk and Y. Bengio

650

1
1

~

6

1

0.13

~

1
character
to classify

input
sequence

score I
0.08

output
sequences

distance
measures

score 7
0.23
decision
module

Figure 2: Architecture of a Diabolo classifier
measure to online character recognition [10]. One autoencoder alone, however, can not
learn efficiently the model of a character if it is written in many different stroke orders and
directions. The architecture can be extended by using several autoencoders per class, each
one specializing on a particular writing style (subclass). For the class ""0"", for instance,
we would have one Diabolo network that learns a model for zeros written clockwise and
another one for zeros written counterclockwise. The assignment of the training examples to
the different subclass models should ideally be done in an unsupervised way. However, this
can be quite difficult since the number of writing styles is not known in advance and usually
the number of examples in each subclass varies a lot. Our training data base contains for
instance 100 zeros written counterclockwise, but only 3 written clockwise (there are also
some more examples written in other strange styles). Classical clustering algorithms would
probably tend to ignore subclasses with very few examples since they aren't responsible
for much of the error, but this may result in poor generalization behavior. Therefore, in
previous work we have manually assigned the subclass labels [10]. Of course, this is not a
generally satisfactory approach, and certainly infeasible when the training set is large. In
the following, we will show that the emphasizing algorithm of AdaBoost can be used to
train multiple Diabolo classifiers per class, performing a soft assignment of examples of
the training set to each network.

4

Results with Diabolo and MLP Classifiers

Experiments have been performed on three data sets: a data base of online handwritten
digits, the UeI Letters database of offline machine-printed alphabetical characters and the
UCI satellite database that is generated from Landsat Multi-spectral Scanner image data.
All data sets have a pre-defined training and test set. The Diabolo classifier was only
applied to the online data set (since it takes advantage of the structure of the input features).
The online data set was collected at Paris 6 University [10]. It is writer-independent (different writers in training and test sets) and there are 203 writers, 1200 training examples
and 830 test examples. Each writer gave only one example per class. Therefore, there are
many different writing styles, with very different frequencies. We only applied a simple
pr~processing: the characters were resamfled to 11 points, centered and size normalized
to a (x,y)-coordinate sequence in [-1, 1]2 . Since the Diabolo classifier with tangent distance [10] is invariant to small transformations we don't need to extract further features.
Table 1 summarizes the results on the test set of different approaches before using AdaBoost. The Diabolo classifier with hand-selected sub-classes in the training set performs
best since it is invariant to transformations and since it can deal with the different writing
styles. The experiments suggest that fully connected neural networks are not well suited
for this task: small nets do poorly on both training and test sets, while large nets overfit.

"
2729,"Online Prediction on Large Diameter Graphs

Mark Herbster, Guy Lever, Massimiliano Pontil
Department of Computer Science
University College London
Gower Street, London WC1E 6BT, England, UK
{m.herbster, g.lever, m.pontil}@cs.ucl.ac.uk

Abstract
We continue our study of online prediction of the labelling of a graph. We show a
fundamental limitation of Laplacian-based algorithms: if the graph has a large diameter then the number of mistakes made by such algorithms may be proportional
to the square root of the number of vertices, even when tackling simple problems.
We overcome this drawback by means of an efficient algorithm which achieves
a logarithmic mistake bound. It is based on the notion of a spine, a path graph
which provides a linear embedding of the original graph. In practice, graphs may
exhibit cluster structure; thus in the last part, we present a modified algorithm
which achieves the ?best of both worlds?: it performs well locally in the presence
of cluster structure, and globally on large diameter graphs.

1

Introduction

We study the problem of predicting the labelling of a graph in the online learning framework. Consider the following game for predicting the labelling of a graph: Nature presents a graph; nature
queries a vertex vi1 ; the learner predicts y?1 ? {?1, 1}, the label of the vertex; nature presents a
label y1 ; nature queries a vertex vi2 ; the learner predicts y?2 ; and so forth. The learner?s goal is to
minimise the total number of mistakes M = |{t : y?t 6= yt }|. If nature is adversarial, the learner
will always mispredict, but if nature is regular or simple, there is hope that a learner may make only
a few mispredictions. Thus, a central goal of online learning is to design algorithms whose total
mispredictions can be bounded relative to the complexity of nature?s labelling. In [9, 8, 7], the cut
size (the number of edges between disagreeing labels) was used as a measure of the complexity of a
graph?s labelling, and mistake bounds relative to this and the graph diameter were derived.
The strength of the methods in [8, 7] is in the case when the graph exhibits ?cluster structure?. The
apparent deficiency of these methods is that they have poor bounds when the graph diameter is large
relative to the number of vertices. We observe that this weakness is not due to insufficiently tight
bounds, but is a problem in their performance. In particular, we discuss an example of a n-vertex
labelled graph with a single edge between disagreeing label sets. On this graph, sequential prediction
using the common method
based upon minimising the Laplacian semi-norm of a labelling, subject to
?
constraints, incurs ?( n) mistakes (see Theorem 3). The expectation is that the number of mistakes
incurred by an optimal online algorithm is bounded by O(ln n).
We solve this problem by observing that there exists an approximate structure-preserving embedding
of any graph into a path graph. In particular the cut-size of any labelling is increased by no more than
a factor of two. We call this embedding a spine of the graph. The spine is the foundation on which we
build two algorithms. Firstly we predict directly on the spine with the 1-nearest-neighbor algorithm.
We demonstrate that this equivalent to the Bayes-optimal classifier for a particular Markov random
field. A logarithmic mistake bound for learning on a path graph follows by the Halving algorithm
analysis. Secondly, we use the spine of the graph as a foundation to add a binary support tree to the
original graph. This enables us to prove a bound which is the ?best of both worlds? ? if the predicted
set of vertices has cluster-structure we will obtain a bound appropriate for that case, but if instead,
the predicted set exhibits a large diameter we will obtain a polylogarithmic bound.

Previous work. The seminal approach to semi-supervised learning over graphs in [3] is to predict
with a labelling which is consistent with a minimum label-separating cut. More recently, the graph
Laplacian has emerged as a key object in semi-supervised learning, for example the semi-norm
induced by the Laplacian is commonly either directly minimised subject to constraints, or used as
a regulariser [14, 2]. In [8, 7] the online graph labelling problem was studied. An aim of those
papers was to provide a natural interpretation of the bound on the cumulative mistakes of the kernel
perceptron when the kernel is the pseudoinverse of the graph Laplacian ? bounds in this case being
relative to the cut and (resistance) diameter of the graph. In this paper we necessarily build directly
on the very recent results in [7] as those results depend on the resistance diameter of the predicted
vertex set as opposed to the whole graph [8]. The online graph labelling problem is also studied in
[13], and here the graph structure is not given initially. A slightly weaker logarithmic bound for the
online graph labelling problem has also been independently derived via a connection to an online
routing problem in the very recent [5].

2

Preliminaries

We study the process of predicting a labelling defined on the vertices of a graph. Following the
classical online learning framework, a sequence of labelled vertices {(vi1 , y1 ), (vi2 , y2 ), . . . }, the
trial sequence, is presented to a learning algorithm such that, on sight of each vertex vit , the learner
makes a prediction y?t for the label value, after which the correct label is revealed. This feedback
information is then used by the learning algorithm to improve its performance on further examples.
We analyse the performance of a learning algorithm in the mistake bound framework [12] ? the aim
is to minimise the maximum possible cumulative number of mistakes made on the training sequence.
A graph G = (V, E) is a collection of vertices V = {v1 , . . . , vn } joined by connecting (possibly
weighted) edges. Denote i ? j whenever vi and vj are connected so that E = {(i, j) : i ? j} is the
set of unordered pairs of connected vertex indices. Associated with each edge (i, j) ? E is a weight
Aij , so that A is the n ? n symmetric adjacency matrix. We say that G is unweighted if Aij = 1
for every (i, j) ? E and is 0 otherwise. In this paper, we consider only connected graphs ? that is,
graphs such that there exists a path between any two vertices. The Laplacian G of a graph
P G is the
n ? n matrix G = D ? A, where D is the diagonal degree matrix such that Dii = j Aij . The
quadratic form associated with the Laplacian relates to the cut size of graph labellings.
Definition 1. Given a labelling u ? IRn of G = (V, E) we define the cut size of u by
1 T
1 X
?G (u) =
u Gu =
Aij (ui ? uj )2 .
(1)
4
4
(i,j)?E

n

In particular, if u ? {?1, 1} we say that a cut occurs on edge (i, j) if ui 6= uj and ?G (u) measures
the number of cuts.
We evaluate the performance of prediction algorithms in terms of the cut size and the resistance
diameter of the graph. There is an established natural connection between graphs and resistive
networks where each edge (i, j) ? E is viewed as a resistor with resistance 1/Aij [4]. Thus the
effective resistance rG (vi , vj ) between vertex vi and vj is the potential difference needed to induce a
unit current flow between vi and vj . The effective resistance may be computed by the formula [11]
rG (vi , vj ) = (ei ? ej )T G+ (ei ? ej ),
(2)
+
n
where ? ? denotes the pseudoinverse and e1 , . . . , en are the canonical basis vectors of IR . The
resistance diameter of a graph RG := maxvi ,vj ?V rG (vi , vj ) is the maximum effective resistance
between any pair of vertices on the graph.

3

Limitations of online minimum semi-norm interpolation

As we will show, it is possible to develop online algorithms for predicting the labelling of a graph
which have a mistake bound that is a logarithmic function of the number of vertices. Conversely, we
first highlight a deficiency in a standard Laplacian based method for predicting a graph labelling.
Given a partially labelled graph G = (V, E) with |V | = n ? that is, such that for some ` ? n,
y` ? {?1, 1}` is a labelling defined on the ` vertices V` = {vi1 , vi2 , . . . , vi` } ? the minimum
semi-norm interpolant is defined by
y? = argmin{uT Gu : u ? IRn , uik = yk , k = 1, . . . , `}.

We then predict using y?i = sgn(?
yi ), for i = 1, . . . , n.
The common justification behind the above learning paradigm [14, 2] is that minimizing the cut (1)
encourages neighbouring vertices to be similarly labelled. However, we now demonstrate that in the
online setting such a regime will perform poorly on ?
certain graph constructions ? there exists a trial
sequence on which the method will make at least ?( n) mistakes.
Definition 2. An octopus graph of size d is defined to be d path graphs (the tentacles) of length d
(that is, with d + 1 vertices) all adjoined at a common end vertex, to which a further single head
vertex is attached, so that n = |V | = d2 + 2. This corresponds to the graph O1,d,d discussed in [8].
Theorem 3. Let G = (V, E) be an octopus graph of size d and y = (y1 , . . . , y|V | ) the labelling
such that yi = 1 if vi is the head vertex and yi = ?1 otherwise.
There exists a trial sequence for
p
which online minimum semi-norm interpolation makes ?( |V |) mistakes.
Proof. Let the first query vertex be the head vertex, and let the end vertex of a tentacle be queried at
each subsequent trial. We show that this strategy forces at least d mistakes. The solution to the minimum semi-norm interpolation with boundary
Pn values problem is precisely the harmonic solution [4]
y? (that is, for every unlabeled vertex vj , i=1 Aij (?
yi ? y?j ) = 0). If the graph is connected y? is
unique and the graph labelling problem is identical to that of identifying the potential at each vertex
of a resistive network defined on the graph where each edge corresponds to a resistor of 1 unit; the
harmonic principle corresponds to Kirchoff?s current law in this case. Using this analogy, suppose
that the end points of k < d tentacles are labelled and that the end vertex vq of an unlabelled tentacle
is queried. Suppose a current of k? flows from the head to the body of the graph. By Kirchoff?s
law, a current of ? flows along each labelled tentacle (in order to obey the harmonic principle at
2
every vertex it is clear that no current flows along the unlabelled tentacles). By Ohm?s law ? = d+k
.
Minimum semi-norm interpolation therefore results in the solution
2k
y?q = 1 ?
? 0 iff k ? d.
d+k
Hence the minimum semi-norm solution predicts incorrectly whenever k < d and the algorithm
makes at least d mistakes.
The above demonstrates a limitation in the method of online Laplacian minimum semi-norm interpolation for predicting a graph labelling ? the mistake bound can be proportional to the square root
of the number of data points. We solve these problems in the following section.

4

A linear graph embedding

We demonstrate a method of embedding data represented as a connected graph G into a path graph,
we call it a spine of G, which partially preserves the structure of G. Let Pn be the set of path graphs
with n vertices. We would like to find a path graph with the same vertex set as G, which solves
?P (u)
.
min
max
P?Pn u?{?1,1}n ?G (u)
If a Hamiltonian path H of G (a path on G which visits each vertex precisely once) exists, then
(u)
the approximation ratio is ??H
? 1. The problem of finding a Hamiltonian path is NP-complete
G (u)
however, and such a path is not guaranteed to exist. As we shall see, a spine S of G may be found
S (u)
efficiently and satisfies ?
?G (u) ? 2.
We now detail the construction of a spine of a graph G = (V, E), with |V | = n. Starting from
any node, G is traversed in the manner of a depth-first search (that is, each vertex is fully explored
before backtracking to the last unexplored vertex), and an ordered list VL = {vl1 , vl2 , . . . , vl2m+1 }
of the vertices (m ? |E|) in the order that they are visited is formed, allowing repetitions when
a vertex is visited more than once. Note that each edge in EG is traversed no more than twice
when forming VL . Define an edge multiset EL = {(l1 , l2 ), (l2 , l3 ), . . . , (l2m , l2m+1 )} ? the set
of pairs of consecutive
vertices in VL . Let u be an P
arbitrary labelling of G and denote, as usual,
P
?G (u) = 41 (i,j)?EG (ui ? uj )2 and ?L (u) = 14 (i,j)?EL (ui ? uj )2 . Since the multiset EL
contains every element of EG no more than twice, ?L (u) ? 2?G (u).
We then take any subsequence VL0 of VL containing every vertex in V exactly once. A spine
S = (V, ES ) is a graph formed by connecting each vertex in V to its immediate neighbours in

the subsequence VL0 with an edge. Since a cut occurs between connected vertices vi and vj in S
only if a cut occurs on some edge in EL located between the corresponding vertices in the list VL
we have
?S (u) ? ?L (u) ? 2?G (u).

(3)

Thus we have reduced the problem of learning the cut on a generic graph to that of learning the
cut on a path graph. In the following we see that 1-nearest neighbour (1-NN) algorithm is a Bayes
optimal algorithm for this problem. Note that the 1-NN algorithm does not perform well
? on general
graphs; on the octopus graph discussed above, for example, it can make at least ?( n) mistakes,
and even ?(n) mistakes on a related graph construction [8].

5

Predicting with a spine

We consider implementing the 1-NN algorithm on a path graph and demonstrate that it achieves a
mistake bound which is logarithmic in the length of the line. Let G = (V, E) be a path graph, where
V = {v1 , v2 , . . . , vn } is the set of vertices and E = {(1, 2), (2, 3), . . . , (n ? 1, n)}. The nearest
neighbour algorithm, in the standard online learning framework described above, attempts to predict
a graph labelling by producing, for each query vertex vit , the prediction y?t which is consistent with
the label of the closest labelled vertex (and predicts randomly in the case of a tie).
Theorem 4. Given the task of predicting the labelling of any unweighted, n-vertex path graph P in
the online framework, the number of mistakes, M , incurred by the 1-NN algorithm satisfies


n?1
?P (u)
M ? ?P (u) log2
+
+ 1,
(4)
?P (u)
ln 2
where u ? {?1, 1}n is any labelling consistent with the trial sequence.
Proof. We shall prove the result by noting that the Halving algorithm [1] (under certain conditions
on the probabilities assigned to each hypothesis) implements the nearest neighbour algorithm on a
path graph. Given any input space X and finite binary concept class C ? {?1, 1}|X| , the Halving
algorithm learns any target concept c? ? C as follows. Each hypothesis c ? C is given an associated
probability p(c). A sequence of labelled examples {(x1 , y1 ), . . . , (xt?1 , yt?1 )} ? X ? {?1, 1}, is
revealed in accordance with the usual online framework. Let Ft be the set of feasible hypotheses at
trial t; Ft = {c : c(xs ) = ys ?s < t}. Given an unlabelled example xtP? X at trial t the predicted
label y?t is that which agrees with the majority vote ? that is, such that
it predicts randomly if this is equal to
most MH mistakes with

1
2 ).

c?Ft ,c(xt )=y
?t

P

c?Ft

p(c)

p(c)

>

1
2

(and

It is well known [1] that the Halving algorithm makes at


MH ? log2

1
p(c? )


.

(5)

We now define a probability distribution over the space of all labellings u ? {?1, 1}n of P such that
the Halving algorithm with these probabilities implements the nearest neighbour algorithm. Let a cut
occur on any given edge with probability ?, independently of all other cuts; Prob(ui+1 6= ui ) = ?
?i < n. The position of all cuts fixes the labelling up to flipping every label, and each of these
two resulting possible arrangements are equally likely. This recipe associates with each possible
labelling u ? {?1, 1}n a probability p(u) which is a function of the labelling?s cut size
1 ?P (u)
?
(1 ? ?)n?1??P (u) .
(6)
2
This induces a full joint probability distribution on the space of vertex labels. In fact (6) is a Gibbs
measure and as such defines a Markov random field over the space of vertex labels [10]. The mass
function p therefore satisfies the Markov property
p(u) =

p(ui = ? | uj = ?j ?j 6= i) = p(ui = ? | uj = ?j ?j ? Ni ),

(7)

where here Ni is the set of vertices neighbouring vi ? those connected to vi by an edge. We will
give an equivalent Markov property which allows a more general conditioning to reduce to that over
boundary vertices.

Definition 5. Given a path graph P = (V, E), a set of vertices V 0 ? V and a vertex vi ? V , we
define the boundary vertices v` , vr (either of which may be vacuous) to be the two vertices in V 0 that
are closest to vi in each direction along the path; its nearest neighbours in each direction.
The distribution induced by (6) satisfies the following Markov property; given a partial labelling of
P defined on a subset V 0 ? V , the label of any vertex vi is independent of all labels on V 0 except
those on the vertices v` , vr (either of which could be vacuous)
p(ui = ? | uj = ?j , ?j : vj ? V 0 )

= p(ui = ? | u` = ?` , ur = ?r ).

(8)

Given the construction of the probability distribution formed by independent cuts on graph edges,
we can evaluate conditional probabilities. For example, p(uj = ? | uk = ?) is the probability of an
even number of cuts between vertex vj and vertex vk . Since cuts occur with probability ? and there

are |k?j|
possible arrangements of s cuts we have
s

p(uj = ? | uk = ?) =

X |k ? j|
1
?s (1 ? ?)|k?j|?s = (1 + (1 ? 2?)|k?j| ).
s
2
s even

(9)

X |k ? j|
1
?s (1 ? ?)|k?j|?s = (1 ? (1 ? 2?)|k?j| ).
s
2

(10)

Likewise we have that
p(uj 6= ? | uk = ?) =

s odd

Note also that for any single vertex we have p(ui = ?) = 12 for ? ? {?1, 1}.
Lemma 6. Given the task of predicting the labelling of an n-vertex path graph online, the Halving
algorithm, with a probability distribution over the labellings defined as in (6) and such that 0 <
? < 12 , implements the nearest neighbour algorithm.
Proof. Suppose that t ? 1 trials have been performed so that we have a partial labelling of a subset
V 0 ? V , {(vi1 , y1 ), (vi2 , y2 ), . . . , (vit?1 , yt?1 )}. Suppose the label of vertex vit is queried so that
the Halving algorithm makes the following prediction y?t for vertex vit : y?t = y if p(uit = y | uij =
yj ? 1 ? j < t) > 21 , y?t = ?y if p(uit = y | uij = yj ? 1 ? j < t) < 21 (and predicts randomly
if this probability is equal to 12 ). We first consider the case where the conditional labelling includes
vertices on both sides of vit . We have, by (8), that
p(uit = y | uij = yj ? 1 ? j < t)

= p(uit = y | u` = y? (`) , ur = y? (r) )
=

p(u` = y? (`) | ur = y? (r) , uit = y)p(ur = y? (r) , uit = y)
p(u` = y? (`) , ur = y? (r) )

=

p(u` = y? (`) | uit = y)p(ur = y? (r) | uit = y)
p(u` = y? (`) | ur = y? (r) )

(11)

where v` and vr are the boundary vertices and ? (`) and ? (r) are trials at which vertices v` and vr
are queried, respectively. We can evaluate the right hand side of this expression using (9, 10). To
show equivalence with the nearest neighbour method whenever ? < 12 , we have from (9, 10, 11)
p(uit = y | u` = y, ur 6= y)

=

(1 + (1 ? 2?)|`?it | )(1 ? (1 ? 2?)|r?it | )
2(1 ? (1 ? 2?)|`?r| )

which is greater than 12 if |` ? it | < |r ? it | and less than 21 if |` ? it | > |r ? it |. Hence, this
produces predictions exactly in accordance with the nearest neighbour scheme. We also have more
simply that for all it , ` and r and ? < 12
p(uit = y | u` = y, ur = y) >

1
1
, and p(uit = y | u` = y) > .
2
2

This proves the lemma for all cases.
A direct application of the Halving algorithm mistake bound (5) now gives




1
2
M ? log2
= log2
p(u)
??P (u) (1 ? ?)n?1??P (u)

P (u) 1
where u is any labelling consistent with the trial sequence. We choose ? = min( ?n?1
, 2 ) (note
P (u)
that the bound is vacuous when ?n?1
> 12 since M is necessarily upper bounded by n) giving




?P (u)
n?1
+ (n ? 1 ? ?P (u)) log2 1 +
+1
M ? ?P (u) log2
?P (u)
n ? 1 ? ?P (u)


n?1
?P (u)
? ?P (u) log2
+
+ 1.
?P (u)
ln 2

This proves the theorem.
The nearest neighbour algorithm can predict the labelling of any graph G = (V, E), by first transferring the data representation to that of a spine S of G, as presented in Section 4. We now apply the
above argument to this method and immediately deduce our first main result.
Theorem 7. Given the task of predicting the labelling of any unweighted, connected, n-vertex graph
G = (V, E) in the online framework, the number of mistakes, M , incurred by the nearest neighbour
algorithm operating on a spine S of G satisfies



n?1
2?G (u)
M ? 2?G (u) max 0, log2
+ 1,
(12)
+
2?G (u)
ln 2
where u ? {?1, 1}n is any labelling consistent with the trial sequence.
Proof. Theorem 4 gives bound (4) for predicting on any path, hence M ? ?S (u) log2
?S (u)
ln 2 + 1. Since this is an increasing function of ?S (u) for ?S (u) ? n
?S (u) ? n ? 1 (M is necessarily upper bounded by n) we upper bound



n?1
?S (u)



+

? 1 and is vacuous at
substituting ?S (u) ?

2?G (u) (equation (3)).
We observe that predicting with the spine is a minimax improvement over Laplacian minimal seminorm interpolation. Recall Theorem 3, there we showed
? that there exists a trial sequence such that
Laplacian
p minimal semi-norm interpolation incurs ?( n) mistakes. In fact this trivially generalizes
to ?( ?G (u)n) mistakes by creating a colony of ?G (u) octopi then identifying each previously
separate head vertex as a single central vertex. The upper bound (12) is smaller than the prior lower
bound.
The computational complexity for this algorithm is O(|E| + |V | ln |V |) time. We compute the spine
in O(|E|) time by simply listing vertices in the order in which they are first visited during a depthfirst search traversal of G. Using online 1-NN requires O(|V | ln |V |) time to predict an arbitrary
vertex sequence using a self-balancing binary search tree (e.g., a red-black tree) as the insertion of
each vertex into the tree and determination of the nearest left and right neighbour is O(ln |V |).

6

Prediction with a binary support tree

The Pounce online label prediction algorithm [7] is designed to exploit cluster structure of a graph
G = (V, E) and achieves the following mistake bound
M ? N (X, ?, rG ) + 4?G (u)? + 1,

(13)

for any ? > 0. Here, u ? IRn is any labelling consistent with the trial sequence, X =
{vi1 , vi2 , . . . } ? V is the set of inputs and N (X, ?, rG ) is a covering number ? the minimum
number of balls of resistance diameter ? (see Section 2) required to cover X. The mistake bound
(13) can be preferable to (12) whenever the inputs are sufficiently clustered and so has a cover of
small diameter sets. For example, consider two (m + 1)-cliques, one labeled ?+1?, one ??1? with
cm arbitrary interconnecting edges (c ? 1) here the bound (12) is vacuous while (13) is M ? 8c + 3
2
(with ? = m
, N (X, ?, rG ) = 2, and ?G (u) = cm). An input space V may have both local cluster structure yet have a large diameter. Imagine a ?universe? such that points are distributed into
many dense clusters such that some sets of clusters are tightly packed but overall the distribution is
quite diffuse. A given ?problem? X ? V may then be centered on a few clusters or alternatively
encompass the entire space. Thus, for practical purposes, we would like a prediction algorithm

which achieves the ?best of both worlds?, that is a mistake bound which is no greater, in order of
magnitude, than the maximum of (12) and (13). The rest of this paper is directed toward this goal.
We now introduce the notion of binary support tree, formalise the Pounce method in the support tree
setting and then prove the desired result.
Definition 8. Given a graph G = (V, E), with |V | = n, and spine S, we define a binary support tree
of G to be any binary tree T = (VT , ET ) of least possible depth, D, whose leaves are the vertices
of S, in order. Note that D < log2 (n) + 1.
We show that there is a weighting of the support tree which ensures that the resistance diameter of
the support tree is small, but also such that any labelling of the leaf vertices can be extended to the
support tree such that its cut size remains small. This enables effective learning via the support tree.
A related construction has been used to build preconditioners for solving linear systems [6].
Lemma 9. Given any spine graph S = (V, E) with |V | = n, and labelling u ? {?1, 1}n , with
? ? [?1, 1]|VT |
support tree T = (VT , ET ), there exists a weighting A of T , and a labelling u
? and u are identical on V , ?T (u)
? < ?S (u) and RT ? (log2 n + 1)(log2 n +
of T such that u
4)(log2 (log2 n + 2))2 .
Proof. Let vr be the root vertex of T . Suppose each edge (i, j) ? ET has a weight Aij , which
is a function of the edge?s depth d = max{dT (vi , vr ), dT (vj , vr )}, Aij = W (d) where dT (v, v 0 )
? such
is the number of edges in the shortest path from v to v 0 . Consider the unique labelling u
that, for 1 ? i ? n we have u
?i = ui and such that for every other vertex vp ? VT , with child
u
? +?
u
vertices vc1 , vc2 , we have u
?p = c1 2 c2 , or u
?p = u
?c in the case where vp has only one child, vc .
Suppose the edges (p, c1 ), (p, c2 ) ? ET are at some depth d in T , and let V 0 ? V correspond to
the leaf vertices of T descended from vp . Define ?S (uV 0 ) to be the cut of u restricted to vertices
in V 0 . If u
? c1 = u
?c2 then (?
up ? u
?c1 )2 + (?
up ? u
?c2 )2 = 0 ? 2?S (uV 0 ), and if u
?c1 6= u
?c2 then
2
2
(?
up ? u
?c1 ) + (?
up ? u
?c2 ) ? 2 ? 2?S (uV 0 ). Hence

W (d) (?
up ? u
?c1 )2 + (?
up ? u
?c2 )2 ? 2W (d)?S (uV 0 )
(14)
(a similar inequality is trivial in the case that vp has only one child). Since the sets of leaf descendants
of all vertices at depth d form a partition of V , summing (14) first over all parent nodes at a given
depth and then over all integers d ? [1, D] gives
? ?2
4?T (u)

D
X

W (d)?S (u).

d=1

(15)
We then choose
1
(d + 1)(log2 (d + 1))2
R?
? 21 + ln2 2 2 x ln12 x dx =

W (d) =
and note that

P?

1
d=1 (d+1)(log2 (d+1))2

(16)
1
2

+ ln 2 < 2.

PD
Further, RT = 2 d=1 (d + 1)(log2 (d + 1))2 ? D(D + 3)(log2 (D + 1))2 and so D ? log2 n + 1
gives the resistance bound.
Definition 10. Given the task of predicting the labelling of an unweighted graph G = (V, E) the
? is formed
augmented Pounce algorithm proceeds as follows: An augmented graph G? = (V? , E)
by attaching a binary support tree of G, with weights defined as in (16), to G; formally let T =
(VT , ET ) be such a binary support tree of G, then G? = (VT , E ? ET ). The Pounce algorithm is
?
then used to predict the (partial) labelling defined on G.
Theorem 11. Given the task of predicting the labelling of any unweighted, connected, n-vertex
graph G = (V, E) in the online framework, the number of mistakes, M , incurred by the augmented
Pounce algorithm satisfies
M ? min{N (X, ?, rG ) + 12?G (u)?} + 1,
(17)
?>0

where N (X, ?, rG ) is the covering number of the input set X = {vi1 , vi2 , . . . } ? V relative to
the resistance distance rG of G and u ? IRn is any labelling consistent with the trial sequence.
Furthermore,
M ? 12?G (u)(log2 n + 1)(log2 n + 4)(log2 (log2 n + 2))2 + 2.
(18)

Proof. Let u be some labelling consistent with the trial sequence. By (3) we have that ?S (u) ?
2?G (u) for any spine S of G. Moreover, by the arguments in Lemma 9 there exists some labelling
? of the weighted support tree T of G, consistent with u on V , such that ?T (u)
? < ?S (u). We then
u
have
? = ?T (u)
? + ?G (u) < 3?G (u).
?G?(u)

(19)

By Rayleigh?s monotonicity law the addition of the support tree does not increase the resistance
between any vertices on G, hence
N (X, ?, rG?) ? N (X, ?, rG ).

(20)

? yields
? on G,
Combining inequalities (19) and (20) with the pounce bound (13) for predicting u
? + 1 ? N (X, ?, rG ) + 12?G (u)? + 1.
M ? N (X, ?, rG?) + 4?G?(u)?
? G? + 2 ?
which proves (17). We prove (18) by covering G? with single ball so that M ? 4?G?(u)R
12?G (u)RT + 2 and the result follows from the bound on RT in Lemma 9.

7

Conclusion

We have explored a deficiency with existing online techniques for predicting the labelling of a graph.
As a solution, we have presented an approximate cut-preserving embedding of any graph G =
(V, E) into a simple path graph, which we call a spine, such that an implementation of the 1nearest-neighbours algorithm is an efficient realisation of a Bayes optimal classifier. This therefore
achieves a mistake bound which is logarithmic in the size of the vertex set for any graph, and the
complexity of our algorithm is of O(|E| + |V | ln |V |). We further applied the insights gained to
a second algorithm ? an augmentation of the Pounce algorithm, which achieves a polylogarithmic
performance guarantee, but can further take advantage of clustered data, in which case its bound is
relative to any cover of the graph.

References
[1] J. M. Barzdin and R. V. Frievald. On the prediction of general recursive functions. Soviet Math. Doklady,
13:1224?1228, 1972.
[2] M. Belkin and P. Niyogi. Semi-supervised learning on riemannian manifolds. Machine Learning, 56:209?
239, 2004.
[3] A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincuts. In Proc. 18th
International Conf. on Machine Learning, pages 19?26. Morgan Kaufmann, San Francisco, CA, 2001.
[4] P. Doyle and J. Snell. Random walks and electric networks. Mathematical Association of America, 1984.
[5] J. Fakcharoenphol and B. Kijsirikul. Low congestion online routing and an improved mistake bound for
online prediction of graph labeling. CoRR, abs/0809.2075, 2008.
[6] K. Gremban, G. Miller, and M. Zagha. Performance evaluation of a new parallel preconditioner. Parallel
Processing Symposium, International, 0:65, 1995.
[7] M. Herbster. Exploiting cluster-structure to predict the labeling of a graph. In The 19th International
Conference on Algorithmic Learning Theory, pages 54?69, 2008.
[8] M. Herbster and M. Pontil. Prediction on a graph with a perceptron. In B. Sch?olkopf, J. Platt, and
T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 577?584. MIT Press,
Cambridge, MA, 2007.
[9] M. Herbster, M. Pontil, and L. Wainer. Online learning over graphs. In ICML ?05: Proceedings of the
22nd international conference on Machine learning, pages 305?312, New York, NY, USA, 2005. ACM.
[10] R. Kinderman and J. L. Snell. Markov Random Fields and Their Applications. Amer. Math. Soc., Providence, RI, 1980.
[11] D. Klein and M. Randi?c. Resistance distance. Journal of Mathematical Chemistry, 12(1):81?95, 1993.
[12] N. Littlestone. Learning when irrelevant attributes abound: A new linear-threshold algorithm. Machine
Learning, 2:285?318, 1988.
[13] K. Pelckmans and J. A. Suykens. An online algorithm for learning a labeling of a graph. In In Proceedings
of the 6th International Workshop on Mining and Learning with Graphs, 2008.
[14] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian fields and harmonic
functions. In 20-th International Conference on Machine Learning (ICML-2003), pages 912?919, 2003.

"
4438,"A message-passing algorithm
for multi-agent trajectory planning
Jos?e Bento ?
jbento@disneyresearch.com

Nate Derbinsky
nate.derbinsky@disneyresearch.com

Javier Alonso-Mora
jalonso@disneyresearch.com

Jonathan Yedidia
yedidia@disneyresearch.com

Abstract
We describe a novel approach for computing collision-free global trajectories for
p agents with specified initial and final configurations, based on an improved version of the alternating direction method of multipliers (ADMM). Compared with
existing methods, our approach is naturally parallelizable and allows for incorporating different cost functionals with only minor adjustments. We apply our
method to classical challenging instances and observe that its computational requirements scale well with p for several cost functionals. We also show that a
specialization of our algorithm can be used for local motion planning by solving
the problem of joint optimization in velocity space.

1

Introduction

Robot navigation relies on at least three sub-tasks: localization, mapping, and motion planning. The
latter can be described as an optimization problem: compute the lowest-cost path, or trajectory,
between an initial and final configuration. This paper focuses on trajectory planning for multiple
agents, an important problem in robotics [1, 2], computer animation, and crowd simulation [3].
Centralized planning for multiple agents is PSPACE hard [4, 5]. To contend with this complexity,
traditional multi-agent planning prioritizes agents and computes their trajectories sequentially [6],
leading to suboptimal solutions. By contrast, our method plans for all agents simultaneously. Trajectory planning is also simplified if agents are non-distinct and can be dynamically assigned to a set
of goal positions [1]. We consider the harder problem where robots have a unique identity and their
goal positions are statically pre-specified. Both mixed-integer quadratic programming (MIQP) [7]
and [more efficient, although local] sequential convex programming [8] approaches have been applied to the problem of computing collision-free trajectories for multiple agents with pre-specified
goal positions; however, due to the non-convexity of the problem, these approaches, especially the
former, do not scale well with the number of agents. Alternatively, trajectories may be found by
sampling in their joint configuration space [9]. This approach is probabilistic and, alone, only gives
asymptotic guarantees. See Appendix A for further comments on discrete search methods.
Due to the complexity of planning collision-free trajectories, real-time robot navigation is commonly decoupled into a global planner and a fast local planner that performs collision-avoidance.
Many single-agent reactive collision-avoidance algorithms are based either on potential fields [10],
which typically ignore the velocity of other agents, or ?velocity obstacles? [11], which provide
improved performance in dynamic environments by formulating the optimization in velocity space
instead of Cartesian space. Building on an extension of the velocity-obstacles approach, recent work
on centralized collision avoidance [12] computes collision-free local motions for all agents whilst
maximizing a joint utility using either a computationally expensive MIQP or an efficient, though
local, QP. While not the main focus of this paper, we show that a specialization of our approach
?

This author would like to thank Emily Hupf and Noa Ghersin for their support while writing this paper.

1

to global-trajectory optimization also applies for local-trajectory optimization, and our numerical
results demonstrate improvements in both efficiency and scaling performance.
In this paper we formalize the global trajectory planning task as follows. Given p agents of different
radii {ri }pi=1 with given desired initial and final positions, {xi (0)}pi=1 and {xi (T )}pi=1 , along with
a cost functional over trajectories, compute collision-free trajectories for all agents that minimize
the cost functional. That is, find a set of intermediate points {xi (t)}pi=1 , t 2 (0, T ), that satisfies the
?hard? collision-free constraints that kxi (t) xj (t)k > ri + rj , for all i, j and t, and that insofar as
possible, minimizes the cost functional.
The method we propose searches for a solution within the space of piece-wise linear trajectories,
wherein the trajectory of an agent is completely specified by a set of positions at a fixed set of time
instants {ts }?s=0 . We call these time instants break-points and they are the same for all agents, which
greatly simplifies the mathematics of our method. All other intermediate points of the trajectories
are computed by assuming that each agent moves with constant velocity in between break-points: if
t1 and t2 > t1 are consecutive break-points, then xi (t) = t2 1 t1 ((t2 t)xi (t1 ) + (t t1 )xi (t2 )) for
t 2 [t1 , t2 ]. Along with the set of initial and final configurations, the number of interior break-points
(? 1) is an input to our method, with a corresponding tradeoff: increasing ? yields trajectories that
are more flexible and smooth, with possibly higher quality; but increasing ? enlarges the problem,
leading to potentially increased computation.
The main contributions of this paper are as follows:
i) We formulate the global trajectory planning task as a decomposable optimization problem.
We show how to solve the resulting sub-problems exactly and efficiently, despite their nonconvexity, and how to coordinate their solutions using message-passing. Our method, based on
the ?three-weight? version of ADMM [13], is easily parallelized, does not require parameter
tuning, and we present empirical evidence of good scalability with p.
ii) Within our decomposable framework, we describe different sub-problems, called minimizers,
each ensuring the trajectories satisfy a separate criterion. Our method is flexible and can consider different combinations of minimizers. A particularly crucial minimizer ensures there are
no inter-agent collisions, but we also derive other minimizers that allow for finding trajectories
with minimal total energy, avoiding static obstacles, or imposing dynamic constraints, such as
maximum/minimum agent velocity.
iii) We show that our method can specialize to perform local planning by solving the problem of
joint optimization in velocity space [12].
Our work is among the few examples where the success of applying ADMM to find approximate
solutions to a large non-convex problems can be judged with the naked eye, by the gracefulness
of the trajectories found. This paper also reinforces the claim in [13] that small, yet important,
modifications to ADMM can bring an order of magnitude increase in speed. We emphasize the
importance of these modifications in our numerical experiments, where we compare the performance
of our method using the three-weight algorithm (TWA) versus that of standard ADMM.
The rest of the paper is organized as follows. Section 2 provides background on ADMM and the
TWA. Section 3 formulates the global-trajectory-planning task as an optimization problem and describes the separate blocks necessary to solve it (the mathematical details of solving these subproblems are left to appendices). Section 4 evaluates the performance of our solution: its scalability
with p, sensitivity to initial conditions, and the effect of different cost functionals. Section 5 explains
how to implement a velocity-obstacle method using our method and compares its performance with
prior work. Finally, Section 6 draws conclusions and suggests directions for future work.

2

Minimizers in the TWA

In this section we provide a short description of the TWA [13], and, in particular, the role of the
minimizer building blocks that it needs to solve a particular optimization problem. Section B of the
supplementary material includes a full description of the TWA.
As a small illustrative example of how the TWA is used to solve optimization problems, suppose we
want to solve minx2R3 f (x) = min{x1 ,x2 ,x3 } f1 (x1 , x3 ) + f2 (x1 , x2 , x3 ) + f3 (x3 ), where fi (.) 2
2

R[{+1}. The functions can represent soft costs, for example f3 (x3 ) = (x3 1)2 , or hard equality
or inequality constraints, such as f1 (x1 , x3 ) = J(x1 ? x3 ), where we are using the notation J(.) = 0
if (.) is true or +1 if (.) is false.
The TWA solves this optimization problem iteratively by passing messages on a bipartite graph, in
the form of a Forney factor graph [14]: one minimizer-node per function fb , one equality-node per
variable xj and an edge (b, j), connecting b and j, if fb depends on xj (see Figure 1-left).
1

g

=

1

2

g

=

2

3

g

=

3

?
n1,1, ?1,1
?
n1,3, ?1,3

g1

?
x1,1, ?1,1
?
x1,3, ?1,3

?
n2,1, ?2,1
?
n2,2 , ?2,2
?
n2,3, ?2,3

g2

?
x2,1, ?2,1
?
x2,2 , ??2,2
x2,3, ?2,3

?
n3,3, ?3,3

g3

?
x3,3, ?3,3

Figure 1: Left: bipartite graph, with one minimizer-node on the left for each function making up
the overall objective function, and one equality-node on the right per variable in the problem. Right:
The input and output variables for each minimizer block.
Apart from the first-iteration message values, and two internal parameters1 that we specify in Section
4, the algorithm is fully specified by the behavior of the minimizers and the topology of the graph.
What does a minimizer do? The minimizer-node g1 , for example, solves a small optimization problem over its local variables x1 and x3 . Without going into the full detail presented in [13] and the
supplementary material, the estimates x1,1 and x1,3 are then combined with running sums of the
differences between the minimizer estimates and the equality-node consensus estimates to obtain
messages m1,1 and m1,3 on each neighboring edge that are sent to the neighboring equality-nodes
along with corresponding certainty weights, !
? 1,2 and !
? 1,3 . All other minimizers act similarly.
The equality-nodes receive these local messages and weights and produce consensus estimates for
all variables by computing an average of the incoming messages, weighted by the incoming certainty
weights !
? . From these consensus estimates, correcting messages are computed and communicated
back to the minimizers to help them reach consensus. A certainty weight for the correcting messages,
? , is also communicated back to the minimizers. For example, the minimizer g1 receives correcting
messages n1,1 and n1,3 with corresponding certainty weights ? 1,1 and ? 1,3 (see Figure 1-right).
When producing new local estimates, the bth minimizer node computes its local estimates {xj } by
choosing a point that minimizes the sum of the local function fb and weighted squared distance from
the incoming messages (ties are broken randomly):
2
3
X
1
{xb,j }j = gb {nb,j }j , { ? kb,j }j ? arg min 4fb ({xj }j ) +
? b,j (xj nb,j )2 5 , (1)
2 j
{xj }j

P
where {}j and j run over all equality-nodes connected to b. In the TWA, the certainty weights
{!
? b,j } that this minimizer outputs must be 0 (uncertain); 1 (certain); or ?0 , set to some fixed
value. The logic for setting weights from minimizer-nodes depends on the problem; as we shall
see, in trajectory planning problems, we only use 0 or ?0 weights. If we choose that all minimizers
always output weights equal to ?0 , the TWA reduces to standard ADMM; however, 0-weights allows
equality-nodes to ignore inactive constraints, traversing the search space much faster.
Finally, notice that all minimizers can operate simultaneously, and the same is true for the consensus
calculation performed by each equality-node. The algorithm is thus easy to parallelize.

3

Global trajectory planning

We now turn to describing our decomposition of the global trajectory planning optimization problem
in detail. We begin by defining the variables to be optimized in our optimization problem. In
1

These are the step-size and ?0 constants. See Section B in the supplementary material for more detail.

3

our formulation, we are not tracking the points of the trajectories by a continuous-time variable
taking values in [0, T ]. Rather, our variables are the positions {xi (s)}i2[p] , where the trajectories
are indexed by i and break-points are indexed by a discrete variable s taking values between 1 and
? 1. Note that {xi (0)}i2[p] and {xi (?)}i2[p] are the initial and final configuration, sets of fixed
values, not variables to optimize.
3.1

Formulation as unconstrained optimization without static obstacles

In terms of these variables, the non-collision constraints2 are
k(?xi (s + 1) + (1 ?)xi (s)) (?xj (s + 1) + (1
for all i, j 2 [p], s 2 {0, ..., ? 1} and ? 2 [0, 1].

?)xj (s))k

ri + rj ,

(2)

The parameter ? is used to trace out the constant-velocity trajectories of agents i and j between
break-points s + 1 and s. The parameter ? has no units, it is a normalized time rather than an
absolute time. If t1 is the absolute time of the break-point with integer index s and t2 is the absolute
time of the break-point with integer index s + 1 and t parametrizes the trajectories in absolute time
then ? = (t t1 )/(t2 t1 ). Note that in the above formulation, absolute time does not appear, and
any solution is simply a set of paths that, when travelled by each agent at constant velocity between
break-points, leads to no collisions. When converting this solution into trajectories parameterized
by absolute time, the break-points do not need to be chosen uniformly spaced in absolute time.
The constraints represented in (2) can be formally incorporated into an unconstrained optimization
problem as follows. We search for a solution to the problem:
min f cost ({xi (s)}i,s ) +

{xi (s)}i,s

n
X1 X

frcoll
(xi (s), xi (s + 1), xj (s), xj (s + 1)),
i ,rj

(3)

s=0 i>j

where {xi (0)}p and {xi (?)}p are constants rather than optimization variables, and where the function f cost is a function that represents some cost to be minimized (e.g. the integrated kinetic energy
coll
or the maximum velocity over all the agents) and the function fr,r
0 is defined as,
coll
0
0
fr,r
0 (x, x, x , x ) = J(k?(x

x0 ) + (1

?)(x

x0 )k

r + r0 8? 2 [0, 1]).

(4)

In this section, x and x represent the position of an arbitrary agent of radius r at two consecutive
break-points and x0 and x0 the position of a second arbitrary agent of radius r0 at the same breakpoints. In the expression above J(.) takes the value 0 whenever its argument, a clause, is true and
coll
takes the value +1 otherwise. Intuitively, we pay an infinite cost in fr,r
0 whenever there is a
collision, and we pay zero otherwise.
In (3) we can set f cost (.), to enforce a preference for trajectories satisfying specific properties. For
example, we might prefer trajectories for which the total kinetic energy spent by the set of agents is
small. In this case, defining fCcost (x, x) = Ckx xk2 , we have,
p n 1

f cost ({xi (s)}i,s ) =

1 X X cost
f
(xi (s), xi (s + 1)).
pn i=1 s=0 Ci,s

(5)

where the coefficients {Ci,s } can account for agents with different masses, different absolute-time
intervals between-break points or different preferences regarding which agents we want to be less
active and which agents are allowed to move faster.
More simply, we might want to exclude trajectories in which agents move faster than a certain
amount, but without distinguishing among all remaining trajectories. For this case we can write,
fCcost (x, x) = J(kx

xk ? C).

(6)

In this case, associating each break-point to a time instant, the coefficients {Ci,s } in expression (5)
would represent different limits on the velocity of different agents between different sections of the
trajectory. If we want to force all agents to have a minimum velocity we can simply reverse the
inequality in (6).
2
We replaced the strict inequality in the condition for non-collision by a simple inequality ? ? to avoid
technicalities in formulating the optimization problem. Since the agents are round, this allows for a single point
of contact between two agents and does not reduce practical relevance.

4

3.2

Formulation as unconstrained optimization with static obstacles

In many scenarios agents should also avoid collisions with static obstacles. Given two points in
space, xL and xR , we can forbid all agents from crossing the line segment from xL to xR by adding
Pp Pn 1
the following term to the function (3): i=1 s=0 fxwall
(xi (s), xi (s + 1)). We recall that ri is
L ,xR ,ri
the radius of agent i and
fxwall
(x, x) = J(k(?x + (1
L ,xR ,r

Notice that f

coll

?)x)

can be expressed using f

( xR + (1

wall

coll
0
0
fr,r
0 (x, x, x , x )

)xL )k

r for all ?,

. In particular,

wall
0
= f0,0,r+r
0 (x

x, x0

2 [0, 1]).

x).

(7)
(8)

We use this fact later to express the minimizer associated with agent-agent collisions using the
minimizer associated with agent-obstacle collisions.
When agents move in the plane, i.e. xi (s) 2 R2 for all i 2 [p] and s+1 2 [? +1], being able to avoid
collisions with a general static line segment allows to automatically avoid collisions with multiple
static obstacles of arbitrary polygonal shape. Our numerical experiments only consider agents in the
plane and so, in this paper, we only describe the minimizer block for wall collision for a 2D world.
In higher dimensions, different obstacle primitives need to be considered.
3.3

Message-passing formulation

To solve (3) using the TWA, we need to specify the topology of the bipartite graph associated with
the unconstrained formulation (3) and the operation performed by every minimizer, i.e. the !
?weight update logic and x-variable update equations. We postpone describing the choice of initial
values and internal parameters until Section 4.
We first describe the bipartite graph. To be concrete, let us assume that the cost functional has the
form of (5). The unconstrained formulation (3) then tells us that the global objective function is
the sum of ?p(p + 1)/2 terms: ?p(p 1)/2 functions f coll and ?p functions fCcost . These functions
involve a total of (? + 1)p variables out of which only (? 1)p are free (since the initial and final
configurations are fixed). Correspondingly, the bipartite graph along which messages are passed has
?p(p + 1)/2 minimizer-nodes that connect to the (? + 1)p equality-nodes. In particular, the equalitynode associated with the break-point variable xi (s), ? > s > 0, is connected to 2(p 1) different
cost
g coll minimizer-nodes and two different gC
minimizer-nodes. If s = 0 or s = ? the equality-node
cost
only connects to half as many g coll nodes and gC
nodes.
We now describe the different minimizers. Every minimizer basically is a special case of (1).
3.3.1

Agent-agent collision minimizer

We start with the minimizer associated with the functions f coll , that we denoted by g coll . This minimizer receives as parameters the radius, r and r0 , of the two agents whose collision it is avoiding. The
minimizer takes as input a set of incoming n-messages, {n, n, n0 , n0 }, and associated ? -weights,
{ ? , ? , ? 0 , ? 0 }, and outputs a set of updated x-variables according to expression (9). Messages n
and n come from the two equality-nodes associated with the positions of one of the agents at two
consecutive break-points and n0 and n0 from the corresponding equality-nodes for the other agent.
g coll (n, n, n0 , n0 , ? , ? , ? 0 , ? 0 , r, r0 ) = arg

min

{x,x,x0 ,x0 }

coll
0
0
fr,r
0 (x, x, x , x )

?
?0 0
?
?0 0
kx nk2 + kx nk2 +
kx
n0 k2 +
kx
n0 k2 .
(9)
2
2
2
2
The update logic for the weights !
? for this minimizer is simple. If the trajectory from n to n for
an agent of radius r does not collide with the trajectory from n0 to n0 for an agent of radius r0 then
set all the outgoing weights !
? to zero. Otherwise set them all to ?0 . The outgoing zero weights
indicate to the receiving equality-nodes in the bipartite graph that the collision constraint for this
pair of agents is inactive and that the values it receives from this minimizer-node should be ignored
when computing the consensus values of the receiving equality-nodes.
+

The solution to (9) is found using the agent-obstacle collision minimizer that we describe next.
5

3.3.2

Agent-obstacle collision minimizer

The minimizer for f wall is denoted by g wall . It is parameterized by the obstacle position {xL , xR }
as well as the radius of the agent that needs to avoid the obstacle. It receives two n-messages,
{n, n}, and corresponding weights { ? , ? }, from the equality-nodes associated with two consecutive positions of an agent that needs to avoid the obstacle. Its output, the x-variables, are defined as
g wall (n, n, r, xL , xR , ? , ? ) = arg min fxwall
(x, x) +
L ,xR ,r
{x,x}

?
kx
2

nk2 +

?
kx
2

nk2 .

(10)

When agents move in the plane (2D), this minimizer can be solved by reformulating the optimization in (10) as a mechanical problem involving a system of springs that we can solve exactly and
efficiently. This reduction is explained in the supplementary material in Section D and the solution
to the mechanical problem is explained in Section I.
The update logic for the !
? -weights is similar to that of the g coll minimizer. If an agent of radius

r going from n and n does not collide with the line segment from xL to xR then set all outgoing
weights to zero because the constraint is inactive; otherwise set all the outgoing weights to ?0 .
Notice that, from (8), it follows that the agent-agent minimizer g coll can be expressed using g wall .
More concretely, as proved in the supplementary material, Section C,
?
?
g coll (n, n, n0 , n0 , ? , ? , ? 0 , ? 0 , r, r0 ) = M2 g wall M1 .{n, n, n0 , n0 , ? , ? , ? 0 , ? 0 , r, r0 } ,
for a constant rectangular matrix M1 and a matrix M2 that depend on {n, n, n0 , n0 , ? , ? , ? 0 , ? 0 }.
3.3.3

Minimum energy and maximum (minimum) velocity minimizer

When f cost can be decomposed as in (5), the minimizer associated with the functions f cost is denoted
by g cost and receives as input two n-messages, {n, n}, and corresponding weights, { ? , ? }. The
messages come from two equality-nodes associated with two consecutive positions of an agent. The
minimizer is also parameterized by a cost factor c. It outputs a set of updated x-messages defined as
g cost (n, n, ? , ? , c) = arg min fccost (x, x) +
{x,x}

?
kx
2

nk2 +

?
kx
2

nk2 .

(11)

The update logic for the !
? -weights of the minimum energy minimizer is very simply: always set all
outgoing weights !
? to ?0 . The update logic for the !
? -weights of the maximum velocity minimizer
is the following. If kn nk ? c set all outgoing weights to zero. Otherwise, set them to ?0 . The
update logic for the minimum velocity minimizer is similar. If kn nk c, set all the !
? -weights
to zero. Otherwise set them to ?0 .
The solution to the minimum energy, maximum velocity and minimum velocity minimizer is written
in the supplementary material in Sections E, F, and G respectively.

4

Numerical results

We now report on the performance of our algorithm (see Appendix J for an important comment on
the anytime properties of our algorithm). Note that the lack of open-source scalable algorithms for
global trajectory planning in the literature makes it difficult to benchmark our performance against
other methods. Also, in a paper it is difficult to appreciate the gracefulness of the discovered trajectory optimizations, so we include a video in the supplementary material that shows final optimized
trajectories as well as intermediate results as the algorithm progresses for a variety of additional
scenarios, including those with obstacles. All the tests described here are for agents in a twodimensional plane. All tests but the last were performed using six cores of a 3.4GHz i7 CPU.
The different tests did not require any special tuning of parameters. In particular, the step-size in
[13] (their ? variable) is always 0.1. In order to quickly equilibrate the system to a reasonable set of
variables and to wash out the importance of initial conditions, the default weight ?0 was set equal to
a small value (?p ? 10 5 ) for the first 20 iterations and then set to 1 for all further iterations.
6

The first test considers scenario CONF1: p (even) agents of radius r, equally spaced around on a
circle of radius R, are each required to exchange position with the corresponding antipodal agent,
r = (5/4)R sin(?/2(p 4)). This is a classical difficult test scenario because the straight line
motion of all agents to their goal would result in them all colliding in the center of the circle. We
compare the convergence time of the TWA with a similar version using standard ADMM to perform
the optimizations. In this test, the algorithm?s initial value for each variable in the problem was set
to the corresponding initial position of each agent. The objective is to minimize the total kinetic
energy (C in the energy minimizer is set to 1). Figure 2-left shows that the TWA scales better with
p than classic ADMM and typically gives an order of magnitude speed-up. Please see Appendix K
for a further comment on the scaling of the convergence time of ADMM and TWA with p.
Convergence time (sec)
?

Number of occurrences

?

?=4
?

?

1500

?=8
?

?
?

0

?

0

?
?

20

?
?
?
?

?

?
?

?

?
?
?
?

?
?

?

40

3

4

5

6

7

2500

60

? = ?6

25

20

15

10

5

? = ?4

80

100

p = 100
2000

1500

?

1000

?
?
500

?

0
100

200

Number of agents, p

300

400

500

600

700

Physical cores (? 12)

?

500

2

Convergence time (sec)

?=6

2000

1000

1

?

?=8

0

0

p = 80

?

?

?

p ?= 60
?
?
p?? ?
? 40
?
?
?

?

?

?
5

Objective value of trajectories

?

?
?

?
?

?

?

?

?

?

?

?
?
?

10

?
?
?

?
?

?
?
?

15

?
?
?

?
?

?
?
?

?
?

?
?
?

20

Number of cores

Figure 2: Left: Convergence time using standard ADMM (dashed lines) and using TWA (solid
lines). Middle: Distribution of total energy and time for convergence with random initial conditions
(p = 20 and ? = 5). Right: Convergence time using a different number of cores (? = 5).
The second test for CONF1 analyzes the sensitivity of the convergence time and objective value
when the variables? value at the first iteration are chosen uniformly at random in the smallest spacetime box that includes the initial and final configuration of the robots. Figure 2-middle shows that,
although there is some spread on the convergence time, our algorithm seems to reliably converge to
relatively similar-cost local minima (other experiments show that the objective value of these minima
is around 5 times smaller than that found when the algorithm is run using only the collision avoidance
minimizers without a kinetic energy cost term). As would be expected, the precise trajectories found
vary widely between different random runs.
Still for CONF1, and fixed initial conditions, we parallelize our method using several cores of
a 2.66GHz i7 processor and a very primitive scheduling/synchronization scheme. Although this
scheme does not fully exploit parallelization, Figure 2-right does show a speed-up as the number
of cores increases and the larger p is, the greater the speed-up. We stall when we reach the twelve
physical cores available and start using virtual cores.

Convergence time, one epoch (sec)

Finally, Figure 3-left compares the convergence time to optimize the total energy with the time to
simply find a feasible (i.e. collision-free) solution. The agents initial and final configuration is
randomly chosen in the plane (CONF2). Error bars indicate ? one standard deviation. Minimizing
the kinetic energy is orders of magnitude computationally more expensive than finding a feasible
solution, as is clear from the different magnitude of the left and right scale of Figure 3-left.
?

?=8

12

?=8
10

1500

?=6
?

?=4

8

1200
?

6

?
?

?

4
?

2
0

?

0

?

20

?
?

?
?
?

40

?

?
?
?
?
?

?
?
?
?
?

?
?
?
?
?

60

?
?
?
?

?
?
?

600

?

?=6
?

900

? 300

?=4

Minimum energy

Feasible

1800

Convergence time (sec)

Convergence time (sec)

Convergence time (sec)

30

?

0
80

3.0

2.5

Pink: MIQP

2.0

Light blue: TWA

1.5

1.0

0.5

0.0
8

100

Number of agents, p

10* 12

14* 16

18* 20

24* 24

30* 32

40* 40

50* 52

Number of agents, p

Figure 3: Left: Convergence time when minimizing energy (blue scale/dashed lines) and to simply
find a feasible solution (red scale/solid lines). Right: (For Section 5). Convergence-time distribution
for each epoch using our method (blue bars) and using the MIQP of [12] (red bars and star-values).

7

5

Local trajectory planning based on velocity obstacles

In this section we show how the joint optimization presented in [12], which is based on the concept
of velocity obstacles [11] (VO), can be also solved via the message-passing TWA. In VO, given
the current position {xi (0)}i2[p] and radius {ri } of all agents, a new velocity command is computed
jointly for all agents minimizing the distance to their preferred velocity {viref }i2[p] . This new velocity
command must guarantee that the trajectories of all agents remain collision-free for at least a time
horizon ? . New collision-free velocities are computed every ?? seconds, ? < 1, until all agents
reach their final configuration. Following [12], and assuming an obstacle-free environment and first
order dynamics, the collision-free velocities are given by,
X
minimize
Ci kvi viref k2 s.t. k(xi (0) + vi t) (xj (0) + vj t)k ri + rj 8 i 2 [p], t 2 [0, ? ].
{vi }i2[p]

i2[p]

Since the velocities {vi }i2[p] are related linearly to the final position of each object after ? seconds,
{xi (? )}i2[p] , a simple change of variables allows us to reformulate the above problem as,
X
2
minimize
Ci0 kxi xref
i k
{xi }i2[p]

i2[p]

s.t. k(1 ?)(xi (0) xj (0)) + ?(xi xj )k ri + rj 8 j > i 2 [p], ? 2 [0, 1]
(12)
2
ref
ref
where
= Ci /? , xi = xi (0) + vi ? and we have dropped the ? in xi (? ). The above problem,
extended to account for collisions with the static line segments {xRk , xLk }k , can be formulated in
an unconstrained form using the functions f cost , f coll and f wall . Namely,
X
X
XX
ref
min
fCcost
frcoll
(xi (0), xi , xj (0), xj ) +
fxwall
(xi (0), xi ). (13)
0 (xi , xi ) +
i ,rj
R k ,xL k ,ri
Ci0

{xi }i

i

i2[p]

i>j

i2[p] k

{xref
i }i

Note that {xi (0)}i and
are constants, not variables being optimized. Given this formulation,
the TWA can be used to solve the optimization. All corresponding minimizers are special cases
of minimizers derived in the previous section for global trajectory planning (see Section H in the
supplementary material for details). Figure 3-right shows the distribution of the time to solve (12)
for CONF1. We compare the mixed integer quadratic programming (MIQP) approach from [12]
with ours. Our method finds a local minima of exactly (13), while [12] finds a global minima of
an approximation to (13). Specifically, [12] requires approximating the search domain by hyperplanes and an additional branch-and-bound algorithm while ours does not. Both approaches use a
mechanism for breaking the symmetry from CONF1 and avoid deadlocks: theirs uses a preferential
rotation direction for agents, while we use agents with slightly different C coefficients in their energy minimizers (Cith agent = 1 + 0.001i). Both simulations were done on a single 2.66GHz core.
The results show the order of magnitude is similar, but, because our implementation is done in Java
while [12] uses Matlab-mex interface of CPLEX 11, the results are not exactly comparable.

6

Conclusion and future work

We have presented a novel algorithm for global and local planning of the trajectory of multiple
distinct agents, a problem known to be hard. The solution is based on solving a non-convex optimization problem using TWA, a modified ADMM. Its similarity to ADMM brings scalability and
easy parallelization. However, using TWA improves performance considerably. Our implementation of the algorithm in Java on a regular desktop computer, using a basic scheduler/synchronization
over its few cores, already scales to hundreds of agents and achieves real-time performance for local
planning.
The algorithm can flexibly account for obstacles and different cost functionals. For agents in the
plane, we derived explicit expressions that account for static obstacles, moving obstacles, and dynamic constraints on the velocity and energy. Future work should consider other restrictions on the
smoothness of the trajectory (e.g. acceleration constraints) and provide fast solvers to our minimizers for agents in 3D.
The message-passing nature of our algorithm hints that it might be possible to adapt our algorithm
to do planning in a decentralized fashion. For example, minimizers like g coll could be solved by
message exchange between pairs of agents within a maximum communication radius. It is an open
problem to build a practical communication-synchronization scheme for such an approach.
8

References
[1] Javier Alonso-Mora, Andreas Breitenmoser, Martin Rufli, Roland Siegwart, and Paul Beardsley. Image
and animation display with multiple mobile robots. 31(6):753?773, 2012.
[2] Peter R. Wurman, Raffaello D?Andrea, and Mick Mountz. Coordinating hundreds of cooperative, autonomous vehicles in warehouses. AI Magazine, 29(1):9?19, 2008.
[3] Stephen J. Guy, Jatin Chhugani, Changkyu Kim, Nadathur Satish, Ming Lin, Dinesh Manocha, and
Pradeep Dubey. Clearpath: highly parallel collision avoidance for multi-agent simulation. In Proceedings
of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, pages 177?187, 2009.
[4] John H. Reif. Complexity of the mover?s problem and generalizations. In IEEE Annual Symposium on
Foundations of Computer Science, pages 421?427, 1979.
[5] John E. Hopcroft, Jacob T. Schwartz, and Micha Sharir. On the complexity of motion planning for
multiple independent objects; pspace-hardness of the ?warehouseman?s problem?. The International
Journal of Robotics Research, 3(4):76?88, 1984.
[6] Maren Bennewitz, Wolfram Burgard, and Sebastian Thrun. Finding and optimizing solvable priority
schemes for decoupled path planning techniques for teams of mobile robots. Robotics and Autonomous
Systems, 41(2?3):89?99, 2002.
[7] Daniel Mellinger, Alex Kushleyev, and Vijay Kumar. Mixed-integer quadratic program trajectory generation for heterogeneous quadrotor teams. In IEEE International Conference on Robotics and Automation,
pages 477?483, 2012.
[8] Federico Augugliaro, Angela P. Schoellig, and Raffaello D?Andrea. Generation of collision-free trajectories for a quadrocopter fleet: A sequential convex programming approach. In IEEE/RSJ International
Conference on Intelligent Robots and Systems, pages 1917?1922, 2012.
[9] Steven M. LaValle and James J. Kuffner. Randomized kinodynamic planning. The International Journal
of Robotics Research, 20(5):378?400, 2001.
[10] Oussama Khatib. Real-time obstacle avoidance for manipulators and mobile robots. The International
Journal of Robotics Research, 5(1):90?98, 1986.
[11] Paolo Fiorini and Zvi Shiller. Motion planning in dynamic environments using velocity obstacles. The
International Journal of Robotics Research, 17(7):760?772, 1998.
[12] Javier Alonso-Mora, Martin Rufli, Roland Siegwart, and Paul Beardsley. Collision avoidance for multiple
agents with joint utility maximization. In IEEE International Conference on Robotics and Automation,
2013.
[13] Nate Derbinsky, Jos?e Bento, Veit Elser, and Jonathan S. Yedidia. An improved three-weight messagepassing algorithm. arXiv:1305.1961 [cs.AI], 2013.
[14] G. David Forney Jr. Codes on graphs: Normal realizations. IEEE Transactions on Information Theory,
47(2):520?548, 2001.
[15] Sertac Karaman and Emilio Frazzoli. Incremental sampling-based algorithms for optimal motion planning. arXiv preprint arXiv:1005.0416, 2010.
[16] R. Glowinski and A. Marrocco. Sur l?approximation, par e? l?ements finis d?ordre un, et la r?esolution, par
p?enalisization-dualit?e, d?une class de probl`ems de Dirichlet non lin?eare. Revue Franc?aise d?Automatique,
Informatique, et Recherche Op?erationelle, 9(2):41?76, 1975.
[17] Daniel Gabay and Bertrand Mercier. A dual algorithm for the solution of nonlinear variational problems
via finite element approximation. Computers & Mathematics with Applications, 2(1):17?40, 1976.
[18] Hugh Everett III. Generalized lagrange multiplier method for solving problems of optimum allocation of
resources. Operations Research, 11(3):399?417, 1963.
[19] Magnus R. Hestenes. Multiplier and gradient methods. Journal of Optimization Theory and Applications,
4(5):303?320, 1969.
[20] Magnus R. Hestenes. Multiplier and gradient methods. In L.A. Zadeh et al., editor, Computing Methods
in Optimization Problems 2. Academic Press, New York, 1969.
[21] M.J.D. Powell. A method for nonlinear constraints in minimization problems. In R. Fletcher, editor,
Optimization. Academic Press, London, 1969.
[22] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization
and statistical learning via the alternating direction method of multipliers. Foundations and Trends in
Machine Learning, 3(1):1?122, 2011.

9

"
3981,"Provable ICA with Unknown Gaussian Noise, with
Implications for Gaussian Mixtures and Autoencoders

Sanjeev Arora?

Rong Ge?

Ankur Moitra ?

Sushant Sachdeva?

Abstract
We present a new algorithm for Independent Component Analysis (ICA) which
has provable performance guarantees. In particular, suppose we are given samples
of the form y = Ax + ? where A is an unknown n ? n matrix and x is a random
variable whose components are independent and have a fourth moment strictly
less than that of a standard Gaussian random variable and ? is an n-dimensional
Gaussian random variable with unknown covariance ?: We give an algorithm that
provable recovers A and ? up to an additive  and whose running time and sample complexity are polynomial in n and 1/. To accomplish this, we introduce
a novel ?quasi-whitening? step that may be useful in other contexts in which the
covariance of Gaussian noise is not known in advance. We also give a general
framework for finding all local optima of a function (given an oracle for approximately finding just one) and this is a crucial step in our algorithm, one that has
been overlooked in previous attempts, and allows us to control the accumulation
of error when we find the columns of A one by one via local search.

1

Introduction

We present an algorithm (with rigorous performance guarantees) for a basic statistical problem.
Suppose ? is an independent n-dimensional Gaussian random variable with an unknown covariance
matrix ? and A is an unknown n ? n matrix. We are given samples of the form y = Ax + ? where
x is a random variable whose components are independent and have a fourth moment strictly less
than that of a standard Gaussian random variable. The most natural case is when x is chosen uniformly at random from {+1, ?1}n , although our algorithms in even the more general case above.
Our goal is to reconstruct an additive approximation to the matrix A and the covariance matrix ?
running in time and using a number of samples that is polynomial in n and 1 , where  is the target
precision (see Theorem 1.1) This problem arises in several research directions within machine learning: Independent Component Analysis (ICA), Deep Learning, Gaussian Mixture Models (GMM),
etc. We describe these connections next, and known results (focusing on algorithms with provable
performance guarantees, since that is our goal).
Most obviously, the above problem can be seen as an instance of Independent Component Analysis
(ICA) with unknown Gaussian noise. ICA has an illustrious history with applications ranging from
econometrics, to signal processing, to image segmentation. The goal generally involves finding a
linear transformation of the data so that the coordinates are as independent as possible [1, 2, 3]. This
is often accomplished by finding directions in which the projection is ?non-Gaussian? [4]. Clearly,
if the datapoint y is generated as Ax (i.e., with no noise ? added) then applying linear transformation
A?1 to the data results in samples A?1 y whose coordinates are independent. This restricted case
was considered by Comon [1] and Frieze, Jerrum and Kannan [5], and their goal was to recover an
?
{arora, rongge, sachdeva}@cs.princeton.edu. Department of Computer Science, Princeton University,
Princeton NJ 08540. Research supported by the NSF grants CCF-0832797, CCF-1117309 and Simons Investigator Grant
?
moitra@ias.edu. School of Mathematics, Institute for Advanced Study, Princeton NJ 08540. Research
supported in part by NSF grant No. DMS-0835373 and by an NSF Computing and Innovation Fellowship.

1

additive approximation to A efficiently and using a polynomial number of samples. (We will later
note a gap in their reasoning, albeit fixable by our methods. See also recent papers by Anandkumar
et al., Hsu and Kakade[6, 7], that do not use local search and avoids this issue.) To the best of our
knowledge, there are currently no known algorithms with provable guarantees for the more general
case of ICA with Gaussian noise (this is especially true if the covariance matrix is unknown, as in
our problem), although many empirical approaches are known. (eg. [8], the issue of ?empirical? vs
?rigorous? is elaborated upon after Theorem 1.1.)
The second view of our problem is as a concisely described Gaussian Mixture Model. Our data is
generated as a mixture of 2n identical Gaussian components (with an unknown covariance matrix)
n
whose centers are the points {Ax : x ? {?1, 1} }, and all mixing weights are equal. Notice, this
n
mixture of 2 Gaussians admits a concise description using O(n2 ) parameters. The problem of
learning Gaussian mixtures has a long history, and the popular approach in practice is to use the
EM algorithm [9], though it has no worst-case guarantees (the method may take a very long time
to converge, and worse, may not always converge to the correct solution). An influential paper of
Dasgupta [10] initiated the program of designing algorithms with provable guarantees, which was
improved in a sequence of papers [11, 12, 13, 14]. But in the current setting, it is unclear how to
apply any of the above algorithms (including EM ) since the trivial application would keep track
of exponentially many parameters ? one for each component. Thus, new ideas seem necessary to
achieve polynomial running time.
The third view of our problem is as a simple form of autoencoding [15]. This is a central notion in
Deep Learning, where the goal is to obtain a compact representation of a target distribution using a
multilayered architecture, where a complicated function (the target) can be built up by composing
layers of a simple function (called the autoencoder [16]). The main tenet is that there are interesting functions which can be represented concisely using many layers, but would need a very large
representation if a ?shallow? architecture is used instead). This is most useful for functions that
are ?highly varying? (i.e. cannot be compactly described by piecewise linear functions or other
?simple? local representations). Formally, it is possible to represent using just (say) n2 parameters,
some distributions with 2n ?varying parts? or ?interesting regions.? The Restricted Boltzmann Machine (RBM) is an especially popular autoencoder in Deep Learning, though many others have been
proposed. However, to the best of our knowledge, there has been no successful attempt to give a
rigorous analysis of Deep Learning. Concretely, if the data is indeed generated using the distribution represented by an RBM, then do the popular algorithms for Deep Learning [17] learn the model
parameters correctly and in polynomial time? Clearly, if the running time were actually found to
be exponential in the number of parameters, then this would erode some of the advantages of the
compact representation.
How is Deep Learning related to our problem? As noted by Freund and Haussler [18] many years
ago, an RBM with real-valued visible units (the version that seems more amenable to theoretical
analysis) is precisely a mixture of exponentially many standard Gaussians. It is parametrized by an
n ? m matrix A and a vector ? ? Rn . It encodes a mixture of n-dimensional standard Gaussians
m
centered at the points {Ax : x ? {?1, 1} }, where the mixing weight of the Gaussian centered at
2
Ax is exp(kAxk2 + ? ? x). This is of course reminiscent of our problem. Formally, our algorithm
can be seen as a nonlinear autoencoding scheme analogous to an RBM but with uniform mixing
weights. Interestingly, the algorithm that we present here looks nothing like the approaches favored
traditionally in Deep Learning, and may provide an interesting new perspective.
1.1 Our results and techniques
We give a provable algorithm for ICA with unknown Gaussian noise. We have not made an attempt
to optimize the quoted running time of this model, but we emphasize that this is in fact the first
algorithm with provable guarantees for this problem and moreover we believe that in practice our
algorithm will run almost as fast as the usual ICA algorithms, which are its close relatives.
Theorem 1.1 (Main, Informally). There is an algorithm that recovers the unknown A and ? up to
additive error  in each entry in time that is polynomial in n, kAk2 , k?k2 , 1/, 1/?min (A) where k ? k2
denotes the operator norm and ?min (?) denotes the smallest eigenvalue.
The classical approach for ICA initiated in Comon [1] and Frieze, Jerrum and Kannan [5]) is for
the noiseless case in which y = Ax. The first step is whitening, which applies a suitable linear
transformation that makes the variance the same in all directions, thus reducing to the case where
2

A is a rotation matrix. Given samples y = Rx where R is a rotation matrix, the rows of R can be
found in principle by computing the vectors u that are local minima of E[(u ? y)4 ]. Subsequently, a
number of works (see e.g. [19, 20]) have focused on giving algorithms that are robust to noise. A
popular approach is to use the fourth order cumulant (as an alternative to the fourth order moment)
as a method for ?denoising,? or any one of a number of other functionals whose local optima reveal
interesting directions. However, theoretical guarantees of these algorithms are not well understood.
The above procedures in the noise-free model can almost be made rigorous (i.e., provably polynomial running time and number of samples), except for one subtlety: it is unclear how to use local
search to find all optima in polynomial time. In practice, one finds a single local optimum, projects
to the subspace orthogonal to it and continues recursively on a lower-dimensional problem. However, a naive implementation of this idea is unstable since approximation errors can accumulate
badly, and to the best of our knowledge no rigorous analysis has been given prior to our work. (This
is not a technicality: in some similar settings the errors are known to blow up exponentially [21].)
One of our contributions is a modified local search that avoids this potential instability and finds all
local optima in this setting. (Section 4.2.)
Our major new contribution however is dealing with noise that is an unknown Gaussian. This is an
important generalization, since many methods used in ICA are quite unstable to noise (and a wrong
estimate for the covariance could lead to bad results). Here, we no longer need to assume we know
even rough estimates for the covariance. Moreover, in the context of Gaussian Mixture Models this
generalization corresponds to learning a mixture of many Gaussians where the covariance of the
components is not known in advance.
We design new tools for denoising and especially whitening in this setting. Denoising uses the fourth
order cumulant instead of the fourth moment used in [5] and whitening involves a novel use of the
Hessian of the cumulant. Even then, we cannot reduce to the simple case y = Rx as above, and are
left with a more complicated functional form (see ?quasi-whitening? in Section 2.) Nevertheless,
we can reduce to an optimization problem that can be solved via local search, and which remains
amenable to a rigorous analysis. The results of the local optimization step can be then used to
simplify the complicated functional form and recover A as well as the noise ?. We defer many of
our proofs to the supplementary material section, due to space constraints.
In order to avoid cluttered notation, we have focused on the case in which x is chosen uniformly at
random from {?1, +1}n , although our algorithm and analysis work under the more general conditions that the coordinates of x are (i) independent and (ii) have a fourth moment that is less
than three (the fourth moment of a Gaussian random variable). In this case, the functional P (u)
(see Lemma 2.2) will take the same form but with weights depending on the exact value of the
fourth moment for each coordinate. Since we already carry through an unknown diagonal matrix D
throughout our analysis, this generalization only changes the entries on the diagonal and the same
algorithm and proof apply.

2

Denoising and quasi-whitening

As mentioned, our approach is based on the fourth order cumulant. The cumulants of a random
variable are the coefficients of the Taylor expansion of the logarithm of the characteristic function
[22]. Let ?r (X) be the rth cumulant of a random variable X. We make use of:
Fact 2.1. (i) If X has mean zero, then ?4 (X) = E[X 4 ] ? 3 E[X 2 ]2 . (ii) If X is Gaussian with mean
? and variance ? 2 , then ?1 (X) = ?, ?2 (X) = ? 2 and ?r (X) = 0 for all r > 2. (iii) If X and Y
are independent, then ?r (X + Y ) = ?r (X) + ?r (Y ).
The crux of our technique is to look at the following functional, where y is the random variable
Ax + ? whose samples are given to us. Let u ? Rn be any vector. Then P (u) = ??4 (uT y).
Note that for any u we can compute P (u) reasonably accurately by drawing sufficient number of
samples of y and taking an empirical average. Furthermore, since x and ? are independent, and ? is
Gaussian, the next lemma is immediate. We call it ?denoising? since it allows us empirical access
to some information about A that is uncorrupted by the noise ?.
Pn
Lemma 2.2 (Denoising Lemma). P (u) = 2 i=1 (uT A)4i .
The intuition is that P (u) = ??4 (uT Ax) since the fourth cumulant does not depend on the additive
Gaussian noise, and then the lemma follows from completing the square.
3

2.1 Quasi-whitening via the Hessian of P (u)
In prior works on ICA, whitening refers to reducing to the case where y = Rx for some some
rotation matrix R. Here we give a technique to reduce to the case where y = RDx + ? 0 where ? 0
is some other Gaussian noise (still unknown), R is a rotation matrix and D is a diagonal matrix that
depends upon A. We call this quasi-whitening. Quasi-whitening suffices for us since local search
using the objective function ?4 (uT y) will give us (approximations to) the rows of RD, from which
we will be able to recover A.
Quasi-whitening involves computing the Hessian of P (u), which recall is the matrix of all 2nd order
partial derivatives of P (u). Throughout this section, we will denote the Hessian operator by H. In
matrix form, the Hessian of P (u) is
n
n
X
X
?2
P (u) = 24
Ai,k Aj,k (Ak ? u)2 ; H(P (U )) = 24
(Ak ? u)2 Ak ATk = ADA (u)AT
?ui ?uj
k=1

k=1

where Ak is the k-th column of the matrix A (we use subscripts to denote the columns of matrices
throught the paper). DA (u) is the following diagonal matrix:
Definition 2.3. Let DA (u) be a diagonal matrix in which the k th entry is 24(Ak ? u)2 .
Of course, the exact Hessian of P (u) is unavailable and we will instead compute an empirical
approximation Pb(u) to P (u) (given many samples from the distribution), and we will show that the
Hessian of Pb(u) is a good approximation to the Hessian of P (u).
0
Definition 2.4. Given 2N samples y1 , y10 , y2 , y20 ..., yN , yN
of the random variable y, let
N
N
3 X T 2 T 0 2
?1 X T 4
(u yi ) +
(u yi ) (u yi ) .
Pb(u) =
N i=1
N i=1
Our first step is to show that the expectation of the Hessian of Pb(u) is exactly the Hessian of P (u).
In fact, since the expectation of Pb(u) is exactly P (u) (and since Pb(u) is an analytic function of the
samples and of the vector u), we can interchange the Hessian operator and the expectation operator.
Roughly, one can imagine the expectation operator as an integral over the possible values of the
random samples, and as is well-known in analysis, one can differentiate under the integral provided
that all functions are suitably smooth over the domain of integration.
Claim 2.5. Ey,y0 [?(uT y)4 + 3(uT y)2 (uT y 0 )2 ] = P (u)
This claim follows immediately from the definition of P (u), and since y and y 0 are independent.
Lemma 2.6. H(P (u)) = Ey,y0 [H(?(uT y)4 + 3(uT y)2 (uT y 0 )2 )]
Next, we compute the two terms inside the expectation:
Claim 2.7. H((uT y)4 ) = 12(uT y)2 yy T
Claim 2.8. H((uT y)2 (uT y 0 )2 ) = 2(uT y 0 )2 yy T + 2(uT y)2 y 0 (y 0 )T + 4(uT y)(uT y 0 )(y(y 0 )T +
(y 0 )y T )
Let ?min (A) denote the smallest eigenvalue of A. Our analysis also requires bounds on the entries
of DA (u0 ):
Claim 2.9. If u0 is chosen uniformly at random then with high probability for all i,
n
log n
n
min kAi k22 n?4 ? DA (u0 ))i,i ? max kAi k22
i=1
i=1
n
Lemma 2.10. If u0 is chosen uniformly at random and furthermore we are given 2N =
poly(n, 1/, 1/?min (A), kAk2 , k?k2 ) samples of y, then with high probability we will have that
(1 ? )ADA (u0 )AT  H(Pb(u0 ))  (1 + )ADA (u0 )AT .
c  (1 + )ADA (u0 )AT , and let M
c = BB T .
Lemma 2.11. Suppose that (1 ? )ADA (u0 )AT  M
?
?
?1
1/2
?
Then there is a rotation matrix R such that kB ADA (u0 ) ? R kF ? n.
The intuition is: if any of the singular values of B ?1 ADA (u0 )1/2 are outside the range [1 ? , 1 + ],
cx are too far apart
we can find a unit vector x where the quadratic forms xT ADA (u0 )AT x and xT M
(which contradicts the condition of the lemma). Hence the singular values of B ?1 ADA (u0 )1/2 can
all be set to one without changing the Froebenius norm of B ?1 ADA (u0 )1/2 too much, and this
yields a rotation matrix.
4

3

Our algorithm (and notation)

In this section we describe our overall algorithm. It uses as a blackbox the denoising and quasiwhitening already described above, as well as a routine for computing all local maxima of some
?well-behaved? functions which is described later in Section 4.
Notation: Placing a hat over a function corresponds to an empirical approximation that we obtain
from random samples. This approximation introduces error, which we will keep track of.
Step 1: Pick a random u0 ? Rn and estimate the Hessian H(Pb(u0 )). Compute B such that
H(Pb(u0 )) = BB T . Let D = DA (u0 ) be the diagonal matrix defined in Definition 2.3.
PN
0
Step
2: Take 2N samples y1 , y2 , ...,yN , y10 , y20 , ..., yN
, and let Pb0 (u) = ? N1 i=1 (uT B ?1 yi )4 +
P
N
3
T ?1
yi )2 (uT B ?1 yi0 )2 which is an empirical estimation of P 0 (u).
i=1 (u B
N
Step 3: Use the procedure A LL OPT(Pb0 (u), ?, ? 0 , ? 0 , ? 0 ) of Section 4 to compute all n local maxima
of the function Pb0 (u).
Step 4: Let R be the matrix whose rows are the n local optima recovered in the previous step. Use
procedure R ECOVER of Section 5 to find A and ?.
Explanation: Step 1 uses the transformation B ?1 computed in the previous Section to quasi-whiten
the data. Namely, we consider the sequence of samples z = B ?1 y, which are therefore of the form
R0 Dx+? 0 where ? = B ?1 ?, D = DA (u0 ) and R0 is close to a rotation matrix R? (by Lemma 2.11).
In Step 2 we look at ?4 ((uT z)), which effectively denoises the new samples (see Lemma 2.2), and
thus is the same as ?4 (R0 D?1/2 x). Let P 0 (u) = ?4 (uT z) = ?4 (uT B ?1 y) which is easily seen to be
E[(uT R0 D?1/2 x)4 ]. Step 2 estimates this function, obtaining Pb0 (u). Then Step 3 tries to find local
optima via local search. Ideally we would have liked access to the functional P ? (u) = (uT R? x)4
since the procedure for local optima works only for true rotations. But since R0 and R? are close
we can make it work approximately with Pb0 (u), and then in Step 4 use these local optima to finally
recover A.
Theorem 3.1. Suppose we are given samples of the form y = Ax + ? where x is uniform on
{+1, ?1}n , A is an n ? n matrix, ? is an n-dimensional Gaussian random variable independent
of x with unknown covariance matrix ?. There is an algorithm that with high probability recovers
b ? A?diag(ki )kF ?  where ? is some permutation matrix and each ki ? {+1, ?1} and
kA
b ? ?kF ? . Furthermore the running time and number of samples needed are
also recovers k?
poly(n, 1/, kAk2 , k?k2 , 1/?min (A))
Note that here we recover A up to a permutation of the columns and sign-flips. In general, this is
all we can hope for since the distribution of x is also invariant under these same operations. Also,
the dependence of our algorithm on the various norms (of A and ?) seems inherent since our goal is
to recover an additive approximation, and as we scale up A and/or ?, this goal becomes a stronger
relative guarantee on the error.

4

Framework for iteratively finding all local maxima

In this section, we first describe a fairly standard procedure (based upon Newton?s method) for
finding a single local maximum of a function f ? : Rn ? R among all unit vectors and an analysis
of its rate of convergence. Such a procedure is a common tool in statistical algorithms, but here we
state it rather carefully since we later give a general method to convert any local search algorithm
(that meets certain criteria) into one that finds all local maxima (see Section 4.2).
Given that we can only ever hope for an additive approximation to a local maximum, one should
be concerned about how the error accumulates when our goal is to find all local maxima. In fact, a
naive strategy is to project onto the subspace orthogonal to the directions found so far, and continue
in this subspace. However, such an approach seems to accumulate errors badly (the additive error
of the last local maxima found is exponentially larger than the error of the first). Rather, the crux
of our analysis is a novel method for bounding how much the error can accumulate (by refining old
estimates).
5

Algorithm 1. L OCAL OPT, Input:f (u), us , ?, ? Output: vector v
1. Set u ? us .
2. Maximize (via Lagrangian methods) Proj?u (?f (u))T ? + 12 ? T Proj?u (H(f (u)))? ?
k?k22

0

1
2



?
f (u)
?u



?

T

Subject to k?k2 ? ? and u ? = 0

3. Let ? be the solution, u
?=

u+?
ku+?k

4. If f (?
u) ? f (u) + ?/2, set u ? u
? and Repeat Step 2
5. Else return u

Our strategy is to first find a local maximum in the orthogonal subspace, then run the local optimization algorithm again (in the original n-dimensional space) to ?refine? the local maximum we have
found. The intuition is that since we are already close to a particular local maxima, the local search
algorithm cannot jump to some other local maxima (since this would entail going through a valley).
4.1 Finding one local maximum
Throughout this section, we will assume that we are given oracle access to a function f (u) and its
gradient and Hessian. The procedure is also given a starting point us , a search range ?, and a step
size ?. For simplicity in notation we define the following projection operator.
Definition 4.1. Proj?u (v) = v ? (uT v)u, Proj?u (M ) = M ? (uT M u)uuT .
The basic step the algorithm is a modification of Newton?s method to find a local improvement
that makes progress so long as the current point u is far from a local maxima. Notice that if we
add a small vector to u, we do not necessarily preserve the norm of u. In order to have control
over how the norm of u changes, during local optimization step the algorithm projects the gradient
?f and Hessian H(f ) to the space perpendicular to u. There is also an additional correction term
??/?u f (u) ? k?k2 /2. This correction term is necessary because the new vector we obtain is (u +
?)/ k(u + ?)k2 which is close to u ? k?k22 /2 ? u + ? + O(? 3 ). Step 2 of the algorithm is just
maximizing a quadratic function and can be solved exactly using Lagrangian Multiplier method. To
increase efficiency it is also acceptable to perform an approximate maximization step by taking ? to
be either aligned with the gradient Proj?u ?f (u) or the largest eigenvector of Proj?u (H(f (u))).
The algorithm is guaranteed to succeed in polynomial time when the function is Locally Improvable
and Locally Approximable:
Definition 4.2 ((?, ?, ?)-Locally Improvable). A function f (u) : Rn ? R is (?, ?, ?)-Locally
Improvable, if for any u that is at least ? far from any local maxima, there is a u0 such that
ku0 ? uk2 ? ? and f (u0 ) ? f (u) + ?.
Definition 4.3 ((?, ?)-Locally Approximable). A function f (u) is locally approximable, if its third
order derivatives exist and for any u and any direction v, the third order derivative of f at point u in
the direction of v is bounded by 0.01?/? 3 .
The analysis of the running time of the procedure comes from local Taylor expansion. When a
function is Locally Approximable it is well approximated by the gradient and Hessian within a ?
neighborhood. The following theorem from [5] showed that the two properties above are enough to
guarantee the success of a local search algorithm even when the function is only approximated.
Theorem 4.4 ([5]). If |f (u) ? f ? (u)| ? ?/8, the function f ? (u) is (?, ?, ?)-Locally Improvable,
f (u) is (?, ?) Locally Approximable, then Algorithm 1 will find a vector v that is ? close to some
local maximum. The running time is at most O((n2 + T ) max f ? /?) where T is the time to evaluate
the function f and its gradient and Hessian.
4.2 Finding all local maxima
Now we consider how to find all local maxima of a given function f ? (u). The crucial condition that
we need is that all local maxima are orthogonal (which is indeed true in our problem, and is morally
true when using local search more generally in ICA). Note that this condition implies that there are
at most n local maxima.1 In fact we will assume that there are exactly n local maxima. If we are
given an exact oracle for f ? and can compute exact local maxima then we can find all local maxima
6

Algorithm 2. A LL OPT, Input:f (u), ?, ?, ? 0 , ? 0 Output: v1 , v2 , ..., vn , ?i kvi ? vi? k ? ?.
1.
2.
3.
4.
5.
6.
7.

Let v1 = L OCAL OPT(f, e1 , ?, ?)
FOR i = 2 TO n DO
Let gi be the projection of f to the orthogonal subspace of v1 , v2 , ..., vi?1 .
Let u0 = L OCAL OPT(g, e1 , ? 0 , ? 0 ).
Let vi = L OCAL OPT(f, u0 , ?, ?).
END FOR
Return v1 , v2 , ..., vn

easily: find one local maximum, project the function into the orthogonal subspace, and continue to
find more local maxima.
Definition 4.5. The projection of a function f to a linear subspace S is a function on that subspace
with value equal to f . More explicitly, if {v1 , v2 , ..., vd } is an orthonormal basis of S, the projection
Pd
of f to S is a function g : Rd ? R such that g(w) = f ( i=1 wi vi ).
The following theorem gives sufficient conditions under which the above algorithm finds all local
maxima, making precise the intuition given at the beginning of this section.
Theorem 4.6. Suppose the function f ? (u) : Rn ? R satisfies the following properties:
1. Orthogonal Local Maxima: The function has n local maxima vi? , and they are orthogonal
to each other.
2. Locally Improvable: f ? is (?, ?, ?) Locally Improvable.
3. Improvable Projection: The projection of the function to any subspace spanned by a subset
of local maxima is (? 0 , ? 0 , ? 0 ) Locally Improvable. The step size ? 0 ? 10?.
?
4. Lipschitz: If ku ? u0 k2 ? 3 n?, then the function value |f ? (u) ? f ? (u0 )| ? ? 0 /20.
?
5. Attraction Radius: Let Rad ? 3 n? + ? 0 , for any local maximum vi? , let T?be min f ? (u)
for ku ? vi? k2 ? Rad, then there exist a set U containing ku ? vi? k2 ? 3 n? + ? 0 and
does not contain any other local maxima, such that for every u that is not in U but is ?
close to U , f ? (u) < T .
If we are given function f such that |f (u) ? f ? (u)| ? ?/8 and f is both (?, ?) and (? 0 , ? 0 ) Locally
Approximable, then Algorithm 2 can find all local maxima of f ? within distance ?.
To prove this theorem, we first notice the projection of the function f in Step 3 of the algorithm
should be close to the projection of f ? to the remaining local maxima. This is implied by Lipschitz
condition and is formally shown in the following two lemmas. First we prove a ?coupling? between
the orthogonal complement of two close subspaces:
Lemma 4.7. Given v1 , v2 , ..., vk , each ?-close respectively to local maxima v1? , v2? , ..., vk? (this is
without loss of generality because we can permute the index of local maxima), then there is an
orthonormal basis vk+1 , vk+2 , ..., vn for the orthogonal space of span{v1 , v2 , ..., vk } such that for
Pn?k
Pn?k
?
?
any unit vector w ? Rn?k , i=1 wk vk+i is 3 n? close to i=1 wk vk+i
.
We prove this lemma using a modification of the Gram-Schmidt orthonormalization procedure. Using this lemma we see that the projected function is close to the projection of f ? in the span of the
rest of local maxima:
Lemma 4.8. Let g ? be the projection of f ? into the space spanned by the rest of local maxima, then
|g ? (w) ? g(w)| ? ?/8 + ? 0 /20 ? ? 0 /8.

5

Local search on the fourth order cumulant

Next, we prove that the fourth order cumulant P ? (u) satisfies the properties above. Then the algorithm given in the previous section will find all of the local maxima, which is the missing step in our
1
Technically, there are 2n local maxima since for each direction u that is a local maxima, so too is ?u but
this is an unimportant detail for our purposes.

7

b  Output: A,
b ?
b
Algorithm 3. R ECOVER, Input:B, Pb0 (u), R,
b A (u) be a diagonal matrix whose ith entry is
1. Let D

1
2



bi )
Pb0 (R

?1/2

.

b = BR
bD
b A (u)?1/2 .
2. Let A
b=
3. Estimate C = E[yy T ] by taking O((kAk2 + k?k2 )4 n2 ?2 ) samples and let C
b=C
b?A
bA
bT
4. Let ?
b
b
5. Return A, ?

1
N

PN

i=1

yi yiT .

main goal: learning a noisy linear transformation Ax + ? with unknown Gaussian noise. We first
use a theorem from [5] to show that properties for finding one local maxima is satisfied.
Also, for notational convenience we set di = 2DA (u0 )?2
i,i and let dmin and dmax denote the minimum and maximum
values
(bounds
on
these
and
their
ratio follow from Claim 2.9). Using this
Pn
notation P ? (u) = i=1 di (uT Ri? )4 .
?
Theorem 5.1 ([5]). When ? < dmin /10dmax n2 , the function P ? (u) is (3 n?, ?, P ? (u)? 2 /100)
Locally Improvable and (?, dmin ? 2 /100n) Locally Approximable. Moreover, the local maxima of
the function is exactly {?Ri? }.
We then observe that given enough samples, the empirical mean Pb0 (u) is close to P ? (u). For
concentration we require every degree four term zi zj zk zl has variance at most Z.
Claim 5.2. Z = O(d2min ?min (A)8 k?k42 + d2min ).
0
, suppose columns of R0 =
Lemma 5.3. Given 2N samples y1 , y2 , ..., yN , y10 , y20 , ..., yN
?1
1/2
?
B ADA (u0 ) are  close to the corresponding columns of R , with high probability the function
Pb0 (u) is O(dmax n1/2  + n2 (N/Z log n)?1/2 ) close to the true function P ? (u).

The other properties required by Theorem 4.6 are also satisfied:
Lemma 5.4. For any ku ? u0 k2 ? r, |P ? (u) ? P ? (u0 )| ? 5dmax n1/2 r. All local maxima of P ?
has attraction radius Rad ? dmin /100dmax .
Applying Theorem 4.6 we obtain the following Lemma (the parameters are chosen so that all properties required are satisfied):
Lemma 5.5. Let ? 0 = ?((dmin /dmax )2 ), ? = min{?n?1/2 , ?((dmin /dmax )4 n?3.5 )}, then the
procedure R ECOVER (f, ?, dmin ? 2 /100n , ? 0 , dmin ? 02 /100n) finds vectors v1 , v2 , ..., vn , so that
there is a permutation matrix ? and ki ? {?1} and for all i: kvi ? (R?Diag(ki ))?i k2 ? ?.
b = [v1 , v2 , ..., vn ] we can use Algorithm 3 to find A and ?:
After obtaining R
b such that there is permutation matrix ? and ki ? {?1} with
Theorem 5.6. Given a matrix R
?
b
b such that kA
b ? A?Diag(ki )kF ?
kRi ? ki (R ?)i k2 ? ? for all i, Algorithm 3 returns matrix A
2 3/2
2 3/2
O(? kAk2 n /?min (A)). If ? ? O(/ kAk2 n ?min (A)) ? min{1/ kAk2 , 1}, we also have
b ? ?kF ? .
k?
Recall that the diagonal matrix DA (u) is unknown
Pn (since it depends on A), but if we are given
R? (or an approximation) and since P ? (u) = i=1 di (uT Ri? )4 , we can recover the matrix DA (u)
approximately from computing P ? (Ri? ). Then given DA (u), we can recover A and ? and this
completes the analysis of our algorithm.
Conclusions
ICA is a vast field with many successful techniques. Most rely on heuristic nonlinear optimization.
An exciting question is: can we give a rigorous analysis of those techniques as well, just as we
did for local search on cumulants? A rigorous analysis of deep learning ?say, an algorithm that
provably learns the parameters of an RBM?is another problem that is wide open, and a plausible
special case involves subtle variations on the problem we considered here.

8

References
[1] P. Comon. Independent component analysis: a new concept? Signal Processing, pp. 287?314,
1994. 1, 1.1
[2] A. Hyvarinen, J. Karhunen, E. Oja. Independent Component Analysis. Wiley: New York,
2001. 1
[3] A. Hyvarinen, E. Oja. Independent component analysis: algorithms and applications. Neural
Networks, pp. 411?430, 2000. 1
[4] P. J. Huber. Projection pursuit. Annals of Statistics pp. 435?475, 1985. 1
[5] A. Frieze, M. Jerrum, R. Kannan. Learning linear transformations. FOCS, pp. 359?368, 1996.
1, 1.1, 4.1, 4.4, 5, 5.1
[6] A. Anandkumar, D. Foster, D. Hsu, S. Kakade, Y. Liu. Two SVDs suffice: spectral decompositions for probabilistic topic modeling and latent Dirichlet allocation. Arxiv:abs/1203.0697,
2012. 1
[7] D. Hsu, S. Kakade. Learning mixtures of spherical Gaussians: moment methods and spectral
decompositions. Arxiv:abs/1206.5766, 2012. 1
[8] L. De Lathauwer; J. Castaing; J.-F. Cardoso, Fourth-Order Cumulant-Based Blind Identification of Underdetermined Mixtures, Signal Processing, IEEE Transactions on, vol.55, no.6,
pp.2965-2973, June 2007 1
[9] A.P. Dempster, N.M. Laird, and D.B. Rubin. Maximum likelihood from incomplete data via
the EM Algorithm. Journal of the Royal Statistical Society Series B, pp. 1?38, 1977. 1
[10] S. Dasgupta. Learning mixtures of Gaussians. FOCS pp. 634?644, 1999. 1
[11] S. Arora and R. Kannan. Learning mixtures of separated nonspherical Gaussians. Annals of
Applied Probability, pp. 69-92, 2005. 1
[12] M. Belkin and K. Sinha. Polynomial learning of distribution families. FOCS pp. 103?112,
2010. 1
[13] A. T. Kalai, A. Moitra, and G. Valiant. Efficiently learning mixtures of two Gaussians. STOC
pp. 553-562, 2010. 1
[14] A. Moitra and G. Valiant. Setting the polynomial learnability of mixtures of Gaussians. FOCS
pp. 93?102, 2010. 1
[15] G. Hinton, R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science pp. 504?507, 2006. 1
[16] Y. Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning,
pp. 1?127, 2009. 1
[17] G. E. Hinton. A Practical Guide to Training Restricted Boltzmann Machines, Version 1, UTML
TR 2010-003, Department of Computer Science, University of Toronto, August 2010 1
[18] Y. Freund , D. Haussler. Unsupervised Learning of Distributions on Binary Vectors using Two
Layer Networks University of California at Santa Cruz, Santa Cruz, CA, 1994 1
[19] S. Cruces, L. Castedo, A. Cichocki, Robust blind source separation algorithms using cumulants, Neurocomputing, Volume 49, Issues 14, pp 87-118, 2002. 1.1
[20] L., De Lathauwer; B., De Moor; J. Vandewalle. Independent component analysis based on
higher-order statistics only Proceedings of 8th IEEE Signal Processing Workshop on Statistical
Signal and Array Processing, 1996. 1.1
[21] S. Vempala, Y. Xiao. Structure from local optima: learning subspace juntas via higher order
PCA. Arxiv:abs/1108.3329, 2011. 1.1
[22] M. Kendall, A. Stuart. The Advanced Theory of Statistics Charles Griffin and Company, 1958.
2

9

"
1674,"Necessary Intransitive Likelihood-Ratio
Classifiers
Gang Ji and Jeff Bilmes
SSLI-Lab, Department of Electrical Engineering
University of Washington
Seattle, WA 98195-2500
{gang,bilmes}@ee.washington.edu

Abstract
In pattern classification tasks, errors are introduced because of differences between the true model and the one obtained via model estimation.
Using likelihood-ratio based classification, it is possible to correct for this
discrepancy by finding class-pair specific terms to adjust the likelihood
ratio directly, and that can make class-pair preference relationships intransitive. In this work, we introduce new methodology that makes necessary corrections to the likelihood ratio, specifically those that are necessary to achieve perfect classification (but not perfect likelihood-ratio
correction which can be overkill). The new corrections, while weaker
than previously reported such adjustments, are analytically challenging
since they involve discontinuous functions, therefore requiring several
approximations. We test a number of these new schemes on an isolatedword speech recognition task as well as on the UCI machine learning
data sets. Results show that by using the bias terms calculated in this
new way, classification accuracy can substantially improve over both the
baseline and over our previous results.

1

Introduction

Statistical pattern recognition is often based on Bayes decision theory [4], which aims to
achieve minimum error rate classification. In previous work [2], we observed that multiclass Bayes classification can be viewed as a tournament style game, where the winner
between players is decided using log likelihood ratios. Supposing the classes (players) are
{c1 , c2 , ? ? ? , cM }, and the observation (game) is x, the winner of each pair of classes is
determined, with the assumption of equal priors, by the sign of the log likelihood ratio
(x|ci )
Lij (x) = ln PP (x|c
, in which case if Lij > 0 class ci wins and otherwise class cj wins.
j)
A practical game strategy can be obtained by fixing a comparison order, {i1 , i2 , ? ? ? , iM },
as a permutation of {1, 2, ? ? ? , M }, where class ci1 plays with class ci2 , the winner plays
with class ci3 , and so on until a final winner is ultimately found. This yields a transitive
game [8] ? assuming no ties, the ultimate winner is identical regardless of the comparison
order.
To perform these procedures optimally, correct likelihood ratios are needed, which requires
correct probabilistic models and sufficient training data. This is never the case given a fi-

nite amount of training data or the wrong model family, typical in practice. In previous
work [2], we introduced a method to correct for the difference between the true and an
approximate log likelihood ratio. In this work, we improve upon the correction method by
using an expression that can still lead to perfect correction, but is weaker than what we used
before. We show that this new condition can achieve a significant improvement over baseline results, both on a medium vocabulary isolated-word automatic speech recognition task
and on the UCI machine learning data sets. The paper is organized as follows: Section 2
describes the general scheme and describes past work. Section 3 discusses the weaker correction condition, and its approximations. Section 4 provides various experimental results
on an isolated-word speech recognition task. Section 5 contains the experimental results
on the UCI data. Finally, Section 6 concludes.

2

Background

A common problem in many probabilistic machine learning settings is the lack of a correct statistical model. In a generative pattern classification setting, this occurs because
only an estimated quantity P? (x|c)1 of a distribution is available, rather than the true classconditional model P (x|c). In the likelihood ratio decision scheme described above, only
? ij (x) = ln(P? (x|ci )/P? (x|cj )), is available for decision
an imperfect log likelihood ratio, L
making rather than the true log likelihood ratio Lij (x).
One approach to correct for this inaccuracy is to use richer class conditional likelihoods,
more complicated parametric forms of Lij (x) itself, and/or more training data. In previous
work [2], we proposed a different approach that requires no change in generative models,
no increase in free parameters, and no additional training data but still yields improved
? ij (x)
accuracy. The key idea is to compensate for the difference between Lij (x) and L
2
using a bias term ?ij (x) computed from test data such that:
? ij (x).
Lij (x) ? ?ij (x) = L

(1)

If it is assumed that a single bias term is used for all data, so that ?ij (x) = ?ij , we found
that the best ?ij is as follows:

1
1?
?
?ij = (D(ikj) ? D(jki)) ?
D(ikj) ? D(jki)
,
(2)
2
2
where D(ikj) = EP (x|ci ) ln Lij (x) is the Kullback-Leibler (KL) divergence [3] between
? ij (x) is its estimation. Under the assumption
?
P (x|ci ) and P (x|cj ) and D(ikj)
= EP (x|ci ) L
(referred to as assumption A in Section 3.1) of symmetric KL-divergence for the true model
(e.g., equal covariance matrices in the Gaussian case), the bias term can be solved explicitly
as

1?
?
?ij = ?
D(ikj) ? D(jki)
.
(3)
2
? ij (x) + ?ij can lead to an inWe saw how the augmented likelihood ratio Sij (x) = L
transitive game [8, 13], since Sij (x) can specify intransitive preferences amongst the set
{1, 2, ? ? ? , M }. We therefore investigated a number of intransitive game playing strategies. Moreover, we observed that if the correction was optimal, the true likelihood ratios
would be obtained which are clearly transitive. We therefore hypothesized and experimentally verified that the existence of intransitivity was a good indicator of the occurrence of a
classification error.
This general approach can be improved upon in several ways. First, better intransitive
strategies can be developed (for detecting, tolerating, and utilizing the intransitivity of a
1
2

In this paper, we use ?hatted? letters to describe estimated quantities.
Note that by bias, we do not mean standard parameter bias in statistical parameter estimation.

classifier); second, the assumption of symmetric KL-divergence could be relaxed; and third,
the above criterion is stricter than required to obtain perfect correction. In this work, we
advance on the latter two of the above three possible avenues for improvement.

3

Necessary Intransitive Scheme

An ?ij (x) that solves Equation 1 is a sufficient condition for a perfect correction of the
estimated likelihood ratio since given such a quantity, the true likelihood ratio would be
attainable. This condition, however, is stricter than required because it is only the sign of
the likelihood ratio that is needed to decide the winning class. We therefore should ask for
a condition that corrects only for the discrepancy in sign between the true and estimated
ratio, i.e., we want to find a function ?ij (x) that minimizes
Z n
o2
? ij (x) ? Pij (x) dx.
J[?ij ] =
sgn [Lij (x) ? ?ij (x)] ? sgnL
Rn

Clearly the ?ij (x) that minimizes J[?ij ] is the one such that
? ij (x),
sgn [Lij (x) ? ?ij (x)] = sgnL

?x ? suppPij = {x : Pij (x) 6= 0}.

(4)

As can be seen, this condition is weaker than Equation 1, weaker in the sense that any
solution to Equation 1 solves Equation 4 but not vice versa. Note also that Equation 4
provides necessary conditions for an additive bias term to achieve perfect correction, since
any such correction must achieve parity in the sign. Therefore, it might make it simpler
to find a better bias term since Equation 4 (and therefore, set of possible ? values) is less
constrained. As will be seen, however, analysis of this weaker condition is more difficult.
In the following sections, therefore we introduce several approximations to this condition.
Note that as in previous work, we henceforth assume ?ij (x) = ?ij is a constant. In this
case, the equation providing the best ?ij values is:
n
o
? ij (x) .
EPij {sgn [Lij (x) ? ?ij ]} = EPij sgnL
(5)
3.1

The difficulty with the sign function

The main problem in trying to solve for ?ij in Equation 5 is the existence of a discontinuous
function. In this section, therefore, we work towards obtaining an analytically tractable
approximation. The {?1, 0, 1}-valued sign function sgn(z) is defined as 2u(z) ? 1, where
u(z) is the Heaviside step function. We obtain an approximation via a Taylor expansion as
follows:
sgn(z + ) = sgn(z) + sgn0 (z) + o() = sgn(z) + 2?(z) + o(),

(6)

where ?(z) is the Dirac delta function [7]. It can be defined as the derivative
of the HeavZ

iside step function u0 (z) = ?(z), and it satisfies the sifting property
f (z0 ). Therefore, it follows that [6, page 263]
Z
Z
f (z)?[g(z)] dz =
Rn

Zg

f (z)?(z ? z0 ) =
R

f (z)
? d?,
|?g(z)|

where ?g is the gradient of g and Zg = {z ? Rn : g(z) = 0} is the zero set of g with
Lebesgue measure ? [12].
Of course, the Taylor expansion is valid only for a differentiable function, otherwise the
error terms can be arbitrarily large. If, however, we find and use a suitable continuous and

differentiable approximation rather than the discrete sign function, the above expansion
becomes more appropriate. There exists a trade-off, however, between the quality of the
sign function approximation (a better sign function should yield a better approximation in
Equation 4) and the error caused by the o() term in Equation 6 (a better sign function
approximation will have a greater error when the higher-order Taylor terms are dropped).
We therefore expect that ideally there will exist an optimal balance between the two. The
shifted sigmoid with free parameter ? (defined and used below) allows us to easily explore
this trade-off simply by varying ?.
Retaining the first-order Taylor term, and applying this to the left side of Equation 5,
EPij sgn [Lij (x) ? ?ij ] ? EPij sgnLij (x) ? 2EPij ?ij ? [Lij (x)] .
The distribution under which the expectation in Equation 5 is taken can also influence our
results. If it is known that the true class of x is always ci , the ci -conditional distribution
(i)
should be used, i.e., Pij (x) = P (x|ci ), yielding a class-conditional correction term ?ij ,
(i)
(i)
? ij (x)+? . The symmetric
and a class-conditional likelihood-ratio correction Sij (x) = L
ij
case arises when x is of class cj . If, on the other hand, neither ci nor cj is the true classes
(i.e., x is sampled from some other class-conditional distribution, say P (x|ck ), k 6= i, j),
it does not matter which distribution for Pij (x) is used since, for a given comparison order
in a game playing strategy, the current winner will ultimately play using the true class
distribution P (x|ck ) of x (when one of i or j will equal k). It is therefore valid to consider
only the case when either x is of class ci (we denote this event by Ci (x)) or when x is of
class cj (event Cj (x)). Note that these two events are disjoint.
In practice, however, we do not know which of the two events is correct. The ideal choice
in either case can be expressed using indicators as follows:
(i)

(j)

Aij (x) = ?ij 1{Ci (x)} + ?ij 1{Cj (x)} .
Taking the expected value of Aij (X) with respect to p(x|Ci (x) ? Cj (x)) yields
(i)

(j)

?ij P (ci ) + ?ij P (cj )
?ij = Ep(x|Ci (x)?Cj (x)) [Aij (X)] =
.
P (ci ) + P (cj )
? ij (x) + ?ij that is obtained simply
This results in a single likelihood correction Sij (x) = L
by integrating in Equation 5 with respect to the average distribution over class ci and cj ,
i.e.,
P (ci )P (x|ci ) + P (cj )P (x|cj )
?
Pij (x) = p(x|Ci (x) ? Cj (x)) =
.
P (ci ) + P (cj )
With these assumptions, and supposing the zero set ZLij = {x ? Rn : P (x|ci ) =
P (x|cj )} of Lij (x) is Lebesgue measurable with measure ?, we get:
Z
Z
{sgnLij (x) ? 2?ij ? [Lij (x)]} Pij (x) dx =
sgnLij (x)Pij (x) dx ? 2?(Pi , Pj )?ij ,
Rn

Rn

where
Z
?(Pi , Pj ) =

Z
Pij (x)? [Lij (x)] dx =

Rn

ZLij

Pij (x)
? d?.
|?Lij (x)|

Therefore,
1
?ij =
?(Pi , Pj )

Z ""
Rn

#
? ij (x)
sgnLij (x) ? sgnL
Pij (x) dx.
2

(7)

As can be seen, ?ij is composed of two factors, the integral and the 1/?(Pi , Pj ) factor.
The integral is bounded between -1 and 1 and determines the direction of the correction.
? ij (x) always agree, the integral is zero and there is no correction. The
When Lij (x) and L
? ij is negative
correction favors i when ?ij is positive. This occurs when Lij is positive and L
?
more often than Lij is negative and Lij is positive, a situation improved upon by giving i
?help.? Similarly, when ?ij is negative, the correction biases towards j.
The maximum amount of absolute likelihood correction possible is determined by the (always positive) 1/?(Pi , Pj ) factor. This is affected by two quantities, the mass around and
the log-likelihood ratio gradient at the decision boundary. Low mass at the decision boundary increases the maximum possible correction because any errors in the integral factor
are being de-weighted. High gradient at the decision boundary also increases the maximum possible correction because any decision boundary deviation causes a higher change
in likelihood ratio than if the gradient was low. Since we are correcting the likelihood ratio
directly, this needs to be reflected in ?ij .
When P (x|ci ) and P (x|cj ) are multivariate Gaussians with means ?i and ?j , identical
covariance matrices ?, and equal priors, this becomes:
1

T

?1

e? 8 (?i ??j ) ? (?i ??j )
2?(?i ? ?j )T ??1 (?i ? ?j )

?(Pi , Pj ) = p

As the means diverge from each other, both the mass at the decision boundary decreases
and the likelihood-ratio gradient increases, thereby increasing the maximum amount of
correction.
Unfortunately, it is quite difficult to explicitly evaluate ?(Pi , Pj ) without knowing the true
probability distributions. In this initial work, therefore, our investigations simplify by only
computing the direction and not the magnitude of the correction. As will be seen, this
assumption yields a likelihood-ratio adjustment that is similar in form to our previous KLdivergence based adjustment. More practically, the assumption significantly simplifies the
derivation and still yields reasonable empirical results. Under this assumption, expression
for ?ij becomes:
1
1
? ij (x)].
EP (x) [sgnLij (x)] ? EPij (x) [sgnL
(8)
2 ij
2
The left term on the right of the equality is quite similar to the left difference on the right
of the equality in the KL-divergence case (Equation 2). Again, because we have no information about the true class conditional models, we assume the left term in Equation 8 to
be zero (denote this as assumption B). Comparing this with the corresponding assumption
for the KL-divergence case (assumption A, Equations 2 and 3), it can be shown that 1) they
are not identical in general, and 2) in the Gaussian case, A implies B but not vice versa,
meaning B is weaker than A.
?ij =

Under assumption B, an expression for the resulting ?ij can be derived using the weak law
of large numbers yielding:
?
?
X
X
?
?
P (x|cj )
P (x|ci ) ?
1
?
sgn ln
?
sgn ln
,
(9)
?ij ?
2(Ni + Nj )
P? (x|ci )
P? (x|cj )
x?Ci

x?Cj

where x ? Ci and x ? Cj correspond to the samples as they are classified in a previous
recognition pass; Ni and Nj are number of samples from model ci and cj respectively. One
can immediately see the similarity between this equation and the one using KLD [2].
Like in [2], since the true classes are unknown, we perform a previous classification pass
(e.g., using the original likelihood ratios) to get estimates and use these in Equation 9.

Note that there are three potential sources of error in the analysis above. The first is the
?(Pi , Pj ) factor that we neglected. The second is assumption B, that (since weaker) can
be less severe than in the corresponding KL-divergence case. The third is the error due to
the discontinuity of the sign function. To address the third problem, rather than using the
sign function in Equation 9, we can approximate it with a continuous differential function
with the goal of balancing the trade-off mentioned above. There are a number of possible
sign-function approximations, including hyperbolic and arc tangent, and shifted sigmoid
function, the latter of which is the most flexible because of its free parameter ?.3
Specifically, the sigmoid function has the form f (z) = 1+e1??z , where the free parameter ?
(an inverse temperature) determines how well the curve will approximate the discontinuous
function. Using the sigmoid function, we can approximate the sign function as sgnz ?
2
? 1. Note that the approximation improves as ? increases. Hence,
1+e??z
?
?
 X

X
1
2
2
? . (10)
?
?ij ?
?
1?
1?
? ij (x)
?L
2(Ni + Nj ) x?c
1 + e? L? ji (x)
1
+
e
x?c
j

i

4

Speech Recognition Evaluation

As in previous work [2], we implemented this technique on NYNEX PHONEBOOK [10,
1], a medium vocabulary isolated-word speech corpus. Gaussian mixture hidden Markov
models (HMMs) produced probability scores P? (x|ci ) where here x is a matrix of feature
values (one dimension as MFCC features and the other as time frames), and ci is a word
identity. The HMMs use four hidden states per phone, and 12 Gaussian mixtures per state
(standard for this task [10]). This yields approximately 200k free model parameters in total.
In our experiments, the steps are: 1) calculate P? (x|ci ) using full inference (no Viterbi
approximation) for each test case and for each word; 2) classify the test examples using
? ij = ln P? (x|ci )/P? (x|cj ); 3) using the hypothesized (and
just the log likelihood ratios L
error-full) class labels, calculate the test-set bias term using one of the techniques described
? ij + ?ij . Since
above; and 4) classify again using the augmented likelihood ratio Sij = L
the procedure is no longer transitive, we run 1000 random tournament-style games (as in
[2]) and choose the most frequent winner as the ultimate winner.
Table 1: Word error rates % on speech data with various sign approximations.
SIZE

ORIG

SIGN

TANH

ATAN

SIG (.1)

SIG (1)

SIG (10)

SIG (100)

SIG (200)

SIG (400)

75
150
300
600

2.34
3.31
5.23
7.39

1.76
2.83
4.75
6.64

1.76
2.84
4.75
6.61

1.76
2.83
4.70
6.60

1.82
2.65
4.74
6.66

1.76
2.83
4.75
6.64

1.56
2.65
4.29
6.04

1.57
2.47
3.95
5.70

1.33
2.68
4.34
6.74

1.34
2.43
4.34
6.74

KLD[2]
1.91
2.72
4.29
5.91

The results are shown in Table 1, where the first column gives the test-set vocabulary size
(number of different classes). The second column shows the baseline word error rates
? ij . The remaining columns are the bias-corrected results with various
(WERs) using only L
sign approximations, namely sign (Equation 9), hyperbolic and arc tangent, and the shifted
sigmoid with various ? values (thus allowing us to investigate the trade-off mentioned in
Section 3.1). From the results we can see that larger-? sigmoid is usually better, with
overall performance increasing with ?. This is because with large ?, the shifted sigmoid
curve better approximates the sign function. For ? = 100, the results are even better than
our previous KL-divergence (KLD) results reported in [2] (right-most column in the table).
It can also been seen that when ? is greater than 100, the WERs are not consistently better.
This indicates that the inaccuracies due to the Taylor error term start adversely affecting
the results at around ? = 100.
3

Note that the other soft sign functions can also be defined to utilize a ? smoothness parameter.

5

UCI Dataset Evaluation
Table 2: Error rates in % (and std where applicable) on the UCI data.
data
australian
breast
chess
cleve
corral
crx
diabetes
flare
german
glass
glass2
heart
hepatitis
iris
letter
lymphography
mofn-3-7-10
pima
satimage
segment
shuttle-small
soybean-large
vehicle
vote
waveform-21

NN baseline
16.75(3.51)
2.94(1.16)
0.56
25.67(3.40)
2.44(1.26)
17.41(3.18)
28.04(3.08)
20.98(2.26)
29.96(3.49)
42.16(2.06)
28.82(2.57)
21.83(3.77)
19.46(7.10)
8.13(1.60)
38.66
24.46(4.86)
0
25.96(2.01)
15.80
7.53
0.87
8.47(1.31)
28.39(4.68)
7.40(2.22)
26.21

KLD
16.33(3.66)
2.62(1.15)
0.46
24.35(2.82)
1.82(1.16)
17.25(2.67)
26.88(3.56)
19.37(2.16)
28.54(3.45)
39.63(1.76)
26.23(2.61)
21.48(4.26)
16.10(6.13)
6.84(1.44)
34.66
23.81(4.57)
0
25.22(2.95)
14.25
7.40
0.77
8.29(1.39)
28.15(4.62)
6.94(1.77)
26.17

sign
16.17(3.63)
2.63(1.15)
0.47
24.01(2.27)
1.19(1.16)
17.11(2.91)
27.41(4.13)
18.29(2.25)
28.82(2.53)
41.92(1.92)
26.95(2.65)
21.19(4.52)
17.16(6.92)
6.26(1.47)
37.10
23.29(4.52)
0
24.82(2.87)
14.40
7.27
0.87
7.18(1.08)
27.70(4.44)
6.94(1.77)
26.12

sig(10)
16.32(3.75)
2.65(1.15)
0.37
24.01(3.94)
1.19(1.16)
17.26(3.00)
27.18(1.98)
18.46(1.85)
28.25(3.71)
40.95(2.00)
26.23(2.57)
21.09(4.23)
15.82(6.94)
6.84(1.44)
37.00
23.29(4.86)
0
25.96(2.19)
14.25
7.53
0.77
8.47(1.31)
28.39(4.75)
7.17(2.05)
26.14

NB baseline
14.89(1.97)
2.45(1.93)
12.66
17.91(2.37)
12.77(3.66)
15.05(3.67)
25.71(2.13)
20.24(2.31)
24.58(2.57)
44.12(7.96)
22.36(9.01)
15.50(6.01)
16.18(5.92)
6.99(1.78)
30.68
16.62(8.64)
8.59
25.71(2.13)
19.15
12.21
1.40
8.71(2.70)
38.92(4.47)
9.91(1.72)
21.45

KLD
14.29(2.45)
2.29(2.02)
12.76
15.55(1.81)
9.57(2.12)
14.02(3.91)
24.79(2.68)
19.55(2.63)
26.55(1.88)
42.24(8.64)
21.15(9.25)
15.11(5.34)
18.29(5.96)
6.99(1.78)
30.88
18.27(9.25)
4.57
24.79(2.68)
19.35
11.73
1.41
9.13(2.60)
38.59(5.05)
9.68(2.49)
21.11

sign
14.76(2.45)
2.13(2.07)
13.04
15.22(1.82)
9.57(2.62)
13.06(3.67)
24.24(3.49)
18.70(1.87)
24.79(2.30)
42.06(9.22)
21.77(9.25)
15.11(5.72)
18.04(5.92)
6.99(1.78)
30.48
17.34(8.91)
1.56
24.24(3.49)
19.25
11.82
1.50
8.35(2.65)
38.79(4.46)
9.68(1.72)
20.15

sig(10)
14.76(2.37)
1.86(2.07)
12.85
16.22(2.61)
12.05(4.80)
15.05(3.67)
24.66(2.59)
16.64(2.34)
24.25(2.50)
42.28(7.93)
22.36(9.01)
15.11(6.01)
15.45(4.56)
6.99(1.78)
30.64
15.31(8.91)
3.42
24.66(2.59)
18.70
12.21
1.50
8.37(2.70)
37.84(4.43)
9.68(1.72)
21.40

In order to show that our methodology is general beyond isolated-word speech recognition,
we also evaluated this technique on the entire UCI machine learning repository [9]. In
our experiments, baseline classifiers are built using one of: 1) the Matlab neural network
(NN) toolbox with feed-forward 3-layer perceptrons having different number of hidden
units and training epochs (optimized over a large set to achieve the best possible baseline
for each test case), and trained using the Levenberg-Marquardt algorithm [11], or 2) the
MLC++ toolbox to produce na??ve Bayes (NB) classifiers that have been smoothed using
Dirichlet priors. In each case (i.e., NN or NB), we augmented the resulting likelihood ratios
with bias correction terms thereby evaluating our technique using quite different forms of
baseline classifiers. Unlike the above, with these data sets we have only tried one random
tournament game to decide the winner so far.
For the NN results, hidden units use logistic sigmoid, and output units use a soft-max
function, making the network outputs interpretable as posterior probabilities P (c|x), where
x is the sample and c is the class. While our bias correction described above is in terms
of likelihoods ratios Lij (x), posteriors can be used as well if the posteriors are divided by
the priors giving the relation P (c|x)/P (c) = P (x|c)/p(x) (i.e., scaled likelihoods) which
produces the standard Lij (x) values when used in a likelihood ratio .
As was done in [5], for the small data sets the experimental results use 5-fold crossvalidation using randomly selected chunks ? results show mean and standard deviation
(std) in parentheses. For the larger data sets, we use the same held out training/test sets
as in [5] (so std is not shown). The experimental procedure is similar to that described in
Section 4, except that scaled likelihoods are used for the NN baselines. Again, first-pass
error-full test-set hypothesized answers are used to compute the bias corrections.
Table 5 shows our results for both the NN (columns 2?5) and NB (columns 6?9) baseline classifiers. Within each baseline group, the first column shows the baseline accuracy
(with the 5-fold standard derivations when the data set is small). The second column shows
results using KL-divergence based bias corrections ? these are the first published KLD
results on the UCI data. The third column shows results with sign-based correction (Equation 9), and the forth column shows the sigmoid (? = 10) case (Equation 10).
While not the point of this paper, one immediately sees that the NB baseline results are
often better than the NN baseline results (15 out of 25 times). Using the NN as a baseline,

the table shows that the KLD results are almost always better than the baseline 24 times
(out of 25). Also, the sign correction is better than the baseline 23 out of 25 times, and
the sigmoid(10) results are better 20 times. Also (not shown in the table), we found that
? = 10 is slightly better than ? = 1 but there is no advantage using ? = 100. These results
therefore show that the NN KLD correction typically beats the sign and sigmoid correction,
possibly owing to the error in the Taylor approximation. Using the NB classifier as the
baseline, however, shows not only improved baseline results in general but also that the
sigmoid(10) improves more often. Specifically, the KLD results are better than the baseline
16 times, sign is better than the baseline 18 times, and sigmoid(10) beats the baseline 19
times, suggesting that sigmoid(10) typically wins over the KLD case.

6

Discussion

We have introduced a new necessary intransitive likelihood ratio classifier. This was done
by using sign-based corrections to likelihood ratios and by using continuous differentiable
approximations of the sign function in order to be able to vary the inherent trade-off between sign-function approximation accuracy and Taylor error. We have applied these techniques to both a speech recognition corpus and the UCI data sets, as well as applying
previous KL-divergence based corrections to the latter data. Results on the UCI data sets
confirm that our techniques reasonably generalize to data sets other than speech recognition. This suggests that the framework could be applied to other machine learning tasks.
This work was supported in part by NSF grant IIS-0093430 and IIS-0121396.

References
[1] Jeff Bilmes. Burried Markov models for speech recognition. In IEEE Intl. Conf. on Acoustics,
Speech, and Signal Processing, March 1999.
[2] Jeff Bilmes, Gang Ji, and M. Meil?a. Intransitive likeilhood-ratio classifiers. In Neural Information Processing Systems: Natural and Synthetic, December 2001.
[3] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley and Sons, Inc.,
1991.
[4] Richard O. Duda, Peter E. Hart, and David G. Stork. Pattern Classification. John Wiley and
Sons, second edition, 2001.
[5] Nir Friedman, Dan Geiger, and Moises Goldszmidt. Bayesian network classifiers. Machine
Learning, 29(2-3):131?163, 1997.
[6] D. S. Jones. Generalised Functions. McCraw-Hill Publishing Company Limited, 1966.
[7] J. Kevorkian. Partial Differential Equations: Analytical Solution Techniques. New York:
Springer, 2000.
[8] R. Duncan Luce and Howard Raiffa. Games and Decisions: Introduction and Critical Survey.
Dover, 1957.
[9] P. M. Murphy and D. W. Aha. UCI Repository of Machine Learning Database, 1995.
[10] J. Pitrelli, C. Fong, S. H. Wong, J. R. Spitz, and H. C. Lueng. PhoneBook: a phonetically-rich
isolated-word telephone-speech database. In IEEE Intl. Conf. on Acoustics, Speech, and Signal
Processing, 1995.
[11] W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. Numerical Recipes in C: The
Art of Scientific Computing. Cambridge University Press, Cambridge, England, second edition,
1992.
[12] M. M. Rao. Measure Theory and Integration. John Wiley and Sons, 1987.
[13] P. D. Straffin. Game Theory and Strategy. The Mathematical Association of America, 1993.

"
1701,"Markov Networks for Detecting
Overlapping Elements in Sequence Data

Joseph Bockhorst
Dept. of Computer Sciences
University of Wisconsin
Madison, WI 53706
joebock@cs.wisc.edu

Mark Craven
Dept. of Biostatistics and Medical Informatics
University of Wisconsin
Madison, WI 53706
craven@biostat.wisc.edu

Abstract
Many sequential prediction tasks involve locating instances of patterns in sequences. Generative probabilistic language models, such
as hidden Markov models (HMMs), have been successfully applied
to many of these tasks. A limitation of these models however, is
that they cannot naturally handle cases in which pattern instances
overlap in arbitrary ways. We present an alternative approach,
based on conditional Markov networks, that can naturally represent arbitrarily overlapping elements. We show how to efficiently
train and perform inference with these models. Experimental results from a genomics domain show that our models are more accurate at locating instances of overlapping patterns than are baseline
models based on HMMs.

1

Introduction

Hidden Markov models (HMMs) and related probabilistic sequence models have
been among the most accurate methods used for sequence-based prediction tasks
in genomics, natural language processing and other problem domains. One key
limitation of these models, however, is that they cannot represent general overlaps
among sequence elements in a concise and natural manner. We present a novel
approach to modeling and predicting overlapping sequence elements that is based on
undirected Markov networks. Our work is motivated by the task of predicting DNA
sequence elements involved in the regulation of gene expression in bacteria. Like
HMM-based methods, our approach is able to represent and exploit relationships
among different sequence elements of interest. In contrast to HMMs, however, our
approach can naturally represent sequence elements that overlap in arbitrary ways.
We describe and evaluate our approach in the context of predicting a bacterial
genome?s genes and regulatory ?signals? (together its regulatory elements). Part
of the process of understanding a given genome is to assemble a ?parts list?, often
using computational methods, of its regulatory elements. Predictions, in this case,
entail specifying the start and end coordinates of subsequences of interest. It is
common in bacterial genomes for these important sequence elements to overlap.

(a)

(b)
prom 1

gene1

prom2 prom 3

gene 2

START

END

term 1
prom

gene

term

Figure 1: (a) Example arrangement of two genes, three promoters and one terminator in
a DNA sequence. (b) Topology of an HMM for predicting these elements. Large circles
represent element-specific sub-models and small gray circles represent inter-element submodels, one for each allowed pair of adjacent elements. Due to the overlapping elements,
there is no path through the HMM consistent with the configuration in (a).

Our approach to predicting overlapping sequence elements, which is based on discriminatively trained undirected graphical models called conditional Markov networks [5, 10] (also called conditional random fields), uses two key steps to make a
set of predictions. In the first step, candidate elements are generated by having a set
of models independently make predictions. In the second step, a Markov network
is constructed to decide which candidate predictions to accept.
Consider the task of predicting gene, promoter, and terminator elements encoded in
bacterial DNA. Figure 1(a) shows an example arrangement of these elements in a
DNA sequence. Genes are DNA sequences that encode information for constructing
proteins. Promoters and terminators are DNA sequences that regulate transcription, the first step in the synthesis of a protein from a gene. Transcription begins
at a promoter, proceeds downstream (left-to-right in Figure 1(a)), and ends at a
terminator. Regulatory elements often overlap each other, for example prom2 and
prom3 or gene1 and prom2 in Figure 1.
One technique for predicting these elements is first to train a probabilistic sequence
model for each element type (e.g. [2, 9]) and then to ?scan? an input sequence
with each model in turn. Although this approach can predict overlapping elements,
it is limited since it ignores inter-element dependencies. Other methods, based on
HMMs (e.g. [11, 1]), explicitly consider these dependencies. Figure 1(b) shows an
example topology of such an HMM. Given an input sequence, this HMM defines a
probability distribution over parses, partitionings of the sequence into subsequences
corresponding to elements and the regions between them. These models are not naturally suited to representing overlapping elements. For the case shown in Figure 1(a)
for example, even if the subsequences for gene1 and prom2 match their respective
sub-models very well, since both elements cannot be in the same parse there is a
competition between predictions of gene1 and prom2 . One could expand the state
set to include states for specific overlap situations however, the number of states increases exponentially with the number of overlap configurations. Alternatively, one
could use the factorized state representation of factorial HMMs [4]. These models,
however, assume a fixed number of loosely connected processes evolving in parallel,
which is not a good match to our genomics domain.
Like HMMs, our method, called CMN-OP (conditional Markov networks for overlapping patterns), employs element-specific sub-models and probabilistic constraints
on neighboring elements qualitatively expressed in a graph. The key difference between CMN-OP and HMMs is the probability distributions they define for an input
sequence. While, as mentioned above, an HMM defines a probability distribution
over partitions of the sequence, a CMN-OP defines a probability distribution over
all possible joint arrangements of elements in an input sequence. Figure 2 illustrates
this distinction.

(b) CMN?OP

(a) HMM
predicted labels

sample space

predicted signals

sample space
end position

1

2

3

4

5

6

7

8

1

2

3

4

5

6

7

1

8

2

3

4

5

6

7

8

1
2

3

4

5

6

7

8

start position

2
1

3
4
5
6
7
8

Figure 2: An illustration of the difference in the sample spaces on which probability
distributions over labelings are defined by (a) HMMs and (b) CMN-OP models. The left
side of (a) shows a sequence of length eight for which an HMM has predicted that an
element of interest occupies two subsequences, [1:3] and [6:7]. The darker subsequences,
[4:5] and [8:8], represent sequence regions between predicted elements. The right side of
(a) shows the corresponding event in the sample space of the HMM, which associates one
label with each position. The left side of (b) shows four predicted elements made by a
CMN-OP model. The right side of (b) illustrates the corresponding event in the CMN-OP
sample space. Each square corresponds to a subsequence, and an event in this sample
space assigns a (possibly empty) label to each sub-sequence.

2

Models

A conditional Markov network [5, 10] (CMN) defines the conditional probability
distribution Pr(Y|X) where X is a set of observable input random variables and Y
is a set of output random variables. As with standard Markov networks, a CMN
consists of a qualitative graphical component G = (V, E) with vertex set V and
edge set E that encodes a set of conditional independence assertions along with a
quantitative component in the form of a set of potentials ? over the cliques of G.
In CMNs, V = X ? Y. We denote an assignment of values to the set of random
variables U with u. Each clique, q = (Xq , Yq ), in the clique set Q(G) has a potential
function ?q (xq , yq ) ? ? that assigns a non-negative number to each of the joint
settings of (Xq , Y
Qq ). A CMN (G, ?) defines the conditional
P Q probability distribution
1
0
Pr(y|x) = Z(x)
?
(x
,
y
)
where
Z(x)
=
q
q
q
q?Q(G)
y0
q?Q(G) ?q (xq , y q ) is the
x dependent normalization factor called the partition function. One benefit of
CMNs for classification tasks is that they are typically discriminatively trained by
maximizing a function based on the conditional likelihood Pr(Y|X) over a training
set rather than the joint likelihood Pr(Y, X).
A common representation
for the potentials ?q (yq , xq ) is with a log-linear model:
P
?q (yq , xq ) = exp{ b wqb fqb (yq , xq )} = exp{wqT ? fq (yq , xq )}. Here wqb is the weight
of feature fqb and wq and fq are column vectors of q?s weights and features.
Now we show how we use CMNs to predict elements in observation sequences.
Given a sequence x of length L, our task is to identify the types and locations of
all instances of patterns in P = {P1 , ..., PN } that are present in x where P is a set
of pattern types. In the genomics domain x is a DNA sequence and P is a set of
regulatory elements such as {gene, promoter, terminator}.
A match m of a pattern to x specifies a subsequence xi:j and a pattern type Pk ? P.
We denote the set of all matches of pattern types in P to x with M(P, x). We call a
subset C = (m1 , m2 , ..., mM ) of M(P, x) a configuration. Matches in C are allowed

(a)

(b)

X

PROM
START

Y1

Y2

GENE

TERM
END

YL+1

Figure 3: (a) The structure of the CMN-OP induced for the sequence x of length L. The
ath pattern match Ya is conditionally independent of its non-neighbors given its neighbors
X, Ya?1 and Ya+1 . (b) The interaction graph we use in the regulatory element prediction
task. Vertices are the pattern types along with START and END. Edges connect pattern
types that may be adjacent. Edges from START connect to pattern types that may be
the first matches Edges into END come from pattern types that may be the last matches.

to overlap however, we assume that no two matches in C have the same start index 1 .
Thus, the maximum size of a configuration C is L, and the elements of C may be
ordered by start position such that ma ? ma+1 . Our models define a conditional
probability distribution over configurations given an input sequence x.
Given a sequence x of length L, the output random variables of our models are
Y = (Y1 , Y2 , ..., YL , YL+1 ). We represent a configuration C = (m1 , m2 , ..., mM )
with Y in the following way. If a is less than or equal to the configuration size
M , we assign Ya to the ath match in C (Ya = ma ), otherwise we set Ya equal to a
special value null. Note that YL+1 will always be null; it is included for notational
convenience. Our models define the conditional distribution Pr(Y|X).
Our models assume that a pattern match is independent of other matches given
its neighbors. That is, Ya is independent of Ya0 for a0 < a ? 1 or a0 > a + 1
given X, Ya?1 and Ya+1 . This is analogous to the HMM assumption that the next
state depends only on the current state. The conditional Markov network structure
associated with this assumption is shown in Figure 3(a). The cliques in this graph
are {Ya , Ya+1 , X} for 1 ? a ? L. We denote the clique {Ya , Ya+1 , X} with qa .
We define the clique potential of qa for a 6= 1 as the product of a pattern match
term g(ya , x) and a pattern interaction term h(ya , ya+1 , x). The functions g() and
h() are shared among all cliques so ?qa (ya , ya+1 , x) = g(ya , x) ? h(ya , ya+1 , x) for
2 ? a ? L. The first clique q1 includes an additional start placement term ?(y1 , x)
that scores the type and position of the first match y1 . To ensure that real matches
come before any null settings and that additional null settings do not affect
Pr(y|x), we require that g(null, x) = 1, h(null,null, x) = 1 and h(null,ya ,
x) = 0 for all x and ya 6= null. The pattern match term measures the agreement
between the matched subsequence and the pattern type associated with y a . In
the genomics domain our representation of the sequence match term is based on
regulatory element specific HMMs. The pattern interaction term measures the
compatibility between the types and spacing (or overlap) of adjacent matches.
A Conditional Markov Network for Overlapping Patterns (CMN-OP) = (g, h, ?)
specifies a pattern match function g, pattern interaction function h and
start placement function ? that define the conditional distribution Pr(y|x) =
QL
?(y1 ) QL
1
a=1 ?a (qa , x) = Z(x)
a=1 g(ya , x)h(ya , ya+1 , x) where Z(x) is the normalZ(x)
izing partition function. Using the log-linear representation for g() and h() we have
PL
1)
T
T
Pr(y|x) = ?(y
a=1 wg ? fg (ya , x) + wh ? fh (ya , ya+1 , x)}. Here wg , fg , wh
Z(x) exp{
and fh are g() and h()?s weights and features.
1
We only need to require configurations to be ordered sets. We make this slightly more
stringent assumption to simplify the description of the model.

2.1

Representation

Our representation of the pattern match function g() is based on HMMs. We
construct an HMM with parameters ?k for each pattern type Pk along with a single
background HMM with parameters ?B . The pattern match score of ya 6= null
with subsequence xi:j and pattern type Pk is the odds Pr(xi:j |?k )/ Pr(xi:j |?B ).
We have a feature fgk (ya , x) for each pattern type Pk whose value is the logarithm
of the odds if the pattern associated with ya is Pk and zero otherwise. Currently,
the weights wg are not trained and are fixed at 1. So, wgT ? fg (ya , x) = fgk (ya , x) =
log(Pr(xi:j |?k )/ Pr(xi:j |?B )) where Pk is the pattern of ya .
Our representation of the pattern interaction function h() consists of two components: (i) a directed graph I called the interaction graph that contains a vertex
for each pattern type in P along with special vertices START and END and (ii)
a set of weighted features for each edge in I. The interaction graph encodes qualitative domain knowledge about allowable orderings of pattern types. The value
of h(ya , ya+1 , x) = whT ? fh (ya , ya+1 , x) is non-zero only if there is an edge in I
from the pattern type associated with ya to the pattern type associated with ya+1 .
Thus, any configuration with non-zero probability corresponds to a path through
I. Figure 3(b) shows the interaction graph we use to predict bacterial regulatory
elements. It asserts that between the start positions of two genes there may be no
element starts, a single terminator start or zero or more promoter starts with the
requirement that all promoters start after the start of the terminator. Note that
in CMN-OP models, the interaction graph indicates legal orderings over the start
position of matches not over complete matches as in an HMM.
Each of the pattern interaction features f ? fh is associated with an edge in the
interaction graph I. Each edge e in I has single bias feature feb and a set of distance
features feD . The value of feb (ya , ya+1 , x) is 1 if the pattern types connected by e
correspond to the types associated with ya and ya+1 and 0 otherwise. The distance
features for edge e provide a discretized representation of the distance between (or
amount of overlap of) two adjacent matches of types consistent with e. We associate
each distance feature fer ? feD with a range r. The value of fer (ya , ya+1 , x) is 1 if
the (possibly negative) difference between the start position of ya+1 and the end
position of ya is in r, otherwise it is 0. The set of ranges for a given edge are nonoverlapping. So, h(ya , ya+1 , x) = exp(whT ? fh (ya , ya+1 , x)) = exp(web + wer ) where e
is the edge for ya and ya+1 , web is the weight of the bias feature feb and wer is the
weight of the single distance feature fer whose range contains the spacing between
the matches of ya and ya+1 .

3

Inference and Training

Given a trained model with weights w and an input sequence x, the inference task
is to determine properties of the distribution Pr(y|x). Since the cliques of a CMNOP form a chain we could perform exact inference with the belief propagation (BP)
algorithm [8]. The number of joint settings in one clique grows O(L4 ), however,
giving BP a running time of O(L5 ) and which is impractical for longer sequences.
The exact inference procedure we use, which is inspired the energy minimization
algorithm for pictorial structures [3], runs in O(L2 ) time.
Our inference procedure exploits two properties of our representation of the pattern
interaction function h(). First, we use the invariance of h(ya , ya+1 , x) to the start
position of ya and the end position of ya+1 . In this section, we make this explicit by
writing h(ya , ya+1 , x) as h(k, k 0 , d) where k and k 0 are the pattern types of ya and

ya+1 respectively and d is the distance between (or overlap of if negative) ya and
ya+1 . The second property we use is the fact that the difference between h(k, k 0 , d)
and h(k, k 0 , d + 1) is non-zero only if d is the maximum value of the range of one of
the distance features fer ? feD associated with the edge e = k ? k 0
The inference procedure we use for our CMN-OP models consists of a forward
pass and a backward pass. Due to space limitations, we only describe the key
aspects of the forward pass. The forward pass fills an L ? L ? N matrix F
where we define F (i, j, k) to be the sum of the scores of all partial configura?
?
tions y
?P
that end with
Q y where y is the match of xi:j to Pk : F (i, j,? k) ?
?
? = (y1 , y2 , ..., y ) and
g(y , x) y? ?(y1 , x) ya ?(?y\y? ) g(ya , x)h(ya , ya+1 , x) Here y
\ denotes set difference.
F has a recursive formulation:
?
?
i?1 X
L X
N
?
?
X
F (i, j, k) = gk (y ? , x) ?k (i) +
F (i0 , j 0 , k 0 )h(k 0 , k, i ? j 0 ) .
?
?
0
0
0 0
i =1 j =i k =1

The triple sum is over all possible adjacent previous matches. Due to the first
property of h just discussed, the value of the triple sum for setting F (i, j, k) and
F (i, j 0 , k) is the same for any j 0 . We cache the value of the triple sum in the L ? N
matrix Fin where Fin (i, k) holds the value needed for setting F (i, j 0 , k) for any j 0 .
We begin the forward pass with i = 1 and set the values of F (1, j, k) for all j and
k before incrementing i. After i is incremented, we use the second property of h to
update Fin in time O(N 2 B), which is independent of the sequence length L, where
B is the number of ?bins? used in our discretized represenation of distance. The
overall time complexity of the forward pass is O(LN 2 B + L2 N ). The first term is
for updating Fin and the second term is for the constant time setting of the O(L2 N )
elements of F . If the sequence length L dominates N and B, as it does in the gene
regulation domain, the effective running time is O(L2 ).
Training involves estimating the weights w from a training set D. An element d of
? d ) where xd is a fully observable sequence and y
? d is a partially
D is a pair (xd , y
observable configuration for xd . To help avoid overfitting we assume a zero-mean
Gaussian prior over the weights and optimize the log of the MAP objective function
P
T
following Taskar et al. [10]: L(w, D) = d?D (log Pr(y?d |xd )) ? w2??w
2 .
?L(w,D)
The
=
?w
P value of the gradient ?L(w, D) win the direction of weight w ? w is:
? d ] ? E[Cw |xd ]) ? ?2 where Cw is a random variable representing
d?D (E[Cw |xd , y
the number of times the binary feature of w is 1. The expectation is relative to
Pr(y|x) defined by the current setting of w. The value in the summation is the
? to the
difference in the expected number of times w is used given both x and y
expected number of times w is used given just x. The last term is the shrinking
effect of the prior. With the gradient in hand, we can use any of a number of
optimization procedures to set w. We use the quasi-Newton method BFGS [6].

4

Empirical Evaluation

In this section we evaluate our Markov network approach by applying it to recognize
regulatory signals in the E. coli genome. Our hypothesis is that the CMN-OP
models will provide more accurate predictions than either of two baselines: (i)
predicting the signals independently, and (ii) predicting the signals using an HMM.
All three approaches we evaluate ? the Markov networks and the two baselines ?
employ two submodels [1]. The first submodel is an HMM that is used to predict

(a)

(b)

(c)

Promoters

0.6
0.4

0.4
0.2

0

0
0.2

0.4 0.6
Recall

0.8

1

CMN-OP
HMM
SCAN

0.8

0.6

0.2

0

Overlapping Terminators
1

CMN-OP
HMM
SCAN

0.8
Precision

0.8
Precision

Terminators
1

CMN-OP
HMM
SCAN

Precision

1

0.6
0.4
0.2
0

0

0.2

0.4 0.6
Recall

0.8

1

0

0.2

0.4 0.6
Recall

0.8

1

Figure 4: Precision-recall curves for the CMN-OP, HMM and SCAN models on (a) the
promoter localization task, (b) the terminator localization task and (c) the terminator
localization task for terminators known to overlap genes or promoters.

candidate promoters and the second submodel is a stochastic context free grammar
(SCFG) that is used to predict candidate terminators. The first baseline approach,
which we refer to as SCAN, involves ?scanning? a promoter model and a terminator
model along each sequence being processed, and at each position producing a score
indicating the likelihood that a promoter or terminator starts at that position. With
this baseline, each prediction is made independently of all other predictions. The
second baseline is an HMM, similar to the one depicted in Figure 1(b). The HMM
that we use here, does not contain the gene submodel shown in Figure 1(b) because
the sequences we use in our experiments do not contain entire genes. We have the
HMM and CMN-OP models make terminator and promoter predictions for each
position in each test sequence. We do this using posterior decoding which involves
having a model compute the probability that a promoter (terminator) ends at a
specified position given that the model somehow explains the sequence.
The data set we use consists of 2,876 subsequences of the E. coli genome that
collectively contain 471 known promoters and 211 known terminators. Using tenfold cross-validation, we evaluate the three methods by considering how well each
method is able to localize predicted promoters and terminators in the test sequences.
Under this evaluation criterion, a correct prediction predicts a promoter (terminator) within k bases of an actual promoter (terminator). We set k to 10 for promoters
and to 25 for terminators. For all methods, we plot precision-recall (PR) curves by
P
varying a threshold on the prediction confidences. Recall is defined as T PT+F
N , and
TP
precision is defined as T P +F P , where T P is the number of true positive predictions,
F N is the number of false negatives, and F P is the number of false positives.
Figures 4(a) and 4(b) show PR curves for the promoter and terminator localization
tasks, respectively. For both cases, the HMM and CMN-OP models are clearly
superior to the SCAN models. This result indicates the value of taking the regularities of relationships among these signals into account when making predictions.
For the case of localizing terminators, the CMN-OP PR curve dominates the curve
for the HMMs. The difference is not so marked for promoter localization, however.
Although the CMN-OP curve is better at high recall levels, the HMM curve is
somewhat better at low recall levels. Overall, we conclude that these results show
the benefits of representing relationships among predicted signals (as is done in the
HMMs and CMN-OP models) and being able to represent and predict overlapping
signals. Figure 4(c) shows the PR curves specifically for a set of filtered test sets
in which each actual terminator overlaps either a gene or a promoter. These curves
indicate that the CMN-OP models have a particular advantage in these cases.

5

Conclusion

We have presented an approach, based on Markov networks, able to naturally represent and predict overlapping sequence elements. Our approach first generates a
set of candidate elements by having a set of models independently make predictions.
Then, we construct a Markov network to decide which candidate predictions to accept. We have empirically validated our approach by using it to recognize promoter
and terminator ?signals? in a bacterial genome. Our experiments demonstrate that
our approach provides more accurate predictions than baseline HMM models.
Although we describe and evaluate our approach in the context of genomics, we
believe that it has other applications as well. Consider, for example, the task of
segmenting and indexing audio and video streams [7]. We might want to annotate
segments of a stream that correspond to specific types of events or to particular
individuals who appear or are speaking. Clearly, there might be overlapping events
and appearances of people, and moreover, there are likely to be dependencies among
events and appearances. Any problem with these two properties is a good candidate
for our Markov-network approach.
Acknowledgments
This research was supported in part by NSF grant IIS-0093016, and NIH grants
T15-LM07359-01 and R01-LM07050-01.

References
[1] J. Bockhorst, Y. Qiu, J. Glasner, M. Liu, F. Blattner, and M. Craven. Predicting
bacterial transcription units using sequence and expression data. Bioinformatics,
19(Suppl. 1):i34?i43, 2003.
[2] M. Ermolaeva, H. Khalak, O. White, H. Smith, and S. Salzberg. Prediction of transcription terminators in bacterial genomes. J. of Molecular Biology, 301:27?33, 2000.
[3] P. Felzenszwalb and D. Huttenlocher. Efficient matching of pictorial structures. In
Proc. of the 2000 IEEE Conf. on Computer Vision and Pattern Recognition, 66?75.
[4] Z. Ghahramani and M. I. Jordan. Factorial hidden markov models. Machine Learning,
29:245?273, 1997.
[5] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In Proc. of the 18th Internat. Conf.
on Machine Learning, pages 282?289, Williamstown, MA, 2001. Morgan Kaufmann.
[6] R. Malouf. A comparison of algorithms for maximum entropy parameter estimation.
Sixth workshop on computational language learning (CoNLL), 2002.
[7] National Institute of Standards and Technology. TREC video retrieval evaluation
(TRECVID), 2004. http://www-nlpir.nist.gov/projects/t01v/.
[8] J. Pearl. Probabalistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo, CA, 1988.
[9] A. Pedersen, P. Baldi, S. Brunak, and Y. Chauvin. Characterization of prokaryotic
and eukaryotic promoters using hidden Markov models. In Proc. of the 4th International Conf. on Intelligent Systems for Molecular Biology, pages 182?191, St. Louis,
MO, 1996. AAAI Press.
[10] B. Taskar, P. Abbeel, and D. Koller. Discriminative probabilistic models for relational
data. In Proc. of the 18th International Conf. on Uncertainty in Artificial Intelligence,
Edmonton, Alberta, 2002. Morgan Kaufmann.
[11] T. Yada, Y. Totoki, T. Takagi, and K. Nakai. A novel bacterial gene-finding system
with improved accuracy in locating start codons. DNA Research, 8(3):97?106, 2001.

"
2887,"Hierarchical Fisher Kernels for Longitudinal Data

Zhengdong Lu Todd K. Leen
Dept. of Computer Science & Engineering
Oregon Health & Science University
Beaverton, OR 97006
luz@cs.utexas.edu,tleen@csee.ogi.edu
Jeffrey Kaye
Layton Aging & Alzheimer?s Disease Center
Oregon Health & Science University
Portland, OR 97201
kaye@ohsu.edu

Abstract
We develop new techniques for time series classification based on hierarchical Bayesian
generative models (called mixed-effect models) and the Fisher kernel derived from them.
A key advantage of the new formulation is that one can compute the Fisher information matrix despite varying sequence lengths and varying sampling intervals. This avoids
the commonly-used ad hoc replacement of the Fisher information matrix with the identity which destroys the geometric invariance of the kernel. Our construction retains the
geometric invariance, resulting in a kernel that is properly invariant under change of coordinates in the model parameter space. Experiments on detecting cognitive decline show
that classifiers based on the proposed kernel out-perform those based on generative models
and other feature extraction routines, and on Fisher kernels that use the identity in place
of the Fisher information.

1 Introduction
Time series classification arises in diverse application. This paper develops new techniques based on hierarchical Bayesian generative models and the Fisher kernel derived from them. A key advantage of the
new formulation is that, despite varying sequence lengths and sampling times, one can compute the Fisher
information matrix. This avoids its common ad hoc replacement with the identity matrix. The latter strategy,
common in the biological sequence literature [4], destroys the geometrical invariance of the kernel. Our construction retains the proper geometric structure, resulting in a kernel that is properly invariant under change
of coordinates in the model parameter space.
This work was motivated by the need to classify clinical longitudinal data on human motor and psychometric
test performance. Clinical studies show that at the population level progressive slowing of walking and the
rate at which a subject can tap their fingers are predictive of cognitive decline years before its manifestation
[1]. Similarly, performance on psychometric tests such as delayed recall of a story or word lists( tests
not used in diagnosis), are predictive of cognitive decline [8]. An early predictor of cognitive decline for
individual patients based on such longitudinal data would improve medical care and planning for assistance.
1

Our new Fisher kernels use mixed-effects models [6] as the generative process. These are hierarchical
models that describe the population (consisting of many individuals) as a whole, and variations between
individuals in the population. The population model parameters (called fixed effects), the covariance of
the between-individual variability (the random effects), and the additive noise variance are fit by maximum
likelihood. The overall population model together with the covariance of the random effects comprise a set
of parameters for the prior on an individual subject model, so the fitting scheme is a hierarchical empirical
Bayesian procedure.
Data Description The data for this study was drawn from the Oregon Brain Aging Study (OBAS) [2], a
longitudinal study spanning up to fifteen years with roughly yearly assessment of subjects. For our work,
we grouped the subjects into two classes: those who remain cognitively healthy through the course of the
study (denoted normal), and those who progress to mild cognitive impairment (MCI) or further to dementia
(denoted impaired). Since we are interested in prediction, we retain only data taken prior to diagnosis of
impairment. We use 97 subjects from the normal group and 46 from the group that becomes impaired.
Motor task data included the time (denoted as seconds) and the number of steps (denoted as steps) to walk
9 meters, and the number of times the subject can tap their forefinger, both dominant (tappingD) and nondominant hands (tappingN) in 10 seconds. Psychometric test data include delayed-recall, which measures
the number of words from a list of 10 that the subject can recall one minute after hearing the list, and logical
memory II in which the subject is graded on recall of a story told 15-20 minutes earlier.

2 Mixed-effect Models
2.1 Mixed-effect Regression Models
In this paper, we confine attention to parametric regression. Suppose there are k individuals (indexed by i =
1, . . . , k) contributing data to the sample, and we have observations {tin , yni }, n = 1, . . . , N i as a function
of time t for individual i. The data are modeled as yni = f (tin ; ? i ) + ?in , where ? i are the regression
parameters and ?in is zero-mean white Gaussian noise with (unknown) variance ? 2 . The superscript on the
model parameters ? i indicates that the regression parameters are different for each individual contributing to
the population. Since the model parameters vary between individuals, it is natural to consider them generated
by the sum of a fixed and a random piece: ? i = ? + ? i , where ? i (called the random effect), is assumed
distributed N (0, D) with unknown covariance D. The expected parameter vector ?, called the fixed effect,
determines the model for the population as a whole, and the random effect ? i accounts for the differences
between individuals. This intuition is most precise for the case in which the model is linear in parameters
f (t; ?) = ? T ?(t) = ?T ?(t) + ? T ?(t)
T

(1)
1

where ?(t) = [?1 (t), ?2 (t), ..., ?d (t)] denotes a vector of basis functions . We use M = {?, D, ?} to
denote the mixed-effect model parameters. The feature values, observation times, and observation noise are
i T
yi ? [y1i , ? ? ? , yN
i] ,

ti ? [ti1 , ? ? ? , tiN i ]T ,

?i ? [?i1 , ? ? ? , ?iN i ]T .

2.2 Maximum Likelihood Fitting
Model fitting uses the entire collection of data {ti , yi }, i = 1, . . . , k to determine the parameters M =
{?, D, ?} by maximum likelihood. The likelihood of the data {ti , yi } given M is
Z
p(yi ; ti , M) =
p(yi | ? i ; ti , ?)p(? i | M)d? i
(2)
=

(2?)?N

i

/2

|?i |?1/2 exp((yi ? ?T ?(ti ))T (?i )?1 (yi ? ?T ?i (ti )))

where
1

More generally, the fixed and random effects can be associated with different basis functions.

2

(3)

Seconds: Impaired

3.5

3.5

2.5
2

3

# of words

3

logical memory II: Normal

2.5
2

10

10

8

8

6
4

1.5

1.5

2

1

1

0

70

80

90

100

Age

70

80

90

100

logical memory II: Impaired

# of words

4

log(seconds)

log(seconds)

Seconds: Normal
4

6
4
2

70

80

90

100

0

70

80

Age

Age

90

100

Age

Figure 1: The fit mixed-effect models for two tests.
p In each panel, the red line stands for the fixed effect
?T ?(t). The two green lines stand for ?T ?(t) ? ?T (t)D?(t), i.e., the population model ? the s.t.d. of
the deviation from the uncertainty of the ?. The black dash line is the s.t.d of the deviation when we consider
the observation noise.
i

i

? =

N
X

?(tin )D?(tin )T + ? 2 I,

and

?(ti ) = [?(ti1 ), ?(ti2 ), ? ? ? , ?(tin )]T .

n=1

The data likelihood for Y = {y1 , y2 , ? ? ? , yk } with T = {t1 , t2 , ? ? ? , tk } is then p(Y; T, M) =
Qk
i i
The maximum likelihood values of {?, D, ?} are found using the Expectationi=1 p(y | t ; M).
Maximization algorithm [6] with {? 1 , ? 2 , ? ? ? , ? k } considered as the latent variable:
E-step:
M-step:

Q(M, Mg ) = E{? i } (log p(Y, {? i }; T, M)|Y; T, Mg )
g

M = arg max Q(M, M ),
M

(4)
(5)

where Mg stands for the model parameters estimated in previous step, and the expectation in the E-step is
with respect to the posterior distribution of on {? i } when Y is known and the model parameter is Mg . For
the linear mixed-effect model in Equation (1), the M-step can be given in a closed form. The details of the
updating equations are given by Laird et al. [6].
We use the linear mixed-effect model with polynomial basis functions
?(t) = [1, t]T . We trained separate mixed-effect models for each of the
six measurements. For the four motor behavior measurements, we use
the logarithm of data to reduce the skew of the residuals. Figure 1 shows
the fit models for seconds and logical memory II, as the representatives of
the six measurements. The plots show the fixed effect regression ?T ?(t)
(red curve), and the standard deviations arising from the random effects
(green curves) and measurement noise (dashed black curve, see caption).
The data are the blue spaghetti plots. The plots confirm that subjects that
become impaired deteriorate faster than those who remain healthy.
With multiple classes (or component subpopulations), it is natural to use
a mixture of mixed-effect models. We have two components: one fit on
the normal group (denoted M0 ) and one fit on impaired group (denoted
f =
M1 ), with Mm = {?m , Dm , ?m }, m = 0, 1. Here, we use M
Figure 2: The graphical model of
{?0 , M0 , ?1 , M1 } to denote the parameters of this mixture, with ?0 and the mixture of mixed-effect models.
?1 being the mixing proportions (prior) estimated from the training data.
The overall generative process for any individual (ti , yi ) is summarized
in Figure 2. Here z i ? {0, 1} is the latent variable indicating which model component is used to generate
yi .
3

3 Hierarchical Fisher Kernel
3.1 Fisher Kernel Background
The Fisher kernel [4] provides a way to extract discriminative features from the generative model. For any
?-parameterized model p(x; ?), the Fisher kernel between xi and xj is defined as
K(xi , xj ) = (?? log p(xi ; ?))T I?1 ?? log p(xj ; ?),
where I is the Fisher information matrix with the (n, m) entry
Z
? log p(x; ?) ? log p(x; ?)
In,m =
p(x; ?)dx.
??n
??m
x

(6)

(7)

The kernel entry K(xi , xj ) can be viewed as the inner product of the natural gradient I?1 ?? log p(x; ?) at
xi and xj with metric I, and is invariant to re-parametrization of ?. Jaakkola and Haussler [4] prove that a
linear classifier based on the Fisher kernel performs at least as well as the generative model.
3.2 Retaining the Fisher Information Matrix
In the bioinformatics literature [3] and for longitudinal data such as ours, p(xi ; ?) is different for each
individual owing to different sequence lengths, and (for longitudinal data) different sampling times ti . The
integral in Equation (7) must therefore include the distribution sequence lengths and observation times.
Where only sequence lengths differ, an empirical average can be used. However where observation times
are non-uniform and vary considerably between individuals (as is the case here), there is insufficient data to
form an estimate by empirical averaging.
The usual response to the difficulty is to replace the Fisher information with the identity matrix [4]. This
spoils the geometric structure, in particular the invariance of the the kernel K(xi , xj ) under change of coordinates in the model parameter space (model re-parameterization). This is a significant flaw: the coordinate
system used to describe the model is immaterial and should not influence the value of K(xi , xj ). For probabilistic kernel regression, the choice of metric is immaterial in the limit of large training sets [4]. However for
our application, which uses a support vector machine (SVM), we found the difference cannot be neglected.
In our case, replacing Fisher information matrix with the identity matrix is grossly unsuitable. For the
mixed-effect model with polynomial basis functions the Fisher score components associated with higher
order terms (such as slope and curvature) are far larger than the entries associated with lower order term
(such as intercept). Without the proper normalization provided by the Fisher information matrix, the kernel
will be dominated by higher order entries2 . A principled extension of the Fisher kernel provided by our
hierarchical model allows proper calculation of the Fisher information matrix.
3.3 Hierarchical Fisher Kernel
Our design of kernel is based on the generative hierarchy of mixture of mixed-effect models, in Figure 2. We
notice that the individual-specific information ti enter into this generative process at the last step, but the ?la? = {?0 , ?0 , D0 , ?1 , ?1 , D1 },
tent? variables ? i and z i are drawn from the Gaussian mixture model (GMM) ?
e = ?zi p(?zi ; ?zi , Dzi ).
with p(z i , ? i ; ?)
We can thus build a standard Fisher kernel for the latent variables, and use it to induce a kernel on the
observed data. Denoting the latent variables by v i , the Fisher kernel between v i and v j is
K(v i , v j ) = (?? log p(v i ; ?))T (Iv )?1 ?? log p(v j ; ?),
2

Our experiments on the OBAS data show that replacing the Fisher information with the identity compromises
classifier performance.

4

i ?
where the Fisher score ??
? log p(v ; ?) is a column vector

? log p ? log p ? log p ? log p ? log p ? log p T
i ?
;
] ,
??
;
;
;
;
? log p(v ; ?) = [
??0
??0
?D0
??1
??1
?D1
and Iv is the well-defined Fisher information matrix for v:
Z
?
? ? log p(v; ?)
? log p(v; ?)
?
p(v|?)dv.
Ivn,m =
?n
?m
??
??
v

(8)

The kernel for yi and yj is the expectation of K(v i , v j ) given the observation yi and yj .
ZZ
j
f =
f
f i dv j
K(yi , yj ) = Evi ,vj [K(v i , v j )| yi , yj ; ti , tj , M]
K(v i , v j )p(v i | yi ; ti , M)p(v
| yj ; tj , M)dv
With different choices of latent variable v, we have three kernel design strategies in the following subsections. This extension to the Fisher kernel, named hierarchical Fisher kernel (HFK), enables us to deal with
time series with irregular sampling and different sequence lengths. To our knowledge it has not been reported
elsewhere in the literature.
Design A: v i = ? i
This kernel design marginalizes out the higher level variable {z i } and constructs Fisher kernel between the
{? i }. This generative process is illustrated in Figure 3 (left panel), which is the same graphical model in
Figure 2 with latent variable z i marginalized out3 . The Fisher kernel for ? is
i ? T ? ?1
i ?
K(? i , ? j ) = (??
??
? log p(? |?)) (I )
? log p(? |?).

(9)

The kernel between yi and yj as the expectation of K(? i , ? j ):
f
K(yi , yj ) = E? i ,? j (K(? i , ? j )| yi , yj ; ti , tj , M)
(10)
Z
Z
j
i ?
i
i i f
i T ? ?1
j ?
j
j j f
= ( ??
??
? log p(? |?)p(? | y ; t , M)d? ) (I )
? log p(? |?)p(? | y ; t M)d? . (11)
R
j ?
j
j j f
j
The computational drawback is that the integral required to evaluate ??
? log p(? |?)p(? | y ; t M)d?
and Ir do not have an analytical solution. In our experiments, we estimated the integral with Monte-Carlo
sampling.
Design B: v i = (z i , ? i )
This design strategy takes both ? i and z i as joint latent variable and build a Fisher kernel for them. The
generative process, as summarized in Figure 3 (middle panel), gives the probability for latent variables
? = ?zi p(?i ; ?zi , Dzi ).
p(z i , ? i ; ?)
The Fisher kernel for the joint variable (? i , z i ) is
i
i ? T z,? ?1
i
i ?
K((z i , ? i ), (z j , ? j )) = (??
) ??
? log p(z , ? ; ?)) (I
? log p(z , ? ; ?),

(12)

? It can be shown that
where Iz,? is the Fisher information matrix associated with distribution p(z, ?; ?).
K((z i , ? i ), (z j , ? j )) =

1
?(z i , z j )(1 + Kzi (? i , ? j ))
?zi

3

Strictly speaking, we cannot sum out z i at this step since the group membership is used later in generating the
observation noise. However this is a reasonable approximation since the noise variance from M0 and M1 are similar.

5

where Km (? i , ? j ) is the Fisher kernel for ? i associated with component m (= 0, 1)
i
Km (? i , ? j ) = (??m log p(? i ; ?m , Dm ))T I?1
m ??m log p(? ; ?m , Dm ),

(13)

The kernel for yi and yj is defined similarly as in Design A:
K(yi , yj ) =

f
Ezi ,? i ,zj ,? j (K((z i , ? i ), (z j , ? j ))| yi , yj ; ti , tj , M)

(14)

where the integral can be evaluated analytically.
f = Mm , m = 0, 1
Design C: M
This design uses one mixed-effect component instead of the mixture as the generative model, as illustrated in Figure 3 (right panel). Although any
single Mm is not a satisfying generative model
for the whole population, the resulting kernel is
still useful for classification as follows. For either
model, m = 0, 1, the Fisher score for the ith individual ??m log p(? i ; ?m ) describes how the probability p(? i ; ?m ) responds to the change of parameters ?m . This is a discriminative feature vector
since the likelihood of ?i for individuals from difDesign A
Design B
Design C
ferent group are likely to have different response to
the change of parameters ?m . The kernel between Figure 3: The graphical model of the mixture of
? i and ? j is Km (? i , ? j ) defined in Equation (13). mixed-effect models for Design A, B, and C.
And then the kernel for yi and yj :
K(yi , yj )

= E? i ,? j (K(? i , ? j )| yi , yj ; ti , tj , Mm )

(15)

Our experiments show that the kernel based on the impaired group is significantly better than others; we
therefore use this kernel as the representative of Design C. It is easy to see that the designed kernel is a
special case of Design A or Design B when ?0 = 1 and ?1 = 0.
3.4 Related Models
Marginalized Kernel Our HFK is related to the marginalized kernel (MK) proposed by Tsuda et. al. [10].
MK uses a distribution with discrete latent variable h (indicating the generating component) and observable
x, which form a complete data pair x = (h, x). The kernel for observable xi and xj is defined as
XX
e i , xj ) =
e i , xj )
K(x
P (hi |xi )P (hj |xj )K(x
(16)
hi

hj

e i , xj ) is the joint kernel for complete data. Tsuda et. al. [10] uses the form:
where K(x
e i , xj ) = ?(hi , hj )Khi (xi , xj ),
K(x
i

j

i

(17)

where Khi (x , x ) is the pre-defined kernel for observables associated the h generative component. Equae i , xj ) takes the value of kernel defined for the mth component model if xi and xj
tion (17) says that K(x
e i , xj ) = 0. HFK can be viewed as a
are generated from the same component hi = hj = m; otherwise, K(x
special case of the generalized marginalized kernel that allows continuous latent variables h. This is clear if
we re-write Equation (16) as
e i , xj ) = Ehi ,hj (K(x
e i , xj )|xi , xj )
K(x
i
j
e , x ) as a generalization of kernel between hi and hj . Nevertheless HFK is different from the
and view K(x
original work in [10], in that MK requires existing kernels for observable, such as Kh (xi , xj ) in Equation
(17). In our problem setting, this kernel does not exist due to the different lengths of time series.
6

Probability Product Kernel We can get a family of kernels by employing various kernel designs
of K(v i , v j ). The simplest example is to let K(v i , v j ) = ?(z i , z j ), which immediately leads to
f = P P (z i = m|yi ; ti , M)P
f (z j = m|yj ; tj , M),
f
K(yi , yj ) = Evi ,vj (K(v i , v j )| yi , yj ; ti , tj , M)
m
which is obviously related to the posterior probabilities of samples, and is essentially a special case of the
probability product kernels [5] proposed by Jebara et. al.

4 Experiments
Performance Evaluation We use the empirical ROC curve (detection rate vs. false alarm rate) to evaluate classifiers. We compare different classifiers using the area under the curve (AUC), and calculate the
statistical significance following the method given by Pepe [9]. We tested the classifiers on the five features:
steps, seconds, tappingD, tappingN, and logical memory II. The results of delayed-recall are omitted, they are
very close to those for logical memory II. The mixed-effect models for each feature were trained separately
with order-1 polynomials (linear) as the basis functions. For each feature, the kernels are used in support
vector machines (SVM) for classification, and the ROC is obtained by thresholding the classifier output with
varying values. The classifiers are evaluated by leave-one-out cross-validation, the left-out sample consisting
of an individual subject?s complete time series (which is also held out of the fitting of the generative model).
Classifiers For comparison, we also examined the following two classifiers. First, we consider the likelihood ratio test based on mixed-effect models {M0 , M1 }. For any given observation (t, y), the likelihood
that it is generated by mixed-effect model Mm is given by p(y; t, Mm ), which is defined similarly as in
Equation (3). The classification decision for a likelihood ratio classifier is made by thresholding the ratio
p(y;t,M0 )
p(y;t,M1 ) . Second, we consider a feature extraction routine independent of any generative model. We summarize each individual i with the least-square fit coefficients for a d-degree polynomial regression model,
denoted as pi . To get a reliable fitting we only consider the case d = 1 since many individuals only have four
? i , as the feature vector,
or five observations. We use the coefficients (normalized to their s.t.d.), denoted as p
i
j 2
||?
p ??
p ||
and build a RBF kernel Gij = exp(? 2s2 2 ), where s is the kernel width estimated with leave-one-out
cross validation in our experiment. The obtained kernel matrix G will be referred to as LSQ kernel.
Results We first compare three HFK designs, using the ROC curves plotted in Figure 4 (upper row). On
all four motor tests, Design A and Design B are very much comparable except on tappingD, on which
Design A is marginally better than Design B with p = 0.136. Also on the motor tests, Design C is slightly
but consistently better than other two designs. On logical memory II (story recall), the three designs have
comparable performance. We thus use Design C as the representative of HFK, and compare it with the
likelihood ratio classifier and SVM based on LSQ kernel, as shown in Figure 4 (lower row). On four motor
test, the classifier based on HFK obviously out-performs the other two classifiers, and on logical memory II,
the three classifiers have very much comparable performance.

5 Discussion
Fisher kernels derived from mixed-effect generative models retain the Fisher information matrix, and hence
the proper invariance of the kernel under change of coordinates in the model parameter space. In additional
experiments, classifiers constructed with the proper kernel out-perform those constructed with the identity
matrix in place of the Fisher information on our data. For example, on seconds, the HKF (Deign C) achieves
AUC = 0.7333, while the Fisher kernel computed with the identity matrix as metric on p(yi ; ti , M) achieves
a AUC = 0.6873, with the p-value (Z-test) 0.0435.
Our classifiers built with Fisher kernels derived from mixed-effect models outperform those based solely
on the generative model (using likelihood ratio tests) for the motor task data, and are comparable on the
psychometric tests. The hierarchical kernels also produce better classifiers than a standard SVM using the
coefficients of a least squares fit to the individual?s data. This shows that the generative model provides real
advantage for classification. The mixed-effect models capture both the population behavior (through ?),
and the statistical variability of the individual subject models (through the covariance of ?). Knowledge of
7

steps
p1 =0.486, p2 =0.326
1

DesignA
DesignB
DesignC

0.9

DesignA
DesignB
DesignC

0.9

tappingN
p1 =0.482, p2 =0.286
1

DesignA
DesignB
DesignC

0.9

0.9

logical memory II
p1 =0.491, p2 =0.452
1

DesignA
DesignB
DesignC

0.7

0.7

0.7

0.7

0.7

0.5
0.4

0.6
0.5
0.4

0.6
0.5
0.4

Detection Rate

0.8

Detection Rate

0.8

Detection Rate

0.8

0.6

0.6
0.5
0.4

0.6
0.5
0.4

0.3

0.3

0.3

0.3

0.3

0.2

0.2

0.2

0.2

0.2

0.1

0.1

0.1

0.1

0.2

0.4
0.6
False Alarm Rate

0.8

0
0

1

p1 =0.041, p2 =0.038

0.4
0.6
False Alarm Rate

0.8

0
0

1

p1 =0.056, p2 =0.083

1
0.9

0.2

0.9

0.4
0.6
False Alarm Rate

0.8

0
0

1

p1 =0.042, p2 =0.085

1
DesignC
LSQK
LKHD

0.2

0.9

0.1
0.2

0.4
0.6
False Alarm Rate

0.8

0
0

1

p1 =0.38, p2 =0.049

1
DesignC
LSQK
LKHD

0.9

0.9

0.7

0.7

0.7

0.7

0.7

0.4

0.6
0.5
0.4

0.6
0.5
0.4

Detection Rate

0.8

Detection Rate

0.8

Detection Rate

0.8

Detection Rate

0.8

0.5

0.6
0.5
0.4
0.3

0.3

0.2

0.2

0.2

0.2

0.2

0.1

0.1

0.1

0.1

0.8

1

0
0

0.2

0.4
0.6
False Alarm Rate

0.8

1

0
0

0.2

0.4
0.6
False Alarm Rate

0.8

1

0
0

DesignC
LSQK
LKHD

0.4

0.3

0.4
0.6
False Alarm Rate

1

0.5

0.3

0.2

0.8

0.6

0.3

0
0

0.4
0.6
False Alarm Rate

1
DesignC
LSQK
LKHD

0.8

0.6

0.2

p1 =0.485, p2 =0.523

1
DesignC
LSQK
LKHD

DesignA
DesignB
DesignC

0.9

0.8

0
0

Detection Rate

tappingD
p1 =0.136, p2 =0.210
1

0.8

Detection Rate

Detection Rate

seconds
p1 =0.387, p2 =0.158
1

0.1
0.2

0.4
0.6
False Alarm Rate

0.8

1

0
0

0.2

0.4
0.6
False Alarm Rate

0.8

1

Figure 4: Comparison of classifiers. Upper row: Three HFK designs. The number in the parenthesis is the
p-value (Z-test) for the null-hypothesis ?the AUC of Classifier 1 is the same as the AUC of Classifier 2?.
Upper row: Three HKF designs. p1 : Design A vs. Design B, p2 : Design C vs. Design A; Lower row:
HFK & other classifiers. p1 : Design C vs. Likelihood ratio, p2 : Design C vs. LSQ kernel.
the statistics of the subject variability is extremely important for classification: although not discussed here,
classifiers based only on the population model (?) perform far worse than those presented here [7].
Acknowledgements
This work was supported by Intel Corp. under the OHSU BAIC award. Milar Moore and to Robin Guariglia
of the Layton Aging & Alzheimer?s Disease Center gave invaluable help with data from the Oregon Brain
Aging Study. We thank Misha Pavel, Tamara Hayes, and Nichole Carlson for helpful discussion.

References
[1] R. Camicioli, D. Howieson, B. Oken, G. Sexton, and J. Kaye. Motor slowing precedes cognitive impairment in the
oldest old. Neurology, 50:1496?1498, 1998.
[2] M. Green, J. Kaye, and M. Ball. The Oregon brain aging study: Neuropathology accompanying healthy aging in
the oldest old. Neurology, 54(1):105?113, 2000.
[3] T. Jaakkola, M. Diekhaus, and D. Haussler. Using the fisher kernel method to detect remote protein homologies.
7th Intell. Sys. Mol. Biol., pages 149?158, 1999.
[4] T. Jaakkola and D. Haussler. Exploiting generative models in discriminative classifiers. Technical report, Dept. of
Computer Science, Univ. of California, 1998.
[5] T. Jebara, R. Kondor, and A. Howard. Probability product kernels. Journal of Machine Learning Research, 5:819?
844, 2004.
[6] N. Laird and J. Ware. Random-effects models for longitudinal data. Biometrics, 38(4):963?974, 1982.
[7] Z. Lu. Constrained Clustering and Cognitive Decline Detection. PhD thesis, OHSU, 2008.
[8] S. Marquis, M. Moore, D. Howieson, G. Sexton, H. Payami, J. Kaye, and R. Camicioli. Independent predictors of
cognitive decline in healthy elderly persons. Arch. Neurol., 59:601?606, 2002.
[9] M. Pepe. The Statistical Evaluation of Medical Tests for Classification and Prediction. Oxford University Press,
Oxford, 2003.
[10] K. Tsuda, T. Kin, and K. Asai. Marginalized kernels for biological sequences. Bioinformatics, 1(1):1?8, 2002.

8

"
3926,"Slice Normalized Dynamic Markov Logic Networks

Tivadar Papai
Henry Kautz
Daniel Stefankovic
Department of Computer Science
University of Rochester
Rochester, NY 14627
{papai,kautz,stefanko}@cs.rochester.edu

Abstract
Markov logic is a widely used tool in statistical relational learning, which uses
a weighted first-order logic knowledge base to specify a Markov random field
(MRF) or a conditional random field (CRF). In many applications, a Markov logic
network (MLN) is trained in one domain, but used in a different one. This paper focuses on dynamic Markov logic networks, where the size of the discretized
time-domain typically varies between training and testing. It has been previously
pointed out that the marginal probabilities of truth assignments to ground atoms
can change if one extends or reduces the domains of predicates in an MLN. We
show that in addition to this problem, the standard way of unrolling a Markov logic
theory into a MRF may result in time-inhomogeneity of the underlying Markov
chain. Furthermore, even if these representational problems are not significant for
a given domain, we show that the more practical problem of generating samples
in a sequential conditional random field for the next slice relying on the samples
from the previous slice has high computational cost in the general case, due to the
need to estimate a normalization factor for each sample. We propose a new discriminative model, slice normalized dynamic Markov logic networks (SN-DMLN),
that suffers from none of these issues. It supports efficient online inference, and
can directly model influences between variables within a time slice that do not
have a causal direction, in contrast with fully directed models (e.g., DBNs). Experimental results show an improvement in accuracy over previous approaches to
online inference in dynamic Markov logic networks.

1

Introduction

Markov logic [1] is a language for statistical relational learning, which employs weighted first-order
logic formulas to compactly represent a Markov random field (MRF) or a conditional random field
(CRF). A Markov logic theory where each predicate can take an argument representing a time point
is called a dynamic Markov logic network (DMLN). We will focus on two-slice dynamic Markov
logic networks, i.e., ones in which each quantified temporal argument is of the form t or t + 1, in
the conditional (CRF) setting. DMLNs are the undirected analogue of dynamic Bayesian networks
(DBN) [13] and akin to dynamic conditional random fields [19].
DMLNs have been shown useful for relational inference in complex dynamic domains; for example,
[17] employed DMLNs for reasoning about the movements and strategies of 14-player games of
Capture the Flag. The usual method for performing offline inference in a DMLN is to simply unroll
it into a CRF and employ a general MLN or CRF inference algorithm. We will show, however, that
the standard unrolling approach has a number of undesirable properties.
The first two negative properties derive from the fact that MLNs are in general sensitive to the
number of constants in each variable domain [6]; and so, in particular cases, unintuitive results can
occur when the length of training and testing sequences differ. First, as one increases the number
of time points in the domain, the marginals can fluctuate, even if the observations have little or no
influence on the hidden variables. Second, the model can become time-inhomogeneous, even if the
ground weighted formulas between the time slices originate from the same weighted first-order logic
formulas.
The third negative property is of greater practical concern. In domains where there are a large number of variables within each slice dynamic programming based exact inference cannot be used. When
1

the number of time steps is high and/or online inference is required, unrolling the entire sequence
(perhaps repeatedly) becomes prohibitively expensive. Kersting et al. [7] suggests reducing the cost
by exploiting symmetries while Nath & Domingos [14] propose reusing previously sent messages
while performing a loopy belief propagation. Both algorithms are restricted by the capabilities of
loopy belief propagation, which can fail to converge to the correct distribution in MLNs. Geier &
Biundo [2] provides a slice-by-slice approximate inference algorithm for DMLNs that can utilize
any inference algorithm as a black box, but assumes that projecting the distribution over the random
variables at every time step to the product of their marginal distributions does not introduce a large
degree of error ? an assumption that does not always hold. Sequential Monte Carlo methods, or
particle filters, are perhaps the most popular methods for online inference in high-dimensional sequential models. However, except for special cases such as, e.g., the Gaussian distributions used in
[11], sampling from a two-slice CRF model can become expensive, due to the need to evaluate a
partition function for each particle (see Sec. 3 for more details).
As a solution to all of these concerns, we propose a novel way of unrolling a Markov logic theory
such that in the resulting probabilistic model a smaller CRF is embedded into a larger CRF making the clique potentials between adjacent slices normalized. We call this model slice normalized
dynamic Markov logic network (SN-DMLN). Because of the embedded CRF and the undirected
components in our proposed model, the distribution represented by a SN-DMLN cannot be compactly captured by conventional chain graph [10], DBN or CRF graph representations, as we will
explain in Sec. 4. The SN-DMLN has none of the negative theoretical or practical properties outlined above, and for accuracy and/or speed of inference matches or outperforms unrolled CRFs and
the slice-by-slice approximate inference methods. Finally, because the maximum likelihood parameter learning for an SN-DMLN can be a non-convex optimization problem, we provide an effective
heuristic for weight learning, along with initial experimental results.

2

Background

Probabilistic graphical models compactly represent probability distributions using a graph structure that expresses conditional independences among the variables. Directed graphical models are
mainly used in the generative setting, i.e., they model the joint distribution of the hidden variables
and the observations, and during training the joint probability of the training data is maximized.
Hidden Markov models are the prototypical directed models used for sequential data with hidden
and observable parts. It has been demonstrated that for classification problems, discriminative models, which model the conditional probability of the hidden variables given the observations, can
outperform generative models [12]. The main justifications for their success are that complex dependencies between observed variables do not have to be modeled explicitly, and the conditional
probability of the training data (which is maximized during parameter learning) is a better objective
function if we eventually want to use our model for classification. Markov random fields (MRFs)
and conditional random fields (CRFs) belong to the class of undirected graphical models. MRFs
are generative models, while CRFs are their discriminative version. (For a more detailed discussion
of the relationships between these models see [8]). Markov logic [1] is a first-order probabilistic
language that allows one to define template features that apply to whole classes of objects at once.
A Markov logic network is a set of weighted first-order logic formulas and a finite set of constants
C = {c1 , c2 , . . . , c|C| } which together define a Markov network ML,C that contains a binary node
for each possible grounding of each predicate (ground atom) and a binary valued feature for each
grounding of each first-order logic formula. We will also call the ground atoms variables (since they
are random variables). In each truth assignment to the variables, each variable or feature (ground
formula) evaluates to 1 (true) or 0 (false). In this paper we assume function-free clauses and Herbrand interpretations. Using the knowledge base we can either create an MRF or a CRF. If we
instantiate the model as a CRF, the conditional probability of a truth assignment y to the hidden
ground atoms (query atoms) in an MLN, given truth assignment x to the observable ground atoms
(evidence atoms), is defined as:
P
P
exp( i wi j fi,j (x, y))
,
(1)
P r(Y = y|X = x) =
Z(x)
where fi,j (x, y) = 1 if the jth grounding of the ith formula is true under truth assignment {x, y},
and fi,j (x, y) = 0 otherwise. wi is the weight of the ith formula and Z(x) is the normalization
factor. Ground atoms share the same weight if they are groundings of
Pthe same weighted firstorder logic formula, and (1) could be expressed in terms of ni (x, y) = j fi,j (x, y). Instantiation
as an MRF can be done similarly, having an empty set of evidence atoms. Dynamic MLNs [7]
are MLNs with distinguished arguments in every predicate representing the flow of time or some
other sequential quantity. In our settings, Yt and Xt will denote the set of hidden and observable
random variables, respectively, at time t, and Y1:t and X1:t from time step 1 to t. Each set can
contain many variables, and we should note that their distribution will be represented compactly
by weighted first-order logic formulas. The formulas in the knowledge base can be partitioned into
2

two sets. The transitions part contains the formulas for which it is true that for any grounding of
each formula, there is a t such that the grounding shares variables only with Yt and Yt+1 . The
emission part represents the formulas which connect the hidden and observable variables, i.e. Yt and
Xt . We will use P? (Yt , Yt+1 ) (or P? (Yt:t+1 )) and P? (Yt , Xt ) to denote the product of the potentials
corresponding to weighted ground formulas at time t of the transition and the observation formulas,
respectively. Since some ground formulas may contain only variables from Yt ( i.e., defined over
hidden variables within the same slice), in order to count the corresponding potentials exactly once,
we always include their potentials P? (Yt , Yt?1 ), and for t = 1 we have a separate P? (Y1 ). Hence, the
distribution defined in (1) in sequential domains can be factorized as:
Qt
Qt
P?1 (Y1 = y1 ) i=2 P? (Yi?1:i = yi?1:i ) i=1 P? (Yi = yi , Xi = xi )
P r(Y1:t = y1:t |X1:t = x1:t ) =
Z(x1:t )
(2)
In the rest of the paper, we only allow the temporal domain to vary, and the rest of the domains are
fixed.

3

Unrolling MLNs into random fields in temporal domains

We now describe disadvantages of the standard definition of DMLNs, i.e., when the knowledge base
is unrolled into a CRF:
1. As one increases the number of time points the marginals can fluctuate, even if all the clique
potentials P? (Yi = yi , Xi = xi ) in (2) are uninformative.
2. The transition probability Pr(Yi+1 |Yi ) can be dependent on i, even if every P? (Yi =
yi , Xi = xi ) is uninformative and we use the same weighted first-order logic formula
responsible for the ground formulas covering the transitions between every i and i + 1.
3. Particle filtering is costly in general, i.e., if we have the marginal probabilities at time t, we
cannot compute them at time t + 1 using particle filtering unless certain special conditions
are satisfied.
Saying that P? (Yi = yi , Xi = xi ) is uninformative is equivalent to saying that P? (Yi = yi , Xi = xi )
is constant. (Note that, if Yi and Xi are independent, i.e., for some q and r P? (Yi = yi , Xi = xi ) =
r(yi )q(xi ) then q could be marginalized out and r(Yi ) could be snapped to P? (Yi , Yi?1 ) in (2).) To
demonstrate Property 1, consider an unrolled MRF with the temporal domain T = {1, . . . , T },
with only predicate P (t) (t ? T ) and with the weighted formulas (+?, P (t) ? P (t + 1))
(hard constraint) and (w, P (t)) (soft constraint). Because of the hard constraint, only the sequences ?t : P (t) and ?t : ?P (t) have non-zero probabilities. The soft weights imply that
Pr(P (t)) = exp(wT )Pr(?P (t)), i.e., Pr(P (t)) converges to 1, 0 or to 0.5 with exponential rate
depending on the sign of w. But we are not always fortunate to have converging marginals, e.g., if
we change the hard constraint to be P (t) ? ?P (t + 1) and w 6= 0 the marginals will diverge. If
T is even, then for every t ? T , Pr(P (t)) = Pr(?P (t)), since in both sequences P (t) has the same
number of true groundings. If T is odd then for every odd t ? T : Pr(P (t)) = exp(w)Pr(?P (t)).
Consequently, we have diverging marginals as T ? +?. This phenomenon not only makes the
inference unreliable, but a weight learning algorithm that maximizes the log-likelihood of the data
would produce different weights depending on whether T is even or odd. A similar effect arising from moving between different sized domains is discussed in more details in [6]. The akin
Property 2 (inhomogeneity) can be demonstrated similarly, consider, e.g., an MLN with a single
first-order logic formula P (t) ? P (t + 1) with weight w. For the sake of simplicity, assume T = 3.
1+exp(w)
The unrolled MRF defines a distribution where Pr(?P (3)|?P (2)) = 1+2exp(w)+exp(2w)
which is
not equal to Pr(?P (2)|?P (1)) =

1+exp(w)
1+exp(w)+2 exp(2w)

for an arbitrary choice of w.

The examples we just gave involved hard constraints. In fact, we can show that if there are no
hard hard constraints, as T increases the marginals converge and the system becomes homogeneous
(except for a finite number of transitions). Consider the matrix ? s.t. ?i,j = P? (Yt = aj , Yt?1 = ai ),
where ai , i = 1, . . . , N is an enumeration of the all the possible truth assignments within each
slice and N is the number of the possible truth assignments in the slice. Let PrT (Y1 = y1 ) =
P
QT ?1 ?
P
QT ?1 ?
1
y2 ,...,yT
y1 ,...,yT
i=1 P (Yi = yi , Yi+1 = yi+1 ), where Z(Y1:T ) =
i=1 P (Yi =
Z(Y1:T )
yi , Yi+1 = yi+1 ).
Proposition 1. limt?? Prt (Y1 = y) exists if ? is a positive matrix, i.e., ?i, j : ?i,j > 0.
3

Proof. Using ? and the notation ~1 for all one vector and e~i for a vector which has 1 at the ith
component and 0 everywhere else, we can express Prt (Y1 = y) as:
P ?
ei ?t?1~1
y2 P (Y1 = ai , Y2 = y2 )~
(3)
Prt (Y1 = y) =
~1T ?t~1
Since ? is positive we can apply theorem 8.2.8. from [5], i.e., if the spectral radius of ? is ?(?)
(which is always positive for positive matrices): limt?? (??1 (?)?)t = L, where L = xy T , ?x =
?(?)x, ?T y = ?(?)y, x > 0,y > 0 and xT y = 1. Dividing both the numerator and the denominator
by ?t (?) in (3) proves the convergence of Prt (Y1 = y).
The issue of diverging marginals and time-inhomogeneity has not been previously recognized as a
practical problem. However, the increasing interest in probabilistic models that contain large numbers of deterministic constraints (see, e.g. [4]) might bring this issues to the fore. This proposition
can serve as an explanation why in practice we do not encounter diverging marginals in linear chain
type CRFsand why except for a finite number of transitions the model becomes time-homogeneous.
A more significant practical challenge is described by Property 3, the problem of sampling from
Pr(Yt |X1:t = x1:t ) using the previously drawn samples from Pr(Yt?1 |X1:t?1 = x1:t?1 ). In a
directed graphical model (e.g., in a hidden Markov model), following standard particle filter design,
having sampled s1:t?1 ? Pr(Y1:t?1 = s1:t?1 |X1:t?1 = x1:t?1 ), and then using s1:t?1 one would
sample st ? Pr(Yt , Y1:t?1 = s1:t?1 |X1:t?1 ). Since
Pr(Y1:t = s1:t |X1:t?1 = x1:t?1 ) = Pr(Yt = st |Yt?1 = st?1 )Pr(Y1:t?1 = s1:t?1 |X1:t?1 = x1:t?1 )
(4)
we do not have any difficulty performing this sampling step, and all that is left is to re-sample
the collection of s1:t with importance weights Pr(Yt = st |Xt = xt ). The analogue of this process does not work in a CRF in general. If one first draws a sample s1:t?1 ? P? (Y1 , X1 =
Qt?1
x1 )P? (Y1 ) i=2 P? (Yi , Yi?1 )P? (Yi , Xi = xi ), and then draws st ? P? (Yt , Yt?1 = st?1 ), we end
up sampling from:
t
Y
1
s ? P? (Y1 , X1 = x1 )P? (Y1 )
P? (Yi , Yi?1 )P? (Yi , Xi = xi )
(5)
Z
(y
t?1 t?1 )
i=2

P ?
where Zt?1 (yt?1 ) =
yt P (Yt = yt , Yt?1 = yt?1 ). Unless Zt?1 (yt?1 ) is the same for every
yt?1 , it is necessary to approximate Zt?1 (st?1 ) for every st?1 . 1 Although several algorithms have
been proposed to estimate partition functions [16, 18], the partition function estimation can increase
both the running time of the sampling algorithm significantly and the error of the approximation of
the sampling algorithm. While there are restricted special cases where the normalization factor can
be ignored [11], in general ignoring the approximation of Zt?1 (yt?1 ) could cause a large error in
the computed marginals. Consider, e.g., when we have three weighted formulas in the previously
used toy domain, namely, w : ?P (Yt ) ? ?P (Yt+1 ), ?w : P (Yt ) ? ?P (Yt+1 ) and w? : P (Yt ) ?
?P (Yt+1 ), where w > 0 and w? < 0. It can be proved that in this setting using particle filtering in a
CRF without accounting for Zt?1 (yt?1 ) would result in limt?? Pr(P (Yt )) = 21 , while in the CRF
exp(w)
the correct marginal would be limt?? Pr(P (Yt )) = 1 ? 1+exp(w)
exp(w? ) + O(exp(2w? )), which
?
gets arbitrarily close to 1 as we decrease w .

4

Slice normalized DMLNs

As we demonstrated in Section 3, the root cause of the weaknesses
of an ordinarily unrolled CRF
P
lies in that P? (Yt = yt , Yt?1 = yt?1 ) is unnormalized, i.e., yt P? (Yt = yt , Yt?1 = yt?1 ) 6= 1 in
general. One approach to introduce normalization could be to use maximum entropy Markov models
(MEMM) [12]. In that case we would directly represent Pr(Yt |Xt , Yt?1 ), hence we could implement
a sequential Monte Carlo algorithm simply directly sampling st ? Pr(Yt |Xt = xt , Yt?1 = st?1 )
from slice to slice. However, in [9], it was pointed out that MEMMs suffer from the label-bias problem to which as a solution CRFs were invented. Chain graphs (see e.g. [10]) have also the advantage
of mixing directed and undirected components, and would be a tempting choice to use, but they could
only model the transition between slices by either representing (i) Pr(Yt |Xt = xt , Yt?1 = st?1 ),
1
Exploiting inner structure according to the graphical model within the slice would in worst case still result
in computation of the expensive partition function, or could result in a higher variance estimator the same way
as, e.g., using a uniform proposal distribution does.

4

in which case the model would again suffer from the label-bias problem, or (ii) Pr(Yt , Xt |Yt?1 )
or (iii) Pr(Xt |Yt ) and Pr(Yt |Yt?1 ). The defined distributions both in (ii) and (iii) do not give any
advantage performing the sampling step in (4), and similarly to CRFs would require the expensive
computation of a normalization factor. We propose a slice normalized dynamic Markov logic network (SN-DMLN) model, which consists of directed and undirected components on the high level,
and can be thought of as a smaller CRF nested into a larger CRF describing the transition probabilities constructed using weighted first-order logic formulas as templates. SN-DMLNs neither suffer
from the label bias problem, nor bear the disadvantageous properties presented in Section 3. The
distribution defined by an unrolled SN-DMLN is as follows:
Pr(Y1:t = y1:t |X1:t = x1:t ) =

t
Y
1
P1 (Y1 )
P? (Yi = yi , Xi = xi )
Z(x1:t )
i=1
t
Y

(6)

P (Yi = yi |Yi?1 = yi?1 ) ,

i=2

where
P? (Y1 = y1 )
P1 (Y1 = y1 ) = P
,
?
?
y ? P (Y1 = y1 )
1

P? (Yi = yi , Yi?1 = yi?1 )
P (Yi = yi |Yi?1 = yi?1 ) = P
,
?
?
y ? P (Yi = yi , Yi?1 = yi?1 )
i

and the partition function is defined by:
)
(
t
t
Y
Y
X
P (Yi = yi |Yi?1 = yi?1 ) .
P1 (Y1 = y1 )
P? (Yi = yi , Xi = xi )
Z(x1:t ) =
y1 ,...,yt

i=1

i=2

P (Yt = yt |Yt?1 = yt?1 ) is defined by a two-slice Markov logic network (CRF), which describes
the state transitions probabilities in a compact way. If we hide the details of this nested CRF component and treat it as one potential, we could represent the distribution in (6) by regular chain graphs or
CRFs; however we would lose then the compactness the nested CRF provides for describing the distribution. Similarly, we could collapse the variables at every time slice into one and could use a DBN
(or again a chain graph), but it would need exponentially more entries in its conditional probability
tables. If P? (Yi = yi , Xi = xi ) does not have any information content , the probability distribution
Qt
defined in (6) reduces to P1 (Y1 = y1 ) i=2 P (Yi = yi |Yi?1 = yi?1 ), which is a time-homogeneous
Markov chain 2 , hence this model clearly does not have Properties 1 and 2, no matter what formulas
are present in the knowledge base. Furthermore, we do not have to compute the partition function
between the slices, because equation (5) shows, drawing a sample yt ? P? (Yt , Yt?1 = yt?1 ) while
keeping the value yt?1 fixed is equivalent to sampling from P (Yt |Yt?1 = yt?1 ), the quantity present
in equation (6). This means that using our model one can avoid estimating Z(yt?1 ). To learn the
parameters of the model we will maximize the conditional log-likelihood (L) of the data. We use a
modified version of a hill climbing algorithm. The modification is needed, because in our case L is
not necessarily concave. We will partition the weights (parameters) of our model based on whether
they belong to transition or to emission part of the model. The gradient of the L of a data sequence
d = y1 , x1 , . . . , yt , xt w.r.t. an emission parameter we (to which feature ne belongs) is:
"" t
#
t
X
X
?Ld
ne (yi , xi ) ? EP r(Y |X=x)
=
ne (Yi , xi ) ,
(7)
?we
i=1
i=1

which is analogous to what one would expect for CRFs. However, for a transition parameter wtr
(belonging to feature ntr ) we get something different:
t?1
t
X
X
?Ld
EP (Yi+1 |yi ) [ntr (Yi+1 , Yi = yi )]
ntr (yi+1 , yi ) ?
=
?wtr
i=1
i=1
?X
t?1
t?1
h
i?
X
?
? EP r(Y |X=x)
EP (Y?i+1 |Yi ) ntr (Yi+1 , Yi ) .
ntr (Yi+1 , Yi ) ?

(8)

i=1

i=1

(Note that, Ld is concave w.r.t. the emission parameters, i.e., when the transition parameters are
kept fixed, allowing the transition parameters to vary makes Ld no longer concave.) In (8) the first
2
Note that, in the SN-DMLN model the uniformity of P? (Yi = yi , Xi = xi ) is a stronger assumption than
the independence of Xi and Yi .

5

friendships reflect
people?s similarity in
smoking habits
symmetry and
reflexivity of
friendship
persistence of
smoking
people with different smoking
habits hang out separately

Smokes(p1 , t) ? ?Smokes(p2 , t) ? (p1 6= p2 ) ? ?F riends(p1 , p2 , t)
Smokes(p1 , t) ? Smokes(p2 , t) ? (p1 6= p2 ) ? F riends(p1 , p2 , t)
?Smokes(p1 , t) ? ?Smokes(p2 , t) ? (p1 6= p2 ) ? F riends(p1 , p2 , t)
?F riends(p1 , p2 , t) ? ?F riends(p2 , p1 , t)
F riends(p1 , p2 , t) ? F riends(p2 , p1 , t)
F riends(p, p, t)
Smokes(p, t) ? Smokes(p, t + 1)
?Smokes(p, t) ? ?Smokes(p, t + 1)
Hangout(p1 , g1 , t) ? Hangout(p2 , g2 , x) ? Smokes(p1 , t)?
(p1 6= p2 ) ? (g1 6= g2 ) ? ?Smokes(p2 , t)
Hangout(p1 , g1 , t) ? Hangout(p2 , g2 , t) ? ?Smokes(p1 , t)?
(p1 6= p2 ) ? (g1 6= g2 ) ? Smokes(p2 , t)

Table 1: Formulas in the knowledge base
two and the last two terms can be grouped together. The first group would represent the gradient
in the case of uninformative observations, i.e., when the model simplifies to a Markov chain with
a compactly represented transition probability distribution. The second group is the expected value
of the expression in the first group. The first three terms correspond to the gradient of a concave
function; while the fourth term corresponds to the gradient of a convex function, so the function as
a whole is not guaranteed to be maximized by convex optimization techniques alone. Therefore, we
chose a heuristic for our optimization algorithm which gradually increases the effects of the second
group in the gradient. More precisely, we always compute the gradient w.r.t. wo according to (7),
but w.r.t. wtr we use:
t?1
t
X
X
?Ld
EP (Yi+1 |yi ) [ntr (Yi+1 , yi )]
ntr (yi+1 , yi ) ?
=
?wtr
i=1
i=1
?X
t?1
t
h
i?
X
? ?EP r(Y |X=x)
EP (Y?i+1 |Yi ) ntr (Y?i+1 , Yi )
ntr (Yi+1 , Yi ) ?

(9)

i=1

i=1

where ? is kept at the value of 0 until convergence, and then gradually increased from 0 to 1 to
converge to the nearest local optimum. In Section 5, we experimentally demonstrate that this heuristic provides reasonably good results, hence we did not turn to more sophisticated algorithms. The
rationale behind our heuristic is that if P? (Yi = yi , Xi = xi ) had truly no information content, then
for ? = 0 we would find the global optimum, and as we increase ? we are taking into account that
the observations are correlated with the hidden variables with an increasing weight.

5

Experiments

For our experiments we extended the Probabilistic Consistency Engine (PCE) [3], a Markov logic
implementation that has been used effectively in different problem domains. For training, we
used 10000 samples for the unrolled CRF and 100 particles and 100 samples for approximating the conditional expectations in (9) for the SN-DMLN to estimate the gradients. For inference we used 10000 samples for the CRF and 10000 particles for the mixed model. The sampling algorithm we relied on was MC-SAT [15]. Our example training data set was a modified version of the dynamic social network example [7, 2]. The hidden predicates in our knowledge base were Smokes(person, time), F riends(person1 , person2 , time) and the observable
was Hangout(person, group, time). The goal of inference was to predict which people could
potentially be friends, based on the similarity in their smoking habits, which similarity could be inferred based on the groups the individuals hang out. We generated training and test data as follows:
there were two groups g1 , g2 , one for smokers and one for non-smokers. Initially 2 people were
randomly chosen to be smokers and 2 to be non-smokers. People with the same smoking habits
can become friends at any time step with probability 1 ? 0.05?, and a smoker and a non-smoker
can become friends with probability 0.05?. Every 5th time step (starting with t = 0) people hang
out in groups and for each person the probability of joining one of the groups is 1 ? 0.05?. With
probability 1? 0.05?, everyone spends time with the group reflecting their smoking habits, and with
probability 0.05? they go to hang out with the other group. The rest of the days people do not hang
out. The smoking habits persist, i.e., a smoker stays a smoker and a non-smoker stays a non-smoker
at the next time step with probability 1 ? 0.05?. In our two configurations we had ? = 0 (deterministic case) and ? = 1 (non-deterministic case). The weights of the clauses we learned using the
SN-DMLN and the CRF unrolled models are in Table 1.
We used chains with length 5, 10, 20 and 40 as training data, respectively. For each chain we had
40, 20, 10 and 5 examples both for the training and for testing, respectively. In our experiments
we compared three types of inference, and measured the prediction quality for the hidden predicate
F riends by assigning true to every ground atom the marginal probability of which was greater than
6

length

5
10
20
40

SN
1.0
1.0
1.0
1.0

?=0
accuracy
MAR MC-SAT SN
0.40
1.0
1.0
0.40
0.97
1.0
0.40
0.67
1.0
0.85
0.60
1.0

f1
MAR MC-SAT
0.14
1.0
0.14
0.95
0.14
0.49
0.72
0.43

SN
0.84
0.84
0.92
0.88

?=1
accuracy
MAR MC-SAT
SN
0.36
0.81
0.75
0.36
0.77
0.74
0.55
0.66
0.85
0.73
0.59
0.78

f1
MAR
0.10
0.11
0.32
0.55

MC-SAT
0.69
0.61
0.47
0.42

Table 2: Accuracy and F-score results when models were trained and tested on chains with the same
length

(a) ? = 0

(b) ? = 1

Figure 1: F-score of models trained and tested on the same length of data
0.55, and false if its probability was less than 0.45; otherwise we considered it as a misclassification.
Prediction of Smokes was impossible in the generated data set, because the data generation was
symmetric w.r.t to smoking and not smoking, and from the observations we could only tell that
certain pairs of people have similar or different smoking habits, but not who smokes and who does
not. The three methods we compared were (i) particle filtering in the SN-DMLN model (SN), (ii) the
approximate online inference algorithm of [2], which projects the inferred distribution of the random
variables at the previous slice to the product of their marginals, and incorporates this information
into a two slice MLN to infer the probabilities at the next slice (we re-implemented the algorithm
in PCE) (MAR), and (iii) using a general inference algorithm (MC-SAT [15]) for a CRF which is
always completely unrolled in every time step (UNR). In UNR and MAR the same CRF models
were used. The training of the SN-DMLN model took approximately for 120 minutes for all the test
cases, while for the CRF model, it took 120, 145, 175 and 240 minutes respectively. The inference
over the entire test set, took approximately 6 minutes for SN and MAR in every test case, while
UNR required 5, 8, 12 and 40 minutes for the different test cases. The accuracy and F-scores for the
different test cases are summarized in Table 2 and the F-scores are plotted in Fig. 1.
SN outperforms MAR, because as we see that in the knowledge base, MAR can only conclude that
people have the same or different smoking habits on the days when people hang out (every 5th time
step), and the marginal distributions of Smokes do not carry enough information about which pair
of people have different smoking habits, hence the quality of MAR?s prediction decreases on days
when people do not hang out. The performance of SN and MAR stays the same as we increase
the length of the chain while the performance of UNR degrades. This is most pronounced in the
deterministic case (? = 0). This can be explained by that MC-SAT requires more sampling steps to
maintain the same performance as the chain length increases.
To demonstrate that if we use the same number of particles in SN as number of samples in UNR,
the performance of SN stays approximately the same while the performance of UNR degrades over
time, we trained both the CRF and SN-DMLN on length 5 chains where both SN and UNR were
performing equally well and used test sets of different lengths up to length 150. F-scores are plotted
in Fig. 2.
We see from Fig. 2 that SN outperforms both UNR and MAR as the chain length increases. Moreover, UNR?s performance is clearly decreasing as the length of the chain increases.

6

Conclusion

In this paper, we explored the theoretical and practical questions of unrolling a sequential Markov
logic knowledge base into different probabilistic models. The theoretical issues arising in a CRF7

(a) ? = 0

(b) ? = 1

Figure 2: F-score of models trained and tested on different length of data
based MLN unrolling are a warning that unexpected results may occur if the observations are weakly
correlated with the hidden variables. We gave a qualitative justification why this phenomenon is
more of a theoretical concern in domains lacking deterministic constraints. We demonstrated that
the CRF based unrolling can be outperformed by a model that mixes directed and undirected components (the proposed model does not suffer from any of the theoretical weaknesses, nor from the
label-bias problem).
From a more practical point of view, we showed that our proposed model provides computational
savings, when the data has to be processed in a sequential manner. These savings are due to that
we do not have to unroll a new CRF at every time step, or estimate a partition function which is responsible for normalizing the product of clique potentials appearing in two consecutive slices. The
previously used approximate inference methods in dynamic MLNs either relied on belief propagation or assumed that approximating the distribution at every time step by the product of the marginals
would not cause any error. It is important to note that, although in our paper we focused on marginal
inference, finding the most likely state sequence could be done using the generated particles. Although the conditional log-likelihood of the training data in our model may be non-concave so that
hill climbing based approaches could fail to settle in a global maximum, we proposed a heuristic
for weight learning and demonstrated that it could train our model so that it performs as well as
conditional random fields. Although training the mixed model might have a higher computational
cost than training a conditional random field, but this cost is amortized over time, since in applications inference is performed many times, while weight learning only once. Designing more scalable
weight learning algorithms is among our future goals.

7

Acknowledgments

We thank Daniel Gildea for his insightful comments.
This research was supported by grants from ARO (W991NF-08-1-0242), ONR (N00014-11-10417),
NSF (IIS-1012017), DOD (N00014-12-C-0263), and a gift from Intel.

References
[1] Pedro Domingos and Daniel Lowd. Markov Logic: An Interface Layer for Artificial Intelligence. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers, 2009.
[2] Thomas Geier and Susanne Biundo. Approximate online inference for dynamic markov logic
networks. In Tools with Artificial Intelligence (ICTAI), 2011 23rd IEEE International Conference on, pages 764?768, 2011.
[3] Shalini Ghosh, Natarajan Shankar, and Sam Owre. Machine reading using markov logic networks for collective probabilistic inference. In In Proceedings of ECML-CoLISD., 2011.
[4] Vibhav Gogate and Rina Dechter. Samplesearch: Importance sampling in presence of determinism. Artif. Intell., 175(2):694?729, 2011.
[5] Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 1990.
[6] Dominik Jain, Andreas Barthels, and Michael Beetz. Adaptive Markov Logic Networks:
Learning Statistical Relational Models with Dynamic Parameters. In 19th European Conference on Artificial Intelligence (ECAI), pages 937?942, 2010.
8

[7] K. Kersting, B. Ahmadi, and S. Natarajan. Counting belief propagation. In J. Bilmes A. Ng,
editor, Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI?09),
Montreal, Canada, June 18?21 2009.
[8] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT
Press, 2009.
[9] John Lafferty. Conditional random fields: Probabilistic models for segmenting and labeling
sequence data. pages 282?289. Morgan Kaufmann, 2001.
[10] Steffen Lauritzen and Thomas S. Richardson. Chain graph models and their causal interpretations. B, 64:321?361, 2001.
[11] B. Limketkai, D. Fox, and Lin Liao. CRF-Filters: Discriminative Particle Filters for Sequential
State Estimation. In Robotics and Automation, 2007 IEEE International Conference on, pages
3142?3147, 2007.
[12] Andrew McCallum, Dayne Freitag, and Fernando C. N. Pereira. Maximum entropy markov
models for information extraction and segmentation. In Proceedings of the Seventeenth International Conference on Machine Learning, ICML ?00, pages 591?598, San Francisco, CA,
USA, 2000. Morgan Kaufmann Publishers Inc.
[13] Kevin Patrick Murphy. Dynamic bayesian networks: representation, inference and learning.
PhD thesis, 2002. AAI3082340.
[14] Aniruddh Nath and Pedro Domingos. Efficient belief propagation for utility maximization and
repeated inference, 2010.
[15] Hoifung Poon and Pedro Domingos. Sound and efficient inference with probabilistic and deterministic dependencies. In Proceedings of the 21st national conference on Artificial intelligence
- Volume 1, AAAI?06, pages 458?463. AAAI Press, 2006.
[16] G. Potamianos and J. Goutsias. Stochastic approximation algorithms for partition function
estimation of Gibbs random fields. IEEE Transactions on Information Theory, 43(6):1948?
1965, 1997.
[17] Adam Sadilek and Henry Kautz. Recognizing multi-agent activities from GPS data. In TwentyFourth AAAI Conference on Artificial Intelligence, 2010.
[18] R. Salakhutdinov. Learning and evaluating Boltzmann machines. Technical Report UTML TR
2008-002, Department of Computer Science, University of Toronto, June 2008.
[19] Charles Sutton, Andrew McCallum, and Khashayar Rohanimanesh. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. J.
Mach. Learn. Res., 8:693?723, May 2007.

9

"
4534,"Predictive PAC Learning and Process Decompositions

Aryeh Kontorovich
Computer Science Department
Ben Gurion University
Beer Sheva 84105 Israel
karyeh@cs.bgu.ac.il

Cosma Rohilla Shalizi
Statistics Department
Carnegie Mellon University
Pittsburgh, PA 15213 USA
cshalizi@cmu.edu

Abstract
We informally call a stochastic process learnable if it admits a generalization error
approaching zero in probability for any concept class with finite VC-dimension
(IID processes are the simplest example). A mixture of learnable processes need
not be learnable itself, and certainly its generalization error need not decay at the
same rate. In this paper, we argue that it is natural in predictive PAC to condition
not on the past observations but on the mixture component of the sample path.
This definition not only matches what a realistic learner might demand, but also
allows us to sidestep several otherwise grave problems in learning from dependent
data. In particular, we give a novel PAC generalization bound for mixtures of
learnable processes with a generalization error that is not worse than that of each
mixture component. We also provide a characterization of mixtures of absolutely
regular (?-mixing) processes, of independent probability-theoretic interest.

1

Introduction

Statistical learning theory, especially the theory of ?probably approximately correct? (PAC) learning, has mostly developed under the assumption that data are independent and identically distributed
(IID) samples from a fixed, though perhaps adversarially-chosen, distribution. As recently as 1997,
Vidyasagar [1] named extending learning theory to stochastic processes of dependent variables as a
major open problem. Since then, considerable progress has been made for specific classes of processes, particularly strongly-mixing sequences and exchangeable sequences. (Especially relevant
contributions, for our purposes, came from [2, 3] on exchangeability, from [4, 5] on absolute regularity1 , and from [6, 7] on ergodicity; others include [8, 9, 10, 11, 12, 13, 14, 15, 16, 17].) Our goals
in this paper are to point out that many practically-important classes of stochastic processes possess
a special sort of structure, namely they are convex combinations of simpler, extremal processes.
This both demands something of a re-orientation of the goals of learning, and makes the new goals
vastly easier to attain than they might seem.
Main results Our main contribution is threefold: a conceptual definition of learning from non-IID
data (Definition 1) and a technical result establishing tight generalization bounds for mixtures of
learnable processes (Theorem 2), with a direct corollary about exchangeable sequences (Corollary
1), and an application to mixtures of absolutely regular sequences, for which we provide a new
characterization.
Notation X1 , X2 , . . . will be a sequence of dependent random variables taking values in a common
measurable space X , which we assume to be ?standard? [18, ch. 3] to avoid technicalities, implying
1
Absolutely regular processes are ones where the joint distribution of past and future events approaches
independence, in L1 , as the separation between events goes to infinity; see ?4 below for a precise statement and
extensive discussion. Absolutely regular sequences are now more commonly called ??-mixing?, but we use the
older name to avoid confusion with the other sort of ?mixing?.

1

that their ?-field has a countable generating basis; the reader will lose little if they think of X as
Rd . (We believe our ideas apply to stochastic processes with multidimensional index sets as well,
but use sequences here.) Xij will stand for the block (Xi , Xi+1 , . . . Xj?1 , Xj ). Generic infinitedimensional distributions, measures on X ? , will be ? or ?; these are probability laws for X1? . Any
such stochastic process can be represented through the shift map ? : X ? 7? X ? (which just drops
the first coordinate, (? x)t = xt+1 ), and a suitable distribution of initial conditions. When we speak
of a set being invariant, we mean invariant under the shift. The collection of all such probability
measures is itself a measurable space, and a generic measure there will be ?.

2

Process Decompositions

Since the time of de Finetti and von Neumann, an important theme of the theory of stochastic processes has been finding ways of representing complicated but structured processes, obeying certain
symmetries, as mixtures of simpler processes with the same symmetries, as well as (typically) some
sort of 0-1 law. (See, for instance, the beautiful paper by Dynkin [19], and the statistically-motivated
[20].) In von Neumann?s original ergodic decomposition [18, ?7.9], stationary processes, whose distributions are invariant over time, proved to be convex combinations of stationary ergodic measures,
ones where all invariant sets have either probability 0 or probability 1. In de Finetti?s theorem [21,
ch. 1], exchangeable sequences, which are invariant under permutation, are expressed as mixtures of
IID sequences2 . Similar results are now also known for asymptotically mean stationary sequences
[18, ?8.4], for partially-exchangeable sequences [22], for stationary random fields, and even for
infinite exchangeable arrays (including networks and graphs) [21, ch. 7].
The common structure shared by these decompositions is as follows.
1. The probability law ? of the composite but symmetric process is a convex combination of
the simpler, extremal processes ? ? M with the same symmetry. The infinite-dimensional
distribution of these extremal processes are, naturally, mutually singular.
2. Sample paths from the composite process are generated hierarchically, first by picking an
extremal process ? from M according to a some measure ? supported on M, and then
generating a sample path from ?. Symbolically,
X1?

? ? ?
|? ? ?

3. Each realization of the composite process therefore gives information about only a single
extremal process, as this is an invariant along each sample path.

3

Predictive PAC

These points raise subtle but key issues for PAC learning theory. Recall the IID case: random
variables X1 , X2 , . . . are all generated from a common distribution ?(1) , leading to an infinitedimensional process distribution ?. Against this, we have a class F of functions f . The goal in PAC
theory is to find a sample complexity function3 s(, ?, F, ?) such that, whenever n ? s,
 n

!
1 X



P? sup 
f (Xt ) ? E? [f ] ?  ? ?
(1)

f ?F  n
t=1

That is, PAC theory seeks finite-sample uniform law of large numbers for F.
Because of its importance, it will be convenient to abbreviate the supremum,

 n
1 X



sup 
f (Xt ) ? E? [f ] ? ?n

f ?F  n
t=1

2

This is actually a special case of the ergodic decomposition [21, pp. 25?26].
Standard PAC is defined as distribution-free, but here we maintain the dependence on ? for consistency
with future notation.
3

2

using the letter ??? as a reminder that when this goes to zero, F is a Glivenko-Cantelli class (for ?).
?n is also a function of F and of ?, but we suppress this in the notation for brevity. We will also
pass over the important and intricate, but fundamentally technical, issue of establishing that ?n is
measurable (see [23] for a thorough treatment of this topic).
What one has in mind, of course, is that there is a space H of predictive models (classifiers, regressions, . . . ) h, and that F is the image of H through an appropriate loss function `, i.e., each f ? F
can be written as
f (x) = `(x, h(x))
for some h ? H. If ?n ? 0 in probability for this ?loss function? class, then empirical risk
P
minimization is reliable. That is, the function f?n which minimizes the empirical risk n?1 t f (Xt )
has an expected risk in the future which is close to the best attainable risk over all of F, R(F, ?) =
inf f ?F E? [f ]. Indeed, since when n ? s, with high (? 1 ? ?) probability all
have
h functions
i
?
empirical risks within  of their true risks, with high probability the true risk E? fn is within 2
of R(F, ?). Although empirical risk minimization is not the only conceivable learning strategy,
it is, in a sense, a canonical one (computational considerations aside). The latter is an immediate
consequence of the VC-dimension characterization of PAC learnability:
Theorem 1 Suppose that the concept class H is PAC learnable from IID samples. Then H is learnable via empirical risk minimization.
P ROOF : Since H is PAC-learnable, it must necessarily have a finite VC-dimension [24]. But for
finite-dimensional H and IID samples, ?n ? 0 in probability (see [25] for a simple proof). This
implies that the empirical risk minimizer is a PAC learner for H. 
In extending these ideas to non-IID processes, a subtle issue arises, concerning which expectation
value we would like empirical means to converge towards. In the IID case, because ? is simply the
infinite product of ?(1) and f is a function on X , we can without trouble identify expectations under
the two measures with each other, and with expectations conditional on the first n observations:
E? [f (X)] = E?(1) [f (X)] = E? [f (Xn+1 ) | X1n ]
Things are not so tidy when ? is the law of a dependent stochastic process.
In introducing a notion of ?predictive PAC learning?, Pestov [3], like Berti and Rigo [2] earlier,
proposes that the target should be the conditional expectation, in our notation E? [f (Xn+1 ) | X1n ].
This however presents two significant problems. First, in general there is no single value for this ?
it truly is a function of the past X1n , or at least some part of it. (Consider even the case of a binary
Markov chain.) The other, and related, problem with this idea of predictive PAC is that it presents
learning with a perpetually moving target. Whether the function which minimizes the empirical risk
is going to do well by this criterion involves rather arbitrary details of the process. To truly pursue
this approach, one would have to actually learn the predictive dependence structure of the process,
a quite formidable undertaking, though perhaps not hopeless [26].
Both of these complications are exacerbated when the process producing the data is actually a mixture over simpler processes, as we have seen is very common in interesting applied settings. This
is because, in addition to whatever dependence may be present within each extremal process, X1n
gives (partial) information about what that process is. Finding E? [Xn+1 | X1n ] amounts to first
finding all the individual conditional expectations, E? [Xn+1 | X1n ], and then averaging them with
respect to the posterior distribution ?(? | X1n ). This averaging over the posterior produces additional dependence between past and future. (See [27] on quantifying how much extra apparent
Shannon information this creates.) As we show in ?4 below, continuous mixtures of absolutely regular processes are far from being absolutely regular themselves. This makes it exceedingly hard,
if not impossible, to use a single sample path, no matter how long, to learn anything about global
expectations.
These difficulties all simply dissolve if we change the target distribution. What a learner should care
about is not averaging over some hypothetical prior distribution of inaccessible alternative dynamical
systems, but rather what will happen in the future of the particular realization which provided the
training data, and must continue to provide the testing data. To get a sense of what this is means,
3

notice that for an ergodic ?,
m

1 X
E [f (Xn+i ) | X1n ]
m?? m
i=1

E? [f ] = lim

(from [28, Cor. 4.4.1]). That is, matching expectations under the process measure ? means controlling the long-run average behavior, and not just the one-step-ahead expectation suggested in [3, 2].
Empirical risk minimization now makes sense: it is attempting to find a model which will work
well not just at the next step (which may be inherently unstable), but will continue to work well, on
average, indefinitely far into the future.
We are thus led to the following definition.
Definition 1 Let X1? be a stochastic process with law ?, and let I be the ?-field generated by all
the invariant events. We say that ? admits predictive PAC learning of a function class F when there
exists a sample-complexity function s(, ?, F, ?) such that, if n ? s, then


!
n
1 X



P? sup 
f (Xt ) ? E? [f |I] ?  ? ?

f ?F  n
t=1

A class of processes P admits of distribution-free predictive PAC learning if there exists a common
sample-complexity function for all ? ? P, in which case we write s(, ?, F, ?) = s(, ?, F, P).
As is well-known, distribution-free predictive PAC learning, in this sense, is possible for IID processes (F must have finite VC dimension). For an ergodic ?, [6] shows that s(, ?, F, ?) exist and is
finite if and only, once again, F has a finite VC dimension; this implies predictive PAC learning, but
not distribution-free predictive PAC. Since ergodic processes can converge arbitrarily slowly, some
extra condition must be imposed on P to ensure that dependence decays fast enough for each ?. A
sufficient restriction is that all processes in P be stationary and absolutely regular (?-mixing), with
a common upper bound on the ? dependence coefficients, as [5, 14] show how to turn algorithms
which are PAC on IID data into ones where are PAC on such sequences, with a penalty in extra sample complexity depending on ? only through the rate of decay of correlations4 . We may apply these
familiar results straightforwardly, because, when ? is ergodic, all invariant sets have either measure
0 or measure 1, conditioning on I has no effect, and E? [f | I] = E? [f ].
Our central result is now almost obvious.
Theorem 2 Suppose that distribution-free prediction PAC learning is possible for a class of functions F and a class of processes M, with sample-complexity function s(, ?, F, P). Then the class
of processes P formed by taking convex mixtures from M admits distribution-free PAC learning
with the same sample complexity function.
P ROOF : Suppose that n ? s(, ?, F). Then by the law of total expectation,
P? (?n ? )

= E? [P? (?n ?  | ?)]
= E? [P? (?n ? )]
? E? [?] = ?

(2)
(3)
(4)


In words, if the same bound holds for each component of the mixture, then it still holds after averaging over the mixture. It is important here that we are only attempting to predict the long-run average
risk along the continuation of the same sample path as that which provided the training data; with
this as our goal, almost all sample paths looks like ? indeed, are ? realizations of single components of the mixture, and so the bound for extremal processes applies directly to them5 . By contrast,
there may be no distribution-free bounds at all if one does not condition on I.
4

We suspect that similar results could be derived for many of the weak dependence conditions of [29].
After formulating this idea, we came across a remarkable paper by Wiener [30], where he presents a qualitative version of highly similar considerations, using the ergodic decomposition to argue that a full dynamical
model of the weather is neither necessary nor even helpful for meteorological forecasting. The same paper
also lays out the idea of sensitive dependence on initial conditions, and the kernel trick of turning nonlinear
problems into linear ones by projecting into infinite-dimensional feature spaces.
5

4

A useful consequence of this innocent-looking result is that it lets us immediately apply PAC learning
results for extremal processes to composite processes, without any penalty in the sample complexity.
For instance, we have the following corollary:
Corollary 1 Let F have finite VC dimension, and have distribution-free sample complexity
s(, ?, F, M) for all IID measures ? ? P. Then the class M of exchangeable processes composed
from P admit distribution-free PAC learning with the same sample complexity,
s(, ?, F, P) = s(, ?, F, M)

This is in contrast with, for instance, the results obtained by [2, 3], which both go from IID sequences (laws in P) to exchangeable ones (laws in M) at the cost of considerably increased sample
complexity. The easiest point of comparison is with Theorem 4.2 of [3], which, in our notation,
shows that
s(, ?, M) ? s(/2, ?, P)
That we pay no such penalty in sample complexity because our target of learning is E? [f | I], not
E? [f | X1n ]. This means we do not have to use data points to narrow the posterior distribution
?(? | X1n ), and that we can give a much more direct argument.
In [3], Pestov asks whether ?one [can] maintain the initial sample complexity? when going from
IID to exchangeable sequences; the answer is yes, if one picks the right target. This holds whenever
the data-generating process is a mixture of extremal processes for which learning is possible. A
particularly important special case of this are the absolutely regular processes.

4

Mixtures of Absolutely Regular Processes

Roughly speaking, an absolutely regular process is one which is asymptotically independent in
a particular sense, where the joint distribution between past and future events approaches, in L1 ,
the product of the marginal distributions as the time-lag between past and future grows. These are
particularly important for PAC learning, since much of the existing IID learning theory translates
directly to this setting.
?
be a two-sided6 stationary process. The ?-dependence coefficient at lag k is
To be precise, let X??

 0

?(k) ? 
P??
? Pk? ? P?(1:k) 
TV
(5)
0
and Xk? , the total variation distance between the
where P?(1:k) is the joint distribution of X??
actual joint distribution of the past and future, and the product of their marginals. Equivalently
[31, 32]
""
#

0
?(k) = E
sup P B | X?? ? P (B)
(6)
B??(Xk? )

which, roughly, is the expected total variation distance between the marginal distribution of the
future and its distribution conditional on the past.
As is well known, ?(k) is non-increasing in k, and of course ? 0, so ?(k) must have a limit as
k ? ?; it will be convenient to abbreviate this as ?(?). When ?(?) = 0, the process is said to
be beta mixing, or absolutely regular. All absolutely regular processes are also ergodic [32].
The importance of absolutely regular processes for learning comes essentially from a result due to
Yu [4]. Let X1n be a part of a sample path from an absolutely regular process ?, whose dependence
coefficients are ?(k). Fix integers m and a such that 2ma = n, so that the sequence is divided into
2m contiguous blocks of length a, and define ?(m,a) to be distribution of m length-a blocks. (That
is, ?(m,a) approximates ? by IID blocks.) Then |?(C) ? ?(m,a) (C)| ? (m ? 1)?(a) [4, Lemma
4.1]. Since in particular the event C could be taken to be {?n > }, this approximation result allows
distribution-free learning bounds for IID processes to translate directly into distribution-free learning
bounds for absolutely regular processes with bounded ? coefficients.
6
We have worked with one-sided processes so far, but the devices for moving between the two representations are standard, and this definition is more easily stated in its two-sided version.

5

If M contains only absolutely regular processes, then a measure ? on M creates a ? which is a
mixture of absolutely regular processes, or a MAR process. It is easy to see that absolute regularity
of the component processes (?? (k) ? 0) does not imply absolute regularity of the mixture processes
(?? (k) 6? 0). To see this, suppose that M consists of two processes ?0 , which puts unit probability
mass on the sequence of all zeros, and ?1 , which puts unit probability on the sequence of all ones;
and that ? gives these two equal probability. Then ??i (k) = 0 for both i, but the past and the future
of ? are not independent of each other.
More generally, suppose ? is supported on just two absolutely regular processes, ? and ?0 . For each
such ?, there exists a set of typical sequences T? ? X ? , in which the infinite sample path of ? lies
almost surely7 , and these sets do not overlap8 , T? ? T?0 = ?. This implies that ?(T? ) = ?(?), but

0
1 X??
? T?
0
?(T? | X??
)=
0 otherwise
Thus the change in probability of T? due to conditioning on the past is ?(?1 ) if the selected component was ?2 , and 1 ? ?(?1 ) = ?(?2 ) if the selected component is ?1 . We can reason in parallel for
T?2 , and so the average change in probability is
?1 (1 ? ?1 ) + ?2 (1 ? ?2 ) = 2?1 (1 ? ?1 )
and this must be ?? (?). Similar reasoning when ? is supported on q extremal processes shows
q
X
?? (?) =
?i (1 ? ?i )
i=1

so that the general case is
Z
?? (?) =

[1 ? ?({?})]d?(?)

This implies that if ? has no atoms, ?? (?) = 1 always. Since ?? (k) is non-increasing, ?? (k) = 1
for all k, for a continuous mixture of absolutely regular processes. Conceptually, this arises because
of the use of infinite-dimensional distributions for both past and future in the definition of the ?dependence coefficient. Having seen an infinite past is sufficient, for an ergodic process, to identify
the process, and of course the future must be a continuation of this past.
MARs thus display a rather odd separation between the properties of individual sample paths, which
approach independence asymptotically in time, and the ensemble-level behavior, where there is
ineradicable dependence, and indeed maximal dependence for continuous mixtures. Any one realization of a MAR, no matter how long, is indistinguishable from a realization of an absolutely
regular process which is a component of the mixture. The distinction between a MAR and a single
absolutely regular process only becomes apparent at the level of ensembles of paths.
It is desirable to characterize MARs. They are stationary, but non-ergodic and have non-vanishing
?(?). However, this is not sufficient to characterize them. Bernoulli shifts are stationary and
ergodic, but not absolutely regular9 . It follows that a mixture of Bernoulli shifts will be stationary,
and by the preceding argument will have positive ?(?), but will not be a MAR.
Roughly speaking, given the infinite past of a MAR, events in the future become asymptotically
independent as the separation between them increases10 . A more precise statement needs to control
?
the approach to independence of the component processes in a MAR. We say that ? is a ?-uniform
?
?
MAR when ? is a MAR, and, for ?-almost-all ?, ?? (k) ? ?(k), with ?(k) ? 0. Then if we con0
dition on finite histories X?n
and let n grow, widely separated future events become asymptotically
independent.
?
Since X is ?standard?, so is X? , and the latter?s ?-field ?(X??
) has a countable generating
basis, say B.
	
?
?1 Pn?1
t
For each B ? B, the set T?,B = x ? X
: limn?? n
measurable,
t=0 1B (? x) = ?(B) exists, is T
is dynamically invariant, and, by Bikrhoff?s ergodic theorem, ?(T?,B ) = 1 [18, ?7.9]. Then T? ? B?B T?,B
also has ?-measure 1, because B is countable.
8
Since ? 6= ?0 , there exists at least one set C with ?(C) 6= ?0 (C). The set T?,C then cannot overlap at all
with the set T?0 ,C , and so T? cannot intersect T?0 .
9
They are, however, mixing in the sense of ergodic theory [28].
10
0
?-almost-surely, X??
belongs to the typical set of a unique absolutely regular process, say ?, and so the
0
?
0
l
?
posterior concentrates on that ?, ?(? | X??
) = ?? . Hence ?(X1l , Xl+k
| X??
) = ?(X??
, Xl+k
), which
l
?
tends to ?(X?? )?(Xl+k ) as k ? ? because ? is absolutely regular.
7

6

?
Theorem 3 A stationary process ? is a ?-uniform
MAR if and only if
""
lim lim E sup

k?? n??

l

sup

?(B |

? )
B??(Xk+l

0
X1l , X?n
)

? ?(B |

#
0
X?n
)

=0

(7)

Before proceeding to the proof, it is worth remarking on the order of the limits: for finite n, condi0
tioning on X?n
still gives a MAR, not a single (albeit random) absolutely-regular process. Hence
the k ? ? limit for fixed n would always be positive, and indeed 1 for a continuous ?.
P ROOF ?Only if?: Re-write Eq. 7, expressing ? in terms of ? and the extremal processes:
#
""
Z

0
0
0
?(B | X1l , X?n
) ? ?(B | X?n
) d?(? | X?n
)
lim lim E sup sup
k?? n??

l

? )
B??(Xk+l

0
0
0
l
As n ? ?, ?(B | X?n
) ? ?(B | X??
), and ?(B | X1l , X?n
) ? ?(B | X??
). But,
?
?
in expectation, both of these are within ?(k) of ?(B), and so within 2?(k). ?If?: Consider the
contrapositive. If ? is not a uniform MAR, either it is a non-uniform MAR, or it is not a MAR at
?
all. If it is not a uniform MAR, then no matter what function ?(k)
tending to zero we propose,
the set of ? with ?? ? ?? must have positive ? measure, i.e., a positive-measure set of processes
must converge arbitrarily slowly. Therefore there must exist a set B (or a sequence of such sets)
witnessing this arbitrarily slow convergence, and hence the limit in Eq. 7 will be strictly positive. If
? is not a MAR at all, we know from the ergodic decomposition of stationary processes that it must
be a mixture of ergodic processes, and so it must give positive ? weight to processes which are not
absolutely regular at all, i.e., ? for which ?? (?) > 0. The witnessing events B for these processes
a fortiori drive the limit in Eq. 7 above zero. 

5

Discussion and future work

We have shown that with the right conditioning, a natural and useful notion of predictive PAC
emerges. This notion is natural in the sense that for a learner sampling from a mixture of ergodic
processes, the only thing that matters is the future behavior of the component he is ?stuck? in ? and
certainly not the process average over all the components. This insight enables us to adapt the recent
PAC bounds for mixing processes to mixtures of such processes. An interesting question then is to
characterize those processes that are convex mixtures of a given kind of ergodic process (de Finetti?s
theorem was the first such characterization). In this paper, we have addressed this question for
mixtures of uniformly absolutely regular processes. Another fascinating question is how to extend
predictive PAC to real-valued functions [33, 34].

References
[1] Mathukumalli Vidyasagar. A Theory of Learning and Generalization: With Applications to
Neural Networks and Control Systems. Springer-Verlag, Berlin, 1997.
[2] Patrizia Berti and Pietro Rigo. A Glivenko-Cantelli theorem for exchangeable random variables. Statistics and Probability Letters, 32:385?391, 1997.
[3] Vladimir Pestov. Predictive PAC learnability: A paradigm for learning from exchangeable
input data. In Proceedings of the 2010 IEEE International Conference on Granular Computing
(GrC 2010), pages 387?391, Los Alamitos, California, 2010. IEEE Computer Society. URL
http://arxiv.org/abs/1006.1129.
[4] Bin Yu. Rates of convergence for empirical processes of stationary mixing sequences. Annals
of Probability, 22:94?116, 1994. URL http://projecteuclid.org/euclid.aop/
1176988849.
[5] M. Vidyasagar. Learning and Generalization: With Applications to Neural Networks.
Springer-Verlag, Berlin, second edition, 2003.
[6] Terrence M. Adams and Andrew B. Nobel. Uniform convergence of Vapnik-Chervonenkis
classes under ergodic sampling. Annals of Probability, 38:1345?1367, 2010. URL http:
//arxiv.org/abs/1010.3162.
7

[7] Ramon van Handel. The universal Glivenko-Cantelli property. Probability Theory and Related
Fields, 155:911?934, 2013. doi: 10.1007/s00440-012-0416-5. URL http://arxiv.org/
abs/1009.4434.
[8] Dharmendra S. Modha and Elias Masry. Memory-universal prediction of stationary random
processes. IEEE Transactions on Information Theory, 44:117?133, 1998. doi: 10.1109/18.
650998.
[9] Ron Meir. Nonparametric time series prediction through adaptive model selection. Machine Learning, 39:5?34, 2000. URL http://www.ee.technion.ac.il/?rmeir/
Publications/MeirTimeSeries00.pdf.
[10] Rajeeva L. Karandikar and Mathukumalli Vidyasagar. Rates of uniform convergence of empirical means with mixing processes. Statistics and Probability Letters, 58:297?307, 2002. doi:
10.1016/S0167-7152(02)00124-4.
[11] David Gamarnik. Extension of the PAC framework to finite and countable Markov chains.
IEEE Transactions on Information Theory, 49:338?345, 2003. doi: 10.1145/307400.307478.
[12] Ingo Steinwart and Andreas Christmann. Fast learning from non-i.i.d. observations. In Y. Bengio, D. Schuurmans, John Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances
in Neural Information Processing Systems 22 [NIPS 2009], pages 1768?1776. MIT Press,
Cambridge, Massachusetts, 2009. URL http://books.nips.cc/papers/files/
nips22/NIPS2009_1061.pdf.
[13] Mehryar Mohri and Afshin Rostamizadeh. Stability bounds for stationary ?-mixing and ?mixing processes. Journal of Machine Learning Research, 11, 2010. URL http://www.
jmlr.org/papers/v11/mohri10a.html.
[14] Mehryar Mohri and Afshin Rostamizadeh. Rademacher complexity bounds for non-I.I.D. processes. In Daphne Koller, D. Schuurmans, Y. Bengio, and L?eon Bottou, editors, Advances
in Neural Information Processing Systems 21 [NIPS 2008], pages 1097?1104, 2009. URL
http://books.nips.cc/papers/files/nips21/NIPS2008_0419.pdf.
[15] Pierre Alquier and Olivier Wintenberger. Model selection for weakly dependent time series
forecasting. Bernoulli, 18:883?913, 2012. doi: 10.3150/11-BEJ359. URL http://arxiv.
org/abs/0902.2924.
[16] Ben London, Bert Huang, and Lise Getoor. Improved generalization bounds for largescale structured prediction. In NIPS Workshop on Algorithmic and Statistical Approaches
for Large Social Networks, 2012. URL http://linqs.cs.umd.edu/basilic/web/
Publications/2012/london:nips12asalsn/.
[17] Ben London, Bert Huang, Benjamin Taskar, and Lise Getoor. Collective stability in structured
prediction: Generalization from one example. In Sanjoy Dasgupta and David McAllester,
editors, Proceedings of the 30th International Conference on Machine Learning [ICML-13],
volume 28, pages 828?836, 2013. URL http://jmlr.org/proceedings/papers/
v28/london13.html.
[18] Robert M. Gray. Probability, Random Processes, and Ergodic Properties. Springer-Verlag,
New York, second edition, 2009. URL http://ee.stanford.edu/?gray/arp.
html.
[19] E. B. Dynkin. Sufficient statistics and extreme points. Annals of Probability, 6:705?730, 1978.
URL http://projecteuclid.org/euclid.aop/1176995424.
[20] Steffen L. Lauritzen. Extreme point models in statistics. Scandinavian Journal of Statistics,
11:65?91, 1984. URL http://www.jstor.org/pss/4615945. With discussion and
response.
[21] Olav Kallenberg. Probabilistic Symmetries and Invariance Principles. Springer-Verlag, New
York, 2005.
[22] Persi Diaconis and David Freedman. De Finetti?s theorem for Markov chains. Annals of Probability, 8:115?130, 1980. doi: 10.1214/aop/1176994828. URL http://projecteuclid.
org/euclid.aop/1176994828.
?
[23] R. M. Dudley. A course on empirical processes. In Ecole
d??et?e de probabilit?es de Saint-Flour,
XII?1982, volume 1097 of Lecture Notes in Mathematics, pages 1?142. Springer, Berlin, 1984.
8

[24] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Learnability
and the Vapnik-Chervonenkis dimension. Journal of the Association for Computing Machinery, 36:929?965, 1989. doi: 10.1145/76359.76371. URL http://users.soe.ucsc.
edu/?manfred/pubs/J14.pdf.
[25] St?ephane Boucheron, Olivier Bousquet, and G?abor Lugosi. Theory of classification: A survey
of recent advances. ESAIM: Probabability and Statistics, 9:323?375, 2005. URL http:
//www.numdam.org/item?id=PS_2005__9__323_0.
[26] Frank B. Knight. A predictive view of continuous time processes. Annals of Probability, 3:
573?596, 1975. URL http://projecteuclid.org/euclid.aop/1176996302.
[27] William Bialek, Ilya Nemenman, and Naftali Tishby. Predictability, complexity and learning.
Neural Computation, 13:2409?2463, 2001. URL http://arxiv.org/abs/physics/
0007070.
[28] Andrzej Lasota and Michael C. Mackey. Chaos, Fractals, and Noise: Stochastic Aspects of Dynamics. Springer-Verlag, Berlin, 1994. First edition, Probabilistic Properties of Deterministic
Systems, Cambridge University Press, 1985.
[29] J?er?ome Dedecker, Paul Doukhan, Gabriel Lang, Jos?e Rafael Le?on R., Sana Louhichi, and
Cl?ementine Prieur. Weak Dependence: With Examples and Applications. Springer, New York,
2007.
[30] Norbert Wiener. Nonlinear prediction and dynamics. In Jerzy Neyman, editor, Proceedings
of the Third Berkeley Symposium on Mathematical Statistics and Probability, volume 3, pages
247?252, Berkeley, 1956. University of California Press. URL http://projecteuclid.
org/euclid.bsmsp/1200502197.
[31] Paul Doukhan. Mixing: Properties and Examples. Springer-Verlag, New York, 1995.
[32] Richard C. Bradley. Basic properties of strong mixing conditions. A survey and some open
questions. Probability Surveys, 2:107?144, 2005. URL http://arxiv.org/abs/math/
0511078.
[33] Noga Alon, Shai Ben-David, Nicol`o Cesa-Bianchi, and David Haussler. Scale-sensitive dimensions, uniform convergence, and learnability. Journal of the ACM, 44:615?631, 1997. doi:
10.1145/263867.263927. URL http://tau.ac.il/?nogaa/PDFS/learn3.pdf.
[34] Peter L. Bartlett and Philip M. Long. Prediction, learning, uniform convergence, and
scale-sensitive dimensions. Journal of Computer and Systems Science, 56:174?190, 1998.
doi: 10.1006/jcss.1997.1557. URL http://www.phillong.info/publications/
more_theorems.pdf.

9

"
4349,"On the Linear Convergence of the Proximal Gradient
Method for Trace Norm Regularization

Ke Hou, Zirui Zhou, Anthony Man?Cho So
Department of Systems Engineering & Engineering Management
The Chinese University of Hong Kong
Shatin, N. T., Hong Kong
{khou,zrzhou,manchoso}@se.cuhk.edu.hk
Zhi?Quan Luo
Department of Electrical & Computer Engineering
University of Minnesota
Minneapolis, MN 55455, USA
luozq@ece.umn.edu

Abstract
Motivated by various applications in machine learning, the problem of minimizing a convex smooth loss function with trace norm regularization has received
much attention lately. Currently, a popular method for solving such problem is
the proximal gradient method (PGM), which is known to have a sublinear rate of
convergence. In this paper, we show that for a large class of loss functions, the
convergence rate of the PGM is in fact linear. Our result is established without any
strong convexity assumption on the loss function. A key ingredient in our proof
is a new Lipschitzian error bound for the aforementioned trace norm?regularized
problem, which may be of independent interest.

1 Introduction
The problem of finding a low?rank matrix that (approximately) satisfies a given set of conditions
has recently generated a lot of interest in many communities. Indeed, such a problem arises in a
wide variety of applications, including approximation algorithms [17], automatic control [5], matrix
classification [20], matrix completion [6], multi?label classification [1], multi?task learning [2],
network localization [7], subspace learning [24], and trace regression [9], just to name a few. Due to
the combinatorial nature of the rank function, the task of recovering a matrix with the desired rank
and properties is generally intractable. To circumvent this, a popular approach is to use the trace
norm1 (also known as the nuclear norm) as a surrogate for the rank function. Such an approach is
quite natural, as the trace norm is the tightest convex lower bound of the rank function over the set
of matrices with spectral norm at most one [13]. In the context of machine learning, the trace norm
is typically used as a regularizer in the minimization of certain convex loss function. This gives rise
to convex optimization problems of the form
min

X?Rm?n

{F (X) = f (X) + ? kXk? } ,

(1)

where f : Rm?n ? R is the convex loss function, kXk? denotes the trace norm of X, and ? > 0
is a regularization parameter. By standard results in convex optimization [4], the above formulation
is tractable (i.e., polynomial?time solvable) for many choices of the loss function f . In practice,
1

Recall that the trace norm of a matrix is defined as the sum of its singular values.

1

however, one is often interested in settings where the decision variable X is of high dimension.
Thus, there has been much research effort in developing fast algorithms for solving (1) lately.
Currently, a popular method for solving (1) is the proximal gradient method (PGM), which exploits
the composite nature of the objective function F and certain smoothness properties of the loss function f [8, 19, 11]. The attractiveness of PGM lies not only in its excellent numerical performance,
but also in its strong theoretical convergence rate guarantees. Indeed, for the trace norm?regularized
problem (1) with f being convex and continuously differentiable and ?f being Lipschitz continuous, the standard PGM will achieve an additive error of O(1/k) in the optimal value after k iterations. Moreover, this error can be reduced to O(1/k 2 ) using acceleration techniques; see, e.g., [19].
The sublinear O(1/k 2 ) convergence rate is known to be optimal if f is simply given by a first?order
oracle [12]. On the other hand, if f is strongly convex, then the convergence rate can be improved to
O(ck ) for some c ? (0, 1) (i.e., a linear convergence rate) [16]. However, in machine learning, the
loss functions of interest are often highly structured and hence not just given by an oracle, but they
are not necessarily strongly convex either. For instance, in matrix completion, a commonly used loss
function is the square loss f (?) = kA(?) ? bk22 /2, where A : Rm?n ? Rp is a linear measurement
operator and b ? Rp is a given set of observations. Clearly, f is not strongly convex when A has a
non?trivial nullspace (or equivalently, when A is not injective). In view of this, it is natural to ask
whether linear convergence of the PGM can be established for a larger class of loss functions.
In this paper, we take a first step towards answering this question. Specifically, we show that when
the loss function f takes the form f (X) = h(A(X)), where A : Rm?n ? Rp is an arbitrary
linear operator and h : Rp ? R is strictly convex with certain smoothness and curvature properties,
the PGM for solving (1) has an asymptotic linear rate of convergence. Note that f need not be
strictly convex even if h is, as A is arbitrary. Our result covers a wide range of loss functions used
in the literature, such as square loss and logistic loss. Moreover, to the best of our knowledge, it
is the first linear convergence result concerning the application of a first?order method to the trace
norm?regularized problem (1) that does not require the strong convexity of f .
The key to our convergence analysis is a new Lipschitzian error bound for problem (1). Roughly,
it says that the distance between a point X ? Rm?n and the optimal solution set of (1) is on the
order of the residual norm kprox? (X ? ?f (X)) ? XkF , where prox? is the proximity operator
associated with the regularization term ? kXk? . Once we have such a bound, a routine application of the powerful analysis framework developed by Luo and Tseng [10] will yield the desired
linear convergence result. Prior to this work, Lipschitzian error bounds for composite function minimization are available for cases where the non?smooth part either has a polyhedral epigraph (such
as the ?1 ?norm) [23] or is the (sparse) group LASSO regularization [22, 25]. However, the question of whether a similar bound holds for trace norm regularization has remained open, despite its
apparent similarity to ?1 ?norm regularization. Indeed, unlike the ?1 ?norm, the trace norm has a non?
polyhedral epigraph; see, e.g., [18]. Moreover, the existing approach for establishing error bounds
for ?1 ?norm or (sparse) group LASSO regularization is based on splitting the decision variables into
groups, where variables from different groups do not interfere with one another, so that each group
can be analyzed separately. However, the trace norm of a matrix is determined by its singular values,
and each of them depends on every single entry of the matrix. Thus, we cannot use the same splitting approach to analyze the entries of the matrix. To overcome the above difficulties, we make the
? is an optimal solution to (1), then both X
? and ??f (X)
? have the same
crucial observation that if X
set of left and right singular vectors; see Proposition 4.2. As a result, we can use matrix perturbation
theory to get hold of the spectral structure of the points that are close to the optimal solution set. This
in turn allows us to establish a Lipschitzian error bound for the trace norm?regularized problem (1),
thereby resolving the aforementioned open question in the affirmative.

2 Preliminaries
2.1 Basic Setup
We consider the trace norm?regularized optimization problem (1), in which the loss function f :
Rm?n ? R takes the form
f (X) = h(A(X)),
(2)
where A : Rm?n ? Rp is a linear operator and h : Rp ? R is a function satisfying the following
assumptions:
2

Assumption 2.1
(a) The effective domain of h, denoted by dom(h), is open and non?empty.
(b) The function h is continuously differentiable with Lipschitz?continuous gradient on dom(h)
and is strongly convex on any convex compact subset of dom(h).
Note that Assumption 2.1(b) implies the strict convexity of h on dom(h) and the Lipschitz continuity
of ?f . Now, let X denote the set of optimal solutions to problem (1). We make the following
assumption concerning X :
Assumption 2.2 The optimal solution set X is non?empty.
The above assumptions can be justified in various applications. For instance, in matrix completion,
the square loss f (?) = kA(?) ? bk22 /2 induced by the linear measurement operator A and the set
of observations b ? Rp is of the form (2), with h(?) = k(?) ? bk22 /2. Moreover, it is clear that
such an h satisfies Assumptions 2.1 and 2.2. In multi?task learning, the loss function takes the form
P
f (?) = Tt=1 ?(At (?), yt ), where T is the number of learning tasks, At : Rm?n ? Rp is the linear
operator defined by the input data for the t?th task, yt ? Rp is the output data for the t?th task, and
? : Rp ? Rp ? R measures the learning error. Note that f can be put into the form (2), where
A : Rm?n ? RT p is given by A(X) = (A1 (X), A2 (X), . . . , AT (X)), and h : RT p ? R is
PT
given by h(z) = t=1 ?(zt , yt ) with zt ? Rp for t = 1, . . . , T and z = (z1 , . . . , zT ). Moreover,
2
in the case where
Pp ? is, say, the square loss (i.e., ?(zt , yt ) = kzt ? yt k2 /2) or the logistic loss (i.e.,
?(zt , yt ) = i=1 log(1 + exp(?zti yti ))), it can be verified that Assumptions 2.1 and 2.2 hold.
2.2 Some Facts about the Optimal Solution Set
Since f (?) = h(A(?)) by (2) and h(?) is strictly convex on dom(h) by Assumption 2.1(b), it is easy
to verify that the map X 7? A(X) is invariant over the optimal solution set X . In other words, there
exists a z? ? dom(h) such that for any X ? ? X , we have A(X ? ) = z?. Thus, we can express X as

	
X = X ? Rm?n : ? kXk? = v ? ? h(?
z ), A(X) = z? ,

where v ? > ?? is the optimal value of (1). In particular, X is a non?empty convex compact set.
? ? X onto X , which is given by the
This implies that every X ? Rm?n has a unique projection X
solution to the following optimization problem:
dist(X, X ) = min kX ? Y kF .
Y ?X

In addition, since X is bounded and F is convex, it follows from [14, Corollary 8.7.1] that the level
set {X ? Rm?n : F (X) ? ?} is bounded for any ? ? R.
2.3 Proximal Gradient Method and the Residual Map
To motivate the PGM for solving (1), we recall an alternative characterization of the optimal solution
set X . Consider the proximity operator prox? : Rm?n ? Rm?n , which is defined as


1
? kZk? + kX ? Zk2F .
prox? (X) = arg min
(3)
2
Z?Rm?n
By comparing the optimality conditions for (1) and (3), it is immediate that a solution X ? ? Rm?n
is optimal for (1) if and only if it satisfies the following fixed?point equation:
X ? = prox? (X ? ? ?f (X ? )).
This naturally lead to the following PGM for solving (1):
 k+1
Y
= X k ? ?k ?f (X k ),
k+1
X
= prox? ?k (Y k+1 ),

(4)

(5)

where ?k > 0 is the step size in the k?th iteration, for k = 0, 1, . . .; see, e.g., [8, 19, 11]. As is
well?known, the proximity operator defined above can be expressed in terms of the so?called matrix
3

shrinkage operator. To describe this result, we introduce some definitions. Let ? > 0 be given. The
non?negative vector shrinkage operator s? : Rp+ ? Rp+ is defined as (s? (z))i = max{0, zi ? ?},
where i = 1, . . . , p. The matrix shrinkage operator S? : Rm?n ? Rm?n is defined as S? (X) =
U ?? V T , where X = U ?V T is the singular value decomposition of X with ? = Diag(?(X)) and
?(X) being the vector of singular values of X, and ?? = Diag(s? (?(X))). Then, it can be shown
that
prox? (X) = S? (X);
(6)
see, e.g., [11, Theorem 3].
Our goal in this paper is to study the convergence rate of the PGM (5). Towards that end, we need a
measure to quantify its progress towards optimality. One natural candidate would be dist(?, X ), the
distance to the optimal solution set X . Despite its intuitive appeal, such a measure is hard to compute
or analyze. In view of (4) and (6), a reasonable alternative would be the norm of the residual map
R : Rm?n ? Rm?n , which is defined as
R(X) = S? (X ? ?f (X)) ? X.

(7)
m?n

Intuitively, the residual map measures how much a solution X ? R
violates the optimality
condition (4). In particular, X is an optimal solution to (1) if and only if R(X) = 0. However, since
kR(?)kF is only a surrogate of dist(?, X ), we need to establish a relationship between them. This
motivates the development of a so?called error bound for problem (1).

3 Main Results
Key to our convergence analysis of the PGM (5) is the following error bound for problem (1), which
constitutes the main contribution of this paper:
Theorem 3.1 (Error Bound for Trace Norm Regularization) Suppose that in problem (1), f is of
the form (2), and Assumptions 2.1 and 2.2 are satisfied. Then, for any ? ? v ? , there exist constants
? > 0 and ? > 0 such that
dist(X, X ) ? ?kR(X)kF

whenever F (X) ? ?, kR(X)kF ? ?.

(8)

Armed with Theorem 3.1 and some standard properties of the PGM (5), we can apply the convergence analysis framework developed by Luo and Tseng [10] to establish the linear convergence of (5). Recall that a sequence of vectors {wk }k?0 is said to converge Q?linearly (resp. R?
linearly) to a vector w? if there exist an index K ? 0 and a constant ? ? (0, 1) such that
kwk+1 ? w? k2 /kwk ? w? k2 ? ? for all k ? K (resp. if there exist constants ? > 0 and ? ? (0, 1)
such that kwk ? w? k2 ? ? ? ?k for all k ? 0).
Theorem 3.2 (Linear Convergence of the Proximal Gradient Method) Suppose that in problem
(1), f is of the form (2), and Assumptions 2.1 and 2.2 are satisfied. Moreover, suppose that the step
size ?k in the PGM (5) satisfies 0 < ? < ?k < ?
? < 1/Lf for k = 0, 1, 2, . . ., where Lf is the
? are given constants. Then, the sequence of solutions {X k }k?0
Lipschitz constant of ?f , and ?, ?
generated by the PGM (5) converges R?linearly to an element in the optimal solution set X , and the
associated sequence of objective values {F (X k )}k?0 converges Q?linearly to the optimal value v ? .
Proof. Under the given setting, it can be shown that there exist scalars ?1 , ?2 , ?3 > 0, which depend
? , and Lf , such that
on ?, ?
F (X k ) ? F (X k+1 )
F (X

k+1

)?v
k

?

kR(X )kF

? ?1 kX k ? X k+1 k2F ,


? ?2 (dist(X k , X ))2 + kX k+1 ? X k k2F ,
k

? ?3 kX ? X
k

k+1

kF ;

(9)
(10)
(11)

see the supplementary material. Since {F (X )}k?0 is a monotonically decreasing sequence by (9)
and F (X k ) ? v ? for all k ? 0, we conclude, again by (9), that X k ? X k+1 ? 0. This, together
with (11), implies that R(X k ) ? 0. Thus, by (9), (10) and Theorem 3.1, there exist an index K ? 0
and a constant ?4 > 0 such that for all k ? K,
?4
F (X k+1 ) ? v ? ? ?4 kX k ? X k+1 k2F ?
(F (X k ) ? F (X k+1 )).
?1
4

It follows that

?4
(F (X k ) ? v ? ),
(12)
?1 + ?4
which establishes the Q?linear convergence of {F (X k )}k?0 to v ? . Using (9) and (12), we can
show that {kX k+1 ? X k k2F }k?0 converges R?linearly to 0, which, together with (11), implies that
{X k }k?0 converges R?linearly to a point in X ; see the supplementary material.

F (X k+1 ) ? v ? ?

4 Proof of the Error Bound
The structure of our proof of Theorem 3.1 largely follows that laid out in [22, Section 6]. However,
as explained in Section 1, some new ingredients are needed in order to analyze the spectral properties
of a point that is close to the optimal solution set X . Before we proceed, let us set up the notation
that will be used in the proof. Let L > 0 denote the Lipschitz constant of ?h and ?k ? k? denote the
subdifferential of k ? k? . Given a sequence {X k }k?0 ? Rm?n \X , define
? k = arg minY ?X kX k ? Y kF , ?k = kX k ? X
? k kF ,
Rk = R(X k ), X
k
k
k
k
?
k
?
?
z = A(X ), G = ?f (X ) = A (?h(z )), G = A (?h(?
z )),

(13)

?

where A is the adjoint operator of A. The crux of the proof of Theorem 3.1 is the following lemma:
Lemma 4.1 Under the setting of Theorem 3.1, suppose that there exists a convergent sequence
{X k }k?0 ? Rm?n \X satisfying
F (X k ) ? ? for all k ? 0,

Rk ? 0,

Rk
? 0.
?k

(14)

Then, the following hold:
? of {X k }k?0 belongs to X .
(a) (Asymptotic Optimality) The limit point X
(b) (Bounded Iterates) There exists a convex compact subset Z of dom(h) such that z k , z? ? Z
for all k ? 0. Consequently, there exists a constant ? ? (0, L] such that for all k ? 0,
(?h(z k ) ? ?h(?
z ))T (z k ? z?) ? ?kz k ? z?k22 .

(15)

(c) (Restricted Invertibility) There exists a constant ? > 0 such that
? k kF ? ?kz k ? z?k2 = ?kA(X k ? X
? k )k2
kX k ? X

for all k ? 0.

(16)

? k )k2 ? kAk ? kX k ? X
? k kF , where kAk = sup
It is clear that kA(X k ? X
kY kF =1 kA(Y )k2 is
the spectral norm of A. Thus, the key element in Lemma 4.1 is the restricted invertibility property
(16). For the sake of continuity, let us proceed to prove Theorem 3.1 by assuming the validity of
Lemma 4.1.
Proof. [Theorem 3.1] We argue by contradiction. Suppose that there exists ? ? v ? such that (8) fails
to hold for all ? > 0 and ? > 0. Then, there exists a sequence {X k }k?0 ? Rm?n \X satisfying (14).
Since {X ? Rm?n : F (X) ? ?} is bounded (see Section 2.2), by passing to a subsequence if
? Hence, the premises of Lemma 4.1
necessary, we may assume that {X k }k?0 converges to some X.
are satisfied. Now, by Fermat?s rule [15, Theorem 10.1], for each k ? 0,
	

(17)
Rk ? arg min hGk + Rk , Di + ? kX k + Dk? .
D

Hence, we have

? k ? X k i + ? kX
? k k? .
hGk + Rk , Rk i + ? kX k + Rk k? ? hGk + Rk , X
? k ? X and ?f (X
? k ) = G,
? we also have ?G
? ? ? ?kX
? k k? , which implies that
Since X
? k k? ? hG,
? X k + Rk ? X
? k i + ? kX k + Rk k? .
? kX
Adding the two inequalities above and simplifying yield
? Xk ? X
? k i + kRk k2 ? hG
? ? Gk , Rk i + hRk , X
? k ? X k i.
hGk ? G,
F
5

(18)

? k ), by Lemma 4.1(b,c),
Since z k = A(X k ) and z? = A(X
?
? k k2 . (19)
? Xk ? X
? k i = (?h(z k ) ? ?h(?
hGk ? G,
z ))T (z k ? z?) ? ?kz k ? z?k22 ? 2 kX k ? X
F
?
Hence, it follows from (15), (18), (19) and the Lipschitz continuity of ?h that
?
? k k2 + kRk k2 ? (?h(?
? k ? Xki
kX k ? X
z ) ? ?h(z k ))T A(Rk ) + hRk , X
F
F
?2
? k kF kRk kF + kX k ? X
? k kF kRk kF .
? LkAk2 kX k ? X
In particular, this implies that
?
? k k2 ? (LkAk2 + 1)kX k ? X
? k kF kRk kF
kX k ? X
F
?2
? k kF , yields a contradiction to (14). 
for all k ? 0, which, upon dividing both sides by kX k ? X
4.1 Proof of Lemma 4.1
We now return to the proof of Lemma 4.1. Since Rk ? 0 by (14) and R is continuous, we have
? = 0, which implies that X
? ? X . This establishes (a). To prove (b), observe that due to (a), the
R(X)
k
sequence {X }k?0 is bounded. Hence, the sequence {A(X k )}k?0 is also bounded, which implies
? lie in a convex compact subset Z of dom(h) for all
that the points z k = A(X k ) and z? = A(X)
k ? 0. The inequality (15) then follows from Assumption 2.1(b). Note that we have ? ? L, as ?h
is Lipschitz continuous with parameter L.
To prove (c), we argue by contradiction. Suppose that (16) is false. Then, by further passing to a
subsequence if necessary, we may assume that

? k kF ? 0.
(20)
kA(X k ) ? z?k2 kX k ? X
In the sequel, we will also assume without loss of generality that m ? n. The following proposition
establishes a property of the optimal solution set X that will play a crucial role in our proof.
? ? X . Let X
? ?G
? = U
? [Diag(?
? ) 0] V? T be the singular
Proposition 4.2 Consider a fixed X
m?m
n?n
? ? G,
? where U
? ? R
value decomposition of X
, V? ? R
are orthogonal matrices and ?
?
? ? G.
? Then, the matrices X
? and ?G
? can be simultaneously
is the vector of singular values of X
? and V? . Moreover, the set Xc ? X , which is defined as
singular?value?decomposed by U

	
? [Diag(?(X)) 0] V? T ,
Xc = X ? X : X = U
is a non?empty convex compact set.
? k ? Xc onto Xc . Let
By Proposition 4.2, for every k ? 0, the point X k has a unique projection X
? k kF = min kX k ? Y kF .
?k = kX k ? X
(21)
Y ?Xc

? k kF ? kX k ? X
? k kF = ?k . It follows from (20) that
Since Xc ? X 
, we have ?k = kX k ? X
k
k
k
?
kA(X ) ? z?k2 kX ? X kF ? 0. This is equivalent to A(Qk ) ? 0, where
?k
Xk ? X
Qk =
for all k ? 0.
(22)
?k
In particular, we have kQk kF = 1 for all k ? 0. By further passing to a subsequence if necessary,
? Clearly, we have A(Q)
? = 0 and kQk
? F = 1.
we will assume that {Qk }k?0 converges to some Q.
?
4.1.1 Decomposing Q
? =
Our goal now is to show that for k sufficiently large and ? > 0 sufficiently small, the point X
k
k
k
k
?
?
?
X + ?Q belongs to Xc and is closer to X than X is to X . This would then contradict the
? k is the projection of X k onto Xc . To begin, let ? k be the vector of singular values of
fact that X
k
k
? ? G,
? the sequence {? k }k?0 is bounded. Hence, for i = 1, . . . , m,
X ? G . Since X k ? Gk ? X
by passing to a subsequence if necessary, we can classify the sequence {?ik }k?0 into one of the
? k ) > 0 for all k ? 0; (C)
following three cases: (A) ?ik ? ? for all k ? 0; (B) ?ik > ? and ?i (X
k
k
? ) = 0 for all k ? 0. The following proposition gives the key structural properties
?i > ? and ?i (X
? that will lead to the desired contradiction:
of Q
6

? admits the decomposition Q
?=U
? [Diag(?) 0] V? T , where
Proposition 4.3 The matrix Q
?
? k)
?i (X
?
?
=
?
lim
? 0 in Case (A),
?
?
k??
?k
?i
for i = 1, . . . , m.
?
?R
in Case (B),
?
?
?
>0
in Case (C),

It should be noted that the decomposition given in Proposition 4.3 is not necessarily the singular
? as ? could have negative components. A proof of Proposition 4.3 can be
value decomposition of Q,
found in the supplementary material.
4.1.2 Completing the Proof
Armed with Proposition 4.3, we are now ready to complete the proof of Lemma 4.1(c). Since
? k , Qi
? > 0 for all k sufficiently large. Fix
Qk 6= 0 for all k ? 0, it follows from (22) that hX k ? X
k
?
?
?
? = 0,
any such k and let X = X + ?Q, where ? > 0 is a parameter to be determined. Since A(Q)
k
k
? = ?f (X
? ) = G.
? Moreover, since X
? ? Xc , by the optimality
it follows from (13) that ?f (X)
condition (4) and Proposition 4.2, we have
n
o
? k ) + ?i (?G)
? ? ? = ?i (X
? k ) for i = 1, . . . , m.
max 0, ?i (X
(23)
? satisfies
Now, we claim that for ? > 0 sufficiently small, X
? ? G)?
? vi = X
? v?i
S? (X
?
u
?Ti S? (X

? =
? G)

?
u
?Ti X

for i = 1, . . . , n,

(24)

for i = 1, . . . , m,

? (resp. V? ). This would then imply that X
? ? Xc . To prove
where u?i (resp. v?i ) is the i?th column of U
the claim, observe that for i = m + 1, . . . , n, both sides of (24) are equal to 0. Moreover, since
? k ? Xc , Propositions 4.2 and 4.3 give
X


? ?G
?=U
? Diag(?(X
? k ) + ?? + ?(?G))
?
X
0 V? T .
Thus, it suffices to show that for ? > 0 sufficiently small,
? k ) + ??i + ?i (?G)
? ?0
?i (X
?k

?k

? = ?i (X ) + ??i
s? (?i (X ) + ??i + ?i (?G))

for i = 1, . . . , m,

(25)

for i = 1, . . . , m.

(26)

Towards that end, fix an index i = 1, . . . , m and consider the three cases defined in Section 4.1.1:
? k ) = 0 for all k sufficiently large, then Proposition 4.3 gives ?i = 0. Moreover,
Case (A). If ?i (X
?
we have ?i (?G) ? ? by (23). This implies that both (25) and (26) are satisfied for any choice of
? k ) > 0 for all k sufficiently large, then Proposition 4.3 gives
? > 0. On the other hand, if ?i (X
?
? k ) + ??i ? 0, we
?i < 0. Moreover, we have ?i (?G) = ? by (23). By choosing ? > 0 so that ?i (X
can guarantee that both (25) and (26) are satisfied.
? k ) > 0 for all k ? 0, we have ?i (?G)
? = ? by (23). Hence, both (25) and (26)
Case (B). Since ?i (X
k
? ) + ??i ? 0.
can be satisfied by choosing ? > 0 so that ?i (X
? ? Xc . Since X k ? X
? and ?k = kX k ? X
? k kF ?
Case (C). By Proposition 4.2, we have X
k
k
k
? F , we have X
? ?X
? as well. It follows that ?i (X)
? = 0, as ?i (X
? ) = 0 for all k ? 0 by
kX ? Xk
? ?G
? and ? k > ? , we have ?
assumption. Now, since X k ? Gk ? X
?
?
?
. Thus, Proposition 4.2
i
i
? ? G)
? = ?i (X)
? + ?i (?G)
? = ?i (?G).
? This, together with (23), yields
implies that ? ? ?
?i = ?i (X
? = ? . Since ?i > 0 by Proposition 4.3, we conclude that both (25) and (26) can be satisfied
?i (?G)
by any choice of ? > 0.
? ? Xc . This, together with
Thus, in all three cases, the claim is established. In particular, we have X
k
k ?
?
?
hX ? X , Qi > 0 and kQkF = 1, yields
? 2F = kX k ? X
? k ? ?Qk
? 2F = kX k ? X
? k k2F ? 2?hX k ? X
? k , Qi
? + ?2 < kX k ? X
? k k2F
kX k ? Xk
? k is the projection of X k onto Xc . This
for ? > 0 sufficiently small, which contradicts the fact that X
completes the proof of Lemma 4.1(c).
7

5 Numerical Experiments
In this section, we complement our theoretical results by testing the numerical performance of the
PGM (5) on two problems: matrix completion and matrix classification.
Matrix Completion: We randomly generate an n ? n matrix M with a prescribed rank r. Then,
we fix a sampling ratio ? ? (0, 1] and sample p = ??n2 ? entries of M uniformly at random. This
induces a sampling operator P : Rm?n ? Rp and an observation vector b ? Rp . In our experiments,
we fix the rank r = 3 and use the square loss f (?) = kP(?) ? bk22 /2 with regularization parameter
? = 1 in problem (1). We then solve the resulting problem for different values of n and ? using the
PGM (5) with a fixed step size ? = 1. We stop the algorithm when F (X k ) ? F (X k+1 ) < 10?8 .
Figure 1 shows the semi?log plots of the error in objective value and the error in solution against the
number of iterations. It can be seen that as long as the iterates are close enough to the optimal set,
both the objective values and the solutions converge linearly.

0

10

?5

10

?10

0

200

400

600

800

1000

10

n=100
n=500
n=1000

0

10

?5

10

?10

10

0

50

100

Iterations

n = 1000

2

10

0

10

?2

10

?4

0

250

300

400

600

?10

10

0

200

400

800

1000

800

2

10

0

10

?2

10

0

1000

1200

Convergence Performance of Solution

1

n=100
n=500
n=1000

10

600

10

10

?=0.2
?=0.5
?=0.8

0

10

?1

10

?2

10

?3

10

?4

50

100

150

200

250

300

10

0

200

400

600

800

Iterations

Iterations

? = 0.3

n = 40

Iterations

n = 1000

?5

10

n = 40

?4

200

0

10

Iterations

Convergence Performance of Solution

4

?=0.1
?=0.3
?=0.5

Log(error of solution)

Log(error of solution)

10

10

200

?=0.2
?=0.5
?=0.8

? = 0.3

Convergence Performance of Solution

4

150

Convergence Performance of Objective Value

5

10

Iterations

Log(Error of Solution)

10

Convergence Performance of Objective Value

5

Log(error of objective value)

Log(error of objective value)

?=0.1
?=0.3
?=0.5

Log(Error of Objective Value)

Convergence Performance of Objective Value

5

10

1000

1200

Figure 2: Matrix Classification

Figure 1: Matrix Completion

Matrix Classification: We consider a matrix classification problem under the setting described
in [21]. Specifically, we first randomly generate a low-rank matrix classifier X ? , which is an n ? n
symmetric matrix of rank r. Then, we specify a sampling ratio ? ? (0, 1] and sample p = ??n2 ?/2
independent n ? n symmetric matrices W1 , . . . , Wp from the standard Wishart distribution with n
degrees of freedom. The label of Wi , denoted by yi , is given by sgn(hX ? , Wi i). In ourPexperiments,
p
we fix the rank r = 3, the dimension n = 40, and use the logistic loss f (?) =
i=1 log(1 +
exp(?yi h?, Wi i)) with regularization parameter ? = 1 in problem (1). Since a good lower bound
on the Lipschitz constant Lf of ?f is not readily available in this case, a backtracking line search
was adopted at each iteration to achieve an acceptable step size; see, e.g., [3]. We stop the algorithm
when F (X k ) ? F (X k+1 ) < 10?6 . Figure 2 shows the convergence performance of the PGM (5)
as ? varies. Again, it can be seen that both the objective values and the solutions converge linearly.

6 Conclusion
In this paper, we have established the linear convergence of the PGM for solving a class of trace
norm?regularized problems. Our convergence result does not require the objective function to be
strongly convex and is applicable to many settings in machine learning. The key technical tool in
the proof is a Lipschitzian error bound for trace norm?regularized problems, which could be of
independent interest. A future direction is to study error bounds for more general matrix norm?
regularized problems and their implications on the convergence rates of first?order methods.
Acknowledgments The authors would like to thank the anonymous reviewers for their careful
reading of the manuscript and insightful comments. The research of A. M.?C. So is supported in
part by a gift grant from Microsoft Research Asia.
8

References
[1] Y. Amit, M. Fink, N. Srebro, and S. Ullman. Uncovering Shared Structures in Multiclass Classification.
In Proc. 24th ICML, pages 17?24, 2007.
[2] A. Argyriou, T. Evgeniou, and M. Pontil. Convex Multi?Task Feature Learning. Mach. Learn., 73(3):243?
272, 2008.
[3] A. Beck and M. Teboulle. A Fast Iterative Shrinkage?Thresholding Algorithm for Linear Inverse Problems. SIAM J. Imaging Sci., 2(1):183?202, 2009.
[4] A. Ben-Tal and A. Nemirovski. Lectures on Modern Convex Optimization: Analysis, Algorithms, and
Engineering Applications. MPS?SIAM Series on Optimization. Society for Industrial and Applied Mathematics, Philadelphia, Pennsylvania, 2001.
[5] M. Fazel, H. Hindi, and S. P. Boyd. A Rank Minimization Heuristic with Application to Minimum Order
System Approximation. In Proc. 2001 ACC, pages 4734?4739, 2001.
[6] D. Gross. Recovering Low?Rank Matrices from Few Coefficients in Any Basis. IEEE Trans. Inf. Theory,
57(3):1548?1566, 2011.
[7] S. Ji, K.-F. Sze, Z. Zhou, A. M.-C. So, and Y. Ye. Beyond Convex Relaxation: A Polynomial?Time
Non?Convex Optimization Approach to Network Localization. In Proc. 32nd IEEE INFOCOM, pages
2499?2507, 2013.
[8] S. Ji and J. Ye. An Accelerated Gradient Method for Trace Norm Minimization. In Proc. 26th ICML,
pages 457?464, 2009.
[9] V. Koltchinskii, K. Lounici, and A. B. Tsybakov. Nuclear?Norm Penalization and Optimal Rates for
Noisy Low?Rank Matrix Completion. Ann. Stat., 39(5):2302?2329, 2011.
[10] Z.-Q. Luo and P. Tseng. Error Bounds and Convergence Analysis of Feasible Descent Methods: A
General Approach. Ann. Oper. Res., 46(1):157?178, 1993.
[11] S. Ma, D. Goldfarb, and L. Chen. Fixed Point and Bregman Iterative Methods for Matrix Rank Minimization. Math. Program., 128(1?2):321?353, 2011.
[12] Yu. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic Publishers, Boston, 2004.
[13] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed Minimum?Rank Solutions of Linear Matrix Equations
via Nuclear Norm Minimization. SIAM Rev., 52(3):471?501, 2010.
[14] R. T. Rockafellar. Convex Analysis. Princeton Landmarks in Mathematics and Physics. Princeton University Press, Princeton, New Jersey, 1997.
[15] R. T. Rockafellar and R. J.-B. Wets. Variational Analysis, volume 317 of Grundlehren der mathematischen Wissenschaften. Springer?Verlag, Berlin Heidelberg, second edition, 2004.
[16] M. Schmidt, N. Le Roux, and F. Bach. Convergence Rates of Inexact Proximal?Gradient Methods for
Convex Optimization. In Proc. NIPS 2011, pages 1458?1466, 2011.
[17] A. M.-C. So, Y. Ye, and J. Zhang. A Unified Theorem on SDP Rank Reduction. Math. Oper. Res.,
33(4):910?920, 2008.
[18] W. So. Facial Structures of Schatten p?Norms. Linear and Multilinear Algebra, 27(3):207?212, 1990.
[19] K.-C. Toh and S. Yun. An Accelerated Proximal Gradient Algorithm for Nuclear Norm Regularized
Linear Least Squares Problems. Pac. J. Optim., 6(3):615?640, 2010.
[20] R. Tomioka and K. Aihara. Classifying Matrices with a Spectral Regularization. In Proc. of the 24th
ICML, pages 895?902, 2007.
[21] R. Tomioka, T. Suzuki, M. Sugiyama, and H. Kashima. A Fast Augmented Lagrangian Algorithm for
Learning Low?Rank Matrices. In Proc. 27th ICML, pages 1087?1094, 2010.
[22] P. Tseng. Approximation Accuracy, Gradient Methods, and Error Bound for Structured Convex Optimization. Math. Program., 125(2):263?295, 2010.
[23] P. Tseng and S. Yun. A Coordinate Gradient Descent Method for Nonsmooth Separable Minimization.
Math. Program., 117(1?2):387?423, 2009.
[24] M. White, Y. Yu, X. Zhang, and D. Schuurmans. Convex Multi?View Subspace Learning. In Proc. NIPS
2012, pages 1682?1690, 2012.
[25] H. Zhang, J. Jiang, and Z.-Q. Luo. On the Linear Convergence of a Proximal Gradient Method for a Class
of Nonsmooth Convex Minimization Problems. J. Oper. Res. Soc. China, 1(2):163?186, 2013.

9

"
4128,"Minimizing Sparse High-Order Energies by
Submodular Vertex-Cover
Andrew Delong
University of Toronto

Olga Veksler
Western University

Anton Osokin
Moscow State University

Yuri Boykov
Western University

andrew.delong@gmail.com

olga@csd.uwo.ca

anton.osokin@gmail.com

yuri@csd.uwo.ca

Abstract
Inference in high-order graphical models has become important in recent years.
Several approaches are based, for example, on generalized message-passing, or
on transformation to a pairwise model with extra ?auxiliary? variables. We focus
on a special case where a much more efficient transformation is possible. Instead
of adding variables, we transform the original problem into a comparatively small
instance of submodular vertex-cover. These vertex-cover instances can then be
attacked by existing algorithms (e.g. belief propagation, QPBO), where they often
run 4?15 times faster and find better solutions than when applied to the original
problem. We evaluate our approach on synthetic data, then we show applications
within a fast hierarchical clustering and model-fitting framework.

1

Introduction

MAP inference on graphical models is a central problem in machine learning, pattern recognition,
and computer vision. Several algorithms have emerged as practical tools for inference, especially
for graphs containing only unary and pairwise factors. Prominent examples include belief propagation [30], more advanced message passing methods like TRW-S [21] or MPLP [33], combinatorial
methods like ?-expansion [6] (for ?metric? factors) and QPBO [32] (mainly for binary problems).
In terms of optimization, these algorithms are designed to minimize objective functions (energies)
containing unary and pairwise terms.
Many inference problems must be modeled using high-order terms, not just pairwise, and such
problems are increasingly important for many applications. Recent developments in high-order inference include, for example, high-arity CRF potentials [19, 38, 25, 31], cardinality-based potentials
[13, 34], global potentials controlling the appearance of labels [24, 26, 7], learning with high-order
loss functions [35], among many others.
One standard approach to high-order inference is to transform the problem to the pairwise case and
then simply apply one of the aforementioned ?pairwise? algorithms. These transformations add many
?auxiliary? variables to the problem but, if the high-order terms are sparse in the sense suggested
by Rother et al. [31], this can still be a very efficient approach. There can be several equivalent
high-order-to-pairwise transformations, and this choice affects the difficulty of the resulting pairwise inference problem. Choosing the ?easiest? transformation is not trivial and has been explicitly
studied, for example, by Gallagher et al. [11].
Our work is about fast energy minimization (MAP inference) for particularly sparse, high-order ?pattern potentials? used in [25, 31, 29]: each energy term prefers a specific (but arbitrary) assignment
to its subset of variables. Instead of directly transforming the high-order problem to pairwise, we
transform the entire problem to a comparatively small instance of submodular vertex-cover ( SVC).
The vertex-cover implicitly provides a solution to the original high-order problem. The SVC instance can itself be converted to pairwise, and standard inference techniques run much faster and are
often more effective on this compact representation.
1

We also show that our ?sparse? high-order energies naturally appear when trying to solve hierarchical clustering problems using the algorithmic approach called fusion moves [27], also conceptually
known as optimized crossover [1]. Fusion is a powerful very large-scale neighborhood search technique [3] that in some sense generalizes ?-expansion. The fusion approach is not standard for the
kind of clustering objective we will consider, but we believe it is an interesting optimization strategy.
The remainder of the paper is organized as follows. Section 2 introduces the class of high-order energies we consider, then derives the transformation to SVC and the subsequent decoding. Section 3
contains experiments that suggest significant speedups, and discusses possible applications.

2

Sparse High-Order Energies Reducible to SVC

?
In what follows
? we use x to denote a vector of binary variables, xP to denote product i?P xi , and
xQ to denote i?Q xi . It will be convenient to adopt the convention that x{} = 1 and x{} = 1. We
always use i to denote a variable index from I, and j to denote a clique index from V.
It is well-known that any pseudo-boolean function (binary energy) can be written in the form
?
?
F (x) =
ai xi ?
bj xPj xQj
i?I

(1)

j?V

where each clique j has coefficient ?bj with bj ? 0, and is defined over variables in sets Pj , Qj ? I.
Our approach will be of practical interest only when, roughly speaking, |V| ? |I|.
For example, if x = (x1 , . . . , x7 ) then a clique j with Pj = {2, 3} and Qj = {4, 5, 6} will explicitly
reward binary configuration (? , 1, 1, 0, 0, 0, ?) by the amount bj (depicted as b1 in Figure 1). If there
are several overlapping (and conflicting) cliques, then the minimization problem can be difficult.
A standard way to minimize F (x) would be to substitute each ?bj xPj xQj term with a collection
of equivalent pairwise
terms. ?
In our experiments, we used the substitution ?xPj xQj = ?1 +
?
miny?{0,1} y + i?Pj xi y + i?Qj xi y where y is an auxiliary variable. This is like the Type-II
transformation in [31], and we found that it worked better than Type-I for our experiments. However,
we aim to minimize F (x) in a novel way, so first we review the submodular vertex-cover problem.
2.1

Review of Submodular Vertex-Cover

The classic minimum-weighted vertex-cover (VC) problem can be stated as a 0-1 integer program
where variable uj = 1 if and only if vertex j is included in the cover.
?
(2)
( VC) minimize
j?V wj uj
subject to uj + uj? ? 1 ?{j, j ? } ? E
uj ? {0, 1}.

(3)

Without loss of generality one can assume wj > 0 and j ?= j ? for all {j, j ? } ? E. If the graph
(V, E) is bipartite, then we call the specialized problem VC - B and it can be solved very efficiently
by specialized bipartite maximum flow algorithms such as [2].
A function f (x) is called submodular if f (x ? y) + f (x ? y) ? f (x) + f (y) for all x, y ? {0, 1}V
where (x ? y)j = xj yj and (x ? y)j = 1 ? xj y j . A submodular function can be minimized in
strongly polynomial time by combinatorial methods [17], but becomes NP-hard when subject to
arbitrary covering constraints like (3).
The submodular vertex-cover ( SVC) problem generalizes VC by replacing the linear (modular)
objective (2) with an arbitrary submodular objective,
(SVC )

minimize f (u)
subject to uj + uj? ? 1 ?{j, j ? } ? E
uj ? {0, 1}.

(4)

Iwata & Nagano [18] recently showed that when f (?) ? 0 a 2-approximation can be found in
polynomial time and that this is the best constant-ratio bound achievable. It turns out that a halfintegral relaxation uj ? {0, 12 , 1} (call this problem SVC - H), followed by upward rounding, gives
2

a 2-approximation much like for standard VC. They also show how to transform any SVC - H
instance into a bipartite instance of SVC (see below); this extends a classic result by Nemhauser &
Trotter [28], allowing specialized combinatorial algorithms like [17] to solve the relaxation.
In the bipartite submodular vertex-cover ( SVC - B) problem, the graph nodes V can be partitioned
into sets J , K so the binary variables are u ? {0, 1}J, v ? {0, 1}K and we solve
( SVC - B)

minimize f (u) + g(v)
subject to uj + vk ? 1 ?{j, k} ? E
uj , vk ? {0, 1} ?j ? J , k ? K

(5)

where both f (?) and g(?) are submodular functions. This SVC - B formulation is a trivial extension
of the construction in [18] (they assume g = f ), and their proof of tractability extends easily to (5).
2.2

Solving Bipartite SVC with Min-Cut

It will be useful to note that if f and g above can be written in a special manner, SVC - B can
be solved by fast s-t minimum cut instead of by [17, 15]. Suppose we have an SVC - B instance
(J , K, E, f, g) where we can write submodular f and g as
?
?
f (u) =
wS uS , and g(v) =
wS vS .
(6)
S?S 0

S?S 1

Here S 0?and S 1 are collections of subsets of J and K respectively, and typescript uS denotes
product j?S uj throughout (as distinct from typescript u, which denotes a vector).
Proposition 1. If wS ? 0 for all |S| ? 2 in (6), then SVC - B reduces to s-t minimum cut.
Proof. We can define an equivalent problem over variables uj and zk = v k . With this substitution,
the covering constraints become uj ? zk . Since ?g(v) submodular in v? implies ?g(1?v) submodular in v,? letting g?(z) = g(z) = g(v) means g?(z) is submodular as a function of z. Minimizing
f (u)+ g?(z) subject to uj ? zk is equivalent to our original problem. Since uj ? zk can be enforced
by large (submodular) penalty on assignment uj zk , SVC - B is equivalent to
?
? uj zk where ? = ?.
minimize f (u) + g?(z) +
(7)
(j,k)?E

When f and g take the form (6), we have g?(z) =

?

S?S 1

wS zS where zS denotes product

?
k?S

zk .

If wS ? 0 for all |S| ? 2, we can build an s-t minimum cut graph corresponding to (7) by directly
applying the constructions in [23, 10]. We can do this because each term has coefficient wS ? 0
when written as u1 ? ? ? u|S| or z 1 ? ? ? z |S| , i.e. either all complemented or all uncomplemented.
2.3

Transforming F (x) to SVC

To get a sense for how our transformation works, see Figure 1. The transformation is reminiscent of
the binary dual of a Constraint Satisfaction Problem (CSP) [37]. The vertex-cover construction of
[4] is actually a special linear (modular) case of our transformation (details in Proposition 2).

?7
Figure 1: Left: factor graph F (x) = i=1 ai xi ?b1 x2 x3 x4 x5 x6 ?b2 x1 x2 x3 x4 x5 ?b3 x3 x4 x5 x6 x7 .
A small white square indicates ai > 0, a black square ai < 0. A hollow edge connecting xi to factor
j indicates i ? Pj , and a filled-in edge indicates i ? Qj . Right: factor graph of our corresponding
SVC instance. High-order factors of the original problem, shown with gray squares on the left, are
transformed into variables of SVC problem. Covering constraints are shown as dashed lines. Two
pairwise factors are formed with coefficients w{1,3} = ?a3 and w{1,2} = a4 + a5 , both ? 0.
3

Theorem 1. For any F (x) there exists an instance of SVC such that an optimum x? ? {0, 1}I for
F can be computed from an optimal vertex-cover u? ? {0, 1}V .
Proof. First we give the construction for SVC instance (V, E, f ). Introduce auxiliary binary variables u ? {0, 1}V where uj = xPj xQj . Because each bj ? 0, minimizing F (x) is equivalent to the
0-1 integer program with non-linear constraints
minimize F (x, u)
subject to uj ? xPj xQj ?j ? V.
(8)
Inequality (8) is sufficient if bj ? 0 because, for any fixed x, equality uj = xPj xQj holds for some
u that minimizes F (x, u).
We try to formulate a minimization problem solely over u. As a consequence of (8) we have uj =
0 ? xPj = 1, xQj = 0. (We use typescript xS to denote vector (xi )i?S , whereas xS denotes a
product?a scalar value.) Notice that, when some Pj and Qj? overlap, not all u ? {0, 1}V can be
feasible with respect to assignments x ? {0, 1}I . For each i ? I, let us collect the cliques that i
participates in: define sets Ji , Ki ? V where Ji = { j | i ? Pj } and Ki = { j | i ? Qj }. We show
that u can be feasible if and only if uJi + uKi ? 1 for all i ? I, where uS denotes a product. In
other words, u can be feasible if and only if, for each i,
? uj = 0, j ? Ji =? uk = 1 ?j ? Ki
(9)
? uk = 0, k ? Ki =? uj = 1 ?j ? Ji .
(?) If uj ? xPj xQj for all j ? V, then having uJi + uKi ? 1 is necessary: if both uJi = 0 and
uKi = 0 for any i it would mean there exists j ? Ji and k ? Ki for which xPj = 1 and xQk = 0,
contradicting any unique assignment to xi .
(?) If uJi + uKi ? 1 for all i ? I, then we can always choose some x ? {0, 1}I for which every
uj ? xPj xQj . It will be convenient to choose a minimum cost assignment for each xi , subject to the
constraints uJi = 0 ? xi = 1 and uKi = 0 ? xi = 0. If both uJi = uKi = 1 then xi could be
either 0 or 1 so choose the best, giving {
0
if uKi = 0
1
if uJi = 0
(10)
x(u)i =
[ai < 0] otherwise.
The assignment x(u) is feasible with respect to (8) because for any uj = 1 we have x(u)Pj = 1
and x(u)Qj = 0.
We have completed the proof that u can be feasible if and only if uJi + uKi ? 1. To express
minimization of F solely in terms of u, first write (10) in equivalent form
{
uKi
if ai < 0
(11)
x(u)i =
1 ? uJi otherwise.
Again, this definition of x(u) minimizes F (x, u) over all x satisfying inequality (8). Use (11) to
write new SVC objective f (u) = F (x(u), u), which becomes
?
?
?
ai (1 ? uJi ) +
ai uKi ?
bj (1 ? uj )
f (u) =
i : ai >0

=

?

i : ai <0

?ai uJi +

i : ai >0

?

ai uKi +

i : ai <0

?

j?V

bj uj + const.

(12)

j?V

To collect coefficients in the first two summands of (12), we must group them by each unique clique
that appears. We define set S = {S ? V | (?Ji = S) ? (?Ki = S)} and write
?
f (u) =
wS uS + const
(13)
S?S

where wS =

?

?ai +

i : ai >0,
Ji =S

?

(
ai

)
+ bj if S = {j} .

(14)

i : ai <0,
Ki =S

Since the high-order terms uS in (13) have non-positive coefficients wS ? 0, then f (u) is submodular [5]. Also note that for each i at most one of Ji or Ki contributes to the sum, so there are at most
|S| ? |I| unique terms uS with wS ?= 0. If |S|, |V| ? |I| then our SVC instance will be small.
Finally, to ensure (9) holds we add a covering constraint uj + uk ? 1 whenever there exists i such
that j ? Ji , k ? Ki . For this SVC instance, an optimal covering u minimizes F (x(u), u).
4

The construction in Theorem 1 suggests the entire minimization procedure below.
M INIMIZE - BY-SVC(F ) where F is a pseudo-boolean function in the form of (1)
1
2
3
4
5
6
7
8

w{j} := bj ?j ? V
for i ? I do
if
ai > 0 then wJi := wJi ? ai
else if ai < 0 then wKi := wKi + ai
E := E ? {{j, k}} ?j ? Ji , k ? Ki
?
let f (u) = S?S wS uS
?
u := S OLVE -SVC(V, E, f )
return x(u? )

(distribute ai to high-order SVC coefficients)
(where index sets Ji and Ki defined in Theorem 1)
(add covering constraints to enforce uJi + uKi ? 1)
(define SVC objective over clique indices V)
(solve with BP, QPBO, Iwata, etc.)
(decode the covering as in (10))

One reviewer suggested an extension that scales better with the number of overlapping cliques. The
idea is to formulate SVC over the
rather than V. Specifically, let y ? {0, 1}S and use
? elements of S?
submodular objective f (y) = S?S wS yS + j?S (bj + 1)yS y{j} , where the inner sum ensures
?
yS = j?S y{j} at a local minimum because w{j} ? bj . For each unique pair {Ji , Ki }, add a
covering constraint yJi + yKi ? 1 (instead of O(|Ji |?|Ki |) constraints). An optimal covering y ? of S
?
then gives an optimal covering of V by assigning uj = y{j}
. Here we use the original construction,
and still report significant speedups. See [8] for discussion of efficient implementation, and an
alternate proof of Theorem 1 based on LP relaxation.
2.4

Special Cases of Note

Proposition 2. If {Pj }j?V are disjoint and, separately, {Qj }j?V are disjoint (equivalently each
|Ji |, |Ki | ? 1), then the SVC instance in Theorem 1 reduces to standard VC .
Proof. Each
? S ? S in objective (13) must be S = {j} for some j ? V. The objective then becomes
f (u) = j?V w{j} uj + const, a form of standard VC .
Proposition 2 shows that the main result of [4] is a special case of our Theorem 1 when Ji = {j}
and Ki = {k} with j, k determined by two labelings being ?fused?. In Section 3, this generalization
of [4] will allow us to apply a similar fusion-based algorithm to hierarchical clustering problems.
Proposition 3. If each particular j ? V has either Pj = {} or Qj = {}, then the construction in
Theorem 1 is an instance of SVC - B. Moreover, it is reducible to s-t minimum cut.
Proof. In this case Ji is disjoint with Ki? for any i, i? ? I, so sets J = {j : |Pj | ? 1} and
K = {j : |Qj | ? 1} are disjoint. Since E contains pairs (j, k) with j ? J and k ? K, graph (V, E)
is bipartite. By the disjointness of any Ji and Ki? , the unique clique sets S can be partitioned into
S 0 = {S ? J | ?Ji = S} and S 1 = {S ? K | ?Ki = S} so that (13) can be written as in
Proposition 1 and thereby reduced to s-t minimum cut.
Corollary 1. If sets {Pj }j?V and {Qj }j?V satisfy the conditions of propositions 2 and 3, then
minimizing F (x) reduces to an instance of VC - B and can be solved by bipartite maximum flow.
We should note that even though SVC has a 2-approximation algorithm [18], this does not give us
a 2-approximation for minimizing F in general. Even if F (x) ? 0 for all x, it does not imply
f (u) ? 0 for configurations of u that violate the covering constraints, as would be required.

3 Applications
Even though any pseudo-boolean function can be expressed in form (1), many interesting problems
would require an exponential number of terms to be expressed in that form. Only certain specific
applications will naturally have |V| ? |I|, so this is the main limitation of our approach. There may
be applications in high-order segmentation. For example, when P n -Potts potentials [19] are incorporated into ?-expansion, the resulting expansion step contains high-order terms that are compact
in this form; in the absence of pairwise CRF terms, Proposition 3 would apply.
The ?-expansion algorithm has also been extended to optimize the facility location objective [7]
commonly used for clustering (e.g. [24]). The resulting high-order terms inside the expansion step
5

lb+5

+7300

+20000

+56000 +120000

1
0.8
0.6
0.4
0.2

lb+5

+7300

+20000

+56000

+120000

ICM

ICM

BP

SVC-ICM

TRWS

SVC-BP

MPLP

SVC-TRWS

QPBO

SVC-MPLP

QPBOP

SVC-QPBO

QPBOI

SVC-QPBOP
SVC-QPBOI

0

?= 1

2

4

8

16

?= 1

2

4

8

16

SVC-Iwata

Figure 2: Effectiveness of each algorithm as strength of high-order coefficients is increased by factor
of ? ? {1..16}. For a fixed ?, the final energy of each algorithm was normalized between 0.0 (best
lower bound) and 1.0 (baseline ICM energy); the true energy gap between lower bound and baseline
is indicated at top, e.g. for ? = 1 the ?lb+5? means ICM was typically within 5 of the lower bound.

also take the form (1) (in fact, Corollary 1 applies here); with no need to build the ?full? high-order
graph, this would allow ?-expansion to work as a fast alternative to the classic greedy algorithm
for facility location, very similar to the fusion-based algorithm in [4]. However, in Section 3.2 we
show that our generalized transformation allows for a novel way to optimize a hierarchical facility
location objective. We will use a recent geometric image parsing model [36] as a specific example.
First, Section 3.1 compares a number of methods on synthetic instances of energy (1).
3.1

Results on Synthetic Instances

Each instance is a function F (x) where x represents a 100 ? 100 grid of binary variables with
random unary coefficients ai ? [?10, 10]. Each instance also has |J | = 50 high-order cliques with
bj ? [250?, 500?] (we will vary ?), where variable sets Pj and Qj each cover a random nj ? nj and
mj ? mj region respectively (here the region size nj , mj ? {10, . . . , 15} is chosen randomly). If
Pj and Qj are not disjoint, then either Pj := Pj \ Qj or Qj := Qj \ Pj , as determined by a coin flip.
We tested the following algorithms: BP [30], TRW-S [21], MPLP [33], QPBO [14], and extensions
QPBO-P and QPBO-I [32]. For BP we actually used the implementation provided by [21] which is
very fast but, we should note, does not support message-damping; convergence of BP may be more
reliable if this were supported. Algorithms were configured as follows: BP for 25 iterations (more
did not help); TRW-S for 800 iterations (epsilon 1); MPLP for 2000 initial iterations + 20 clusters
added + 100 iterations per tightening; QPBO-I with 5 random improve steps. We ran MPLP for a
particularly long time to ensure it had ample time to tighten and converge; indeed, it always yielded
the best lower bound. We also tested M INIMIZE - BY-SVC by applying each of these algorithms to
solve the resulting SVC problem, and in this case also tried the Iwata-Nagano construction [18].
To transform high-order potentials to quadratic, we report results using Type-II binary reduction [31]
because for TRW-S/MPLP it dominated the Type-I reduction in our experiments, and for BP and the
others it made no difference. This runs counter to the conventional used of ?number of supermodular
terms? as an estimate of difficulty: the Type-I reduction would generate one supermodular
edge per
?
high-order term, whereas Type-II generates |Pj | supermodular edges for each term ( i?Pj xi y).
One minor detail is how to evaluate the ?partial? labelings returned by QPBO and QPBO-P. In the
case of minimizing F directly, we simply assigned such variables xi = [ai < 0]. In the case of
M INIMIZE - BY-SVC we included all unlabeled nodes in the cover, which means a variable xi with
uJi and uKi all unlabeled will similarly be assigned xi = [ai < 0].
Figure 2 shows the relative performance of each algorithm, on average. When ? = 1 the high-order
coefficients are relatively weak compared to the unary terms, so even ICM succeeds at finding a
near-optimal energy. For larger ? the high-order terms become more important, and we make a
number of observations:
?
?
?
?

ICM, BP, TRW-S, MPLP all perform much better when applied to the SVC problem.
QPBO-based methods do not perform better when applied to the SVC problem.
QPBO-I consistently gives good results; BP also gives good results if applied to SVC.
The Iwata-Nagano construction is effectively the same as QBPO applied to SVC.
6

We also observed that the TRW-S lower bound was the same with or without transformation to
SVC , but convergence took much fewer iterations when applied to SVC. In principle, TRW on
binary problems solves the same LP relaxation as QPBO [22]. The TRW-S code finds much better
solutions because it uses the final messages as hints to decode a good solution, unlike for QPBO.
Table 1 gives typical running times for each of the cases in Figure 2 on a 2.66 GHz Intel Core2
processor. Code was written in C++, but the SVC transformation was not optimized at all. Still,
SVC-QBPOI is 20 times faster than QPBOI while giving similar energies on average. The overall
results suggest that SVC-BP or SVC-QPBOI are the fastest ways to find a low-energy solution (bold
in Table 1) on problems containing many conflicting high-order terms of the form (1). Running
times were relatively consistent for all ? ? 2.
Table 1: Typical running times of each algorithm. First row uses Type-II binary reduction on F ,
then directly runs each algorithm. Second row first transforms to SVC, does Type-II reduction, runs
the algorithm, and decodes the result; times shown include all these steps.
directly minimize F
M INIMIZE - BY-SVC(F )

3.2

BP
22ms
5.2ms

TRW-S
670ms
19ms

MPLP
25min
80sec

QPBO
30ms
5.4ms

QPBO-P
25sec
99ms

QPBO-I
140ms
7.2ms

Iwata
N/A
5ms

Application: Hierarchical Model-Estimation / Clustering

In clustering and multi-model estimation, it is quite common to either explicitly constrain the number of clusters, or?more relevant to our work?to penalize the number of clusters in a solution.
Penalizing the number of clusters is a kind of complexity penalty on the solution. Recent examples
include [24, 7, 26], but the basic idea has been used in many contexts over a long period. A classic
operations research problem with the same fundamental components is facility location: the clients
(data points) must be assigned to a nearby facility (cluster) but each facility costs money to open.
This can be thought of as a labeling problem, where each data point is a variable, and there is a label
for each cluster.
For hard optimization problems there is a particular algorithmic approach called fusion [27] or optimized crossover [1]. The basic idea is two take two candidate solutions (e.g. two attempts at clustering), and to ?fuse? the best parts of each solution, effectively stitching them together. To see this
more concretely, imagine a labeling problem where we wish to minimize E(l) where l = (li )i?I
is a vector of label assignments. If l0 is the first candidate labeling, and l1 is the second candidate
labeling, a fusion operation seeks a binary string x? such that the crossover labeling l(x) = (lixi )i?I
minimizes E(l(x)). In other words, x? identifies the best possible ?stitching? of the two candidate
solutions with respect to the energy.
In [4] we derived a fusion operation based on the greedy formulation of facility location, and found
that the subproblem reduced to minimum-weighted vertex-cover. We will now show that the fusion
operation for hierarchical facility location objectives requires minimizing an energy of the form (1),
which we have already shown can be transformed to a submodular vertex-cover problem. Givoni
et al. [12] recently proposed a message-passing scheme for hierarchical facility location, with experiments on synthetic and HIV strain data. We focus on more a computer vision-centric application:
detecting a hierarchy of lines and vanishing points in images using the geometric image parsing
objective proposed by Tretyak et al. [36].
The hierarchical energy proposed by [36] contains five ?layers?: edges, line segments, lines, vanishing points, and horizon. Each layer provides evidence for subsequent (higher) layers, and at each
level their is a complexity cost that regulates how much evidence is needed to detect a line, to detect
a vanishing point, etc. For simplicity we only model edges, lines, and vanishing points, but our
fusion-based framework easily extends to the full model. The purpose of our experiments are, first
and foremost, to demonstrate that M INIMIZE - BY-SVC speeds up inference and, secondly, to suggest that a hierarchical clustering framework based on fusion operations (similar to non-hierarchical
[4]) is an interesting and potentially worthwhile alternative to the greedy and local optimization used
in state-of-the-art methods like [36].
7

Let {y i }i?I be a set of oriented edges y i = (xi , yi , ?i ) where (x, y) is position in the image and ?
is an angle; these bottom-level features are generated by a Canny edge detector. Let L be a set of
candidate lines, and let V be a set of candidate vanishing points. These sets are built by randomly
sampling: one oriented edge to generate each candidate line, and pairs of lines to generate each
candidate vanishing point. Each line j ? L is associated with one vanishing point kj ? V. (If a line
passes close to multiple vanishing points, a copy of the line is made for each.) We seek a labeling l
where li ? L ? ? identifies the line (and vanishing point) that edge i belongs to, or assigns outlier
label ?. Let Di (j) = distj (xi , yi ) + distj (?i ) denote the spatial distance and angular deviation of
edge y i to line j, and let the outlier cost be Di (?) = const. Similarly, let Dj = distj (kj ) be the
distance of line j and its associated vanishing point projected onto the Gaussian sphere (see [36]).
Finally let Cl and Cv denote positive constants that penalize the detection of a line and a vanishing
point respectively. The hierarchical energy we minimize is
?
?
?
Cv ?[?kli = k].
(15)
E(l) =
Di (li ) +
(Cl + Dj )?[?li = j] +
i?I

j?L

k?V

This energy penalizes the number of unique lines, and the number of unique vanishing points that
labeling l depends on. Given two candidate labelings l0 , l1 , writing the fusion energy for (15) gives
?
?
?
E(l(x)) =
Di0 + (Di1 ? Di0 )xi +
(Cl + Dj )?(1?xPj xQj ) +
Cv ?(1?xPk xQk ) (16)
i?I

j?L

k?V

where Pj = { i | = j }, Qj = { i | = j }, and Pk = { i | kli0 = k }, Qk = { i | kli1 = k }.
Notice that sets {Pj } are disjoint with each other, but each Pj is nested in subset Pkj , so overall
Proposition 2 does not apply, and so neither does the algorithm in [4].
li0

li1

For each image we used 10,000 edges, generated 8,000 candidate lines and 150 candidate vanishing
points. We then generated 4 candidate labelings, each by allowing vanishing points to be detected
in randomized order, and their associated lines to be detected in greedy order, and then we fused
the labelings together by minimizing (16). Overall inference with QPBOI took 2?6 seconds per
image, whereas SVC-QPBOI took 0.5-0.9 seconds per image with relative speedup of 4?6 times.
The simplified model is enough to show that hierarchical clustering can be done in this new and
potentially powerful way. As argued in [27], fusion is a robust approach because it combines the
strengths?quite literally?of all methods used to generate candidates.

Figure 3: (Best seen in color.) Edge features color-coded by their detected vanishing point. Not
shown are the detected lines that make up the intermediate layer of inference (similar to [36]).
Images taken from York [9] and Eurasia [36] datasets.
Acknowledgements We thank Danny Tarlow for helpful discussion regarding MPLP, and an anonymous
reviewer for suggesting a more efficient way to enforce covering constraints(!). This work supported by NSERC
Discovery Grant R3584A02, Canadian Foundation for Innovation (CFI), and Early Researcher Award (ERA).

References
[1] Aggarwal, C.C., Orlin, J.B., & Tai, R.P. (1997) Optimized Crossover for the Independent Set Problem.
Operations Research 45(2):226?234.
[2] Ahuja, R.K., Orlin, J.B., Stein, C. & Tarjan, R.E. (1994) Improved algorithms for bipartite network flow.
SIAM Journal on Computing 23(5):906?933.
? Orlin, J.B., & Punnen, A.P. (2002) A survey of very large-scale neighborhood
[3] Ahuja, R.K., Ergun, O.,
search techniques. Discrete Applied Mathematics 123(1?3):75?202.
[4] Delong, A., Veksler, O. & Boykov, Y. (2012) Fast Fusion Moves for Multi-Model Estimation. European
Conference on Computer Vision.
[5] Boros, E. & Hammer, P.L. (2002) Pseudo-Boolean Optimization. Discrete App. Math. 123(1?3):155?225.
[6] Boykov, Y., Veksler, O., & Zabih, R. (2001) Fast Approximate Energy Minimization via Graph Cuts.
IEEE Transactions on Pattern Recognition and Machine Intelligence. 23(11):1222?1239.

8

[7] Delong, A., Osokin, A., Isack, H.N., & Boykov, Y. (20120) Fast Approximate Energy Minimization with
Label Costs. International Journal of Computer Vision 96(1):127. Earlier version in CVPR 2010.
[8] Delong, A., Veksler, O., Osokin, A., & Boykov, Y. (2012) Minimizing Sparse High-Order Energies by
Submodular Vertex-Cover. Technical Report, Western University.
[9] Denis, P., Elder, J., & Estrada, F. (2008) Efficient Edge-Based Methods for Estimating Manhattan Frames
in Urban Imagery. European Conference on Computer Vision.
[10] Freedman, D. & Drineas, P. (2005) Energy minimization via graph cuts: settling what is possible. IEEE
Conference on Computer Vision and Pattern Recognition.
[11] Gallagher, A.C., Batra, D., & Parikh, D. (2011) Inference for order reduction in Markov random fields.
IEEE Conference on Computer Vision and Pattern Recognition.
[12] Givoni, I.E., Chung, C., & Frey, B.J. (2011) Hierarchical Affinity Propagation. Uncertainty in AI.
[13] Gupta, R., Diwan, A., & Sarawagi, S. (2007) Efficient inference with cardinality-based clique potentials.
International Conference on Machine Learning.
[14] Hammer, P.L., Hansen, P., & Simeone, B. (1984) Roof duality, complementation and persistency in
quadratic 0-1 optimization. Mathematical Programming 28:121?155.
[15] Hochbaum, D.S. (2010) Submodular problems ? approximations and algorithms. Arxiv preprint
arXiv:1010.1945.
[16] Iwata, S., Fleischer, L. & Fujishige, S. (2001) A combinatorial, strongly polynomial-time algorithm for
minimizing submodular functions. Journal of the ACM 48:761?777.
[17] Iwata, S. & Orlin, J.B. (2009) A simple combinatorial algorithm for submodular function minimization.
ACM-SIAM Symposium on Discrete Algorithms.
[18] Iwata, S. & Nagano, K. (2009) Submodular Function Minimization under Covering Constraints. IEEE
Symposium on Foundations of Computer Science.
[19] Kohli, P., Kumar, M.P. & Torr, P.H.S. (2007) P 3 & Beyond: Solving Energies with Higher Order Cliques.
IEEE Conference on Computer Vision and Pattern Recognition.
[20] Kolmogorov, V. (2010) Minimizing a sum of submodular functions. Arxiv preprint arXiv:1006.1990.
[21] Kolmogorov, V. (2006) Convergent Tree-Reweighted Message Passing for Energy Minimization. IEEE
Transactions on Pattern Analysis and Machine Intelligence 28(10):1568?1583.
[22] Kolmogorov, V., & Wainwright, M.J. (2005) On the optimality of tree-reweighted max-product messagepassing. Uncertainty in Artificial Intelligence.
[23] Kolmogorov, V. & Zabih, R. (2004) What Energy Functions Can Be Optimized via Graph Cuts? IEEE
Transactions on Pattern Analysis and Machine Intelligence 26(2):147?159.
[24] Komodakis, N., Paragios, N., & Tziritas, G. (2008) Clustering via LP-based Stabilities. Neural Information Processing Systems.
[25] Komodakis, N., & Paragios, N. (2009) Beyond pairwise energies: Efficient optimization for higher-order
MRFs. IEEE Computer Vision and Pattern Recognition.
[26] Ladick?y, L., Russell, C., Kohli, P., & Torr, P.H.S (2010) Graph Cut based Inference with Co-occurrence
Statistics. European Conference on Computer Vision.
[27] Lempitsky, V., Rother, C., Roth, S., & Blake, A. (2010) Fusion Moves for Markov Random Field
Optimization. IEEE Transactions on Pattern Analysis and Machine Inference. 32(9):1392?1405.
[28] Nemhauser, G.L. and Trotter, L.E. (1975) Vertex packings: Structural properties and algorithms.
Mathematical Programming 8(1):232?248.
[29] Osokin, A., & Vetrov, D. (2012) Submodular relaxations for MRFs with high-order potentials. HiPot:
ECCV Workshop on Higher-Order Models and Global Constraints in Computer Vision.
[30] Pearl, J. (1988) Fusion, propagation, and structuring in belief networks. Artificial Intell. 29(3):251?288.
[31] Rother, C., Kohli, P., Feng, W., & Jia, J. (2009) Minimizing sparse higher order energy functions of
discrete variables. IEEE Conference on Computer Vision and Pattern Recognition.
[32] Rother, C., Kolmogorov, V., Lempitsky, V., & Szummer, M. (2007) Optimizing Binary MRFs via
Extended Roof Duality. IEEE Conference on Computer Vision and Pattern Recognition.
[33] Sontag, D., Meltzer, T., Globerson, A., Jaakkola, T., & Weiss, Y. (2008) Tightening LP relaxations for
MAP using message passing. Uncertainty in Artificial Intelligence.
[34] Tarlow, D., Givoni, I.E., & Zemel, R.S. (2010) HOPMAP: Efficient message passing with high order
potentials. International Conference on Artificial Intelligence and Statistics.
[35] Tarlow, D., & Zemel, R. (2012) Structured Output Learning with High Order Loss Functions. International Conference on Artificial Intelligence and Statistics.
[36] Tretyak, E., Barinova, O., Kohli, P., & Lempitsky, V. (2011) Geometric Image Parsing in Man-Made
Environments. International Journal of Computer Vision 97(3):305?321.
[37] Tsang, E. (1993) Foundations of constraint satisfaction. Academic Press, London.
[38] Werner, T. (2008) High-arity Interactions, Polyhedral Relaxations, and Cutting Plane Algorithm for Soft
Constraint Optimisation (MAP-MRF). IEEE Conference on Computer Vision and Pattern Recognition.

9

"
3322,"Object Bank: A High-Level Image Representation for Scene
Classification & Semantic Feature Sparsification

Li-Jia Li*1 , Hao Su*1 , Eric P. Xing2 , Li Fei-Fei1
1 Computer Science Department, Stanford University
2 Machine Learning Department, Carnegie Mellon University

Abstract
Robust low-level image features have been proven to be effective representations
for a variety of visual recognition tasks such as object recognition and scene classification; but pixels, or even local image patches, carry little semantic meanings.
For high level visual tasks, such low-level image representations are potentially
not enough. In this paper, we propose a high-level image representation, called the
Object Bank, where an image is represented as a scale-invariant response map of a
large number of pre-trained generic object detectors, blind to the testing dataset or
visual task. Leveraging on the Object Bank representation, superior performances
on high level visual recognition tasks can be achieved with simple off-the-shelf
classifiers such as logistic regression and linear SVM. Sparsity algorithms make
our representation more efficient and scalable for large scene datasets, and reveal
semantically meaningful feature patterns.

1 Introduction
Understanding the meanings and contents of images remains one of the most challenging problems
in machine intelligence and statistical learning. Contrast to inference tasks in other domains, such
as NLP, where the basic feature space in which the data lie usually bears explicit human perceivable
meaning, e.g., each dimension of a document embedding space could correspond to a word [21], or
a topic, common representations of visual data seem to primarily build on raw physical metrics of
the pixels such as color and intensity, or their mathematical transformations such as various filters,
or simple image statistics such as shape, edges orientations etc. Depending on the specific visual
inference task, such as classification, a predictive method is deployed to pool together and model the
statistics of the image features, and make use of them to build some hypothesis for the predictor. For
example, Fig.1 illustrates the gradient-based GIST features [25] and texture-based Spatial Pyramid
representation [19] of two different scenes (foresty mountain vs. street). But such schemes often
fail to offer sufficient discriminative power, as one can see from the very similar image statistics in
the examples in Fig.1.
Original Image

Gist (filters)

SIFT-SPM (L=2)

Object Filters in OB
Tree

Mountain

Tower

Sky

Tree

Mountain

Tower

Sky

Figure 1: (Best viewed in colors and magnification.) Comparison of object bank (OB) representation with
two low-level feature representations, GIST and SIFT-SPM of two types of images, mountain vs. city street.
From left to right, for each input image, we show the selected filter responses in the GIST representation [25],
a histogram of the SPM representation of SIFT patches [19], and a selected number of OB responses.
*indicates equal contributions.

1

While more sophisticated low-level feature engineering and recognition model design remain important sources of future developments, we argue that the use of semantically more meaningful feature
space, such as one that is directly based on the content (e.g., objects) of the images, as words for textual documents, may offer another promising venue to empower a computational visual recognizer
to potentially handle arbitrary natural images, especially in our current era where visual knowledge
of millions of common objects are readily available from various easy sources on the Internet.
In this paper, we propose ?Object Bank? (OB), a new representation of natural images based on
objects, or more rigorously, a collection of object sensing filters built on a generic collection of labeled objects. We explore how a simple linear hypothesis classifier, combined with a sparse-coding
scheme, can leverage on this representation, despite its extreme high-dimensionality, to achieve
superior predictive power over similar linear prediction models trained on conventional representations. We show that an image representation based on objects can be very useful in high-level visual
recognition tasks for scenes cluttered with objects. It provides complementary information to that of
the low-level features. As illustrated in Fig.1, these two different scenes show very different image
responses to objects such as tree, street, water, sky, etc. Given the availability of large-scale image
datasets such as LabelMe [30] and ImageNet [5], it is no longer inconceivable to obtain trained object detectors for a large number of visual concepts. In fact we envision the usage of thousands if
not millions of these available object detectors as the building block of such image representation in
the future.
While the OB representation offers a rich, high-level description of images, a key technical challenge due to this representation is the ?curse of dimensionality?, which is severe because of the size
(i.e., number of objects) of the object bank and the dimensionality of the response vector for each
object. Typically, for a modest sized picture, even hundreds of object detectors would result into a
representation of tens of thousands of dimensions. Therefore to achieve robust predictor on practical dataset with typically only dozens or a couple of hundreds of instances per class, structural risk
minimization via appropriate regularization of the predictive model is essential.
In this paper, we propose a regularized logistic regression method, akin to the group lasso approach
for structured sparsity, to explore both feature sparsity and object sparsity in the Object Bank representation for learning and classifying complex scenes. We show that by using this high-level image
representation and a simple sparse coding regularization, our algorithm not only achieves superior
image classification results in a number of challenging scene datasets, but also can discover semantically meaningful descriptions of the learned scene classes.

2 Related Work
A plethora of image descriptors have been developed for object recognition and image classification [25, 1, 23]. We particularly draw the analogy between our object bank and the texture filter
banks [26, 10].
Object detection and recognition also entail a large body of literature [7]. In this work, we mainly
use the current state-of-the-art object detectors of Felzenszwalb et. al. [9], as well as the geometric
context classifiers (?stuff? detectors) of Hoeim et. al. [13] for pre-training the object detectors.
The idea of using object detectors as the basic representation of images is analogous [12, 33, 35]. In
contrast to our work, in [12] and [33] each semantic concept is trained by using the entire images or
frames of video. As there is no localization of object concepts in scenes, understanding cluttered images composed of many objects will be challenging. In [35], a small number of concepts are trained
and only the most probable concept is used to form the representation for each region, whereas in
our approach all the detector responses are used to encode richer semantic information.
The idea of using many object detectors as the basic representation of images is analogous to approaches applying a large number of ?semantic concepts? to video and image annotation and retrieval [12, 33, 35]. In contrast to our work, in [12, 33, 35] each semantic concept is trained by using
entire images or frames of videos. There is no sense of localized representation of meaningful object
concepts in scenes. As a result, this approach is difficult to use for understanding cluttered images
composed of many objects.
Combinations of small set of (? a dozen of) off-the-shelf object detectors with global scene context
have been used to improve object detection [14, 28, 29]. Also related to our work is a very recent
exploration of using attributes for recognition [17, 8, 16]. But we emphasize such usage is not a
2

Object Detector Responses

Spatial Pyramid

Object Bank Representation

Sailboat

Max Response (OB)
Sailboat

Original Image

Response

Water

Water Sky
Bear

Bear

Objects
le

te
de

ca
rs
cto

Figure 2: (Best viewed in colors and magnification.) Illustration of OB. A large number of object detectors
are first applied to an input image at multiple scales. For each object at each scale, a three-level spatial pyramid
representation of the resulting object filter map is used, resulting in No.Objects ? No.Scales ? (12 + 22 + 42 )
grids; the maximum response for each object in each grid is then computed, resulting in a No.Objects length
feature vector for each grid. A concatenation of features in all grids leads to an OB descriptor for the image.

universal representation of images as we have proposed. To our knowledge, this is the first work that
use such high-level image features at different image location and scale.

3

The Object Bank Representation of Images

Object Bank (OB) is an image representation constructed from the responses of many object detectors, which can be viewed as the response of a ?generalized object convolution.? We use two
state-of-the-art detectors for this operation: the latent SVM object detectors [9] for most of the
blobby objects such as tables, cars, humans, etc, and a texture classifier by Hoiem [13] for more
texture- and material-based objects such as sky, road, sand, etc. We point out here that we use the
word ?object? in its very general form ? while cars and dogs are objects, so are sky and water. Our
image representation is agnostic to any specific type of object detector; we take the ?outsourcing?
approach and assume the availability of these pre-trained detectors.
Fig. 2 illustrates the general setup for obtaining the OB representation. A large number of object
detectors are run across an image at different scales. For each scale and each detector, we obtain an
initial response map of the image (see Appendix for more details of using the object detectors [9,
13]). In this paper, we use 200 object detectors at 12 detection scales and 3 spatial pyramid levels
(L=0,1,2) [19]. We note that this is a universal representation of any images for any tasks. We use
the same set of object detectors regardless of the scenes or the testing dataset.
3.1

Implementation Details of Object Bank

So what are the ?objects? to use in the object bank? And how many? An obvious answer to this
question is to use all objects. As the detectors become more robust, especially with the emergence
of large-scale datasets such as LabelMe [30] and ImageNet [5], this goal becomes more reachable.
But time is not fully ripe yet to consider using all objects in, say, the LabelMe dataset. Not enough
research has yet gone into building robust object detector for tens of thousands of generic objects.
And even more importantly, not all objects are of equal importance and prominence in natural images. As Fig.1 in Appendix shows, the distribution of objects follows Zipf?s Law, which implies
that a small proportion of object classes account for the majority of object instances.
For this paper, we will choose a few hundred most useful (or popular) objects in images1 . An important practical consideration for our study is to ensure the availability of enough training images for
each object detectors. We therefore focus our attention on obtaining the objects from popular image
datasets such as ESP [31], LabelMe [30], ImageNet [5] and the Flickr online photo sharing community. After ranking the objects according to their frequencies in each of these datasets, we take
the intersection set of the most frequent 1000 objects, resulting in 200 objects, where the identities
and semantic relations of some of them are illustrated in Fig.2 in the Appendix. To train each of the
200 object detectors, we use 100?200 images and their object bounding box information from the
LabelMe [30] (86 objects) and ImageNet [5] datasets (177 objects). We use a subset of LabelMe
scene dataset to evaluate the object detector performance. Final object detectors are selected based
on their performance on the validation set from LabelMe (see Appendix for more details).
1
This criterion prevents us from using the Caltech101/256 datasets to train our object detectors [6, 11] where
the objects are chosen without any particular considerations of their relevance to daily life pictures.

3

4

Scene Classification and Feature/Object Compression via Structured
Regularized Learning

We envisage that with the avalanche of annotated objects on the web, the number of object detectors in our object bank will increase quickly from hundreds to thousands or even millions, offering
increasingly rich signatures for each images based on the identity, location, and scale of the objectbased content of the scene. However, from a learning point of view, it also poses a challenge on how
to train predictive models built on such high-dimensional representation with limited number of examples. We argue that, with an ?overcomplete? OB representation, it is possible to compress ultrahigh dimensional image vector without losing semantic saliency. We refer this semantic-preserving
compression as content-based compression to contrast the conventional information-theoretic compression that aims at lossless reconstruction of the data.
In this paper, we intend to explore the power of OB representation in the context of Scene Classification, and we are also interested in discovering meaningful (possibly small subset of) dimensions during regularized learning for different classes of scenes. For simplicity, here we present our
model in the context of linear binary classier in a 1-versus-all classification scheme for K classes.
Generalization to a multiway softmax classifier is slightly more involved under structured regularization and thus deferred to future work. Let X = [xT1 ; xT2 ; . . . ; xTN ] ? RN ?J , an N ? J
matrix, represent the design built on the J-dimensional object bank representation of N images;
and let Y = (y1 , . . . , yN ) ? {0, 1}N denote the binary classification labels of N samples. A
linear classifier is a function h? : RJ ? {0, 1} defined as h? (x) , arg maxy?{0,1} x?, where
? = (?1 , . . . , ?J ) ? RJ is a vector ofP
parameters to be estimated. This leads to the following
m
1
learning problem min??RJ ?R(?) + m
i=1 L(?; xi , yi ), where L(?; x, y) is some non-negative,
convex loss, m is the number of training images, R(?) is a regularizer that avoids overfitting, and
? ? R is the regularization coefficient, whose value can be determined by cross validation.
A common choice of L is the Log loss, L = log(1/P (yi |xi , ?)), where P (yi |xi , ?)) is the logistic function P (y|x, ?)) = Z1 exp( 12 y(x ? ?)). This leads to the popular logistic regression (LR)
classifier2 . Structural risk minimization schemes over LR via various forms of regularizations have
been widely studied and understood in the literature. In particular, recent asymptotic analysis of the
`1 norm and `1 /`2 mixed norm regularized LR proved that under certain conditions the estimated
sparse coefficient vector ? enjoys a property called sparsistency [34], suggesting their applicability for meaningful variable selection in high-dimensional feature space. In this paper, we employ
an LR classifier for our scene classification problem. We investigate content-based compression
of the high-dimensional OB representation that exploits raw feature-, object-, and (feature+object)sparsity, respectively, using LR with appropriate regularization.
PJ
Feature sparsity via `1 regularized LR (LR1) By letting R(?) , k?k1 =
j=1 |?j |, we
obtain an estimator of ? that is sparse. The shrinkage function on ? is applied indistinguishably
to all dimensions in the OB representation, and it does not have a mechanism to incorporate any
potential coupling of multiple features that are possibly synergistic, e.g., features induced by the
same object detector. We call such a sparsity pattern feature sparsity, and denote the resultant
coefficient estimator by ? F .
Object sparsity via `1 /`2 (group) regularized LR (LRG) Recently, a mixed-norm (e.g., `1 /`2 )
regularization [36] has been used for recovery of joint sparsity across input dimensions. By letting
PJ
R(?) , k?k1,2 = j=1 k? j k2 , where ? j is the j-th group (i.e., features grouped by an object j),
and k ? k2 is the vector `2 -norm, we set the feature group to be corresponding to that of all features
induced by the same object in the OB. This shrinkage tends to encourage features in the same group
to be jointly zero. Therefore, the sparsity is now imposed on object level, rather than merely on raw
feature level. Such structured sparsity is often desired because it is expected to generate semantically
more meaningful lossless compression, that is, out of all the objects in the OB, only a few are needed
to represent any given natural image. We call such a sparsity pattern object sparsity, and denote the
resultant coefficient estimator by ? O .
2
We choose not to use the popular SVM which correspond to L being a hinge loss and R(?) being a
`2 -regularizer, because under SVM, content-based compression via structured regularization is much harder.

4

0.6

Gist

BOW

SPM

OB-SVM OB-LR

Classification on MIT Indoor

0.36

0.7

0.32

0.6

0.28

0.5

0.24

0.4

Gist

BOW

SPM

0.20

OB-SVM OB-LR

0.8

average percent correctness

0.7

0.40

Classification on LabelMe Scenes

average percent correctness

0.8

0.5

0.8

Classification on 15-Scenes

average percent correctness

average percent correctness

0.9

Gist

BOW

SPM

OB-SVM OB-LR

Classification on UIUC-Sports

0.7

0.6

0.5

Gist BOW

SPM Pseudo OB OB-SVM OB-LR

Figure 3: (Best viewed in colors and magnification.) Comparison of classification performance of different
features (GIST vs. BOW vs. SPM vs. OB) and classifiers (SVM vs. LR) on (top to down) 15 scene, LabelMe,
UIUC-Sports and MIT-Indoor datasets. In the LabelMe dataset, the ?ideal? classification accuracy is 90%,
where we use the human ground-truth object identities to predict the labels of the scene classes. The blue bar
in the last panel is the performance of ?pseudo? object bank representation extracted from the same number
of ?pseudo? object detectors. The values of the parameters in these ?pseudo? detectors are generated without
altering the original detector structures. In the case of linear classifier, the weights of the classifier are randomly
generated from a uniform distribution instead of learned. ?Pseudo? OB is then extracted with exactly the same
setting as OB.

Joint object/feature sparsity via `1 /`2 + `1 (sparse group) regularized LR (LRG1) The groupregularized LR does not, however, yield sparsity within a group (object) for those groups with nonzero total weights. That is, if a group of parameters is non-zero, they will all be non-zero. Translating
to the OB representation, this means there is no scale or spatial location selection for an object. To
remedy this, we proposed a composite regularizer, R(?) , ?1 k?k1,2 + ?2 k?k1 , which conjoin the
sparsification effects of both shrinkage functions, and yields sparsity at both the group and individual
feature levels. This regularizer necessitates determination of two regularization parameters ?1 and
?2 , and therefore is more difficult to optimize. Furthermore, although the optimization problem for
`1 /`2 + `1 regularized LR is convex, the non-smooth penalty function makes the optimization highly
nontrivial. In the Appendix, we derive a coordinate descent algorithm for solving this problem. To
conclude, we call the sparse group shrinkage patten object/feature sparsity, and denote the resultant
coefficient estimator by ? OF .

5

Experiments and Results

Dataset We evaluate the OB representation on 4 scene datasets, ranging from generic natural scene
images (15-Scene, LabelMe 9-class scene dataset3 ), to cluttered indoor images (MIT Indoor Scene),
and to complex event and activity images (UIUC-Sports). Scene classification performance is evaluated by average multi-way classification accuracy over all scene classes in each dataset. We list
below the experiment setting for each dataset:
? 15-Scene: This is a dataset of 15 natural scene classes. We use 100 images in each class for training
and rest for testing following [19].
? LabelMe: This is a dataset of 9 classes. 50 images randomly drawn images from each scene classes
are used for training and 50 for testing.
? MIT Indoor: This is a dataset of 15620 images over 67 indoor scenes assembled by [27]. We follow
their experimental setting in [27] by using 80 images from each class for training and 20 for testing.
? UIUC-Sports: This is a dataset of 8 complex event classes. 70 randomly drawn images from each
classes are used for training and 60 for testing following [22].

Experiment Setup We compare OB in scene classification tasks with different types of conventional image features, such as SIFT-BoW [23, 3], GIST [25] and SPM [19]. An off-the-shelf SVM
classifier, and an in-house implementation of the logistic regression (LR) classifier were used on
all feature representations being compared. We investigate the behaviors of different structural risk
minimization schemes over LR on the OB representation. As introduced in Sec 4, we experimented
`1 regularized LR (LR1), `1 /`2 regularized LR (LRG) and `1 /`2 + `1 regularized LR (LRG1).
5.1

Scene Classification

Fig.3 summarizes the results on scene classification based on OB and a set of well known lowlevel feature representations: GIST [25], Bag of Words (BOW) [3] and Spatial Pyramid Matching
3
From 100 popular scene names, we obtained 9 classes from the LabelMe dataset in which there are more
than 100 images: beach, mountain, bathroom, church, garage, office, sail, street, forest. The maximum number
of images in those classes is 1000.

5

(SPM) [19] on four challenging scene datasets. We show the results of OB using both an LR classifier and a linear SVM 4 We achieve substantially superior performances on three out of four datasets,
and are on par with the 15-Scene dataset. The substantial performance gain on the UIUC-Sports
and the MIT-Indoor scene datasets illustrates the importance of using a semantically meaningful
representation for complex scenes cluttered with objects. For example, the difference between a livingroom and a bedroom is less so in the overall texture (easily captured by BoW or GIST), but more
so in the different objects and their arrangements. This result underscores the effectiveness of OB,
highlighting the fact that in high-level visual tasks such as complex scene recognition, a higher level
image representation can be very useful. We further decompose the spatial structure and semantic
meaning encoded in OB by using a ?pseudo? OB without semantic meaning. The significant improvement of OB in classification performance over the ?pseudo object bank? is largely attributed
to the effectiveness of using object detectors trained from image. For each of the existing scene
datasets (UIUC-Sports, 15-Scene and MIT-Indoor), we also compare the reported state of the arts
performances to our OB algorithm (using a standard LR classifier). This result is shown in Tab.15
5.2

Control Experiment: Object Recognition by OB vs. Classemes [33]

15-Scene
UIUCMITOB is constructed from the responses of many objects,
Sports
Indoor
which encodes the semantic and spatial information of
state-of
72.2%[19]
66.0% [32]
26% [27]
-the-art
81.1%[19]
73.4% [22]
objects within images. It can be naturally applied to obOB
80.9%
76.3%
37.6%
ject recognition task. We compare the object recognition
performance on the Caltech 256 dataset to [33], a high Table 1: Comparison of classification relevel image representation obtained as the output of a sults using OB with reported state-of-thelarge number of weakly trained object classifiers on the art algorithms. Many of the algorithms use
image. By encoding the spatial locations of the objects more complex model and supervised information, whereas our results are obtained by
within an image, OB (39%) significantly outperforms applying simple logistic regression.
[33] (36%) on the 256-way classification task, where performance is measured as the average of the diagonal values of a 256?256 confusion matrix.

5.3 Semantic Feature Sparsification Over OB
In this subsection, we systematically investigate semantic feature sparsification of the OB representation. We focus on the practical issues directly relevant to the effectiveness of OB representation
and quality of feature sparsification, and study the following three aspects of the scene classifier:
1) robustness, 2) feasibility of lossless content-based compression, 3) profitability over growing
OB.interpretability of predictive features.
5.3.1

Robustness with Respect to Training Sample Size

80

LR1
LG

Compression of Image Representation

80

80

Compression of Image Representation

80

70

50

40

Accuracy

Accuracy

Accuracy

60

50

70

LR
LR1
LRG
LRG1

60

Classification Accuracy

70

40

LR
LR1
LRG
LRG1

60
50
40

30

30

30

20

20

20
10

10
0

25%

50%

(a)

75%

100%

0

0.2

0.4
0.6
0.8
Dimension Percentage

1

(b)

10

75
70

60
55
50
45
40

?8

?6
?4
?2
Dimension Percentage Log Scale

(c)

0

LRG

65

0

50
100
Number of Objects

150

(d)

Figure 4: (a) Classification performance (and s.t.d.) w.r.t number of training images. Each pair represents performances of LR1 and LRG respectively. X-axis is the ratio of the training images over the full training dataset
(70 images/class). (b) Classification performance w.r.t feature dimension. X-axis is the size of compressed
feature dimension, represented as the ratio of the compressed feature dimension over the full OB representation
dimension (44604). (c) Same as (b), represented in Log Scale to contrast the performances of different algorithms. (d) Classification performance w.r.t number of object filters. X-axis is the number of object filters. 3
rounds of randomized sampling is performed to choose the object filters from all the object detectors.

The intrinsic high-dimensionness of the OB representation raises a legitimate concern on its demand
on training sample size. We investigate the robustness of the logistic regression classifier built on
4
We also evaluate the classification performance of using the detected object location and its detection score
of each object detector as the image representation. The classification performance of this representation is
62.0%, 48.3%, 25.1% and 54% on the 15 scene, LabelMe, UIUC-Sports and MIT-Indoor datasets respectively.
5
We refer to the Appendix for a further discussion of the issue of comparing different algorithms based on
different training strategies.

6

features selected by LR1 and LRG in this experiment. We train LR1 and LRG on the UIUC-Sports
dataset by using multiple sizes of training examples, ranging from 25%, 50%, 75% to 100% of the
full training data.
As shown in Fig. 4(a), we observe only moderate drop of performance when the number of training
samples decreases from 100% to 25% of the training examples, suggesting that the OB representation is a rich representation where discriminating information residing in a lower dimensional ?informative? feature space, which are likely to be retained during feature sparsification, and thereby
ensuring robustness under small training data. We explore this issue further in the next experiment.
5.3.2

Near Losslessness of Content-based Compression via Regularized Learning

We believe that the OB can offer an over complete representation of any natural image. Therefore,
there is great room for possibly (near) lossless content-based compression of the image features into
a much lower-dimensional, but equally discriminative subspace where key semantic information of
the images are preserved, and the quality of inference on images such as scene classification are not
compromised significantly. Such compression can be attractive in reducing representation cost of
image query, and improving the speed of query inference.
In this experiment, we use the classification performance as a measurement to show how different
regularization schemes over LR can preserve the discriminative power. For LR1, LRG and LRG1,
cross-validation is used to decide the best regularization parameters. To study the extend of information loss as a function of different number of features being retained in the classifier, we re-train
an LR classifier using features from the top x% percentile of the rank list, where x is a compression
scale ranging from 0.05% to 100%. One might think that LR itself when fitted on full input dimensional can also produce a rank list of features for subsequent selection. For comparison purpose, we
also include results from the LR-ranked features, as can be seen in Fig.4(b,c), indeed its performance
drops faster than all the regularization methods.
In Fig.4 (b), we observe that the classification accuracy drops very slowly as the number of selected
features decreases. By excluding 75% feature dimensions, classification performance of each algorithm decreases less than 3%. One point to notice here is that, the non-zero entries only appear in
dimensions corresponding to no more than 45 objects for LRG at this point. Even more surprisingly,
LR1 and LRG preserve accuracies above 70% when 99% of the feature dimensions are excluded.
Fig. 4 (c) shows more detailed information in the low feature dimension range, which corresponds
to a high compression ratio. We observe that algorithms imposing sparsity in features (LR1, LRG,
and LRG1) outperform unregularized algorithm (LR) with a larger margin when the compression
ratio becomes higher. This reflects that the sparsity learning algorithms are capable of learning the
much lower-dimensional, but highly discriminative subspace.
5.3.3

Profitability Over Growing OB

We envisage the Object Bank will grow rapidly and constantly as more and more labeled web images
become available. This will naturally lead to increasingly richer and higher-dimensional representation of images. We ask, are image inference tasks such as scene classification going to benefit from
this trend?
As group regularized LR imposes sparsity on object level, we choose to use it to investigate how the
number of objects will affect the discriminative power of OB representation. To simulate what happens when the size of OB grows, we randomly sample subsets of object detectors at 1%, 5%, 10%,
25%, 50% and 75% of total number of objects for multiple rounds. As in Fig.4(d), the classification
performance of LRG continuously increases when more objects are incorporated in the OB representation. We conjecture that this is due to the accumulation of discriminative object features, and
we believe that future growth of OB will lead to stronger representation power and discriminability
of images models build on OB.
5.4

Interpretability of the Compressed Representation

Intuitively, a few key objects can discriminate a scene class from another. In this experiment, we aim
to discover the object sparsity and investigate its interpretability. Again, we use group regularized
LR (LRG) since the sparsity is imposed on object level and hence generates a more semantically
meaningful compression.
7

3500
3000

building

1400
1200

2500

1000

2000

800

tree

1000

cloud

800

4000
3500

boat

3000
2500

600

600

2000

1500
400

400

1000

1500

200

1000
500

200

0

500
?200

0

1 2 3 4 5 6 7 8 9 10 11 12

1 2 3 4 5 6 7 8 9 101112

0

1 2 3 4 5 6 7 8 9 10 11 12

0

1 2 3 4 5 6 7 8 9 10 11 12

Figure 6: Illustration of the learned ? OF by LRG1 within
an object group. Columns from left to right correspond to
?building? in ?church? scene, ?tree? in ?mountain?, ?cloud?
in ?beach?, and ?boat? in ?sailing?. Top Row: weights of
OB dimensions corresponding to different scales, from small
to large. The weight of a scale is obtained by summing up
the weights of all features corresponding to this scale in ? OF .
Middle: Heat map of feature weights in image space at the
scale with the highest weight (purple bars above). We project
the learned feature weights back to the image by reverting the
OB extraction procedure. The purple bounding box shows the
size of the object filter at this scale, centered at the peak of
the heat map. Bottom: example scene images masked by the
feature weights in image space (at the highest weighted scale),
highlighting the most relevant object dimension.

We show in Fig.5 the object-wise coefficients of the comsailing
beach
pression results for 4 sample scene classes. The object
weight is obtained by accumulating the coefficient of ? O
from the feature dimensions of each object (at different
scales and spatial locations) learned by LRG. Objects
with all zero coefficients in the resultant coefficient estimator are not displayed. Fig.5 shows that objects that are
church
mountain
?representative? for each scene are retained by LRG. For
example, ?sailboat?, ?boat?, and ?sky? are objects with
very high weight in the ?sailing? scene class. This suggests that the representation compression via LRG is virtually based upon the image content and is semantically
meaningful; therefore, it is nearly ?semantically lossless?. Figure 5: Object-wise coefficients given
5000

3500

4000

3000

3000

2500

2000

2000

1000

1500

0

1000

?1000

500

?2000

0

?3000
?4000

?500

?5000

?1000

g

r
he
ot
g
in
ild
bu
e
tre
r
rk
fo ape
cr
ys
sk
n
ea
oc
ud
clo
s
as

gr

nd

er
at
w
y
sk

sa

n

t

r
he
ot alk
ew
sid

e
tre

r

rso

pe

o
flo

in
ild

bu

r
ca

oa

ilb

y
sk

sa

8000

7000

7000

6000

6000
5000

5000

4000

4000

3000

3000

2000

2000

1000

1000

0

0

?1000

?1000

?2000

ot

er

6

Conclusion

As we try to tackle higher level visual recognition problems, we show that Object Bank representation is powerful on scene classification tasks because it carries rich semantic level image information. We also apply structured regularization schemes on the OB representation, and achieve nearly
lossless semantic-preserving compression. In the future, we will further test OB representation in
other useful vision applications, as well as other interesting structural regularization schemes.
Acknowledgments L. F-F is partially supported by an NSF CAREER grant (IIS-0845230), a Google research award, and a Microsoft Research Fellowship. E. X is supported by AFOSR FA9550010247, ONR
N0001140910758, NSF Career DBI-0546594, NSF IIS- 0713379 and Alfred P. Sloan Fellowship. We thank
Wei Yu, Jia Deng, Olga Russakovsky, Bangpeng Yao, Barry Chai, Yongwhan Lim, and anonymous reviewers
for helpful comments.

References
[1] S. Belongie, J. Malik, and J. Puzicha. Shape matching and object recognition using shape contexts. IEEE
PAMI, pages 509?522, 2002.

8

r

he

s
as
gr

r
ca

r
ca

e

tre

y
sk

g

ap
cr

in
ild

bu

ys
sk

le

r
he
ot ng
i
ild
bu

ud

n

ai

nt

op

r
ca

clo

pe

k

e

c
ro

ou
m

tre

y
sk

scene class. Selected objects correspond to

Knowing the important objects learned by the compres- non-zero ? values learned by LRG.
sion algorithm, we further investigate the discriminative
dimensions within the object level. We use LRG1 to examine the learned weights within an object. In Sec.3, we introduce that each feature dimension in the OB representation is directly related
to a specific scale, geometric location and object identity. Hence, the weights in ? OF reflects the
importance of an object at a certain scale and location. To verify the hypothesis, we examine the importance of objects across scales by summing up the weights of related spatial locations and pyramid
resolutions. We show one representative object in a scene and visualize the feature patterns within
the object group. As it is shown in Fig.6(Top), LRG1 has achieved joint object/feature sparsification
by zero-out less relevant scales, thus only the most discriminative scales are retained. To analyze
how ? OF reflects the geometric location, we further project the learned coefficient back to the image space by reversing the OB representation extraction procedure. In Fig.6(Middle), we observe
that the regions with high intensities are also the locations where the object frequently appears. For
example, cloud usually appears in the upper half of a scene in the beach class.

[2] L. Bourdev and J. Malik. Poselets: Body Part Detectors Trained Using 3D Human Pose Annotations.
ICCV, 2009.
[3] G. Csurka, C. Bray, C. Dance, and L. Fan. Visual categorization with bags of keypoints. Workshop on
Statistical Learning in Computer Vision, ECCV, 2004.
[4] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. CVPR, 2005.
[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. CVPR, 2009.
[6] L. Fei-Fei, R. Fergus, and P. Perona. One-Shot learning of object categories. TPAMI, 2006.
[7] L. Fei-Fei, R. Fergus, and A. Torralba. Recognizing and learning object categories. Short Course CVPR
[8] A. Farhadi, I. Endres, D. Hoiem and D. Forsyth. Describing objects by their attributes. CVPR, 2009.
[9] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object Detection with Discriminatively
Trained Part Based Models. JAIR, 29, 2007.
[10] W.T. Freeman and E.H. Adelson. The design and use of steerable filters. IEEE PAMI, 1991.
[11] G. Griffin, A. Holub, and P. Perona. Caltech-256 Object Category Dataset. 2007.
[12] A. Hauptmann, R. Yan, W. Lin, M. Christel, and H. Wactlar. Can high-level concepts fill the semantic
gap in video retrieval? a case study with broadcast news. IEEE TMM, 9(5):958, 2007.
[13] D. Hoiem, A.A. Efros, and M. Hebert. Automatic photo pop-up. SIGGRAPH 2005, 24(3):577?584, 2005.
[14] D. Hoiem, A.A. Efros, and M. Hebert. Putting Objects in Perspective. CVPR, 2006.
[15] T. Kadir and M. Brady. Scale, saliency and image description. IJCV, 45(2):83?105, 2001.
[16] N. Kumar, A. C. Berg, P. N. Belhumeur and S. K. Nayar. Attribute and Simile Classifiers for Face
Verification. ICCV, 2009.
[17] C.H. Lampert, H. Nickisch and S. Harmeling. Learning to detect unseen object classes by between-class
attribute transfer. CVPR, 2009.
[18] C.H. Lampert, M.B. Blaschko, T. Hofmann, and S. Zurich. Beyond sliding windows: Object localization
by efficient subwindow search. CVPR, 2008.
[19] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing
natural scene categories. CVPR, 2006.
[20] H.Lee, R.Grosse, R.Ranganath and A. Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. ICML, 2009.
[21] D.Lewis. Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval. ECML, 1998.
[22] L-J. Li and L. Fei-Fei. What, where and who? classifying events by scene and object recognition. ICCV,
2007.
[23] D. Lowe. Object recognition from local scale-invariant features. ICCV, 1999.
[24] K. Mikolajczyk and C. Schmid. An affine invariant interest point detector. ECCV, 2002.
[25] A. Oliva and A. Torralba. Modeling the shape of the scene: a holistic representation of the spatial envelope. IJCV, 42, 2001.
[26] P. Perona and J. Malik. Scale-space and edge detection using anisotropic diffusion. PAMI, 1990.
[27] A. Quattoni and A. Torralba. Recognizing indoor scenes. CVPR, 2009.
[28] A. Rabinovich, A. Vedaldi, C. Galleguillos, E. Wiewiora and S. Belongie. Objects in context. ICCV,
2007.
[29] D. Ramanan C. Desai and C. Fowlkes. Discriminative models for multi-class object layout. ICCV, 2009.
[30] B.C. Russell, A. Torralba, K.P. Murphy, and W.T. Freeman. Labelme: a database and web-based tool for
image annotation. MIT AI Lab Memo, 2005.
[31] L. Von Ahn. Games with a purpose. Computer, 39(6):92?94, 2006.
[32] C. Wang, D. Blei, and L. Fei-Fei. Simultaneous image classification and annotation. CVPR, 2009.
[33] L. Torresani, M. Szummer, and A. Fitzgibbon. Efficient Object Category Recognition Using Classemes.
European Conference of Computer Vision 2010, pages 776?789, 2010.
[34] P.Ravikumar, M.Wainwright, J.Lafferty. High-Dimensional Ising Model Selection Using L1-Regularized
Logistic Regression. Annals of Statistics, 2009.
[35] J. Vogel and B. Schiele. Semantic modeling of natural scenes for content-based image retrieval. International Journal of Computer Vision, 2007.
[36] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 2006.

9

"
4625,"Adaptive Step?Size for Policy Gradient Methods

Matteo Pirotta
Dept. Elect., Inf., and Bio.
Politecnico di Milano, ITALY

Marcello Restelli
Dept. Elect., Inf., and Bio.
Politecnico di Milano, ITALY

Luca Bascetta
Dept. Elect., Inf., and Bio.
Politecnico di Milano, ITALY

matteo.pirotta@polimi.it

marcello.restelli@polimi.it

luca.bascetta@polimi.it

Abstract
In the last decade, policy gradient methods have significantly grown in popularity
in the reinforcement?learning field. In particular, they have been largely employed
in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient
researches have been mainly focused on the identification of effective gradient
directions and the proposal of efficient estimation algorithms. Nonetheless, the
performance of policy gradient methods is determined not only by the gradient direction, since convergence properties are strongly influenced by the choice of the
step size: small values imply slow convergence rate, while large values may lead
to oscillations or even divergence of the policy parameters. Step?size value is usually chosen by hand tuning and still little attention has been paid to its automatic
selection. In this paper, we propose to determine the learning rate by maximizing
a lower bound to the expected performance gain. Focusing on Gaussian policies,
we derive a lower bound that is second?order polynomial of the step size, and
we show how a simplified version of such lower bound can be maximized when
the gradient is estimated from trajectory samples. The properties of the proposed
approach are empirically evaluated in a linear?quadratic regulator problem.

1

Introduction

Policy gradient methods have established as the most effective reinforcement?learning techniques
in robotic applications. Such methods perform a policy search to maximize the expected return of a
policy in a parameterized policy class. The reasons for their success are many. Compared to several
traditional reinforcement?learning approaches, policy gradients scale well to high?dimensional continuous state and action problems, and no changes to the algorithms are needed to face uncertainty
in the state due to limited and noisy sensors. Furthermore, policy representation can be properly designed for the given task, thus allowing to incorporate domain knowledge into the algorithm useful
to speed up the learning process and to prevent the unexpected execution of dangerous policies that
may harm the system. Finally, they are guaranteed to converge to locally optimal policies.
Thanks to these advantages, from the 1990s policy gradient methods have been widely used to learn
complex control tasks [1]. The research in these years has focused on obtaining good model?free
estimators of the policy gradient using data generated during the task execution. The oldest policy
gradient approaches are finite?difference methods [2], that estimate gradient direction by resolving
a regression problem based on the performance evaluation of policies associated to different small
perturbations of the current parameterization. Finite?difference methods have some advantages:
they are easy to implement, do not need assumptions on the differentiability of the policy w.r.t. the
policy parameters, and are efficient in deterministic settings. On the other hand, when used on real
systems, the choice of parameter perturbations may be difficult and critical for system safeness.
Furthermore, the presence of uncertainties may significantly slow down the convergence rate. Such
drawbacks have been overcome by likelihood ratio methods [3, 4, 5], since they do not need to generate policy parameters variations and quickly converge even in highly stochastic systems. Several
1

studies have addressed the problem to find minimum variance estimators by the computation of optimal baselines [6]. To further improve the efficiency of policy gradient methods, natural gradient
approaches (where the steepest ascent is computed w.r.t. the Fisher information metric) have been
considered [7, 8]. Natural gradients still converge to locally optimal policies, are independent from
the policy parameterization, need less data to attain good gradient estimate, and are less affected by
plateaus.
Once an accurate estimate of the gradient direction is obtained, policy parameters are updated by:
? t+1 = ? t + ?t ?? J ?=?t , where ?t ? R+ is the step size in the direction of the gradient. Although,
given an unbiased gradient estimate, convergence to a local optimum can be guaranteed under mild
conditions over the learning?rate values [9], their choice may significantly affect the convergence
speed or the behavior during the transient. Updating the policy with large step sizes may lead to
policy oscillations or even divergence [10], while trying to avoid such phenomena by using small
learning rates determines a growth in the number of iterations that is unbearable in most real?world
applications. In general unconstrained programming, the optimal step size for gradient ascent methods is determined through line?search algorithms [11], that require to try different values for the
learning rate and evaluate the function value in the corresponding updated points. Such an approach
is unfeasible for policy gradient methods, since it would require to perform a large number of policy
evaluations. Despite these difficulties, up to now, little attention has been paid to the study of step?
size computation for policy gradient algorithms. Nonetheless, some policy search methods based
on expectation?maximization have been recently proposed; such methods have properties similar to
the ones of policy gradients, but the policy update does not require to tune the step size [12, 13].
In this paper, we propose a new approach to compute the step size in policy gradient methods that
guarantees an improvement at each step, thus avoiding oscillation and divergence issues. Starting
from a lower bound to the difference of performance between two policies, in Section 3 we derive a
lower bound in the case where the new policy is obtained from the old one by changing its parameters along the gradient direction. Such a new bound is a (polynomial) function of the step size, that,
for positive values of the step size, presents a single, positive maximum ( i.e., it guarantees improvement) which can be computed in closed form. In Section 4, we show how the bound simplifies to a
quadratic function of the step size when Gaussian policies are considered, and Section 5 studies how
the bound needs to be changed in approximated settings (e.g., model?free case) where the policy
gradient needs to be estimated directly from experience.

2

Preliminaries

A discrete?time continuous Markov decision process (MDP) is defined as a 6-tuple
hS, A, P, R, ?, Di, where S is the continuous state space, A is the continuous action space, P
is a Markovian transition model where P(s0 |s, a) defines the transition density between state s and
s0 under action a, R : S ? A ? [0, R] is the reward function, such that R(s, a) is the expected
immediate reward for the state-action pair (s, a) and R is the maximum reward value, ? ? [0, 1) is
the discount factor for future rewards, and D is the initial state distribution. The policy of an agent
is characterized by a density distribution ?(?|s) that specifies for each state s the density distribution
over the action space A. To measure the distance between two policies we will use this norm:
Z
0
k? ? ?k? = sup
|? 0 (a|s) ? ?(a|s)|da,
s?S

A

that is the superior value over the state space of the total variation between the distributions over the
action space of policy ? 0 and ?.
We consider infinite horizon problems where the future rewards are exponentially discounted with
?. For each state s, we define the utility of following a stationary policy ? as:
""?
#
X
?
t
V (s) = E at ? ?
? R(st , at )|s0 = s .
st ? P

It is known that V

?

t=0

solves the following recursive (Bellman) equation:
Z
Z
V ? (s) =
?(a|s)R(s, a) + ?
P (s0 |s, a)V ? (s0 )ds0 da.
A

S

2

Policies can be ranked by their expected discounted reward starting from the state distribution D:
Z
Z
Z
?
?
?
JD =
D(s)V (s)ds) =
dD (s)
?(a|s)R(s, a)dads,
S

d?D (s)

P?

S

A

t

where
= (1 ? ?) t=0 ? P r(st = s|?, D) is the ??discounted future state distribution
for a starting state distribution D [5]. Solving an MDP means to find a policy ? ? that maximizes
?
the expected long-term reward: ? ? ? arg max??? JD
. For any MDP there exists at least one
deterministic optimal policy that simultaneously maximizes V ? (s), ?s ? S. For control purposes, it
is better to consider action values Q? (s, a), i.e., the value of taking action a in state s and following
a policy ? thereafter:
Z
Z
?
0
Q (s, a) = R(s, a) + ?
P(s |s, a)
?(a0 |s0 )Q? (s0 , a0 )da0 ds0 .
S

A

Furthermore, we define the advantage function:
A? (s, a) = Q? (s, a) ? V ? (s),
that quantifies the advantage (or disadvantage) of taking action a in state s instead of following
policy ?. In
for each state s, we define the advantage of a policy ? 0 over policy ? as
R particular,
?0
0
?
A? (s) = A ? (a|s)A (s, a)da and, following [14], we define its expected value w.r.t. an initial
R
0
0
state distribution ? as A??,? = S d?? (s)A?? (s)ds.
We consider the problem of finding a policy that maximizes the expected discounted reward over
a class of parameterized policies ?? = {?? : ? ? Rm }, where ?? is a compact representation of
?(a|s, ?). The exact gradient of the expected discounted reward w.r.t. the policy parameters [5] is:
Z
Z
1
d??? (s)
?? ?(a|s, ?)Q?? (s, a)dads.
?? J? (?) =
1?? S
A
The policy parameters can be updated by following the direction of the gradient of the expected
discounted reward: ? 0 = ? + ??? J? (?). In the following, we will denote with k?? J? (?)k1 and
k?? J? (?)k2 the L1? and L2?norm of the policy gradient vector, respectively.

3

Policy Gradient Formulation

In this section we provide a lower bound to the improvement obtained by updating the policy parameters along the gradient direction as a function of the step size. The idea is to start from the
general lower bound on the performance difference between any pair of policies introduced in [15]
and specialize it to the policy gradient framework.
Lemma 3.1 (Continuous MDP version of Corollary 3.6 in [15]). For any pair of stationary policies corresponding to parameters ? and ? 0 and for any starting state distribution ?, the difference
between the performance of policy ??0 and policy ?? can be bounded as follows
Z
1
?
2
0
(1)
J? (? ) ? J? (?) ?
d??? (s)A????0 (s)ds ?
k??0 ? ?? k? kQ?? k? ,
1?? S
2(1 ? ?)2
where kQ?? k? is the supremum norm of the Q?function: kQ?? k? =

sup

Q?? (s, a)

s?S,a?A

As we can notice from the above bound, to maximize the performance improvement, we need to
?
find a new policy ??0 that is associated to large average advantage A???0,? , but, at the same time, is
not too different from the current policy ?? . Policy gradient approaches provide search directions
characterized by increasing advantage values and, through the step size value, allow to control the
difference between the new policy and the target one. Exploiting a lower bound to the first order
Taylor?s expansion, we can bound the difference between the current policy and the new policy,
whose parameters are adjusted along the gradient direction, as a function of the step size ?.
Lemma 3.2. Let the update of the policy parameters be ? 0 = ? + ??? J? (?). Then
0

T

?(a|s, ? ) ? ?(a|s, ?) ???? ?(a|s, ?) ?? J? (?) + ?

where ?? = ??? J? (?).
3

2

inf
c?(0,1)

!

m
X
? 2 ?(a|s, ?) 
??i ??j
,
??i ??j ?+c?? 1 + I(i = j)
i,j=1

By combining the two previous lemmas, it is possible to derive the policy performance improvement
obtained following the gradient direction.
Theorem 3.3. Let the update of the parameters be ? 0 = ? + ??? J? (?). Then for any stationary
policy ?(a|s, ?) and any starting state distribution ?, the difference in performance between ?? and
??0 is lower bounded by:
J? (? 0 ) ? J? (?) ? ? k?? J? (?)k22
!

Z
Z
m
X
? 2 ?(a|s, ?) 
?2
??i ??j
??
+
d? (s)
inf
Q?? (s, a)dads
1?? S
??i ??j ?+c?? 1 + I(i = j)
A c?(0,1) i,j=1

Z


? kQ?? k?
?? ?(a|s, ?)T ?? J? (?) da
?
?
sup
2
2(1 ? ?)
s?S A
! !2

Z 
m

X
? 2 ?(a|s, ?) 
??i ??j


+?2 sup
sup
 da .



??
1
+
I(i
=
j)
i ??j
s?S A c?(0,1)
?+c??
i,j=1

The above bound is a forth?order polynomial of the step size, whose stationary points, being the
roots of a third?order polynomial ax3 + bx2 + cx + d, can be expressed in closed form. It is worth to
notice that, for positive values of ?, the bound presents a single stationary point that corresponds to
a local maximum. In fact, since a, b ? 0 and d ? 0, the Descartes? rule of signs gives the existence
and uniqueness of the real positive root.
In the following section, we will show, in the case of Gaussian policies, how the bound in Theorem 3.3 can be reduced to a second?order polynomial in ?, thus obtaining a simpler closed-form
solution for optimal (w.r.t. the bound) step size.

4

The Gaussian Policy Model

In this section we consider the Gaussian policy model with fixed standard deviation ? and the mean
is a linear combination of the state feature vector ?(?) using a parameter vector ? of size m:

2 !
1
1 a ? ? T ?(s)
?(a|s, ?) = ?
exp ?
.
2
?
2?? 2
In the case of Gaussian policies, each second?order derivative of policy ?? can be easily bounded.
Lemma 4.1. For any Gaussian policy ?(a|s, ?) ? N (? T ?(s), ? 2 ), the second order derivative of
the policy can be bounded as follows:

 2
 ? ?(a|s, ?)  |?i (s)?j (s)|
m


 ??i ??j  ? ?2?? 3 , ?? ? R , ?a ? A.
This result allows to restate Lemma 3.2 in the case of Gaussian policies:
T
?(a|s, ? 0 ) ? ?(a|s, ?) ? ??? ?(a|s, ?) ?? J? (?) ? ?

2
?2
T
|?? J? (?)| |?(s)| .
3
2??

In the following we will assume that features ? are uniformly bounded:
Assumption 4.1. All the basis functions are uniformly bounded by M? : |?i (s)|< M? , ?s ?
S, ?i = 1, . . . , m.
Exploiting Pinsker?s inequality [16] (which upper bounds the total variation between two distributions with their Kullback?Liebler divergence), it is possible to provide the following upper bound to
the supremum norm between two Gaussian policies.
Lemma 4.2. For any pair of stationary policies ?? and ??0 , so that ? 0 = ? +??? J? (?), supremum
norm of their difference can be upper bounded as follows:
k??0 ? ?? k? ?

?M?
k?? J? (?)k1 .
?
4

By plugging the results of Lemmas 4.1 and 4.2 into Equation (1) we can obtain a lower bound to
the performance difference between a Gaussian policy ?? and another policy along the gradient
direction that is quadratic in the step size ?.
Theorem 4.3. For any starting state distribution ?, and any pair of stationary Gaussian policies
T
?? ? N (? T ?(s), ? 2 ) and ??0 ? N (? 0 ?(s), ? 2 ), so that ? 0 = ? + ??? J? (?) and under Assumption 4.1, the difference between the performance of ??0 and the one of ?? can be lower bounded as
follows:
2

J? (? 0 ) ? J? (?) ? ? k?? J? (?)k2

Z
Z
2
1
T
?
? ?2
d??? (s) |?? J? (?)| |?(s)|
Q?? (s, a)dads
(1 ? ?) 2?? 3 S
A
!
?M?2
2
??
k?? J? (?)k1 kQ k? .
+
2(1 ? ?)2 ? 2
Since the linear coefficient is positive and the quadratic one is negative, the bound in Theorem 4.3
has a single maximum attained for some positive value of ?.
Corollary 4.4. The performance lower bound provided in Theorem 4.3 is maximized by choosing
the following step size:
?? =

?

? 2??M?2

?
(1 ? ?)2 2?? 3 k?? J? (?)k22
,
2 R
R ?
Q?? (s, a)dads
k?? J? (?)k21 kQ?? k? + 2(1 ? ?) S d?? (s) |?? J? (?)|T |?(s)|
A

that guarantees the following policy performance improvement
1
2
J? (? 0 ) ? J? (?) ? ?? k?? J? (?)k2 .
2

5

Approximate Framework

The solution for the tuning of the step size presented in the previous section depends on some
constants (e.g., discount factor and the variance of the Gaussian policy) and requires to be able to
compute some quantities (e.g., the policy gradient and the supremum value of the Q?function). In
many real?world applications such quantities cannot be computed (e.g., when the state?transition
model is unknown or too large for exact methods) and need to be estimated from experience samples.
In this section, we study how the step size can be chosen when the gradient is estimated through
sample trajectories to guarantee a performance improvement in high probability.
For sake of easiness, we consider a simplified version of the bound in Theorem 4.3, in order to obtain
a bound where the only element that needs to be estimated is the policy gradient ?? J? (?).
Corollary 5.1. For any starting state distribution ?, and any pair of stationary Gaussian policies
T
?? ? N (? T ?(s), ? 2 ) and ??0 ? N (? 0 ?(s), ? 2 ), so that ? 0 = ? + ??? J? (?) and under Assumption 4.1, the difference between the performance of ??0 and ?? is lower bounded by:

2 
RM?2 k?? J? (?)k1
?
|A|
2
?
J? (? 0 ) ? J? (?) ? ? k?? J? (?)k2 ? ?2
+
,
2
2?? 2(1 ? ?)
(1 ? ?) ? 2
that is maximized by the following step size value:
?
2
(1 ? ?)3 2?? 3 k?? J? (?)k2
?
?
? = ?

2.
? 2?? + 2(1 ? ?)|A| RM?2 k?? J? (?)k1
Since we are assuming that the policy gradient ?? J? (?) is estimated through trajectory samples,
the lower bound in Corollary 5.1 must take into consideration the associated approximation error.
b ? J? (?) of
Given a set of trajectories obtained following policy ?? , we can produce an estimate ?
T
the policy gradient and we assume to be able to produce a vector  = [1 , . . . , m ] , so that the i?th
component of the approximation error is bounded at least with probability 1 ? ?:




b ? J? (?) ? i ? ?.
P ??i J? (?) ? ?
i
5

Given the approximation error vector , we can adjust the bound in Corollary 5.1 to produce a new
m
bound that holds at least with probability (1 ? ?) . In particular, to preserve the inequality sign,
the estimated approximation error must be used to decrease the L2?norm of the policy gradient in
the first term (the one that provides the positive contribution to the performance improvement) and
to increase the L1?norm in the penalization term. To lower bound the L2?norm, we introduce the
b ? J? (?) whose components are a lower bound to the absolute value of the policy gradient
vector ?
built on the basis of the approximation error :
b ? J? (?) = max(|?
b ? J? (?)| ? , 0),
?
where 0 denotes the m?size vector with all zeros, and max denotes the component?wise maximum.
b ? J? (?):
Similarly, to upper bound the L1?norm of the policy gradient, we introduce the vector ?
b ? J? (?) = |?
b ? J? (?)| + .
?
Theorem 5.2. Under the same assumptions 
of Corollary 5.1, and provided

that it is available a


b
b
policy gradient estimate ?? J? (?), so that P ??i J? (?) ? ??i J? (?) ? i ? ?, the difference
m

between the performance of ??0 and ?? can be lower bounded at least with probability (1 ? ?) :

2


 

b



2
RM?2 
?
? J? (?)
?
|A|

b

0
2
1
?
J? (? ) ? J? (?) ? ? 
?? J? (?)
 ? ?
+
,
2
2
2?? 2(1 ? ?)
(1 ? ?) ? 2
that is maximized by the following step size value:


2
?

b

(1 ? ?)3 2?? 3 
?
? J? (?)
?
2
?
b = ?


2 .


b

2
? 2?? + 2(1 ? ?)|A| RM? 
?? J? (?)
1

In the following, we will discuss how the approximation error of the policy gradient can be bounded.
Among the several methods that have been proposed over the years, we focus on two well?
understood policy?gradient estimation approaches: REINFORCE [3] and G(PO)MDP [4]/policy
gradient theorem (PGT) [5].
5.1

Approximation with REINFORCE gradient estimator

The REINFORCE approach [3] is the main exponent of the likelihood?ratio family. The episodic
REINFORCE gradient estimator is given by:
!!
H
N
H
X
X
X
1
RF
n
n
l?1
n
b ? J? (?) =
?? log ? (ak ; sk , ?)
?
? rl ? b
,
N n=1
k=1

l=1

where N is the number of H?step trajectories generated from a system by roll?outs and b ? R is
a baseline that can be chosen arbitrary, but usually with the goal of minimizing the variance of the
gradient estimator. The main drawback of REINFORCE is its variance, that is strongly affected by
the length of the trajectory horizon H.
The goal is to determine the number of trajectories N in order to obtain the desired accuracy of
the gradient estimate. To achieve this, we exploit the upper bound to the variance of the episodic
REINFORCE gradient estimator introduced in [17] for Gaussian policies.
Lemma 5.3 (Adapted
from Theorem 2 in [17]). Given a Gaussian policy ?(a|s, ?) ?

N ? T ?(s), ? 2 , under the assumption of uniformly bounded rewards and basis functions (Assumption 4.1), we have the following upper bound to the variance of the i?th component of the episodic
b ? J?RF (?):
REINFORCE gradient estimate ?
i


 R2 M 2 H 1 ? ? H 2
?
RF
b ? J? (?) ?
V ar ?
.
i
2
N ? 2 (1 ? ?)
6

The result in the previous Lemma combined with the Chebyshev?s inequality allows to provide a
high?probability upper bound to the gradient approximation error using the episodic REINFORCE
gradient estimator.

Theorem 5.4. Given a Gaussian policy ?(a|s, ?) ? N ? T ?(s), ? 2 , under the assumption of
uniformly bounded rewards and basis functions (Assumption 4.1), using the following number of
H?step trajectories:
2
R2 M?2 H 1 ? ? H
N=
,
2
?2i ? 2 (1 ? ?)
b ? J?RF (?) generated by REINFORCE is such that with probability 1 ? ?:
the gradient estimate ?
i



b
??i J?RF (?) ? ??i J? (?) ? i .
5.2

Approximation with G(PO)MDP/PGT gradient estimator

Although the REINFORCE method is guaranteed to converge at the true gradient at the fastest possible pace, its large variance can be problematic in practice. Advances in the likelihood ratio gradient
estimators have produced new approaches that significantly reduce the variance of the estimate. Focusing on the class of ?vanilla? gradient estimator, two main approaches have been proposed: policy
gradient theorem (PGT) [5] and G(PO)MDP [4]. In [6], the authors show that, while the algorithms
b ? J?G(PO)MDP (?). For this
b ? J?P GT (?) = ?
look different, their gradient estimate are equal, i.e., ?
reason, we can limit our attention to the PGT formulation:
!!
H
H
H
X
X
X
1
b ? J?P GT (?) =
?
?? log ? (ank ; snk , ?)
? l?1 rln ? bnl
,
N n=1
k=1

l=k

bnl

where
? R have the objective to reduce the variance of the gradient estimate. Following the
procedure used to bound the approximation error of REINFORCE, we need an upper bound to the
variance of the gradient estimate of PGT that is provided by the following lemma (whose proof is
similar to the one used in [17] for the REINFORCE case).

Lemma 5.5. Given a Gaussian policy ?(a|s, ?) ? N ? T ?(s), ? 2 , under the assumption of uniformly bounded rewards and basis functions (Assumption 4.1), we have the following upper bound
b ? J?P GT (?):
to the variance of the i?th component of the PGT gradient estimate ?
i



b ? J?P GT (?) ?
V ar ?
i

R2 M?2



2

N (1 ? ?) ? 2


H
1 ? ? 2H
2H
H1??
+ H? ? 2?
.
1 ? ?2
1??

As expected, since the variance of the gradient estimate obtained with PGT is smaller than the one
with REINFORCE, also the upper bound of the PGT variance is smaller than REINFORCE one. In
particular, while the variance with REINFORCE grows linearly with the time horizon, using PGT
the dependence on the time horizon is significantly smaller. Finally, we can derive the upper bound
for the approximation error of the gradient estimated of PGT.

Theorem 5.6. Given a Gaussian policy ?(a|s, ?) ? N ? T ?(s), ? 2 , under the assumption of
uniformly bounded rewards and basis functions (Assumption 4.1), using the following number of
H?step trajectories:


H
R2 M?2
1 ? ? 2H
2H
H1??
N= 2
+ H? ? 2?
2
1 ? ?2
1??
?i ? 2 (1 ? ?)
b ? J?P GT (?) generated by PGT is such that with probability 1 ? ?:
the gradient estimate ?
i


b

??i J?P GT (?) ? ??i J? (?) ? i .

7

?const

?t =

?0
t

1e ? 07
1e ? 06
1e ? 05
1e ? 04
1e ? 03
1e ? 05
1e ? 04

?

?

0.50
itmax
itmax
17138
1675
?
itmax
itmax
24106

0.75
itmax
itmax
8669
697
?
itmax
itmax
7271

1.00
itmax
itmax
5120
499
?
itmax
itmax
3279

1.25
itmax
itmax
3348
?
?
itmax
?
1838

?
1.50
itmax
23651
2342
?
?
itmax
?
1172

1.75
itmax
17516
1714
?
?
itmax
?
813

2.00
itmax
13480
1287
?
?
itmax
?
598

5.00
21888
2163
?
?
?
?
?
1

7.50
9740
849
?
?
?
?
?
58

Table 1: Convergence speed in exact LQG scenario with ? = 0.95. The table reports the number of
iterations required by the exact gradient approach, starting from ? = 0, to learn the optimal policy
parameter ?? = ?0.6037 with an accuracy of 0.01, for different step?size values. Three different
set of experiments are shown: constant step size, decreasing step size, and the step size proposed in
Corollary 4.4. The table contains itmax when no convergence happens in 30, 000 iterations, and ?
when the algorithm diverges (? < ?1 or ? > 0). Best performances are reported in boldface.
10, 000
RF
PGT

it
822
29, 761

?
?0.0030
?0.2176

Number of trajectories
100, 000
it
?
51, 731
?0.3068
63, 985
?0.4013

500, 000
it
?
75, 345
?0.4088
83, 983
?0.4558

Table 2: Convergence speed in approximate LQG scenario with ? = 0.9. The table reports, starting
from ? = 0 and fixed ? = 1, the number of iterations performed before the proposed step size ?
b
becomes 0 and the last value of the policy parameter. Results are shown for different number of
trajectories (of 20 steps each) used in the gradient estimation by REINFORCE and PGT.

6

Numerical Simulations and Discussion

In this section we show results related to some numerical simulations of policy gradient in the
linear?quadratic Gaussian regulation (LQG) problem as formulated
in [6]. The LQG problem is

characterized by a transition model st+1 ? N st + at , ? 2 , Gaussian policy at ? N ? ? s, ? 2
and quadratic reward rt = ?0.5(s2t + a2t ). The range of state and action spaces is bounded to
the interval [?2, 2] and the initial state is drawn uniformly at random. This scenario is particularly
instructive since it allows to exactly compute all terms involved in the bounds. We first present
results in the exact scenario and then we move toward the approximated one.
Table 1 shows how the number of iterations required to learn a near?optimal value of the policy
parameter changes according to the standard deviation of the Gaussian policy and the step?size
value. As expected, very small values of the step size allow to avoid divergence, but the learning
process needs many iterations to reach a good performance (this can be observed both when the step
size is kept constant and when it decreases). On the other hand, larger step?size values may lead to
divergence. In this example, the higher the policy variance, the lower is the step size value that allows
to avoid divergence, since, in LQG, higher policy variance implies larger policy gradient values.
Using the step size ?? from Corollary 4.4 the policy gradient algorithm avoids divergence (since
it guarantees an improvement at each iteration), and the speed of convergence is strongly affected
by the variance of the Gaussian policy. In general, when the policy are nearly deterministic (small
variance in the Gaussian case), small changes in the parameters lead to large distances between
the policies, thus negatively affecting the lower bound in Equation 1. As we can notice from the
expression of ?? in Corollary 4.4, considering policies with high variance (that might be a problem in
real?world applications) allows to safely take larger step size, thus speeding up the learning process.
Nonetheless, increasing the variance over some threshold (making policies nearly random) produces
very bad policies, so that changing the policy parameter has a small impact on the performance,
and as a result slows down the learning process. How to identify an optimal variance value is
an interesting future research direction. Table 2 provides numerical results in the approximated
settings, showing the effect of varying the number of trajectories used to estimate the gradient by
REINFORCE and PGT. Increasing the number of trajectories reduces the uncertainty on the gradient
estimates, thus allowing to use larger step sizes and reaching better performances. Furthermore, the
smaller variance of PGT w.r.t. REINFORCE allows the former to achieve better performances.
However, even with a large number of trajectories, the approximated errors are still quite large
preventing to reach very high performance. For this reason, future studies will try to derive tighter
bounds. Further developments include extending these results to other policy models (e.g., Gibbs
policies) and to other policy gradient approaches (e.g., natural gradient).
8

References
[1] Jan Peters and Stefan Schaal. Policy gradient methods for robotics. In Intelligent Robots and
Systems, 2006 IEEE/RSJ International Conference on, pages 2219?2225. IEEE, 2006.
[2] James C Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. Automatic Control, IEEE Transactions on, 37(3):332?341, 1992.
[3] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229?256, May 1992.
[4] Jonathan Baxter and Peter L. Bartlett. Infinite-horizon policy-gradient estimation. Journal of
Artificial Intelligence Research, 15:319?350, 2001.
[5] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12(22), 2000.
[6] Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients.
Neural Networks, 21(4):682?697, 2008.
[7] Sham Kakade. A natural policy gradient. Advances in neural information processing systems,
14:1531?1538, 2001.
[8] Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7):1180?1190, 2008.
[9] Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical Statistics, pages 400?407, 1951.
[10] P. Wagner. A reinterpretation of the policy oscillation phenomenon in approximate policy
iteration. Advances in Neural Information Processing Systems, 24, 2011.
[11] Jorge J Mor?e and David J Thuente. Line search algorithms with guaranteed sufficient decrease.
ACM Transactions on Mathematical Software (TOMS), 20(3):286?307, 1994.
[12] J. Kober and J. Peters. Policy search for motor primitives in robotics. In Advances in Neural
Information Processing Systems 22 (NIPS 2008), Cambridge, MA: MIT Press, 2009.
[13] Nikos Vlassis, Marc Toussaint, Georgios Kontes, and Savas Piperidis. Learning model-free
robot control by a monte carlo em algorithm. Autonomous Robots, 27(2):123?130, 2009.
[14] S.M. Kakade. On the sample complexity of reinforcement learning. PhD thesis, PhD thesis,
University College London, 2003.
[15] Matteo Pirotta, Marcello Restelli, Alessio Pecorino, and Daniele Calandriello. Safe policy
iteration. In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International Conference on Machine Learning (ICML-13), volume 28, pages 307?315. JMLR
Workshop and Conference Proceedings, May 2013.
[16] S. Pinsker. Information and Information Stability of Random Variable and Processes. HoldenDay Series in Time Series Analysis. Holden-Day, Inc., 1964.
[17] Tingting Zhao, Hirotaka Hachiya, Gang Niu, and Masashi Sugiyama. Analysis and improvement of policy gradient estimation. Neural Networks, 26(0):118 ? 129, 2012.

9

"
4084,"Spectral Learning of General Weighted Automata
via Constrained Matrix Completion

Borja Balle
Universitat Polit`ecnica de Catalunya

Mehryar Mohri
Courant Institute and Google Research

bballe@lsi.upc.edu

mohri@cims.nyu.edu

Abstract
Many tasks in text and speech processing and computational biology require estimating functions mapping strings to real numbers. A broad class of such functions can be defined by weighted automata. Spectral methods based on the singular value decomposition of a Hankel matrix have been recently proposed for
learning a probability distribution represented by a weighted automaton from a
training sample drawn according to this same target distribution. In this paper, we
show how spectral methods can be extended to the problem of learning a general
weighted automaton from a sample generated by an arbitrary distribution. The
main obstruction to this approach is that, in general, some entries of the Hankel
matrix may be missing. We present a solution to this problem based on solving a
constrained matrix completion problem. Combining these two ingredients, matrix
completion and spectral method, a whole new family of algorithms for learning
general weighted automata is obtained. We present generalization bounds for a
particular algorithm in this family. The proofs rely on a joint stability analysis of
matrix completion and spectral learning.

1

Introduction

Many tasks in text and speech processing, computational biology, or learning models of the environment in reinforcement learning, require estimating a function mapping variable-length sequences to
real numbers. A broad class of such functions can be defined by weighted automata. The mathematical and algorithmic properties of weighted automata have been extensively studied in the most
general setting where they are defined in terms of an arbitrary semiring [28, 9, 23]. Weighted automata are widely used in applications ranging from natural text and speech processing [24] to
optical character recognition [12] and image processing [1]. This paper addresses the problem of
learning weighted automata from a finite set of labeled examples.
The particular instance of this problem where the objective is to learn a probabilistic automaton
from examples drawn from this same distribution has recently drawn much attention: starting with
the seminal work of Hsu et al. [19], the so-called spectral method has proven to be a valuable tool in
developing novel and theoretically-sound algorithms for learning HMMs and other related classes
of distributions [5, 30, 31, 10, 6, 4]. Spectral methods have also been applied to other probabilistic
models of practical interest, including probabilistic context-free grammars and graphical models
with hidden variables [26, 22, 16, 3, 2]. The main idea behind these algorithms is that, under
an identifiability assumption, the method of moments can be used to formulate a set of equations
relating the parameters defining the target to observable statistics. Given enough training data, these
statistics can be accurately estimated. Then, solving the corresponding approximate equations yields
a model that closely estimates the target distribution. The spectral term takes its origin from the use
of a singular value decomposition in solving those equations.
1

This paper tackles a significantly more general and more challenging problem than the specific
instance just mentioned. Indeed, in general, there seems to be a large gap separating the scenario
of learning a probabilistic automaton using data drawn according to the distribution it generates,
from that of learning an arbitrary weighted automaton from labeled data drawn from some unknown
distribution. For a start, in the former setting there is only one object to care about because the
distribution from which examples are drawn is the target machine. In contrast, the latter involves two
distinct objects: a distribution according to which strings are drawn, and a target weighted automaton
assigning labels to these strings. It is not difficult in this setting to conceive that, for a particular
target, an adversary could find a distribution over strings making the learner?s task insurmountably
difficult. In fact, this is the core idea behind the cryptography-based hardness results for learning
deterministic finite automata given by Kearns and Valiant [20] ? these same results apply to our
setting as well.
But, even in cases where the distribution ?cooperates,? there is still an obstruction in leveraging the
spectral method for learning general weighted automata. The statistics used by the spectral method
are essentially the probabilities assigned by the target distribution to each string in some fixed finite
set B. In the case where the target is a distribution, increasingly large samples yield uniformly
convergent estimates for these probabilities. Thus, it can be safely assumed that the probability of
any string from B not present in the sample is zero. When learning arbitrary weighted automata,
however, the value assigned by the target to an unseen string is unknown. Furthermore, one cannot
expect that a sample would contain the values of the target function for all the strings in B. This
observation raises the question of whether it is possible at all to apply the spectral method in a
setting with missing data, or, alternatively, whether there is a principled way to ?estimate? this
missing information and then apply the spectral method.
As it turns out, the latter approach can be naturally formulated as a constrained matrix completion
problem. When applying the spectral method, the (approximate) values of the target on B are arranged in a matrix H. Thus, the main difference between the two settings can be restated as follows:
when learning a weighted automaton representing a distribution, unknown entries of H can be filled
in with zeros, while in the general setting there is a priori no straightforward method to fill in the
missing values. We propose to use a matrix completion algorithm for solving this last problem. In
particular, since H is a Hankel matrix whose entries must satisfy some equality constraints, it turns
out that the problem of learning weighted automata under an arbitrary distribution leads to what
we call the Hankel matrix completion problem. This is essentially a constrained matrix completion
problem where entries of valid hypotheses need to satisfy a set of equalities. We give an algorithm
for solving this problem via convex optimization. Many existing approaches to matrix completion,
e.g., [14, 13, 27, 18], are also based on convex optimization. Since the set of valid hypotheses for our
constrained matrix completion problem is convex, many of these algorithms could also be modified
to deal with the Hankel matrix completion problem.
In summary, our approach leverages two recent techniques for learning a general weighted automaton: matrix completion and spectral learning. It consists of first predicting the missing entries in
H and then applying the spectral method to the resulting matrix. Altogether, this yields a family
of algorithms parametrized by the choice of the specific Hankel matrix completion algorithm used.
These algorithms are designed for learning an arbitrary weighted automaton from samples generated
by an unknown distribution over strings and labels.
We study a special instance of this family of algorithms and prove generalization guarantees for
its performance based on a stability analysis, under mild conditions on the distribution. The proof
contains two main novel ingredients: a stability analysis of an algorithm for constrained matrix
completion, and an extension of the analysis of spectral learning to an agnostic setting where data
is generated by an arbitrary distribution and labeled by a process not necessarily modeled by a
weighted automaton.
The rest of the paper is organized as follows. Section 2 introduces the main notation and definitions
used in subsequent sections. In Section 3, we describe a family of algorithms for learning general
weighted automata by combining constrained matrix completion and spectral methods. In Section 4, we give a detailed analysis of one particular algorithm in this family, including generalization
bounds.

2

2

Preliminaries

This section introduces the main notation used in this paper. Bold letters will be used for vectors v and matrices M. For vectors, kvk denotes the standard euclidean norm. For matrices, kMk denotes the operator norm. For p ? [1, +?], kMkp denotes the Schatten p-norm:
P
kMkp = ( n?1 ?np (M))1/p , where ?n (M) is the nth singular value of M. The special case
p = 2 coincides with the Frobenius norm which will be sometimes also written as kMkF . The
Moore?Penrose pseudo-inverse of a matrix M is denoted by M+ .
2.1

Functions over Strings and Hankel Matrices

We denote by ? = {a1 , . . . , ak } a finite alphabet of size k ? 1 and by  the empty string. We also
write ?0 = {} ? ?. The set of all strings over ? is denoted by ?? and the length of a string x
denoted by |x|. For any n ? 0, ??n denotes the set of all strings of length at most n. Given two
sets of strings P, S ? ?? we denote by PS the set of all strings uv obtained by concatenation of a
string u ? P and a string v ? S. A set of strings P is called ?-complete when P = P 0 ?0 for some
set P 0 . P 0 is then called the root of P. A pair (P, S) with P, S ? ?? is said to form a basis of ??
if  ? P ? S and P is ?-complete. We define the dimension of a basis (P, S) as the cardinality of
PS, that is |PS|.
For any basis B = (P, S), we denote by HB the vector space of functions RPS whose dimension
is the dimension of B. We will simply write H instead of HB when the basis B is clear from the
context. The Hankel matrix H ? RP?S associated to a function h ? H is the matrix whose entries
are defined by H(u, v) = h(uv) for all u ? P and v ? S. Note that the mapping h 7? H is linear.
In fact, H is isomorphic to the vector space formed by all |P| ? |S| real Hankel matrices and we can
thus write by identification

	
H = H ? RP?S : ?u1 , u2 ? P, ?v1 , v2 ? S, u1 v1 = u2 v2 ? H(u1 , v1 ) = H(u2 , v2 ) .
It is clear from this characterization that H is a convex set because it is a subset of a convex
space defined by equality constraints. In particular, a matrix in H contains |P||S| coefficients with
|PS| degrees of freedom, and the dependencies can be specified as a set of equalities of the form
H(u1 , v1 ) = H(u2 , v2 ) when u1 v1 = u2 v2 . We will use both characterizations of H indistinctly for
the rest of the paper. Also, note that different orderings of P and S may result in different sets of
matrices. For convenience, we will assume for all that follows an arbitrary fixed ordering, since the
choice of that order has no effect on any of our results.
Matrix norms extend naturally to norms in H. For any p ? [1, +?], the Hankel?Schatten p-norm
on H is defined as khkp = kHkp . It is straightforward to verify that khkp is a norm by the linearity
of h 7? H. In particular, this implies that the function k ? kp : H ? R is convex. In the case p = 2,
it can be seen that khk22 = hh, hiH , with the inner product on H defined by
X
hh, h0 iH =
cx h(x)h0 (x) ,
x?PS

where cx = |{(u, v) ? P ? S : x = uv}| is the number of possible decompositions of x into a prefix
in P and a suffix in S.
2.2

Weighted finite automata

A widely used class of functions mapping strings to real numbers is that of functions defined by
weighted finite automata (WFA) or in short weighted automata [23]. These functions are also known
as rational power series [28, 9]. A WFA over ? with n states can be defined as a tuple A =
h?, ?, {Aa }a?? i, where ?, ? ? Rn are the initial and final weight vectors, and Aa ? Rn?n the
transition matrix associated to each alphabet symbol a ? ?. The function fA realized by a WFA A
is defined by
fA (x) = ?> Ax1 ? ? ? Axt ? ,
for any string x = x1 ? ? ? xt ? ?? with t = |x| and xi ? ? for all i ? [1, t]. We will say that a WFA
A = h?, ?, {Aa }i is ?-bounded if k?k, k?k, kAa k ? ? for all a ? ?. This property is convenient
to bound the maximum value assigned by a WFA to any string of a given length.
3

1/2

a, 3/4
b, 6/5

a, 0
b, 2/3

1

1/2

-1

?> = [1/2 1/2] ? > = [1 ?1]




3/4 0
6/5 2/3
Aa =
Ab =
0 1/3
3/4 1

a, 1/3
b, 1

a, 0
b, 3/4

(a)

(b)

Figure 1: Example of a weighted automaton over ? = {a, b} with 2 states: (a) graph representation;
(b) algebraic representation.
WFAs can be more generally defined over an arbitrary semiring instead of the field of real numbers
and are also known as multiplicity automata (e.g., [8]). To any function f : ?? ? R, we can
?
?
associate its Hankel matrix Hf ? R? ?? with entries defined by Hf (u, v) = f (uv). These are
just the bi-infinite versions of the Hankel matrices we introduced in the case P = S = ?? . Carlyle
?
and Paz [15] and Fliess [17] gave the following characterization of the set of functions f in R?
defined by a WFA in terms of the rank of their Hankel matrix rank(Hf ).1
Theorem 1 ([15, 17]) A function f : ?? ? R can be defined by a WFA iff rank(Hf ) is finite and in
that case rank(Hf ) is the minimal number of states of any WFA A such that f = fA .
Thus, WFAs can be viewed as those functions whose Hankel matrix can be finitely ?compressed?.
Since finite sub-blocks of a Hankel matrix cannot have a larger rank than its bi-infinite extension,
this justifies the use of a low-rank-enforcing regularization in the definition of a Hankel matrix
completion.
Note that deterministic finite automata (DFA) with n states can be represented by a WFA with at
most n states. Thus, the results we present here can be directly applied to classification problems in
?? . However, specializing our results to this particular setting may yield several improvements.
2.2.1

Example

Figure 1 shows an example of a weighted automaton A = h?, ?, {Aa }i with two states defined
over the alphabet ? = {a, b}, with both its algebraic representation (Figure 1(b)) in terms of vectors
and matrices and the equivalent graph representation (Figure 1(a)) useful for a variety of WFA
algorithms [23]. Let W = {, a, b}, then B = (W?0 , W) is a ?-complete basis. The following is
the Hankel matrix of A on this basis shown with three-digit precision entries:
?
?

a
b
aa
ab
ba
bb
?  0.00 0.20 0.14 0.22 0.15 0.45 0.31?
H>
B = ?a 0.20 0.22 0.45 0.19 0.29 0.45 0.85? .
b 0.14 0.15 0.31 0.13 0.20 0.32 0.58
By Theorem 1, the Hankel matrix of A has rank at most 2. Given HB , the spectral method described
in [19] can be used to recover a WFA A? equivalent to A, in the sense that A and A? compute the
same function. In general, one may be given a sample of strings labeled using some WFA that does
not contain enough information to fully specify a Hankel matrix over a complete basis. In that case,
Theorem 1 motivates the use of a low-rank matrix completion algorithm to fill in the missing entries
in HB prior to the application of the spectral method. This is the basis of the algorithm we describe
in the following section.

3

The HMC+SM Algorithm

In this section we describe our algorithm HMC+SM for learning weighted automata. As input,
the algorithm takes a sample Z = (z1 , . . . , zm ) containing m examples zi = (xi , yi ) ? ?? ? R,
1
The construction of an equivalent WFA with the minimal number of states from a given WFA was first
given by Sch?utzenberger [29].

4

1 ? i ? m, drawn i.i.d. from some distribution D over ?? ? R. There are three parameters a user
can specify to control the behavior of the algorithm: a basis B = (P, S) of ?? , a regularization
parameter ? > 0, and the desired number of states n in the hypothesis. The output returned by
HMC+SM is a WFA AZ with n states that computes a function fAZ : ?? ? R.
The algorithm works in two stages. In the first stage, a constrained matrix completion algorithm
with input Z and regularization parameter ? is used to return a Hankel matrix HZ ? HB . In the
second stage, the spectral method is applied to HZ to compute a WFA AZ with n states. These two
steps will be described in detail in the following sections.
As will soon become apparent, HMC+SM defines in fact a whole family of algorithms. In particular,
by combining the spectral method with any algorithm for solving the Hankel matrix completion
problem, one can derive a new algorithm for learning WFAs. For concreteness, in the following,
we will only consider the Hankel matrix completion algorithm described in Section 3.1. Through
its parametrization by a number 1 ? p ? ? and a convex loss ` : R ? R ? R+ , this completion
algorithm already gives rise to a family of learning algorithms that we denote by HMCp,` +SM.
However, it is important to keep in mind that for each existing matrix completion algorithm that can
be modified to solve the Hankel matrix completion problem, a new algorithm for learning WFAs
can be obtained via the general scheme we describe below.
3.1

Hankel Matrix Completion

We now describe our Hankel matrix completion algorithm. Given a basis B = (P, S) of ?? and a
sample Z over ?? ? R, the algorithm solves a convex optimization problem and returns a matrix
HZ ? HB . We give two equivalent descriptions of this optimization, one in terms of functions
h : PS ? R, and another in terms of Hankel matrices H ? RP?S . While the former is perhaps
conceptually simpler, the latter is easier to implement within the existing frameworks of convex
optimization.
e the subsample of Z formed by examples z = (x, y) with x ? PS and by m
We will denote by Z
e
e For any p ? [1, +?] and a convex loss function ` : R ? R ? R+ , we consider the
its size |Z|.
objective function FZ defined for any h ? H by
X
b e (h) = ? khk2p + 1
FZ (h) = ? N (h) + R
`(h(x), y) ,
Z
m
e
e
(x,y)?Z

where ? > 0 is a regularization parameter. FZ is a convex function, by the convexity of k ? kp and `.
Our algorithm seeks to minimize this loss function over the finite-dimensional vector space H and
returns a function hZ satisfying
hZ ? argmin FZ (h) .
(HMC-h)
h?H

To define an equivalent optimization over the matrix version of H, we introduce the following notation. For each string x ? PS, fix a pair of coordinate vectors (ux , vx ) ? RP ? RS such that
u>
x Hvx = H(x) for any H ? H. That is, ux and vx are coordinate vectors corresponding respectively to a prefix u ? P and a suffix v ? S, and such that uv = x. Now, abusing our previous
notation, we define the following loss function over matrices:
X
b e (H) = ? kHk2p + 1
`(u>
FZ (H) = ? N (H) + R
x Hvx , y) .
Z
m
e
e
(x,y)?Z

This is a convex function defined over the space of all |P| ? |S| matrices. Optimizing FZ over the
convex set of Hankel matrices H leads to an algorithm equivalent to (HMC-h):
HZ ? argmin FZ (H) .
(HMC-H)
H?H

We note here that our approach shares some common aspects with some previous work in matrix
completion. The fact that there may not be a true underlying Hankel matrix makes it somewhat close
to the agnostic setting in [18], where matrix completion is also applied under arbitrary distributions.
Nonetheless, it is also possible to consider other learning frameworks for WFAs where algorithms
for exact matrix completion [14, 27] or noisy matrix completion [13] may be useful. Furthermore,
since most algorithms in the literature of matrix completion are based on convex optimization problems, it is likely that most of them can be adapted to solve constrained matrix completions problems
such as the one we discuss here.
5

3.2

Spectral Method for General WFA

Here, we describe how the spectral method can be applied to HZ to obtain a WFA. We use the
same notation as in [7] and a version of the spectral method working with an arbitrary basis (as in
[5, 4, 7]), in contrast to versions restricted to P = ??2 and S = ? like [19].
We first need to partition HZ into k + 1 blocks as follows. Since B is a basis, P is ?-complete
0
and admits a root P 0 . We define a block Ha ? RP ?S for each a ? ?0 , whose entries are given by
0
Ha (u, v) = HZ (ua, v), for any u ? P and v ? S. Thus, after suitably permuting the rows of HZ ,
>
>
>
we can write H>
Z = [H , Ha1 , . . . , Hak ]. We will use the following specific notation to refer to the
rows and columns of H corresponding to  ? P 0 ? S: h,S ? RS with h,S (v) = H (, v) and
0
hP 0 , (u) ? RP with hP 0 , (u) = H (u, ).
Using this notation, the spectral method can be described as follows. Given the desired number
of states n, it consists of first computing the truncated SVD of H corresponding to the n largest
singular values: Un Dn Vn> . Thus, matrix Un Dn Vn> is the best rank n approximation to H with
respect to the Frobenius norm. Then, using the right singular vectors Vn of H , the next step
consists of computing a weighted automaton AZ = h?, ?, {Aa }i as follows:
?> = h>
,S Vn

? = (H Vn )+ hP 0 ,

Aa = (H Vn )+ Ha Vn .

(SM)

The fact that the spectral method is based on a singular value decomposition justifies in part the use
of a Schatten p-norm as a regularizer in (HMC-H). In particular, two very natural choices are p = 1
and p = 2. The first one corresponds to a nuclear norm regularized optimization, which is known
to enforce a low rank constraint on HZ . In a sense, this choice can be justified in view of Theorem
1 when the target is known to be generated by some WFA. On the other hand, choosing p = 2 also
has some effect on the spread of singular values, while at the same time enforcing the coefficients
in HZ ? especially those that are completely unknown ? to be small. As our analysis suggests, this
last property is important for preventing errors from accumulating on the values assigned by AZ to
long strings.

4

Generalization Bound

In this section, we study the generalization properties of HMCp,` +SM. We give a stability analysis
for a special instance of this family of algorithms and use it to derive a generalization bound. We
study the specific case where p = 2 and `(y, y 0 ) = |y ? y 0 | for all (y, y 0 ). But, much of our analysis
can be used to derive similar bounds for other instances of HMCp,` +SM. The proofs of the technical
results presented are given in the Appendix.
We first introduce some notation needed for the presentation of our main result. For any ? > 0, let
t? be the function defined by t? (x) = x for |x| ? ? and t? (x) = ? sign(x) for |x| > ?. For any
distribution D over ?? ? R, we denote by D? its marginal distribution over ?? . The probability that
a string x ? D? belongs to PS is denoted by ? = D? (PS).
We assume that the parameters B, n, and ? are fixed. Two parameters that depend on D will appear
in our bound. In order to define these parameters, we need to consider the output HZ of (HMC-H)
>
>
>
as a random variable that depends on the sample Z. Writing H>
Z = [H , Ha1 , . . . , Hak ], as in
Section 3.2, we define:


? = E m [?n (H )]
? = E m ?n (H )2 ? ?n+1 (H )2 ,
Z?D

Z?D

where ?n (M) denotes the nth singular value of matrix M. Note that these parameters may vary
with m, n, ? and B.
In contrast to previous learning results based on the spectral method, our bound holds in an agnostic
setting. That is, we do not require that the data was generated from some (probabilistic) unknown
WFA. However, in order to prove our results we do need to make two assumptions about the tails of
the distribution. First, we need to assume that there exists a bound on the magnitude of the labels
generated by the distribution.
Assumption 1 There exists a constant ? > 0 such that if (x, y) ? D, then |y| ? ? almost surely.
6

Second, we assume that the strings generated by the distribution will not be too long. In particular,
that the length of the strings generated by D? follows a distribution whose tail is slightly lighter than
sub-exponential.
Assumption 2 There exist constants c, ? > 0 such that Px?D? [|x| ? t] ? exp(?ct1+? ) holds for
all t ? 0.
We note that in the present context both assumptions are quite reasonable. Assumption 1 is equivalent to assumptions made in other contexts where a stability analysis is pursued, e.g., in the analysis
of support vector regression in [11]. Furthermore, in our context, this assumption can be relaxed
to require only that the distribution over labels be sub-Gaussian, at the expense of a more complex
proof.
Assumption 2 is required by the fact already pointed out in [19] that errors in the estimation of
operator models accumulate exponentially with the length of the string. Moreover, it is well known
that the tail of any probability distribution generated by a WFA is sub-exponential. Thus, though we
do not require D? to be generated by a WFA, we do need its distribution over lengths to have a tail
behavior similar to that of a distribution generated by a WFA. This seems to be a limitation common
to all known learnability proofs based on the spectral method.
We can now state our main result, which is a bound on the average loss R(f ) = Ez?D [`(f (x), y)]
bZ (f ) = |Z|?1 P
in terms of the empirical loss R
z?Z `(f (x), y).
Theorem 2 Let Z be a sample formed by m i.i.d. examples generated from some distribution D
satisfying Assumptions 1 and 2. Let AZ be the WFA returned by algorithm HMCp,` +SM with p = 2
and loss function `(y, y 0 ) = |y ? y 0 |. Then, for any ? > 0, the following holds with probability at
least 1 ? ? for fZ = t? ? fAZ :
r
 4 2 3/2

? |P| |S|
ln m
1
b
R(fZ ) ? RZ (fZ ) + O
ln
.
? ? 3 ??
?
m1/3
The proof of this theorem is based on an algorithmic stability analysis. Thus, we will consider
two samples of size m, Z ? Dm consisting of m i.i.d. examples drawn from D, and Z 0 differing
0
0
). The
in Z 0 = (z1 , . . . , zm?1 , zm
from Z by just one point: say zm in Z = (z1 , . . . , zm ) and zm
0
new example zm is an arbitrary point the support of D. Throughout the analysis we use the shorter
notation H = HZ and H0 = HZ 0 for the Hankel matrices obtained from (HMC-H) based on
samples Z and Z 0 respectively.
The first step in the analysis is to bound the stability of the matrix completion algorithm. This is
done in the following lemma, that gives a sample-dependent and a sample-independent bound for
the stability of H.
Lemma 3 Suppose D satisfies Assumption 1. Then, the following holds:


p
1
kH ? H0 kF ? min 2? |P||S|,
.
? min{m,
e m
e 0}
The standard method for deriving generalization bounds from algorithmic stability results could be
applied here to obtain a generalization bound for our Hankel matrix completion algorithm. However,
our goal is to give a generalization bound for the full HMC+SM algorithm.
Using the bound on the Frobenius norm kH ? H0 kF , we are able to analyze the stability of ?n (H ),
?n (H )2 ? ?n+1 (H )2 , and Vn using well-known results on the stability of singular values and
singular vectors. These results are used to bound the difference between the operators of WFA AZ
and AZ 0 . The following lemma can be proven by modifying and extending some of the arguments
of [19, 4], which were given in the specific case of WFAs representing a probability distribution.
Lemma 4pLet ? = kH?H0 kF , ?
b = min{?n (H ), ?n (H0 )}, and ?b = ?n (H )2 ??n+1 (H )2 . Suppose ? ? ?b/4. Then, there exists some constant C > 0 such that the following three inequalities
7

hold:
?a ? ? : kAa ? A0a k ? C?? 3 |P|3/2 |S|1/2 /b
??
b2 ;
k? ? ?0 k ? C?? 2 |P|1/2 |S|/b
?;
k? ? ? 0 k ? C?? 3 |P|3/2 |S|1/2 /b
??
b2 .
The other half of the proof results from combining Lemmas 3 and 4 to obtain a bound for
|fZ (x) ? fZ 0 (x)|. This is a delicate step, because some of the bounds given above involve quantities
that are defined in terms of Z. Therefore, all these parameters need to be controlled in order to
ensure that the bounds do not grow too large. Furthermore, to obtain the desired bounds we need to
extend the usual tools for analyzing spectral methods to the current setting. In particular, these tools
need to be adapted to the agnostic settings where there is no underlying true WFA. The analysis is
further complicated by the fact that now the functions we are trying to learn and the distribution that
generates the data are not necessarily related.
Once all this is achieved, it remains to combine these new tools to show an algorithmic stability
result for HMCp,` +SM. In the following lemma, we first define ?bad? samples Z and show that bad
samples have a very low probability.
Lemma 5 Suppose D satisfies Assumptions 1 and 2. If Z is a large enough i.i.d. sample from
D, then with probability at least 1 ? 1/m3 the following inequalities hold simultaneously: |xi | ?
((1/c) ln(4m4 ))1/1+? for all i, ? ? 4/(? ?m), ?
b ? ?/2, and ?b ? ?/2.
After that we give two upper bounds for |fZ (x) ? fZ 0 (x)|: a tighter bound that holds for ?good?
samples Z and Z 0 and a another one that holds for all samples. These bounds are combined using
a variant of McDiarmid?s inequality for dealing with functions that do not satisfy the bounded differences assumption almost surely [21]. The rest of the proof then follows the same scheme as the
standard one for deriving generalization bounds for stable algorithms [11, 25].

5

Conclusion

We described a new algorithmic solution for learning arbitrary weighted automata from a sample of labeled strings drawn from an unknown distribution. Our approach combines an algorithm
for constrained matrix completion with the recently developed spectral learning methods for learning probabilistic automata. Using our general scheme, a broad family of algorithms for learning
weighted automata can be obtained. We gave a stability analysis of a particular algorithm in that
family and used it to prove generalization bounds that hold for all distributions satisfying two reasonable assumptions. The particular case of Schatten p-norm with p = 1, which corresponds to
a regularization with the nuclear norm, can be analyzed using similar techniques. Our results can
be further extended by deriving generalization guarantees for all algorithms in the family we introduced. An extensive and rigorous empirical comparison of all these algorithms will be an important
complement to the research we presented. Finally, learning DFAs under an arbitrary distribution
using the algorithms we presented deserves a specific study since the problem is of interest in many
applications and since it may benefit from improved learning guarantees.

Acknowledgments
Borja Balle is partially supported by an FPU fellowship (AP2008-02064) and project TIN201127479-C04-03 (BASMATI) of the Spanish Ministry of Education and Science, the EU PASCAL2
NoE (FP7-ICT-216886), and by the Generalitat de Catalunya (2009-SGR-1428). The work of
Mehryar Mohri was partly funded by the NSF grant IIS-1117591.

8

References
[1] J. Albert and J. Kari. Digital image compression. In Handbook of Weighted Automata. Springer, 2009.
[2] A. Anandkumar, D. P. Foster, D. Hsu, S. M. Kakade, and Y-K. Liu. Two SVDs suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation. CoRR, abs/1204.6703, 2012.
[3] A. Anandkumar, D. Hsu, and S. M. Kakade. A method of moments for mixture models and hidden
Markov models. COLT, 2012.
[4] R. Bailly. Quadratic weighted automata: Spectral algorithm and likelihood maximization. ACML, 2011.
[5] R. Bailly, F. Denis, and L. Ralaivola. Grammatical inference as a principal component analysis problem.
ICML, 2009.
[6] B. Balle, A. Quattoni, and X. Carreras. A spectral learning algorithm for finite state transducers. ECML?
PKDD, 2011.
[7] B. Balle, A. Quattoni, and X. Carreras. Local loss optimization in operator models: A new insight into
spectral learning. ICML, 2012.
[8] A. Beimel, F. Bergadano, N.H. Bshouty, E. Kushilevitz, and S. Varricchio. Learning functions represented
as multiplicity automata. JACM, 2000.
[9] J. Berstel and C. Reutenauer. Rational Series and Their Languages. Springer, 1988.
[10] B. Boots, S. Siddiqi, and G. Gordon. Closing the learning planning loop with predictive state representations. I. J. Robotic Research, 2011.
[11] O. Bousquet and A. Elisseeff. Stability and generalization. JMLR, 2002.
[12] T. M. Breuel. The OCRopus open source OCR system. IS&T/SPIE Annual Symposium, 2008.
[13] E.J. Candes and Y. Plan. Matrix completion with noise. Proceedings of the IEEE, 2010.
[14] E.J. Candes and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Transactions on Information Theory, 2010.
[15] Jack W. Carlyle and Azaria Paz. Realizations by stochastic finite automata. J. Comput. Syst. Sci., 5(1):26?
40, 1971.
[16] S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Ungar. Spectral learning of latent-variable
PCFGs. ACL, 2012.
[17] M. Fliess. Matrices de Hankel. Journal de Math?ematiques Pures et Appliqu?ees, 53:197?222, 1974.
[18] R. Foygel, R. Salakhutdinov, O. Shamir, and N. Srebro. Learning with the weighted trace-norm under
arbitrary sampling distributions. NIPS, 2011.
[19] D. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden Markov models. COLT,
2009.
[20] M. Kearns and L. Valiant. Cryptographic limitations on learning boolean formulae and finite automata.
JACM, 1994.
[21] S. Kutin. Extensions to McDiarmid?s inequality when differences are bounded with high probability.
Technical report, TR-2002-04, University of Chicago, 2002.
[22] F.M. Luque, A. Quattoni, B. Balle, and X. Carreras. Spectral learning in non-deterministic dependency
parsing. EACL, 2012.
[23] M. Mohri. Weighted automata algorithms. In Handbook of Weighted Automata. Springer, 2009.
[24] M. Mohri, F. C. N. Pereira, and M. Riley. Speech recognition with weighted finite-state transducers. In
Handbook on Speech Processing and Speech Communication. Springer, 2008.
[25] M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. The MIT Press, 2012.
[26] A.P. Parikh, L. Song, and E.P. Xing. A spectral algorithm for latent tree graphical models. ICML, 2011.
[27] B. Recht. A simpler approach to matrix completion. JMLR, 2011.
[28] Arto Salomaa and Matti Soittola. Automata-Theoretic Aspects of Formal Power Series. Springer-Verlag:
New York, 1978.
[29] M.P. Sch?utzenberger. On the definition of a family of automata. Information and Control, 1961.
[30] S. M. Siddiqi, B. Boots, and G. J. Gordon. Reduced-rank hidden Markov models. AISTATS, 2010.
[31] L. Song, B. Boots, S. Siddiqi, G. Gordon, and A. Smola. Hilbert space embeddings of hidden Markov
models. ICML, 2010.

9

"
449,"Generalized Prioritized Sweeping

David Andre

Nir Friedman

Ronald Parr

Computer Science Division, 387 Soda Hall
University of California, Berkeley, CA 94720
{dandre,nir,parr}@cs.berkeley.edu

Abstract
Prioritized sweeping is a model-based reinforcement learning method
that attempts to focus an agent's limited computational resources to
achieve a good estimate of the value of environment states. To choose effectively where to spend a costly planning step, classic prioritized sweeping uses a simple heuristic to focus computation on the states that are
likely to have the largest errors. In this paper, we introduce generalized
prioritized sweeping, a principled method for generating such estimates
in a representation-specific manner. This allows us to extend prioritized
sweeping beyond an explicit, state-based representation to deal with compact representations that are necessary for dealing with large state spaces.
We apply this method for generalized model approximators (such as
Bayesian networks), and describe preliminary experiments that compare
our approach with classical prioritized sweeping.

1 Introduction
In reinforcement learning, there is a tradeoff between spending time acting in the environment and spending time planning what actions are best. Model-free methods take one
extreme on this question- the agent updates only the state most recently visited. On the
other end of the spectrum lie classical dynamic programming methods that reevaluate the
utility of every state in the environment after every experiment. Prioritized sweeping (PS)
[6] provides a middle ground in that only the most ""important"" states are updated, according
to a priority metric that attempts to measure the anticipated size of the update for each state.
Roughly speaking, PS interleaves perfonning actions in the environment with propagating
the values of states. After updating the value of state s, PS examines all states t from which
the agent might reach s in one step and assigns them priority based on the expected size of
the change in their value.
A crucial desideratum for reinforcement learning is the ability to scale-up to complex
domains. For this, we need to use compact (or generalizing) representations of the model and
the value function. While it is possible to apply PS in the presence of such representations
(e.g., see [1]), we claim that classic PS is ill-suited in this case. With a generalizing model,
a single experience may affect our estimation of the dynamics of many other states. Thus,
we might want to update the value of states that are similar, in some appropriate sense, to
s since we have a new estimate of the system dynamics at these states. Note that some of
these states might never have been reached before and standard PS will not assign them a
priority at all.

D. Andre, N. Friedman and R Parr

1002

In this paper, we present generalized prioritized sweeping (GenPS), a method that utilizes
a fonnal principle to understand and extend PS and extend it to deal with parametric
representations for both the model and the value function. If GenPS is used with an explicit
state-space model and value function representation, an algorithm similar to the original
(classic) PS results. When a model approximator (such as a dynamic Bayesian network
[2]) is used, the resulting algorithm prioritizes the states of the environment using the
generalizations inherent in the model representation.

2 The Basic Principle
We assume the reader is familiar with the basic concepts of Markov Decision Processes
(MDPs); see, for example, [5]. We use the following notation: A MDP is a 4-tuple,
(S,A,p,r) where S is a set of states, A is a set of actions, p(t I s,a) is a transition
model that captures the probability of reaching state t after we execute action a at state
s, and r( s) is a reward function mapping S into real-valued rewards. In this paper, we
focus on infinite-horizon MDPs with a discount factor ,. The agent's aim is to maximize
the expected discounted total reward it will receive. Reinforcement learning procedures
attempt to achieve this objective when the agent does not know p and r.
A standard problem in model-based reinforcement learning is one of balancing between
planning (Le., choosing a policy) and execution. Ideally, the agent would compute the
optimal value function for its model of the environment each time the model changes. This
scheme is unrealistic since finding the optimal policy for a given model is computationally
non-trivial. Fortunately, we can approximate this scheme if we notice that the approximate
model changes only slightly at each step. Thus, we can assume that the value function
from the previous model can be easily ""repaired"" to reflect these changes. This approach
was pursued in the DYNA [7] framework, where after the execution of an action, the
agent updates its model of the environment, and then performs some bounded number
of value propagation steps to update its approximation of the value function . Each vaiuepropagation step locally enforces the Bellman equation by setting V(s) ~ maxaEA Q(s, a),
where Q(s,a) = f(s) + ,Ls'ESP(s' I s,a)V(s'), p(s' I s,a) and f(s) are the agent's
approximation of the MDP, and V is the agent's approximation of the value function.
This raises the question of which states should be updated. In this paper we propose the
following general principle:
GenPS Principle: Update states where the approximation of the value
function will change the most. That is, update the states with the largest
Bellmanerror,E(s)
IV(s) -maxaEAQ(s,a)l.
The motivation for this principle is straightforward. The maximum Bellman error can be
used to bound the maximum difference between the current value function, V(s) and the
optimal value function, V""'(s) [9]. This difference bounds the policy loss, the difference
between the expected discounted reward received under the agent's current policy and the
expected discounted reward received under the optimal policy.
To carry out this principle we have to recognize when the Bellman error at a state changes.
This can happen at two different stages. First, after the agent updates its model of the world,
new discrepancies between V (s) and max a Q( s, a) might be introduced, which can increase
the Bellman error at s. Second, after the agent performs some value propagations, V is
changed, which may introduce new discrepancies.
We assume that the agent maintains a value function and a model that are parameterized
by Dv and DM . (We will sometimes refer to the vector that concatenates these vectors
together into a single, larger vector simply as D.) When the agent observes a transition from
state s to s' under action a, the agent updates its environment model by adjusting some
of the parameters in DM. When perfonning value-propagations, the agent updates V by
updating parameters in Dv. A change in any of these parameters may change the Bellman
error at other states in the model. We want to recognize these states without explicitly

=

Generalized Prioritized Sweeping

1003

computing the Bellman error at each one. Formally, we wish to estimate the change in
error, I~E(B) I, due to the most recent change ~() in the parameters.
We propose approximating I~E(8) 1 by using the gradient of the right hand side of the
Bellman equation (i.e. max a Q(8,a). Thus, we have: I~E(s)1 ~ lV'max a Q(8,a) . ~()I
which estimates the change in the Bellman error at state 8 as a function of the change in
Q( 8, a). The above still requires us to differentiate over a max, which is not differentiable.
In general, we want to to overestimate the change, to avoid ""starving"" states with nonnegligible error. Thus, we use the following upper bound: 1V'(max a Q(8, a)) . ~81 ~
max a IV'Q(s,a). ~81?
We now define the generalized prioritized sweeping procedure. The procedure maintains
a priority queue that assigns to each state 8 a priority,pri( 8). After making some changes, we
can reassign priorities by computing an approximation of the change in the value function.
Ideally, this is done using a procedure that implements the following steps:
procedure update-priorities (&)
for all s E S pri(s) +- pri(s) + max a IV'Q(s, a) . &1.
Note that when the above procedure updates the priority for a state that has an existing
priority, the priorities are added together. This ensures that the priority being kept is an
overestimate of the priority of each state, and thus, the procedure will eventually visit all
states that require updating.
Also, in practice we would not want to reconsider the priority of all states after an update
(we return to this issue below).
Using this procedure, we can now state the general learning procedure:
procedure GenPS 0
loop
perform an action in the environment
update the model; let & be the change in ()
call update-priorities( &)
while there is available computation time
arg max s pri( s)
let smax
perform value-propagation for V(smax); let & be the change in ()
call update-priorities( &)
pri(smax) +- W(smax) - max a Q(smax,a)11

=

Note that the GenPS procedure does not determine how actions are selected. This issue,
which involves the problem of exploration, is orthogonal to the our main topic. Standard
approache!;, such as those described in [5, 6, 7], can be used with our procedure.
This abstract description specifies neither how to update the model, nor how to update the
value function in the value-propagation steps. Both of these depend on the choices made
in the corresponding representation of the model and the value function. Moreover, it is
clear that in problems that involve a large state space, we cannot afford to recompute the
priority of every state in update-priorities. However, we can simplify this computation
by exploiting sparseness in the model and in the worst case we may resort to approximate
methods for finding the states that receive high priority after each change.

3 Explicit, State-based Representation
In this section we briefly describe the instantiation of the generalized procedure when the
rewards, values, and transition probabilities are explicitly modeled using lookup tables. In
this representation, for each state 8, we store the expected reward at 8, denoted by Of(s)' the
estimated value at 8, denoted by Bv (s), and for each action a and state t the number of times
the execution of a at 8 lead to state t, denoted NS,Q.,t. From these transition counts we can
I In general, this will assign the state a new priority of 0, unless there is a self loop. In this case it
will easy to compute the new Bellman error as a by-product of the value propagation step.

D. Andre, N. Friedman and R. Parr

1004

reconstruct the transition probabilities pACt

I 8, a) =

N?. a.! +N~! a .t

O
W t' N "",a ,t' +N"" , a , t l

""

'

where NO

8,a,t

are

fictional counts that capture our prior information about the system's dynamics. 2 After each
step in the worJd, these reward and probability parameters are updated in the straightforward
manner. Value propagation steps in this representation set 8Y (t) to the right hand side of
the Bellman equation.
To apply the GenPS procedure we need to derive the gradient of the Bellman equation
for two situations: (a) after a single step in the environment, and (b) after a value update.
In case (a), the model changes after performing action 8~t . In this case, it is easy to
verify that V'Q(s,a) 'do = dOr(t) +
N.}t+N~.a.t (V(t) - 2:~p(t' I s,a)V(t')), and

tt

that V' Q( s', a') . do = 0 if s' =1= s or a' =1= a. Thus, s is the only state whose priority
changes.
In case (b), the value function changes after updating the value of a state t. In this case,
V'Q(s, a) ?do = ,pet I s, a)l1o v (t)' It is easy to see that this is nonzero only ift is reachable
from 8. In both cases, it is straightforward to locate the states where the Bellman error
might have have changed, and the computation of the new priority is more efficient than
computing the Bellman-error. 3
Now we can relate GenPS to standard prioritized sweeping. The PS procedure has the
general form of this application of GenPS with three minor differences. First, after performing a transition s~t in the environment, PS immediately performs a value propagation
for state s, while GenPS increments the priority of s. Second, after performing a value
propagation for state t, PS updates the priority of states s that can reach t with the value
max a p( tis, a) ./1y( t). The priority assigned by GenPS is the same quantity multiplied by
,. Since PS does not introduce priorities after model changes, this multiplicative constant
does not change the order of states in the queue. Thirdly, GenPS uses addition to combine
the old priority of a state with a new one, which ensures that the priority is indeed an upper
bound. In contrast, PS uses max to combine priorities.
This discussion shows that PS can be thought of as a special case of GenPS when the
agent uses an explicit, state-based representation. As we show in the next section, when
the agent uses more compact representations, we get procedures where the prioritization
strategy is quite different from that used in PS. Thus, we claim that classic PS is desirable
primarily when explicit representations are used.

4 Factored Representation
We now examine a compact representation of p( s' I s, a) that is based on dynamic Bayesian
networks (DBNs) [2]. DBNs have been combined with reinforcement learning before in
[8], where they were used primarily as a means getting better generalization while learning.
We will show that they also can be used with prioritized sweeping to focus the agent's
attention on groups of states that are affected as the agent refines its environment model.
We start by assuming that the environment state is described by a set of random variables,
XI, . .. , X n ? For now, we assume that each variable can take values from a finite set
Val(Xi). An assignment of values XI, .? ? , Xn to these variables describes a particular
environment state. Similarly, we assume that the agent's action is described by random
variables AI, ... ,A k . To model the system dynamics, we have to represent the probability
of transitions s~t, where sand t are two assignments to XI, .. . , Xn and a is an assignment
to AI, ... ,A k ? To simplify the discussion, we denote by Yi, .. . , Yn the agent's state after
2Formally, we are using multinomial Dirichlet priors. See, for example, [4] for an introduction to
these Bayesian methods.
3 Although ~~(s .a ) involves a summation over all states, it can be computed efficiently. To see
.. ,a ,t
this, note that the summation is essentially the old value of Q( s, a) (minus the immediate reward)
which can be retained in memory.

Generalized Prioritized Sweeping

1005

the action is executed (e.g., the state t). Thus, p(t I s,a) is represented as a conditional
probability P(YI , ... , Y n I XI, ... ,Xn , AI , ... ,Ak).
A DBN model for such a conditional distribution consists of two components. The
first is a directed acyclic graph where each vertex is labeled by a random variable and in
which the vertices labeled XI, . .. ,Xn and AI, .. . , Ak are roots. This graph speoifies the
factorization of the conditional distribution:
n

P(Yi, ... , Y n I XI'? .. ' X n , AI,???, Ak) =

II P(Yi I Pai),

(1)

i=I

where Pai are the parents of Yi in the graph. The second component of the DBN model is
a description of the conditional probabilities P(Yi I Pai). Together, these two components
describe a unique conditional distribution. The simplest representation of P(Yi I Pai) is a
table that contains a parameter (}i ,y,z = P(Yi = y I Pai = z) for each possible combination
of y E Val(Yi) and z E Val(Pai) (note that z is a joint assignment to several random
variables). It is easy to see that the ""density"" of the DBN graph determines the number of
parameters needed. In particular, a complete graph, to which we cannot add an arc without
violating the constraints, is equivalent to a state-based representation in terms of the number
of parameters needed. On the other hand, a sparse graph requires few parameters.
In this paper, we assume that the learneris supplied with the DBN structure and only has to
learn the conditional probability entries. It is often easy to assess structure information from
experts even when precise probabilities are not available. As in the state-based representation, we learn the parameters using Dirichlet priors for each multinomial distribution [4].
In this method, we assess the conditional probability (}i,y,z using prior knowledge and the
frequency of transitions observed in the past where Yi = y among those transitions where
Pai = z. Learning amounts to keeping counts Ni ,y,z that record the number of transitions
where Yi = y and Pai = z for each variable Yi and values y E Val(Yi) and z E Val(Pai).
Our prior knowledge is represented by fictional counts Np,y,z. Then we estimate probabil. . USIng
. the 10rmu
~
Ia Ui,y,z
LI
, z+N?,y , z
h
N?~, ' ,z ?
ltles
- N; ,1I N;,.
,z
' W ere
- ""
L....,y' N t,y',z
+ NOi,y' ,z?
We now identify which states should be reconsidered after we update the DBN parameters.
Recall that this requires estimating the term V Q( s, a) ./1n. Since!!:.o is sparse, after making
the transition s* ~t*, we have that VQ(s, a) . !!:.O =

2:i a~Q(s:a). ' where yi and zi are the
I,V i '%i

assignments to Yi and Pai, respectively, in s* ~t*. (Recall that s*, a* and t* jointly assign
values to all the variables in the DBN.)
We say that a transition s~t is consistent with an assignment X = x for a vector of
random variables X, denoted (s,a, t) F= (X = x), if X is assigned the value x in s~t.
We also need a similar notion for a partial description of a transition. We say that s and
a are consistent with X = x, denoted (s,a,?) F= (X = x), if there is a t such that
(s, a, t) F= (X = x).
Using this notation, we can show that if (s, a, .) F (Pai = zi), then

8Q(s, a)
=

-: Nt'~-''-':- [8,.?:.,: t(,.J~.: .': p(t I s, a)1l(t) - t(,. ~I=': p(t I s, a)1l(t)]

and if s, a are inconsistent with Pai =

zi, then aa:.(8:a).
""""i

=

o.

'Z i

This expression shows that if s is similar to s* in that both agree on the values they assign
to the parents of some Yi (i.e., (s, a*) is consistent with zi), then the priority of s would
change after we update the model. The magnitude of the priority change depends upon both
the similarity of sand s* (i.e. how many of the terms in VQ(s, a) ? !!:'o will be non-zero),
and the value of the states that can be reached from s.

D. Andre, N. Friedman and R. Parr

1006

PSPS+fact<X'ed --- .-

G:nps ??..:.~.._~_..::.:.;. ::::.:....~..:.....:._--I---...?---

2.S

?? oX-'--

~

'3

""

0

~

I.S

?

005

./
,.l

___

/~

o~~~--~~--~~~--~~

1000 2000 3000 4000 sooo 6000 7000 8000 9000 10000
Number of iterations

(b)

(a)

(c)

Figure 1: (a) The maze used in the experiment. S marks the start space, G the goal state, and 1, 2
and 3 are the three flags the agent has to set to receive the reward. (b) The DBN structure that captures
the independencies in this domain. (c) A graph showing the performance of the three procedures on
this example. PS is GenPS with a state-based model, PS+factored is the same procedure but with a
factored model, and GenPS exploits the factored model in prioritization. Each curve is the average
of 5 runs.
The evaluation of 8~(8:a). requires us to sum over a subset of the states -namely, those
1,1Ii

.z,

states t that are consistent with zi. Unfortunately, in the worst case this will be a large
fragment of the state space. If the number of environment states is not large, then this might
be a reasonable cost to pay for the additional benefits of GenPS. However, this might be a
burdensome when we have a large state space, which are the cases where we expect to gain
the most benefit from using generalized representations such as DBN.
In these situations, we propose a heuristic approach for estimating V'Q(s, a)~ without
summing over large numbers of states for computing the change of priority for each possible
state. This can be done by finding upper bounds on or estimates of 8~( 8:a ).. Once we
I,ll, ' %i

have computed these estimates, we can estimate the priority change for each state s. We
use the notation s '""Vi s* if sand s* both agree on the assignment to Pai . If Ci is an upper
bound on (or an estimate of)

18~~~:t:: I, we have that IV'Q(8, a)/loM ~ Li:s~iS. C i .
1

Thus, to evaluate the priority of state s, we simply find how ""similar"" it is to s*. Note
that it is relatively straightforward to use this equation to enumerate all the states where the
priority change might be large. Finally, we note that the use of a DBN as a model does not
change the way we update priorities after a value propagation step. If we use an explicit
table of values, then we would update priorities as in the previous section. If we use a
compact description of the value function, then we can apply GenPS to get the appropriate
update rule.

S An Experiment
We conducted an experiment to evaluate the effect of using GenPS with a generalizing
model. We used a maze domain similar to the one described in [6]. The maze, shown
in Figure 1(a), contains 59 cells, and 3 binary flags, resulting in 59 x 2 3 = 472 possible
states. Initially the agent is at the start cell (marked by S) and the flags are reset. The
agent has four possible actions, up, down, left, and right, that succeed 80% of the time,
and 20% of the time the agent moves in an unintended perpendicular direction. The i'th
flag is set when the agent leaves the cell marked by i. The agent receives a reward when
it arrives at the goal cell (marked by G) and all of the flags are set. In this situation, any
action resets the game. As noted in [6], this environment exhibits independencies. Namely,
the probability of transition from one cell to another does not depend on the flag settings.

Generalized Prioritized Sweeping

1007

These independencies can be captured easily by the simple DBN shown in Figure I(b) Our
experiment is designed to test the extent to which GenPS exploits the knowledge of these
independencies for faster learning.
We tested three procedures. The first is GenPS, which uses an explicit state-based
model. As explained above, this variant is essentially PS. The second procedure uses a
factored model of the environment for learning the model parameters, but uses the same
prioritization strategy as the first one. The third procedure uses the GenPS prioritization
strategy we describe in Section 4. All three procedures use the Boltzman exploration
strategy (see for example [5]). Finally, in each iteration these procedures process at most
10 states from the priority queue.
The results are shown in Figure l(c). As we can see, the GenPS procedure converged
faster than the procedures that used classic PS. As we can see, by using the factored model
we get two improvements. The first improvement is due to generalization in the model.
This allows the agent to learn a good model of its environment after fewer iterations. This
explains why PS+factored converges faster than PS. The second improvement is due to
the better prioritization strategy. This explains the faster convergence of GenPS.

6 Discussion
We have presented a general method for approximating the optimal use of computational
resources during reinforcement learning . Like classic prioritized sweeping, our method
aims to perform only the most beneficial value propagations. By using the gradient of the
Bellman equation our method generalizes the underlying principle in prioritized sweeping.
The generalized procedure can then be applied not only in the explicit, state-based case,
but in cases where approximators are used for the model. The generalized procedure also
extends to cases where a function approximator (such as that discussed in [3]) is used for
the value function, and future work will empirically test this application of GenPS. We are
currently working on applying GenPS to other types of model and function approximators.

Acknowledgments
We are grateful to Geoff Gordon, Daishi Harada, Kevin Murphy, and Stuart Russell for
discussions related to this work and comments on earlier versions of this paper. This
research was supported in part by ARO under the MURI program ""Integrated Approach to
Intelligent Systems,"" grant number DAAH04-96-I-0341. The first author is supported by a
National Defense Science and Engineering Graduate Fellowship.

References
[I] S. Davies. Multidimensional triangulation and interpolation for reinforcement learning. In
Advances in Neurallnfonnation Processing Systems 9. 1996.
[2] T. Dean and K. Kanazawa. A model for reasoning about persistence and causation. Computationallntelligence, 5:142-150, 1989.
[3] G. J. Gordon. Stable function approximation in dynamic programming. In Proc. 12th
Int. Con! on Machine Learning, 1995.

[4] D. Heckerman. A tutorial on learning with Bayesian networks. Technical Report MSR-TR-9506, Microsoft Research, 1995. Revised November 1996.
[5] L. P. Kaelbling, M. L. Littman and A. W. Moore. Reinforcement learning: A survey. Journal
of Artificial Intelligence Research, 4:237-285, 1996.
[6] A. W. Moore and C. G. Atkeson. Prioritized sweeping-reinforcement learning with less data
and less time. Machine Learning, 13:103-130, 1993.
[7] R. S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating
dynamic programming. In Machine Learning: Proc. 7th Int. Con!, 1990.
[8] P. Tadepalli and D. Ok. Scaling up average reward reinforcement learning by approximating the
domain models and the value function. In Proc. 13th Int. Con! on Machine Learning, 1996.
[9] R. J. Williams and L. C. III Baird. Tight performance bounds on greedy policies based on
imperfect value functions. Technical report, Computer Science, Northeastern University. 1993.

"
3870,"The Lov?asz ? function, SVMs and finding large dense
subgraphs

Vinay Jethava ?
Computer Science & Engineering Department,
Chalmers University of Technology
412 96, Goteborg, SWEDEN
jethava@chalmers.se
Chiranjib Bhattacharyya
Department of CSA,
Indian Institute of Science
Bangalore, 560012, INDIA
chiru@csa.iisc.ernet.in

Anders Martinsson
Department of Mathematics,
Chalmers University of Technology
412 96, Goteborg, SWEDEN
andemar@student.chalmers.se

Devdatt Dubhashi
Computer Science & Engineering Department,
Chalmers University of Technology
412 96, Goteborg, SWEDEN
dubhashi@chalmers.se

Abstract
The Lov?asz ? function of a graph, a fundamental tool in combinatorial optimization and approximation algorithms, is computed by solving a SDP. In this paper
we establish that the Lov?asz ? function is equivalent to a kernel learning problem
related to one class SVM. This interesting connection opens up many opportunities bridging graph theoretic algorithms and machine learning. We show that there
exist graphs, which we call SVM ? ? graphs, on which the Lov?asz ? function
can be approximated well by a one-class SVM. This leads to novel use of SVM
techniques for solving algorithmic
problems in large graphs e.g. identifying a
?
planted clique of size ?( n) in a random graph G(n, 21 ). A classic approach for
this problem involves computing the ? function, however it is not scalable due
to SDP computation. We show that the random graph with a planted clique is an
example of SVM ? ? graph. As a consequence a SVM based approach easily
identifies the clique in large graphs and is competitive with the state-of-the-art.
We introduce the notion of common orthogonal labelling and show that it can be
computed by solving a Multiple Kernel learning problem. It is further shown that
such a labelling is extremely useful in identifying a large common dense subgraph
in multiple graphs, which is known to be a computationally difficult problem. The
proposed algorithm achieves an order of magnitude scalability compared to state
of the art methods.

1

Introduction

The Lov?asz ? function [19] plays a fundamental role in modern combinatorial optimization and
in various approximation algorithms on graphs, indeed Goemans was led to say It seems all
roads lead to ? [10]. The function is an instance of semidefinite programming(SDP) and
hence computing it is an extremely demanding task even for moderately sized graphs. In this paper
we establish that the ? function is equivalent to solving a kernel learning problem in the one-class
SVM setting. This surprising connection opens up many opportunities which can benefit both graph
theory and machine learning. In this paper we exploit this novel connection to show an interesting
application of the SVM setup for identfying large dense subgraphs. More specifically we make the
following contributions.
?
Relevant code and datasets
jethava/svm-theta.html

can

be

found

1

on

http://www.cse.chalmers.se/ e

1.1

Contributions:

1.We give a new SDP characterization of Lov?asz ? function,
min ?(K) = ?(G)
K?K(G)

where ?(K) is computed by solving an one-class SVM. The matrix K is a kernel matrix, associated
with any orthogonal labelling of G. This is discussed in Section 2.
2. Using an easy to compute orthogonal labelling we show that there exist graphs, which we call
SVM ? ? graphs, on which Lov?asz ? function can be well approximated by solving an one-class
SVM. This is discussed in Section 3.
3. The problem of finding a large common dense subgraph in multiple graphs arises in a variety
of domains including Biology, Internet, Social Sciences [18]. Existing state-of-the-art methods
[14] are enumerative in nature and has complexity exponential in the size of the subgraph. We
introduce the notion of common orthogonal labelling which can be used to develop a formulation
which is close in spirit to a Multiple Kernel Learning based formulation. Our results on the well
known DIMACS benchmark dataset show that it can identify large common dense subgraphs in
wide variety of settings, beyond the realm of state-of-the-art methods. This is discussed in Section
4.
4. Lastly, in Section 5, we show that the famous planted clique problem, can be easily solved for
large graphs by solving an one-class SVM. Many problems of interest in the area of machine learning
can be reduced to the problem of detecting planted clique, e.g detecting correlations [1, section 4.6],
correlation clustering [21] etc. The planted clique problem consists of identifying a large clique in
a random graph. There is an elegant approach for identifying the planted clique by computing the
Lov?asz ? function [8], however it is not practical for large graphs as it requires solving an SDP.
We show that the graph associated with the planted clique problem is a SVM ? ? graph, paving
the way for identifying the clique by solving an one-class SVM. Apart from the method based on
computing the ? function, there are other methods for planted clique identification, which do not
require solving an SDP [2, 7, 24]. Our result is also competitive with the state-of-the-art non-SDP
based approaches [24].
Notation We denote the Euclidean norm by k ? k and the infinity norm by k ? k? . Let S d?1 =
{u ? Rd | kuk = 1} denote a d dimensional sphere. Let Sn denote the set of n?n square symmetric
matrices and S+
n denote n ? n square symmetric positive semidefinite matrices. For any A ? Sn
we denote the eigenvalues ?1 (A) ? . . . ? ?n (A). diag(r) will denote a diagonal matrix with
diagonal entries defined by components of r. We denote the one-class SVM objective function by
!
n
n
X
X
?i ?j Kij
?i ?
?(K) =
max
2
(1)
?i ?0,i=1,...,n

i=1

|

i=1

{z

f (?;K)

}

where K ? S+
n . Let G = (V, E) be a graph on vertices V = {1, . . . , n} and edge set E. Let
A ? Sn denote the adjacency matrix of G where Aij = 1 if edge (i, j) ? E, and 0 otherwise. An
? denote the
eigenvalue of graph G would mean the eigenvalue of the adjacency matrix of G. Let G
? = ee> ? I ? A, where e = [1, 1, . . . , 1]>
? is A
complement graph of G. The adjacency matrix of G
is a vector of length n containing all 1?s, and I denotes the identity matrix. Let GS = (S, ES ) denote

the subgraph induced by S ? V in graph G; having density ?(GS ) is given by ?(GS ) = |ES |/ |S|
2 .
Let Ni (G) = {j ? V : (i, j) ? E} denote the set of neighbours of vertex i in graph G, and degree
? is a subset of vertices
of node i to be di (G) = |Ni (G)|. An independent set in G (a clique in G
? The notation is standard e.g.
S ? V for which no (every) pair of vertices has an edge in G (in G).
see [3].

2

Lov?asz ? function and Kernel learning

Consider the problem of embedding a graph G = (V, E) on a d dimensional unit sphere S d?1 . The
study of this problem was initiated in [19] which introduced the idea of orthogonal labelling: An
2

orthogonal labelling of graph G = (V, E) with |V | = n, is a matrix U = [u1 , . . . , un ] ? Rd?n such
d?1
that u>
? i = 1, . . . , n.
i uj = 0 whenever (i, j) 6? E and ui ? S
An orthogonal labelling defines an embedding of a graph on a d dimensional unit sphere: for every
vertex i there is a vector ui on the unit sphere and for every (i, j) 6? E ui and uj are orthogonal.
Using the notion of orthogonal labellings, [19] defined a function, famously known as Lov?asz ?
function, which upper bounds the size of maximum independent set. More specifically
for any graph G : ALPHA(G) ? ?(G),
where ALPHA(G) is the size of the largest independent set. Finding large independent sets is
a fundamental problem in algorithm design and analysis and computing ALPHA(G) is a classic
NP-hard problem which is even very hard even to approximate [11]. However, the Lov?asz function
?(G) gives a tractable upper-bound and since then Lov?asz ? function has been extensively used
in solving a variety of algorithmic problems e.g. [6]. It maybe useful to recall the definition of
Lov?asz ? function. Denote the set of all possible orthogonal labellings of G by Lab(G) = {U =
[u1 , . . . , un ]|ui ? S d?1 , u>
i uj = 0 ?(i, j) 6? E}.
?(G) =

min

min max

U?Lab(G) c?S d?1

i

1
(c> ui )2

(2)

There exist several other equivalent definitions of ?, for a comprehensive discussion see [16].
However computation of Lov?asz ? function is not practical even for moderately sized graphs as it
requires solving a semidefinite program on a matrix which is of the size of the graph. In the following
theorem, we show that there exist connections between the ? function and the SVM formulation.
Theorem 2.1. For a undirected graph G = (V, E), with |V | = n, let K(G) := {K ? S+
n | Kii =
1, i ? [n], Kij = 0, (i, j) 6? E} Then, ?(G) = minK?K(G) ?(K)
Proof. We begin by noting that any K ? K(G) is positive semidefinite and hence there exists
U ? Rd?n such that K = U> U. Note that Kij = u>
i uj where ui is a column of U. Hence by
inspection it is clear that the columns of U defines an orthogonal labelling on G, i.e U ? Lab(G).
Using a similar argument we can show that for any U ? Lab(G), the matrix K = U> U, is an
element of K(G). The set of valid kernel matrices K(G) is thus equivalent to Lab(G). Note that if
U is a labelling then U = Udiag() is also an orthogonal labelling for any > = [1 , . . . , n ], i =
?1 i = 1, . . . , n. It thus suffices to consider only those labellings for which c> ui ? 0 ? i =
1, . . . , n holds. For a fixed c one can write maxi (c>1ui )2 = mint t2 subject to c>1ui ? t. This is
true because the minimum over t is attained at maxi c>1ui . Setting w = 2tc yields the following
2

relation minc?S d?1 maxi (c>1ui )2 = minw?Rd kwk
with constraints w> ui ? 2. This establishes
4
that for a labelling, U, the optimal c is obtained by solving an one-class SVM. Application of
strong duality immediately leads to the claim minc?S d?1 maxi (c>1ui )2 = ?(K) where K = U> U
and ?(K) is defined in (1). As there is a correspondence between each element of Lab(G) and K
minimization of ?(K) over K is equivalent to computing the ?(G) function.
This is a significant result which establishes connections between two well studied formulations,
namely ? function and the SVM formulation. An important consequence of Theorem 2.1 is an
easily computable upperbound on ?(G) namely that for any graph G
ALPHA(G) ? ?(G) ? ?(K) ?K ? K(G)

(3)

Since solving ?(K) is a convex quadratic program, it is indeed a computationally efficient alternative
to the ? function. In fact we will show that there exist families of graphs for which ?(G) can be
approximated to within a constant factor by ?(K) for suitable K. Theorem 2.1 is closely related to
the following result proved in [20].
Theorem 2.2. [20] For a graph G = (V, E) with |V | = n let C ? Sn matrix with Cij = 0
whenever (i, j) 6? E. Then,


 
C
>
>
+I x
?(G) = minC v(G, C) = max 2x e ? x
x?0
??n (C)
3

Proof. See [20]
See that for any feasible C the matrix I + ??nC(C) ? K(G). Theorem 2.1 is a restatement of
Theorem 2.2, but has the additional advantage that the stated optimization problem can be solved
as an SDP. The optimization problem minC v(G, C) with constraints on C is not an SDP. If we
fix C = A, the adjacency matrix, we obtain a very interesting orthogonal labelling, which we will
refer to as LS labelling, introduced in [20]. Indeed there exists family of graphs, called Q graphs
for which LS labelling yields the interesting result ALPHA(G) = v(G, A), see [20]. Indeed
on a Q graph one does not need to compute a SDP, but can solve an one-class SVM, which has
obvious computational benefits. Inspired by this result, in the remaining part of the paper, we study
this labelling more closely. As a labelling is completely defined by the associated kernel matrix, we
refer to the following kernel as the LS labelling,
K=

3

A
+ I where ? ? ??n (A).
?

(4)

SVM ? ? graphs: Graphs where ? function can be approximated by SVM

We now introduce a class of graphs on which ? function can be well approximated by ?(K) for K
defined by (4). In the spirit of approximation algorithms we define:
Definition 3.1. A graph G is a SVM ? ? graph if ?(K) ? (1 + O(1))?(G) where K is a LS labelling.
Such classes of graphs are interesting because on them, one can approximate the Lov?asz ? function
by solving an SVM, instead of an SDP, which in turn can be extremely useful in the design and analysis of approximation algorithms. We will demosntrate two examples of SVM ? ? graphs namely
(a.) the Erd?os?Renyi random graph G(n, 1/2) and (b.) a planted variation. Here the relaxation
?(K) could be used in place of ?(G), resulting in algorithms with the same quality guarantees but
with faster running time ? in particular, this will allow the algorithms to be scaled to large graphs.
The classical Erd?os-Renyi random graph G(n, 1/2) has n vertices and each edge (i, j) is present
independently with probability 1/2. We list a few facts about G(n, 1/2) that will be used repeatedly.
Fact 3.1. For G(n, 1/2),
?
? With probability 1 ? O(1/n), the degree of each vertex is in the range n/2 ? O( n log n).
c

? With probability 1 ? e?n for some c >?0, the
? maximum eigenvalue is n/2 ? o(n) and the
minimum eigenvalue is in the range [? n, n] [9].
?
Theorem 3.1. Let  > 2 ? 1. For G = G(n, 1/2)
? , with probability 1 ? O(1/n), ?(K) ?
?
(1 + )?(G) where K is defined in (4) with ? = 1+
n.
2
?
Proof. We begin by considering the case for ? = (1 + 2? ) n. By Fact 3.1 for all choices of ? > 0,
the minimum eigenvalue of ?1 A + I is, almost surely, greater than 0 which implies that f (?, K)
(see (1)) is strongly concave. For such functions KKT conditions are neccessary and sufficient for
optimality. The KKT conditions for a G(n, 21 ) are given by the following equation
?i +

1 X
Ai,j ?j = 1 + ?i , ?i ?i = 0, ?i ? 0
?

(5)

(i,j)?E

As A is random we begin by analyzing the case for expectation of A. Let E(A) = 12 (ee> ? I), be
? = E(A) + I is positive definite. More
the expectation of A. For the given choice of ?, the matrix K
?
? is again strongly concave and attains maximum at a KKT point. By direct
importantly f (?, K)
? where ?? = 2? satisfies
verification ?
? = ?e
n?1+2?

1
? + E(A)? = e.
?
4

(6)

Thus ?
? is the KKT point for the problem,
? =
f? = max f (?, K)
??0

n
X

?
???
?

i=1

>




E(A)
+I ?
? = n??
?

(7)

?
with the optimal objective function value f?. By choice of ? = (1 + 2? ) n we can write ?? =
2?/n + O(1/n). Using the fact about degrees of vertices in G(n, 1/2), we know that
p
n?1
a>
+ ?i with |?i | ? n log n
(8)
i e=
2
where a>
i is the ith row of the adjacency matrix A. As a consequence we note that




X
 ??

1
?
Aij ?
? j ? 1 = ?i
 ?i + ?
?


j

(9)

Recalling the definition of f and using the above equation along with (8) gives
|f (?
?; K) ? f?| ? n

??2 p
n log n
?

(10)

?
As noted before the function f (?; K) is strongly concave with ?2? f (?; K)  ? 2+?
I for all feasible
?. Recalling a useful result from convex optimization, see Lemma 3.1, we obtain


1
?(K) ? f (?
?; K) ? 1 +
k?f (?
?; K)k2
(11)
?

Observing that ?f (?; K) = 2(e ? ? ? A
? ?) and using the relation between k ? k? and 2 norm along
?
??
with (9) and (8) gives k?f (?
?; K)k ? nk?f (?
? ; K)k? = 2n ?? log n. Plugging this estimate in
?
(11) and using equation (10) we obtain ?(K) ? f? + O(log n) = (2 + ?) n + O(log n) The
second
? ?
equality follows by plugging in the value of ?? in (7). It is well known [6] that ?(G) = 2 n for
?
? ?(G) + o( n) and the theorem
G(n, 12 ) with high probability. One concludes that ?(K) ? 2+?
2
follows by choice of ?.
Discussion: Theorem 3.1 establishes that instead of SDP one can solve an SVM to evaluate
? function on G(n, 1/2). Although it is well known that ALPHA(G(n, 1/2)) = 2 log n whp,
there is no known polynomial time algorithm for computing the maximum independent set. [6]
gives an approximation algorithm that finds an independent set in G(n, p) which runs in expected
polynomial time, via a computation of ?(G(n, p)),which also applies to p = 1/2. The ? function
also serves as a guarantee of the approximation which other algorithms a simple Greedy algorithm
cannot give. Theorem 3.1 allows us to obtain similar guarantees but without the computational overhead of solving an SDP. Apart from finding independent sets computing ?(G(n, p)) is also used as
a subroutine in colorability [6], and here again one can use the SVM based approach to approximate
the ? function.
Similar arguments show also that other families of graphs such as the 11 families of pseudo?random
graphs described in [17] are SVM ? ? graphs.
Lemma 3.1. [4] A function g : C ? Rd ? R is said to be strongly concave over S if there
exists t > 0 such that ?2 g(x)  ?tI ? x ? C. For such functions one can show that if p? =
maxx?C g(x) < ? then
1
?x ? C p? ? g(x) ? k?g(x)k2
2t

4

Dense common subgraph detection

The problem of finding a large dense subgraph in multiple graphs has many applications [23, 22,
18]. We introduce the notion of common orthogonal labelling, and show that it is indeed possible
to recover dense regions in large graphs by solving a MKL problem. This constitutes significant
progress with respect to state of the art enumerative methods [14].
5

Problem definition Let G = {G(1) , . . . , G(M ) } be a set of simple, undirected graphs G(m) =
(V, E (m) ) defined on vertex set V = {1, . . . , n}. Find a common subgraph which is dense in all the
graphs.
Most algorithms which attempts the problem of finding a dense region are enumerative in nature and
hence do not scale well to finding large cliques. [14], first studied a related problem of finding all
possible common subgraphs for a given choice of parameters {? (1) , . . . , ? (M ) } which is atleast ?i
dense in G(i) . In the worst case, the algorithm performs depth first search over space of nnT possible

cliques of size nT . This has ?( nnT ) space and time complexity, which makes it impractical for
moderately large nT . For example, finding quasicliques of size 60 requires 8 hours (see Section 6).
In the remainder of this section, we focus on finding a large common sparse subgraph in a given
collection of graphs; with the observation that this is equivalent to finding a large common dense
subgraph in the set of complement graphs. To this end we introduce the following definition
Definition 4.1. Given simple unweighted graphs, G(m) = (V, E (m) ) m = 1, . . . , M on a common
vertex set V with |V | = n, the common orthogonal labelling on all the labellings is given by
ui ? S d?1 such that u>
/ E (m) ? m = 1, . . . , M }.1
i uj = 0 if (i, j) ?
Following the arguments of Section 2 it is immediate that size of the largest common independent
set is upper bounded by minK?L ?(K) where L = {K ? S+
: Kii = 1 ?i ? [n], Kij =
n
0 whenever (i, j) ?
/ E (m) ? m = 1, . . . , M }. We wish to exploit this fact in identifying large
common sparse regions in general graphs. Unfortunately this problem is a SDP and will not scale
well to large graphs. Taking cue from MKL literature we pose a restricted version of the problem
namely
min
?(K)
(12)
P
P
K=

(m)

M
m=1

?m K(m) , ?m ?0

M
m=1

?m =1

(m)

where K
is an orthogonal labelling of G . Direct verification shows that any feasible K is
also a common orthogonal labelling. Using the fact that ?x ? RM minpm ?0,PM
p> x =
m=1 pm =1
minm xm = max{t|xm ? t ? m = 1, . . . , M } one can recast the optimization problem in (12) as
follows
max t s.t. f (?; K(m) ) ? t ? m = 1, . . . , M
(13)
t?R,?i ?0

(m)

where K
is the LS labelling for G(m) , ?m = 1, . . . , M . The above optimization can be readily
solved by state of the art MKL solvers. This result allows us to build a parameter free common
sparse subgraph (CSS) algorithm shown in Figure 1 having following advantages: it provides a
theoretical bound on subgraph density (Claim 4.1 below); and, it requires no parameters from the
user beyond the set of graphs G(1) , . . . , G(M ) .
Let ?? be the optimal solution in (13); and SV = {i : ?i? > 0} and S1 =
{i : ?i? = 1} with
P
(m)

cardinalities nsv = |SV | and n1 = |S1 | respectively. Let ?
? min,S = mini?S

(m)
j?Ni (G
)
S
(m)
di (GS )

??
j

denote

(m)
Ni (GS )

the average of the support vector coefficients in the neighbourhood
of vertex i in induced
(m)
(m)
(m)
subgraph GS having degree di (GS ) = |Ni (GS )|. We define
(
)
(m)
(1
?
c)?
(m)
T (m) = i ? SV : di (GSV ) <
where c = min ?i?
(14)
(m)
i?SV
?
? min,SV
Claim 4.1. Let T ? V be computed as in Al(m)
gorithm 1. The subgraph GT induced by T ,
in graph G(m) , has density at most ? (m) where
(1?c)?(m)
? (m) = ?? min,SV
(nT ?1)

?? = Use MKL solvers to solve eqn. (13)
T = ?m T (m) {eqn. (14)}
Return T

Figure 1: Algorithm for finding common sparse
Pn
subgraph: T = CSS(G(1) , . . . , G(M ) )
?
Proof. (Sketch) At optimality, t =
?
.
i=1 i
P
P
P
(m) ?
?
?
This allows us to write 0 ?
i?S ?i (2 ? ?i ?
j6=i Kij ?j ) ? t as 0 ?
i?T (1 ? c ?
(m)

di (GT ) (m)
nT
?
? min,SV ) Dividing by 2 completes the proof.
?(m)
1

This is equivalent to defining an orthogonal labelling on the Union graph of G(1) , . . . , G(M )

6

5

Finding Planted Cliques in G(n, 1/2) graphs

Finding large cliques or independent sets is a computationally difficult problem even in random
graphs. While it is known that the size of the largest clique or independent set in G(n, 1/2) is 2 log n
with high probability, there is no known efficient algorithm to find a clique of size significantly larger
than log n - even a cryptographic application was suggested based on this (see the discussion and
references in the introduction of [8]).
Hidden planted clique A random graph G(n, 1/2) is chosen first and then a clique of size k is
introduced in the first 1, . . . , k vertices. The problem is to identify the clique.
?
[8] showed that if k = ?( n), then the hidden clique can be discovered in polynomial time by computing the Lov?asz ? function. There are other approaches [2, 7, 24] which do not require computing
the ? function.
We consider the (equivalent) complement model G(n, 1/2,?k) where a independent set is planted on
? 1/2, k) is a SVM ? ? graph.
the set of k vertices. We show that in the regime k = ?( n), G(n,
We will further demonstrate that as a consequence one can identify the hidden independent set with
high probability by solving an SVM. The following is the main result of the section.
?
?
Theorem 5.1.
? For G = G(n, 1/2, k) and k = 2t n for large enough constant t ? 1 with K as in
(4) and ? = n + k/2,


?
1
?(K) = 2(t + 1) n + O(log n) = 1 + + o(1) ?(G)
t
with probability at least 1 ? O(1/n).
?
Proof. The proof is analogous to that of Theorem 3.1. Note that |?n (G)| ? n + k/2. First we
consider the expected case where all vertices outside the planted part S are adjacent to k/2 vertices
in S and (n ? k)/2 vertices outside
part
? have degree (n ? k)/2.
? S. and all verties in the planted
n for i 6? S and ?i = 2(t + 1)2 / n for i ? S satisfy KKT
We check that ?i = 2(t + 1)/ ?
conditions with an error of O(1/ n). Now apply p
Chernoff bounds to conclude that with high
probability, all vertices in S have degree (n ? k)/2 ? (n ? k)
p log(n ? k) and those outside S are
?
adjacent to k/2 ? k log k vertices in S and to (n ? k)/2 ? (n ? k) log(n ? k) vertices ouside
?
S. Nowwe check
 that the same solution satisfies KKT conditions of G(n, 1/2, k) with an error of
q
log n
 = O
. Using the same arguments as in the proof of Theorem 3.1, we conclude that
n
?
?
?(K) ? 2(t + 1) n + O(log n). Since ?(G) = 2t n for this case [8], the result follows.
The above theorem suggests that the planted independent set can be recovered by taking the top k
values in the optimal solution. In the experimental section we will discuss the performance of this
recovery algorithm. The runtime of this algorithm is one call to SVM solver, which is considerably
cheaper than the SDP option. Indeed the algorithm due to [8], requires computation of ? function.
The current best known algorithm for ? computation has an O(n5 log n)[5], run time complexity. In
contrast the proposed approach needs to solve an SVM and hence scales well to large graphs. Our
approach is competitive with the state of the art [24] as it gives the same high probability guarantees
and have the same running time, O(n2 ). Here we have assumed that we are working with a SVM
solver which has a time complexity of O(n2 ) [13].

6

Experimental evaluation

Comparison with exhaustive approach [14] We generate synthetic m = 3 random graphs over
?
n vertices with average density ? = 0.2, and having single (common) quasi-clique of size k = 2 n
with density ? = 0.95 in all the three graphs. This is similar to the synthetic graphs generated
in the original paper [see 14, Section 6.1.2]. We note that both our MKL-based approach and
exhaustive search in [14] recovers the quasi-clique. However, the time requirements are drastically
different. All experiments were conducted on a computer with 16 GB RAM and Intel X3470 quadcore processor running at 2.93 GHz. Three values of k namely k = 50, 60 and k = 100 were used.
It is interesting to note that CROCHET [14] took 2 hours and 9 hours for k = 50 and k = 60 sized
cliques and failed to find a clique of size of 100. The corresponding numbers for MKL are 47.5,54.8
and 137.6 seconds respectively.
7

Common dense subgraph detection We evaluate our algorithm for finding large dense regions on
the DIMACS Challenge graphs 2 [15], which is a comprehensive benchmark for testing of clique
finding and related algorithms. For the families of dense graphs (brock, san, sanr), we focus on
finding large dense region in the complement of the original graphs.
We run Algorithm 1 using SimpleMkl3 to find large common dense subgraph. In order to evaluate the performance of our algorithm, we compute a
? = maxm a(m) and a = minm a(m) where
(m)
(m)
(m)
a
= ?(GT )/?(G ) is relative density of induced subgraph (compared to original graph density); and nT /N is relative size of induced subgraph compared to original graph size. We want
a high value of nT /N ; while a should not be lower than 1. Table 1 shows evaluation of Algorithm 1 on DIMACS dataset. We note that our algorithm finds a large subgraph (large nT /N )
with higher density compared to original graph in all of DIMACS graph classes making it suitable for finding large dense regions in multiple graphs. In all cases the size of the subgraph, nT
was more than 100. The MKL experiments reported in Table 1 took less than 1 minute (for each
graph family); while the algorithm in [14] aborts after several hours due to memory constraints.

Planted clique recovery We generate 100
random graphs based on planted clique
model G(n, 1/2, k) where
? n = 30000 and
hidden clique size k = 2t n for each choice
of t. We evaluate the recovery algorithm
discussed in Section 4.2. The SVM problem is solved using Libsvm4 . For t ? 2 we
find perfect recovery of the clique on all the
graphs, which is agreement with Theorem
5.1.
It is worth noting that the approach takes 10
minutes to recover the clique in this graph of
30000 vertices which is far beyond the scope
of SDP based procedures.

Graph family
c-fat200
c-fat500
brock200?
brock400?
brock800?
p hat300
p hat500
p hat700
p hat1000
p hat1500
san200?
san400?
sanr200?
sanr400?

N
200
500
200
400
800
300
500
700
1000
1500
200
400
200
400

M
3
4
4
4
4
3
3
3
3
3
5
3
2
2

nT
N

0.50
0.31
0.41
0.50
0.50
0.53
0.48
0.45
0.43
0.38
0.50
0.42
0.39
0.43

a
?
2.12
3.57
1.36
1.15
1.08
1.53
1.55
1.58
1.60
1.63
1.51
1.19
1.86
1.20

a
0.99
1.01
0.99
1.05
1.01
1.15
1.17
1.18
1.19
1.20
1.08
1.02
1.04
1.02

Table 1: Common dense subgraph recovery on multiple graphs in DIMACS dataset. Here a
? and a denote
the maximum and minimum relative density of the
In this paper we have established that the induced subgraph (relative to density of the original
Lov?asz ? function, well studied in graph the- graph) and nT /N is the relative size of the induced
ory can be linked to the one-class SVM for- subgraph compared to original graph size.
mulation. This link allows us to design scalable algorithms for computationally difficult
problems. In particular we have demonstrated that finding a common dense region
in multiple graphs can be solved by a MKL problem, while finding a large planted clique can be
solved by an one class SVM.

7

Conclusion

Acknowledgements
CB is grateful to Department of CSE, Chalmers University of Technology for their hospitality and
was supported by grants from ICT and Transport Areas of Advance, Chalmers University. VJ and
DD were supported by SSF grant Data Driven Secure Business Intelligence.

2

ftp://dimacs.rutgers.edu/pub/challenge/graph/benchmarks/clique/
http://asi.insa-rouen.fr/enseignants/?arakotom/code/mklindex.html
4
http://www.csie.ntu.edu.tw/?cjlin/libsvm/

3

8

References
[1] Louigi Addario-berry, Nicolas Broutin, Gbor Lugosi, and Luc Devroye. Combinatorial testing
problems. Annals of Statistics, 38:3063?3092, 2010.
[2] Noga Alon, Michael Krivelevich, and Benny Sudakov. Finding a large hidden clique in a
random graph. Random Structures and Algorithms, pages 457?466, 1998.
[3] B. Bollob?as. Modern graph theory, volume 184. Springer Verlag, 1998.
[4] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press,
New York, NY, USA, 2004.
[5] T.-H. Hubert Chan, Kevin L. Chang, and Rajiv Raman. An sdp primal-dual algorithm for
approximating the lov?asz-theta function. In ISIT, 2009.
[6] Amin Coja-Oghlan and Anusch Taraz. Exact and approximative algorithms for coloring g(n,
p). Random Struct. Algorithms, 24(3):259?278, 2004.
[7] U. Feige and D. Ron. Finding hidden cliques in linear time. In AofA10, 2010.
[8] Uriel Feige and Robert Krauthgamer. Finding and certifying a large hidden clique in a semirandom graph. Random Struct. Algorithms, 16:195?208, March 2000.
[9] Z. F?uredi and J. Koml?os. The eigenvalues of random symmetric matrices. Combinatorica,
1:233?241, 1981.
[10] Michel X. Goemans. Semidefinite programming in combinatorial optimization. Math. Program., 79:143?161, 1997.
[11] J. H?astad. Clique is hard to approximate within n1?? . Acta Mathematica, 182(1):105?142,
1999.
[12] Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 1990.
[13] Don R. Hush, Patrick Kelly, Clint Scovel, and Ingo Steinwart. Qp algorithms with guaranteed
accuracy and run time for support vector machines. Journal of Machine Learning Research,
7:733?769, 2006.
[14] D. Jiang and J. Pei. Mining frequent cross-graph quasi-cliques. ACM Transactions on Knowledge Discovery from Data (TKDD), 2(4):16, 2009.
[15] D.S. Johnson and M.A. Trick. Cliques, coloring, and satisfiability: second DIMACS implementation challenge, October 11-13, 1993, volume 26. Amer Mathematical Society, 1996.
[16] Donald Knuth. The sandwich theorem. Electronic Journal of Combinatorics, 1(A1), 1994.
[17] Michael Krivelevich and Benny Sudakov. Pseudo-random graphs. In More Sets, Graphs and
Numbers, volume 15 of Bolyai Society Mathematical Studies, pages 199?262. Springer Berlin
Heidelberg, 2006.
[18] V.E. Lee, N. Ruan, R. Jin, and C. Aggarwal. A survey of algorithms for dense subgraph
discovery. Managing and Mining Graph Data, pages 303?336, 2010.
[19] L. Lovasz. On the Shannon capacity of a graph. Information Theory, IEEE Transactions on,
25(1):1?7, 1979.
[20] C.J. Luz and A. Schrijver. A convex quadratic characterization of the lov?asz theta number.
SIAM Journal on Discrete Mathematics, 19(2):382?387, 2006.
[21] Claire Mathieu and Warren Schudy. Correlation clustering with noisy input. In Proceedings
of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ?10, pages
712?728, Philadelphia, PA, USA, 2010. Society for Industrial and Applied Mathematics.
[22] P. Pardalos and S. Rebennack. Computational challenges with cliques, quasi-cliques and clique
partitions in graphs. Experimental Algorithms, pages 13?22, 2010.
[23] V. Spirin and L.A. Mirny. Protein complexes and functional modules in molecular networks.
Proceedings of the National Academy of Sciences, 100(21):12123, 2003.
[24] Dekel Yael, Gurel-Gurevich Ori, and Peres Yuval. Finding hidden cliques in linear time with
high probability. In ANALCO11, 2011.

9

"
5306,"Learning Bayesian Networks with Thousands of
Variables
Mauro Scanagatta
IDSIA? , SUPSI? , USI?
Lugano, Switzerland
mauro@idsia.ch

Cassio P. de Campos
Queen?s University Belfast
Northern Ireland, UK
c.decampos@qub.ac.uk

Giorgio Corani
IDSIA? , SUPSI? , USI?
Lugano, Switzerland
giorgio@idsia.ch

Marco Zaffalon
IDSIA?
Lugano, Switzerland
zaffalon@idsia.ch

Abstract
We present a method for learning Bayesian networks from data sets containing
thousands of variables without the need for structure constraints. Our approach
is made of two parts. The first is a novel algorithm that effectively explores the
space of possible parent sets of a node. It guides the exploration towards the
most promising parent sets on the basis of an approximated score function that
is computed in constant time. The second part is an improvement of an existing
ordering-based algorithm for structure optimization. The new algorithm provably
achieves a higher score compared to its original formulation. Our novel approach
consistently outperforms the state of the art on very large data sets.

1

Introduction

Learning the structure of a Bayesian network from data is NP-hard [2]. We focus on score-based
learning, namely finding the structure which maximizes a score that depends on the data [9]. Several
exact algorithms have been developed based on dynamic programming [12, 17], branch and bound
[7], linear and integer programming [4, 10], shortest-path heuristic [19, 20].
Usually structural learning is accomplished in two steps: parent set identification and structure
optimization. Parent set identification produces a list of suitable candidate parent sets for each
variable. Structure optimization assigns a parent set to each node, maximizing the score of the
resulting structure without introducing cycles.
The problem of parent set identification is unlikely to admit a polynomial-time algorithm with a
good quality guarantee [11]. This motivates the development of effective search heuristics. Usually
however one decides the maximum in-degree (number of parents per node) k and then simply computes the score of all parent sets. At that point one performs structural optimization. An exception is
the greedy search of the K2 algorithm [3], which has however been superseded by the more modern
approaches mentioned above.
A higher in-degree implies a larger search space and allows achieving a higher score; however it
also requires higher computational time. When choosing the in-degree the user makes a trade-off
between these two objectives. However when the number of variables is large, the in-degree is
?

Istituto Dalle Molle di studi sull?Intelligenza Artificiale (IDSIA)
Scuola universitaria professionale della Svizzera italiana (SUPSI)
?
Universit`a della Svizzera italiana (USI)
?

1

generally set to a small value, to allow the optimization to be feasible. The largest data set analyzed
in [1] with the Gobnilp1 software contains 413 variables; it is analyzed setting k = 2. In [5] Gobnilp
is used for structural learning with 1614 variables, setting k = 2. These are among the largest
examples of score-based structural learning in the literature.
In this paper we propose an algorithm that performs approximated structure learning with thousands
of variables without constraints on the in-degree. It is constituted by a novel approach for parent set
identification and a novel approach for structure optimization.
As for parent set identification we propose an anytime algorithm that effectively explores the space
of possible parent sets. It guides the exploration towards the most promising parent sets, exploiting
an approximated score function that is computed in constant time. As for structure optimization,
we extend the ordering-based algorithm of [18], which provides an effective approach for model
selection with reduced computational cost. Our algorithm is guaranteed to find a solution better than
or equal to that of [18].
We test our approach on data sets containing up to ten thousand variables. As a performance indicator we consider the score of the network found. Our parent set identification approach outperforms
consistently the usual approach of setting the maximum in-degree and then computing the score of
all parent sets. Our structure optimization approach outperforms Gobnilp when learning with more
than 500 nodes. All the software and data sets used in the experiments are available online. 2 .

2

Structure Learning of Bayesian Networks

Consider the problem of learning the structure of a Bayesian Network from a complete data set of N
instances D = {D1 , ..., DN }. The set of n categorical random variables is X = {X1 , ..., Xn }. The
goal is to find the best DAG G = (V, E), where V is the collection of nodes and E is the collection
of arcs. E can be defined as the set of parents ?1 , ..., ?n of each variable. Different scores can be
used to assess the fit of a DAG. We adopt the BIC, which asymptotically approximates the posterior
probability of the DAG. The BIC score is decomposable, namely it is constituted by the sum of the
scores of the individual variables:
BIC(G) =
Xn
Xn
=
BIC(Xi , ?i ) =
i=1

i=1

X

X
??|?i |

x?|Xi |

log N
Nx,? log ??x|? ?
(|Xi | ? 1)(|?i |) ,
2

where ??x|? is the maximum likelihood estimate of the conditional probability P (Xi = x|?i = ?),
and Nx,? represents the number of times (X = x ? ?i = ?) appears in the data set, and | ? | indicates
the size of the Cartesian product space of the variables given as arguments (instead of the number of
variables) such that |Xi | is the number of states of Xi and |?| = 1.
Exploiting decomposability, we first identify independently for each variable a list of candidate
parent sets (parent set identification). Then by structure optimization we select for each node the
parent set that yields the highest score without introducing cycles.

3

Parent set identification

For parent set identification usually one explores all the possible parent sets, whose number however
increases as O(nk ), where k denotes the maximum in-degree. Pruning rules [7] do not considerably
reduce the size of this space.
Usually the parent sets are explored in sequential order: first all the parent size of size one, then all
the parent sets of size two, and so on, up to size k. We refer to this approach as sequential ordering.
If the solver adopted for structural optimization is exact, this strategy allows to find the globally
optimum graph given the chosen value of k. In order to deal with a large number of variables it
is however necessary setting a low in-degree k. For instance [1] adopts k=2 when dealing with
the largest data set (diabetes), which contains 413 variables. In [5] Gobnilp is used for structural
learning with 1614 variables, again setting k = 2. A higher value of k would make the structural
1
2

http://www.cs.york.ac.uk/aig/sw/gobnilp/
http://blip.idsia.ch

2

learning not feasible. Yet a low k implies dropping all the parent sets with size larger than k. Some
of them possibly have a high score.
In [18] it is proposed to adopt the subset ?corr of the most correlated variables with the children
variable. Then [18] consider only parent sets which are subsets of ?corr . However this approach
is not commonly adopted, possibly because it requires specifying the size of ?corr . Indeed [18]
acknowledges the need for further innovative approaches in order to effectively explore the space of
the parent sets.
We propose two anytime algorithms to address this problem. The first is the simplest; we call it
greedy selection. It starts by exploring all the parent sets of size one and adding them to a list. Then
it repeats the following until time is expired: pops the best scoring parent set ? from the list, explores
all the supersets obtained by adding one variable to ?, and adds them to the list. Note that in general
the parent sets chosen at two adjoining step are not related to each other. The second approach
(independence selection) adopts a more sophisticated strategy, as explained in the following.
3.1

Parent set identification by independence selection

Independence selection uses an approximation of the actual BIC score of a parent set ?, which we
denote as BIC? , to guide the exploration of the space of the parent sets. The BIC? of a parent set
constituted by the union of two non-empty parent sets ?1 and ?2 is defined as follows:
BIC? (X, ?1 , ?2 ) = BIC(X, ?1 ) + BIC(X, ?2 ) + inter(X, ?1 , ?2 ) ,
(1)
with ?1 ??2 = ? and inter(X, ?1 , ?2 ) = log2 N (|X|?1)(|?1 |+|?2 |?|?1 ||?2 |?1)?BIC(X, ?).
If we already know BIC(X, ?1 ) and BIC(X, ?2 ) from previous calculations (and we know
BIC(X, ?)), then BIC? can be computed in constant time (with respect to data accesses). We
thus exploit BIC? to quickly estimate the score of a large number of candidate parent sets and to
decide the order to explore them.
We provide a bound for the difference between BIC? (X, ?1 , ?2 ) and BIC(X, ?1 ? ?2 ). To this
end, we denote by ii the Interaction Information [14]: ii(X; Y ; Z) = I(X; Y |Z) ? I(X; Y ), namely
the difference between the mutual information of X and Y conditional on Z and the unconditional
mutual information of X and Y .
Theorem 1. Let X be a node of G and ? = ?1 ? ?2 be a parent set for X with ?1 ? ?2 = ?
and ?1 , ?2 non-empty. Then BIC(X, ?) = BIC? (X, ?1 , ?2 ) + N ? ii(?1 ; ?2 ; X), where ii is the
Interaction Information estimated from data.
Proof. BIC(X, ?1 ? ?2 ) ? BIC? (X, ?1 , ?2 ) =
BIC(X, ?1 ? ?2 ) ? BIC(X, ?1 ) ? BIC(X, ?2 ) ? inter(X, ?1 , ?2 ) =
i X
h
X
Nx log ??x =
Nx,?1 ,?2 log ??x|?1 ,?2 ? log(??x|?1 ??x|?2 ) +
x,?1 ,?2
x
!#
""
X
??x|?1 ??x|?2
?
Nx,?1 ,?2 log ?x|?1 ,?2 ? log
=
x,?1 ,?2
??x
!
!
?? ,? |x ??? ???
X
X
??x|?1 ,?2 ??x
?
1
2
1
2
=
Nx,?1 ,?2 log
=
N ? ??x,?1 ,?2 log
x,?1 ,?2
x,?1 ,?2
??x|?1 ??x|?2
???1 |x ???2 |x ???1 ,?2
!
!!
X
X
???1 ,?2 |x
???1 ,?2
?
?
N
?x,?1 ,?2 log
?
??1 ,?2 log
=
x,?1 ,?2
?1 ,?2
???1 |x ???2 |x
???1 ???2
N ? (I(?1 ; ?2 |X) ? I(?1 ; ?2 )) = N ? ii(?1 ; ?2 ; X) ,
where I(?) denotes the (conditional) mutual information estimated from data.
Corollary 1. Let X be a node of G, and ? = ?1 ? ?2 be a parent set of X such that ?1 ? ?2 = ?
and ?1 , ?2 non-empty. Then
|BIC(X, ?) ? BIC? (X, ?1 , ?2 )| ? N min{H(X), H(?1 ), H(?2 )} .
Proof. Theorem 1 states that BIC(X, ?) = BIC? (X, ?1 , ?2 ) + N ? ii(?1 ; ?2 ; X). We now devise
bounds for interaction information, recalling that mutual information and conditional mutual information are always non-negative and achieve their maximum value at the smallest entropy H of their
3

argument: ?H(?2 ) ? ?I(?1 ; ?2 ) ? ii(?1 ; ?2 ; X) ? I(?1 ; ?2 |X) ? H(?2 ). The theorem is
proven by simply permuting the values ?1 ; ?2 ; X in the ii of such equation. Since
ii(?1 ; ?2 ; X) = I(?1 ; ?2 |X)?I(?1 ; ?2 ) = I(X; ?1 |?2 )?I(X; ?1 ) = I(?2 ; X|?1 )?I(?2 ; X) ,
the bounds for ii are valid.
We know that 0 ? H(?) ? log(|?|) for any set of nodes ?, hence the result of Corollary 1 could
be further manipulated to achieve a bound for the difference between BIC and BIC? of at most
N log(min{|X|, |?1 |, |?2 |}). However, Corollary 1 is stronger and can still be computed efficiently
as follows. When computing BIC? (X, ?1 , ?2 ), we assumed that BIC(X, ?1 ) and BIC(X, ?2 ) had
been precomputed. As such, we can also have precomputed the values H(?1 ) and H(?2 ) at the
same time as the BIC scores were computed, without any significant increase of complexity (when
computing BIC(X, ?) for a given ?, just use the same loop over the data to compute H(?)).
Corollary 2. Let X be a node of G, and ? = ?1 ??2 be a parent set for that node with ?1 ??2 = ?
and ?1 , ?2 non-empty. If ?1 ?
? ?2 , then BIC(X, ?1 ? ?2 ) ? BIC? (X, ?1 ? ?2 ). If ?1 ?
? ?2 |X,
then BIC(X, ?1 ? ?2 ) ? BIC? (X, ?1 ? ?2 ). If the interaction information ii(?1 ; ?2 ; X) = 0,
then BIC(X, ?1 ? ?2 ) = BIC? (X, ?1 , ?2 ).
Proof. It follows from Theorem 1 considering that mutual information I(?1 , ?2 ) = 0 if ?1 and ?2
are independent, while I(?1 , ?2 |X) = 0 if ?1 and ?2 are conditionally independent.
We now devise a novel pruning strategy for BIC based on the bounds of Corollaries 1 and 2.
Theorem 2. Let X be a node of G, and ? = ?1 ? ?2 be a parent set for that node with ?1 ?
?2 = ? and ?1 , ?2 non-empty. Let ?0 ? ?. If BIC? (X, ?1 , ?2 ) + log2 N (|X| ? 1)|?0 | >
N min{H(X), H(?1 ), H(?2 )}, then ?0 and its supersets are not optimal and can be ignored.
Proof. BIC? (X, ?1 , ?2 ) ? N min{H(X), H(?1 ), H(?2 )} + log2 N (|X| ? 1)|?0 | > 0 implies
BIC(?) + log2 N (|X| ? 1)|?0 | > 0, and Theorem 4 of [6] prunes ?0 and all its supersets.
Thus we can efficiently check whether large parts of the search space can be discarded based on
these results. We note that Corollary 1 and hence Theorem 2 are very generic in the choice of ?1
and ?2 , even though usually one of them is taken as a singleton.
3.2

Independence selection algorithm

We now describe the algorithm that exploits the BIC? score in order to effectively explore the space
of the parent sets. It uses two lists: (1) open: a list for the parent sets to be explored, ordered by their
BIC? score; (2) closed: a list of already explored parent sets, along with their actual BIC score.
The algorithm starts with the BIC of the empty set computed. First it explores all the parent sets of
size one and saves their BIC score in the closed list. Then it adds to the open list every parent set of
size two, computing their BIC? scores in constant time on the basis of the scores available from the
closed list. It then proceeds as follows until all elements in open have been processed, or the time is
expired. It extracts from open the parent set ? with the best BIC? score; it computes its BIC score
and adds it to the closed list. It then looks for all the possible expansions of ? obtained by adding
a single variable Y , such that ? ? Y is not present in open or closed. It adds them to open with
their BIC? (X, ?, Y ) scores. Eventually it also considers all the explored subsets of ?. It safely [7]
prunes ? if any of its subsets yields a higher BIC score than ?. The algorithm returns the content of
the closed list, pruned and ordered by the BIC score. Such list becomes the content of the so-called
cache of scores for X. The procedure is repeated for every variable and can be easily parallelized.
Figure 1 compares sequential ordering and independence selection. It shows that independence
selection is more effective than sequential ordering because it biases the search towards the highestscoring parent sets.

4

Structure optimization

The goal of structure optimization is to choose the overall highest scoring parent sets (measured
by the sum of the local scores) without introducing directed cycles in the graph. We start from the
approach proposed in [18] (which we call ordering-based search or OBS), which exploits the fact
4

?1,400

?1,600

?1,600

BIC

BIC

?1,400
?1,800
?2,000

?1,800
?2,000

500

1,000

500

Iteration

1,000

Iteration

(a) Sequential ordering.

(b) Indep. selection ordering.

Figure 1: Exploration of the parent sets space for a given variable performed by sequential ordering
and independence selection. Each point refers to a distinct parent set.
Pn
that the optimal network can be found in time O(Ck), where C = i=1 ci and ci is the number of
elements in the cache of scores of Xi , if an ordering over the variables is given.3 ?(k) is needed to
check whether all the variables in a parent set for X come before X in the ordering (a simple array
can be used as data structure for this checking). This implies working on the search space of the
possible orderings, which is convenient as it is smaller than the space of network structures. Multiple
orderings are sampled and evaluated (different techniques can be used for guiding the sampling). For
each sampled total ordering ? over variables X1 , . . . , Xn , the network is consistent with the order
if ?Xi : ?X ? ?i : X ? Xi . A network consistent with a given ordering automatically satisfies
the acyclicity constraint. This allows us to choose independently the best parent set of each node.
Moreover, for a given total ordering V1 , . . . , Vn of the variables, the algorithm tries to improve the
network by a greedy search swapping procedure: if there is a pair Vj , Vj+1 such that the swapped
ordering with Vj in place of Vj+1 (and vice versa) yields better score for the network, then these
nodes are swapped and the search continues. One advantage of this swapping over extra random
orderings is that searching for it and updating the network (if a good swap is found) only takes time
O((cj + cj+1 ) ? kn) (which can be sped up as cj only is inspected for parents sets containing Vj+1 ,
and cj+1 is only processed if Vj+1 has Vj as parent in the current network), while a new sampled
ordering would take O(n + Ck) (the swapping approach is usually favourable if ci is ?(n), which is
a plausible assumption). We emphasize that the use of k here is sole with the purpose of analyzing
the complexity of the methods, since our parent set identification approach does not rely on a fixed
value for k.
However, the consistency rule of OBS is quite restricting. While it surely refuses all cyclic structures,
it also rules out some acyclic ones which could be captured by interpreting the ordering in a slightly
different manner. We propose a novel consistency rule for a given ordering which processes the
nodes in V1 , . . . , Vn from Vn to V1 (OBS can do it in any order, as the local parent sets can be
chosen independently) and we define the parent set of Vj such that it does not introduce a cycle in
the current partial network. This allows back-arcs in the ordering from a node Vj to its successors, as
long as this does not introduce a cycle. We call this idea acyclic selection OBS (or simply ASOBS).
Because we need to check for cycles at each step of constructing the network for a given ordering, at
a first glance the algorithm seems to be slower (time complexity of O(Cn) against O(Ck) for OBS;
note this difference is only relevant as we intend to work with large values n). Surprisingly, we can
implement it in the same overall time complexity of O(Ck) as follows.
1. Build and keep a Boolean square matrix m to mark which are the descendants of nodes
(m(X, Y ) tells whether Y is descendant of X). Start it all false.
2. For each node Vj in the order, with j = n, . . . , 1:
(a) Go through the parent sets and pick the best scoring one for which all contained parents are not descendants of Vj (this takes time O(ci k) if parent sets are kept as lists).
(b) Build a todo list with the descendants of Vj from the matrix representation and associate an empty todo list to all ancestors of Vj .
(c) Start the todo lists of the parents of Vj with the descendants of Vj .
(d) For each ancestor X of Vj (ancestors will be iteratively visited by following a depthfirst graph search procedure using the network built so far; we process a node after
3

O(?), ?(?) and ?(?) shall be understood as usual asymptotic notation functions.

5

its children with non-empty todo lists have been already processed; the search stops
when all ancestors are visited):
i. For each element Y in the todo list of X, if m(X, Y ) is true, then ignore Y and
move on; otherwise set m(X, Y ) to true and add Y to the todo of parents of X.
Let us analyze the complexity of the method. Step 2a takes overall time O(Ck) (already considering
the outer loop). Step 2b takes overall time O(n2 ) (already considering the outer loop). Steps 2c
and 2(d)i will be analyzed based on the number of elements on the todo lists and the time to process
them in an amortized way. Note that the time complexity is directly related to the number of elements
that are processed from the todo lists (we can simply look to the moment that they leave a list, as
their inclusion in the lists will be in equal number). We will now count the number of times we
process an element from a todo list. This number is overall bounded (over all external loop cycles)
by the number of times we can make a cell of matrix m turn from false to true (which is O(n2 )) plus
the number of times we ignore an element because the matrix cell was already set to true (which is
at most O(n) per each Vj , as this is the maximum number of descendants of Vj and each of them
can fall into this category only once, so again there are O(n2 ) times in total). In other words, each
element being removed from a todo list is either ignored (matrix already set to true) or an entry
in the matrix of descendants is changed from false to true, and this can only happen O(n2 ) times.
Hence the total time complexity is O(Ck + n2 ), which is O(Ck) for any C greater than n2 /k (a
very plausible scenario, as each local cache of a variable usually has more than n/k elements).
Moreover, we have the following interesting properties of this new method.
Theorem 3. For a given ordering ?, the network obtained by ASOBS has score equal than or
greater to that obtained by OBS.
Proof. It follows immediately from the fact that the consistency rule of ASOBS generalizes that of
OBS, that is, for each node Vj with j = n, . . . , 1, ASOBS allows all parent sets allowed by OBS
and also others (containing back-arcs).
Theorem 4. For a given ordering ? defined by V1 , . . . , Vn and a current graph G consistent with
?, if OBS consistency rule allows the swapping of Vj , Vj+1 and leads to improving the score of G,
then the consistency rule of ASOBS allows the same swapping and achieves the same improvement
in score.
Proof. It follows immediately from the fact that the consistency rule of ASOBS generalizes that of
OBS, so from a given graph G, if a swapping is possible under OBS rules, then it is also possible
under ASOBS rules.

5

Experiments

We compare three different approaches for parent set identification (sequential, greedy selection and
independence selection) and three different approaches (Gobnilp, OBS and ASOBS) for structure
optimization. This yields nine different approaches for structural learning, obtained by combining
all the methods for parent set identification and structure optimization. Note that OBS has been
shown in [18] to outperform other greedy-tabu search over structures, such as greedy hill-climbing
and optimal-reinsertion-search methods [15].
We allow one minute per variable to each approach for parent set identification. We set the maximum
in-degree to k = 6, a high value that allows learning even complex structures. Notice that our novel
approach does not need a maximum in-degree. We set a maximum in-degree to put our approach
and its competitors on the same ground. Once computed the scores of the parent sets we run each
solver (Gobnilp, OBS, ASOBS) for 24 hours. For a given data set the computation is performed on
the same machine.
The explicit goal of each approach for both parent set identification and structure optimization is
to maximize the BIC score. We then measure the BIC score of the Bayesian networks eventually
obtained as performance indicator. The difference in the BIC score between two alternative networks
is an asymptotic approximation of the logarithm of the Bayes factor. The Bayes factor is the ratio
of the posterior probabilities of two competing models. Let us denote by ?BIC1,2 =BIC1 -BIC2 the
difference between the BIC score of network 1 and network 2. Positive values of ?BIC1,2 imply
6

Data set

n

Data set

n

Data set

n

Data set

n

Audio
Jester
Netflix
Accidents

100
100
100
111

Retail
Pumsb-star
DNA
Kosarek

135
163
180
190

MSWeb
Book
EachMovie
WebKB

294
500
500
839

Reuters-52
C20NG
BBC
Ad

889
910
1058
1556

Table 1: Data sets sorted according to the number n of variables.
evidence in favor of network 1. The evidence in favor of network 1 is respectively [16] {weak,
positive, strong, very strong} if ?BIC1,2 is between {0 and 2; 2 and 6; 6 and 10 ; beyond 10}.
5.1

Learning from datasets

We consider 16 data sets already used in the literature of structure learning, firstly introduced in [13]
and [8]. We randomly split each data set into three subsets of instances. This yields 48 data sets.
The approaches for parent set identification are compared in Table 2. For each fixed structure optimization approach, we learn the network starting from the list of parent sets computed by independence selection (IS), greedy selection (GS) and sequential selection (SQ). In turn we analyze
?BICIS,GS and ?BICIS,SQ . A positive ?BIC means that independence selection yields a network
with higher BIC score than the network obtained using an alternative approach for parent set identification; vice versa for negative values of ?BIC. In most cases (see Table 2) ?BIC>10, implying
very strong support for the network learned using independence selection. We further analyze the
results through a sign-test. The null hypothesis of the test is that the BIC score of the network
learned under independence selection is smaller than or equivalent to the BIC score of the network
learned using the alternative approach (greedy selection or sequential selection depending on the
case). If a data set yields a ?BIC which is {very negative, strongly negative, negative, neutral}, it
supports the null hypothesis. If a data sets yields a BIC score which is {positive, strongly positive,
extremely positive}, it supports the alternative hypothesis. Under any fixed structure solver, the sign
test rejects the null hypothesis, providing significant evidence in favor of independence selection.
In the following when we further cite the sign test we refer to same type of analysis: the sign test
analyzes the counts of the ?BIC which are in favor and against a given method.
As for structure optimization, ASOBS achieves higher BIC score than OBS in all the 48 data sets,
under every chosen approach for parent set identification. These results confirm the improvement of
ASOBS over OBS, theoretically proven in Section 4. In most cases the ?BIC in favor of ASOBS
is larger than 10. The difference in favor of ASOBS is significant (sign test, p < 0.01) under every
chosen approach for parent set identification.
We now compare ASOBS and Gobnilp. On the smaller data sets (27 data sets with n < 500),
Gobnilp significantly outperforms (sign test, p < 0.01) ASOBS under every chosen approach for
parent set identification. On most of such data sets, the ?BIC in favor of the network learned by
Gobnilp is larger than 10. This outcome is expected, as Gobnilp is an exact solver and those data

Gobnilp
GS
SQ

ASOBS
GS
SQ

GS

SQ

?BIC (K)
Very positive (K >10)
Strongly positive (6<K <10)
Positive (2 <K <6)
Neutral (-2 <K <2)
Negative (-6 <K <-2)
Strongly negative (-10 <K <-6)
Very negative (K <-10)

44
0
0
2
0
1
1

38
0
4
3
1
1
1

44
0
2
0
2
0
0

30
4
3
4
1
5
1

44
1
0
2
0
0
1

32
0
2
4
2
4
4

p-value

<0.01

<0.01

<0.01

<0.01

<0.01

<0.01

structure solver
parent identification: IS vs

OBS

Table 2: Comparison of the approaches for parent set identification on 48 data sets. Given any fixed
solver for structural optimization, IS results in significantly higher BIC scores than both GS and SQ.

7

parent identification
structure solver: AS vs

Independence sel.
GP
OB

Forward sel
GP
OB

Sequential sel.
GP
OB

?BIC (K)
Very positive (K >10)
Strongly positive (6<K<10)
Positive (2<K<6)
Neutral (-2<K<2)
Negative (-6<K<-2)
Strongly negative (-10<K<-6)
Very negative (K<-10)

21
0
0
0
0
0
0

21
0
0
0
0
0
0

20
0
0
0
0
0
1

21
0
0
0
0
0
0

19
0
0
0
0
0
2

21
0
0
0
0
0
0

p-value

<0.01

<0.01

<0.01

<0.01

<0.01

<0.01

Table 3: Comparison between the structure optimization approaches on the 21 data sets with n ?
500. ASOBS (AS) outperforms both Gobnilp (GB) and OBS (OB), under any chosen approach for
parent set identification.
sets imply a relatively reduced search space. However the focus of this paper is on large data sets.
On the 21 data sets with n ? 500, ASOBS outperforms Gobnilp (sign test, p < 0.01) under every
chosen approach for parent set identification (Table 3).
5.2

Learning from data sets sampled from known networks

In the next experiments we create data sets by sampling from known networks. We take the largest
networks available in the literature: 4 andes (n=223), diabetes (n=413), pigs (n=441), link (n=724),
munin (n=1041). Additionally we randomly generate other 15 networks: five networks of size
2000, five networks of size 4000, five networks of size 10000. Each variable has a number of states
randomly drawn from 2 to 4 and a number of parents randomly drawn from 0 to 6. Overall we
consider 20 networks. From each network we sample a data set of 5000 instances.
We perform experiments and analysis as in the previous section. For the sake of brevity we do not
add further tables of results. As for parent set identification, independence selection outperforms
both greedy selection and sequential selection. The difference in favor of independence selection
is significant (sign test, p-value <0.01) under every chosen structure optimization approach. The
?BIC of the learned network is >10 in most cases. Take for instance Gobnilp for structure optimization. Then independence selection yields a ?BIC>10 in 18/20 cases when compared to GS
and ?BIC>10 in 19/20 cases when compared to SQ. Similar results are obtained using the other
solvers for structure optimization.
Strong results support also ASOBS against OBS and Gobnilp. Under every approach for parent set
identification, ?BIC>10 is obtained in 20/20 cases when comparing ASOBS and OBS. The number
of cases in which ASOBS obtains ?BIC>10 when compared against Gobnilp ranges between 17/20
and 19/20 depending on the approach adopted for parent set selection. The superiority of ASOBS
over both OBS and Gobnilp is significant (sign test, p < 0.01) under every approach for parent set
identification.
Moreover, we measured the Hamming distance between the moralized true structure and the learned
structure. On the 21 data sets with n ? 500 ASOBS outperforms Gobnilp and OBS and IS outperforms GS and SQ (sign test, p < 0.01). The novel framework is thus superior in terms of both score
and correctness of the retrieved structure.

6

Conclusion and future work

Our novel approximated approach for structural learning of Bayesian Networks scales up to thousands of nodes without constraints on the maximum in-degree. The current results refer to the BIC
score, but in future the methodology could be extended to other scoring functions.
Acknowledgments
Work partially supported by the Swiss NSF grant n. 200021 146606 / 1.
4

http://www.bnlearn.com/bnrepository/

8

References
[1] M. Bartlett and J. Cussens. Integer linear programming for the Bayesian network structure
learning problem. Artificial Intelligence, 2015. in press.
[2] D. M. Chickering, C. Meek, and D. Heckerman. Large-sample learning of Bayesian networks
is hard. In Proceedings of the 19st Conference on Uncertainty in Artificial Intelligence, UAI03, pages 124?133. Morgan Kaufmann, 2003.
[3] G. F. Cooper and E. Herskovits. A Bayesian method for the induction of probabilistic networks
from data. Machine Learning, 9(4):309?347, 1992.
[4] J. Cussens. Bayesian network learning with cutting planes. In Proceedings of the 27st Conference Annual Conference on Uncertainty in Artificial Intelligence, UAI-11, pages 153?160.
AUAI Press, 2011.
[5] J. Cussens, B. Malone, and C. Yuan. IJCAI 2013 tutorial on optimal algorithms for learning
Bayesian networks (https://sites.google.com/site/ijcai2013bns/slides), 2013.
[6] C. P. de Campos and Q. Ji. Efficient structure learning of Bayesian networks using constraints.
Journal of Machine Learning Research, 12:663?689, 2011.
[7] C. P. de Campos, Z. Zeng, and Q. Ji. Structure learning of Bayesian networks using constraints.
In Proceedings of the 26st Annual International Conference on Machine Learning, ICML-09,
pages 113?120, 2009.
[8] J. V. Haaren and J. Davis. Markov network structure learning: A randomized feature generation
approach. In Proceedings of the 26st AAAI Conference on Artificial Intelligence, 2012.
[9] D. Heckerman, D. Geiger, and D.M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20:197?243, 1995.
[10] T. Jaakkola, D. Sontag, A. Globerson, and M. Meila. Learning Bayesian Network Structure
using LP Relaxations. In Proceedings of the 13st International Conference on Artificial Intelligence and Statistics, AISTATS-10, pages 358?365, 2010.
[11] M. Koivisto. Parent assignment is hard for the MDL, AIC, and NML costs. In Proceedings of
the 19st annual conference on Learning Theory, pages 289?303. Springer-Verlag, 2006.
[12] M. Koivisto and K. Sood. Exact Bayesian Structure Discovery in Bayesian Networks. Journal
of Machine Learning Research, 5:549?573, 2004.
[13] D. Lowd and J. Davis. Learning Markov network structure with decision trees. In Geoffrey I.
Webb, Bing Liu 0001, Chengqi Zhang, Dimitrios Gunopulos, and Xindong Wu, editors, Proceedings of the 10st Int. Conference on Data Mining (ICDM2010), pages 334?343, 2010.
[14] W. J. McGill. Multivariate information transmission. Psychometrika, 19(2):97?116, 1954.
[15] A. Moore and W. Wong. Optimal reinsertion: A new search operator for accelerated and
more accurate Bayesian network structure learning. In T. Fawcett and N. Mishra, editors,
Proceedings of the 20st International Conference on Machine Learning, ICML-03, pages 552?
559, Menlo Park, California, August 2003. AAAI Press.
[16] A. E. Raftery. Bayesian model selection in social research. Sociological methodology, 25:111?
164, 1995.
[17] T. Silander and P. Myllymaki. A simple approach for finding the globally optimal Bayesian
network structure. In Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence, UAI-06, pages 445?452, 2006.
[18] M. Teyssier and D. Koller. Ordering-based search: A simple and effective algorithm for learning Bayesian networks. In Proceedings of the 21st Conference on Uncertainty in Artificial
Intelligence, UAI-05, pages 584?590, 2005.
[19] C. Yuan and B. Malone. An improved admissible heuristic for learning optimal Bayesian
networks. In Proceedings of the 28st Conference on Uncertainty in Artificial Intelligence,
UAI-12, 2012.
[20] C. Yuan and B. Malone. Learning optimal Bayesian networks: A shortest path perspective.
Journal of Artificial Intelligence Research, 48:23?65, 2013.

9

"
5169,"Networks with Learned Unit Response Functions

John Moody and Norman Yarvin
Yale Computer Science, 51 Prospect St.
P.O. Box 2158 Yale Station, New Haven, CT 06520-2158

Abstract
Feedforward networks composed of units which compute a sigmoidal function of a weighted sum of their inputs have been much investigated. We
tested the approximation and estimation capabilities of networks using
functions more complex than sigmoids. Three classes of functions were
tested: polynomials, rational functions, and flexible Fourier series. Unlike sigmoids, these classes can fit non-monotonic functions. They were
compared on three problems: prediction of Boston housing prices, the
sunspot count, and robot arm inverse dynamics. The complex units attained clearly superior performance on the robot arm problem, which is
a highly non-monotonic, pure approximation problem. On the noisy and
only mildly nonlinear Boston housing and sunspot problems, differences
among the complex units were revealed; polynomials did poorly, whereas
rationals and flexible Fourier series were comparable to sigmoids.

1

Introduction

A commonly studied neural architecture is the feedforward network in which each
unit of the network computes a nonlinear function g( x) of a weighted sum of its
inputs x
wtu. Generally this function is a sigmoid, such as g( x)
tanh x or
g(x) = 1/(1 + e(x-9?). To these we compared units of a substantially different
type: they also compute a nonlinear function of a weighted sum of their inputs,
but the unit response function is able to fit a much higher degree of nonlinearity
than can a sigmoid. The nonlinearities we considered were polynomials, rational
functions (ratios of polynomials), and flexible Fourier series (sums of cosines.) Our
comparisons were done in the context of two-layer networks consisting of one hidden
layer of complex units and an output layer of a single linear unit.

=

1048

=

Networks with Learned Unit Response Functions

This network architecture is similar to that built by projection pursuit regression
(PPR) [1, 2], another technique for function approximation. The one difference is
that in PPR the nonlinear function of the units of the hidden layer is a nonparametric smooth. This nonparametric smooth has two disadvantages for neural modeling:
it has many parameters, and, as a smooth, it is easily trained only if desired output
values are available for that particular unit. The latter property makes the use of
smooths in multilayer networks inconvenient. If a parametrized function of a type
suitable for one-dimensional function approximation is used instead of the nonparametric smooth, then these disadvantages do not apply. The functions we used are
all suitable for one-dimensional function approximation.

2

Representation

A few details of the representation of the unit response functions are worth noting.

Polynomials: Each polynomial unit computed the function
g(x) = alX + a2x2 + ... + anx n

=

with x wT u being the weighted sum of the input. A zero'th order term was not
included in the above formula, since it would have been redundant among all the
units. The zero'th order term was dealt with separately and only stored in one
location.

Rationals: A rational function representation was adopted which could not have
zeros in the denominator. This representation used a sum of squares of polynomials,
as follows:
ao + alx + ... + anx n
9 (x ) - 1 + (b o + b1x)2 + (b 2x + b3 x2)2 + (b 4x + b5x 2 + b6X3 + b7x4)2 + .,.
This representation has the qualities that the denominator is never less than 1,
and that n parameters are used to produce a denominator of degree n. If the above
formula were continued the next terms in the denominator would be of degrees eight,
sixteen, and thirty-two. This powers-of-two sequence was used for the following
reason: of the 2( n - m) terms in the square of a polynomial p = am xm + '"" + anx n ,
it is possible by manipulating am ... a n to determine the n - m highest coefficients,
with the exception that the very highest coefficient must be non-negative. Thus
if we consider the coefficients of the polynomial that results from squaring and
adding together the terms of the denominator of the above formula, the highest
degree squared polynomial may be regarded as determining the highest half of the
coefficients, the second highest degree polynomial may be regarded as determining
the highest half of the rest of the coefficients, and so forth. This process cannot set
all the coefficients arbitrarily; some must be non-negative.

Flexible Fourier series: The flexible Fourier series units computed
n

g(x) =

L: ai COS(bi X + Ci)
i=O

where the amplitudes ai, frequencies bi and phases Ci were unconstrained and could
assume any value.

1049

1050

Moody and Yarvin

Sigmoids: We used the standard logistic function:

g(x) = 1/(1 + e(x-9))

3

Training Method

All the results presented here were trained with the Levenberg-Marquardt modification of the Gauss-Newton nonlinear least squares algorithm. Stochastic gradient
descent was also tried at first, but on the problems where the two were compared,
Levenberg- Marquardt was much superior both in convergence time and in quality of
result. Levenberg-Marquardt required substantially fewer iterations than stochastic gradient descent to converge. However, it needs O(p2) space and O(p 2n) time
per iteration in a network with p parameters and n input examples, as compared
to O(p) space and O(pn) time per epoch for stochastic gradient descent. Further
details of the training method will be discussed in a longer paper.
With some data sets, a weight decay term was added to the energy function to be
optimized. The added term was of the form A L~=l
When weight decay was
used, a range of values of A was tried for every network trained.

w;.

Before training, all the data was normalized: each input variable was scaled so that
its range was (-1,1), then scaled so that the maximum sum of squares of input
variables for any example was 1. The output variable was scaled to have mean zero
and mean absolute value 1. This helped the training algorithm, especially in the
case of stochastic gradient descent.

4

Results

We present results of training our networks on three data sets: robot arm inverse
dynamics, Boston housing data, and sunspot count prediction. The Boston and
sunspot data sets are noisy, but have only mild nonlinearity. The robot arm inverse
dynamics data has no noise, but a high degree of nonlinearity. Noise-free problems
have low estimation error. Models for linear or mildly nonlinear problems typically
have low approximation error. The robot arm inverse dynamics problem is thus a
pure approximation problem, while performance on the noisy Boston and sunspots
problems is limited more by estimation error than by approximation error.
Figure la is a graph, as those used in PPR, of the unit response function of a oneunit network trained on the Boston housing data. The x axis is a projection (a
weighted sum of inputs wT u) of the 13-dimensional input space onto 1 dimension,
using those weights chosen by the unit in training. The y axis is the fit to data. The
response function of the unit is a sum ofthree cosines. Figure Ib is the superposition
of five graphs of the five unit response functions used in a five-unit rational function
solution (RMS error less than 2%) of the robot arm inverse dynamics problem. The
domain for each curve lies along a different direction in the six-dimensional input
space. Four of the five fits along the projection directions are non-monotonic, and
thus can be fit only poorly by a sigmoid.
Two different error measures are used in the following. The first is the RMS error,
normalized so that error of 1 corresponds to no training. The second measure is the

Networks with Learned Unit Response Functions
Robot arm fit to data
40

20
~
.; 2
o

o

~
o

.

!

c

o

-zo

. .' .. .""'.
-2

-40

-2.0

1.0

Figure 1:

-4

b

a

square of the normalized RMS error, otherwise known as the fraction of explained
varIance. We used whichever error measure was used in earlier work on that data
set.
4.1

Robot arm inverse dynamics

This problem is the determination of the torque necessary at the joints of a twojoint robot arm required to achieve a given acceleration of each segment of the
arm , given each segment's velocity and position. There are six input variables to
the network, and two output variables. This problem was treated as two separate
estimation problems, one for the shoulder torque and one for the elbow torque. The
shoulder torque was a slightly more difficult problem, for almost all networks. The
1000 points in the training set covered the input space relatively thoroughly. This,
together with the fact that the problem had no noise, meant that there was little
difference between training set error and test set error.
Polynomial networks of limited degree are not universal approximators, and that
is quite evident on this data set; polynomial networks of low degree reached their
minimum error after a few units. Figure 2a shows this. If polynomial, cosine, rational, and sigmoid networks are compared as in Figure 2b, leaving out low degree
polynomials , the sigmoids have relatively high approximation error even for networks with 20 units. As shown in the following table, the complex units have more
parameters each, but still get better performance with fewer parameters total.
Type
degree 7 polynomial
degree 6 rational
2 term cosine
sigmoid
sigmoid

Units
5
5
6
10
20

Parameters
65
95
73
81
161

Error
.024
.027
.020
.139
.119

Since the training set is noise-free, these errors represent pure approximation error .

1051

1052

Moody and Yarvin

~.Iilte ......

+ootII1n.. 3 ler....
0.8

0.8

0.8

O.S

de,

.

?~ 0.4

E

0

0.4

0.2

0.0

Ooooln.. 4 tel'lNl
opoJynomleJ
7
XrationeJ do, 8
? ...""'0101

0.2

L---,b-----+--~::::::::8~~?=t::::::!::::::1J

10

numbel' of WIIt11

Figure 2:

number

Dr

111

20

WIIt11

b

a

The superior performance of the complex units on this problem is probably due to
their ability to approximate non-monotonic functions.

4.2

Boston housing

The second data set is a benchmark for statistical algorithms: the prediction of
Boston housing prices from 13 factors [3]. This data set contains 506 exemplars and
is relatively simple; it can be approximated well with only a single unit. Networks
of between one and six units were trained on this problem. Figure 3a is a graph
of training set performance from networks trained on the entire data set; the error
measure used was the fraction of explained variance. From this graph it is apparent
o polJDomll1 d., fi

dec 2
02 term.....m.

+raUo,,""1
03 tenD coolh.
x.itmold

0 .20

1.0

0 3 term COllin.
x.tpnotd

O. lfi

?~
0.5

0.10

0.05

Figure 3:

a

b

Networks with Learned Unit Response Functions

1053

that training set performance does not vary greatly between different types of units,
though networks with more units do better.
On the test set there is a large difference. This is shown in Figure 3b. Each point
on the graph is the average performance of ten networks of that type. Each network
was trained using a different permutation of the data into test and training sets, the
test set being 1/3 of the examples and the training set 2/3. It can be seen that the
cosine nets perform the best, the sigmoid nets a close second, the rationals third,
and the polynomials worst (with the error increasing quite a bit with increasing
polynomial degree.)
It should be noted that the distribution of errors is far from a normal distribution,
and that the training set error gives little clue as to the test set error. The following
table of errors, for nine networks of four units using a degree 5 polynomial, is
somewhat typical:

Set
training
test

Error
0.091
0.395

I

Our speculation on the cause of these extremely high errors is that polynomial approximations do not extrapolate well; if the prediction of some data point results in
a polynomial being evaluated slightly outside the region on which the polynomial
was trained, the error may be extremely high. Rational functions where the numerator and denominator have equal degree have less of a problem with this, since
asymptotically they are constant. However, over small intervals they can have the
extrapolation characteristics of polynomials. Cosines are bounded, and so, though
they may not extrapolate well if the function is not somewhat periodic, at least do
not reach large values like polynomials.
4.3

Sunspots

The third problem was the prediction of the average monthly sunspot count in a
given year from the values of the previous twelve years. We followed previous work
in using as our error measure the fraction of variance explained, and in using as
the training set the years 1700 through 1920 and as the test set the years 1921
through 1955. This was a relatively easy test set - every network of one unit which
we trained (whether sigmoid, polynomial, rational, or cosine) had, in each of ten
runs, a training set error between .147 and .153 and a test set error between .105
and .111. For comparison, the best test set error achieved by us or previous testers
was about .085. A similar set of runs was done as those for the Boston housing
data, but using at most four units; similar results were obtained. Figure 4a shows
training set error and Figure 4b shows test set error on this problem.
4.4

Weight Decay

The performance of almost all networks was improved by some amount of weight
decay. Figure 5 contains graphs of test set error for sigmoidal and polynomial units,

1054

Moody and Yarvin
0.18 ,..-,------=..::.;==.::.....:::...:=:..:2..,;:.::.:..----r--1 0.25

~---..::.S.::.:un:::;;a.!:..po.:...:l:....:t:.::e.:...:Bt:....:lI:.::e..:..l..:.:,mre.::.:an~_ _--,-,
OP0lr.!:0mt.. dea

Opolynomlal d ?? 1\
""""""allon.. de. 2
02 term co.lne
cs term coolne
x.tamcld

0.14

.
..I:

tC ~?leO::
o~:~~
3 hrm corlne
X_lamold

0.20

O.IZ

0

0.15
0.10

0.10

O.OB

0.08 ' - - + 1 - - - - - ? 2 - - - - - ! S e - - - - - - + - - '
number of WIlle

Figure 4:

a

2

3

Dumb .... of unit.

b

using various values of the weight decay parameter A. For the sigmoids, very little
weight decay seems to be needed to give good results, and there is an order of
magnitude range (between .001 and .01) which produces close to optimal results.
For polynomials of degree 5, more weight decay seems to be necessary for good
results; in fact, the highest value of weight decay is the best. Since very high values
of weight decay are needed, and at those values there is little improvement over
using a single unit, it may be supposed that using those values of weight decay
restricts the multiple units to producing a very similar solution to the one-unit
solution. Figure 6 contains the corresponding graphs for sunspots. Weight decay
seems to help less here for the sigmoids, but for the polynomials, moderate amounts
of weight decay produce an improvement over the one-unit solution.
Acknowledgements
The authors would like to acknowledge support from ONR grant N00014-89-J1228, AFOSR grant 89-0478, and a fellowship from the John and Fannie Hertz
Foundation. The robot arm data set was provided by Chris Atkeson.

References
[1] J. H. Friedman, W. Stuetzle, ""Projection Pursuit Regression"", Journal of the
American Statistical Association, December 1981, Volume 76, Number 376,
817-823
[2] P. J. Huber, ""Projection Pursuit"", The Annals of Statistics, 1985 Vol. 13 No.
2,435-475
[3] L. Breiman et aI, Classification and Regression Trees, Wadsworth and Brooks,
1984, pp217-220

Networks with Learned Unit Response Functions

0.30

Boston housin

hi decay

r-T""=::...:..:;.:;:....:r:-=::;.5I~;=::::..:;=:-;;..:..:..::.....;;-=..:.!ar:......::=~...,

00

+.0001
0.001
0.01
)(.1
'.3

00

+.0001
0.001

1.0

0.01
X.l

?.3

0.25

~0.20

?

0.5

0.15

Figure 5: Boston housing test error with various amounts of weight decay

moids wilh wei hl decay

0. 16

0. 111

1.8

0.14

.1:

O.IB

00
+.0001
0 .001
0 .01
><.1
? .3

0.1?
0 . 12

~

D

0.10

~~

0. 12

sea
::::::,.

0.08

3

2
Dum be .. of 1IJlIt,

<4

0. 10

0.08

2

3
Dumb.,. 01 WIll'

Figure 6: Sunspot test error with various amounts of weight decay

1055

"
2266,"Inferring Network Structure from Co-Occurrences

Michael G. Rabbat
Electrical and Computer Eng.
University of Wisconsin
Madison, WI 53706
rabbat@cae.wisc.edu

M?ario A.T. Figueiredo
Instituto de Telecomunicac?o? es
Instituto Superior T?ecnico
Lisboa, Portugal
mtf@lx.it.pt

Robert D. Nowak
Electrical and Computer Eng.
University of Wisconsin
Madison, WI 53706
nowak@ece.wisc.edu

Abstract
We consider the problem of inferring the structure of a network from cooccurrence data: observations that indicate which nodes occur in a signaling pathway but do not directly reveal node order within the pathway. This problem is
motivated by network inference problems arising in computational biology and
communication systems, in which it is difficult or impossible to obtain precise
time ordering information. Without order information, every permutation of the
activated nodes leads to a different feasible solution, resulting in combinatorial
explosion of the feasible set. However, physical principles underlying most networked systems suggest that not all feasible solutions are equally likely. Intuitively, nodes that co-occur more frequently are probably more closely connected.
Building on this intuition, we model path co-occurrences as randomly shuffled
samples of a random walk on the network. We derive a computationally efficient
network inference algorithm and, via novel concentration inequalities for importance sampling estimators, prove that a polynomial complexity Monte Carlo version of the algorithm converges with high probability.

1

Introduction

The study of complex networked systems is an emerging field impacting nearly every area of engineering and science, including the important domains of biology, cognitive science, sociology, and
telecommunications. Inferring the structure of signalling networks from experimental data precedes
any such analysis and is thus a basic and fundamental task. Measurements which directly reveal network structure are often beyond experimental capabilities or are excessively expensive. This paper
addresses the problem of inferring the structure of a network from co-occurrence data: observations
which indicate nodes that are activated in each of a set of signaling pathways but do not directly reveal the order of nodes within each pathway. Co-occurrence observations arise naturally in a number
of interesting contexts, including biological and communication networks, and networks of neuronal
colonies.
Biological signal transduction networks describe fundamental cell functions and responses to environmental stress [1]. Although it is possible to test for individual, localized interactions between
gene pairs, this approach (called genetic epistatic analysis) is expensive and time-consuming. Highthroughput measurement techniques such as microarrays have successfully been used to identify
the components of different signal transduction pathways [2]. However, microarray data only reflects order information at a very coarse, unreliable level. Developing computational techniques for
inferring pathway orders is a largely unexplored research area [3].
A similar problem has been studied in telecommunication networks [4]. In this context, each path
corresponds to a transmission between an origin and destination. The origin and destination are observed, in addition to the activated switches/routers carrying the transmission through the network.

However, due to the geographically distributed nature of the measurement infrastructure and the rapidity at which transmissions are completed, it is not possible to obtain precise ordering information.
Another exciting potential application arises in neuroimaging [5, 6]. Functional magnetic resonance
imaging provides images of brain activity with high spatial resolution but has relatively poor temporal resolution. Treating distinct brain regions as nodes in a functional brain network that co-activate
when a subject performs different tasks may lead to a similar network inference problem.
Given a collection of co-occurrences, a feasible network (consistent with the observations) is easily
obtained by assigning an order to the elements of each co-occurrence, thereby specifying a path
through the hypothesized network. Since any arbitrary order of each co-occurrence leads to a feasible network, the number of feasible solutions is proportional to the number of permutations of all the
co-occurrence observations. Consequently we are faced with combinatorial explosion of the feasible
set, and without additional assumptions or side information there is no reason to prefer one particular
feasible network over the others. See the supplementary document [7] for further discussion.
Despite the apparent intractability of the problem, physical principles governing most networks
suggest that not all feasible solutions are equally plausible. Intuitively, nodes that co-occur more
frequently are more likely to be connected in the underlying network. This intuition has been used
as a stepping stone by recent approaches proposed in the context of telecommunications [4], and in
learning networks of collaborators [8]. However, because of their heuristic nature, these approaches
do not produce easily interpreted results and do not readily lend themselves to analysis or to the
incorporation of side information.
In this paper, we model co-occurrences as randomly permuted samples of a random walk on the
underlying network. The random permutation accounts for lack of observed order. We refer to this
process as the shuffled Markov model. In this framework, network inference amounts to maximum
likelihood estimation of the parameters governing the random walk (initial state distribution and
transition matrix). Direct maximization is intractable due to the highly non-convex log-likelihood
function and exponential feasible set arising from simultaneously considering all permutations of all
co-occurrences. Instead, we derive a computationally efficient EM algorithm, treating the random
permutations as hidden variables. In this framework the likelihood factorizes with respect to each
pathway/observation, so that the computational complexity of the EM algorithm is determined by
the E-step which is only exponential in the longest path. In order to handle networks with long
paths, we propose a Monte Carlo E-step based on a simple, linear complexity importance sampling
scheme. Whereas the exact E-step has computational complexity which is exponential in path length,
we prove that a polynomial number of importance samples suffices to retain desirable convergence
properties of the EM algorithm with high probability. In this sense, our Monte Carlo EM algorithm
breaks the curse of dimensionality using randomness.
It is worth noting that the approach described here differs considerably from that of learning the
structure of a directed graphical model or Bayesian network [9, 10]. The aim of graphical modelling is to find a graph corresponding to a factorization of a high-dimensional distribution which
predicts the observations well. These probabilistic models do not directly reflect physical structures,
and applying such an approach to co-occurrences would ignore physical constraints inherent to the
observations: co-occurring vertices must lie along a path in the network.

2
2.1

Model Formulation and EM Algorithm
The Shuffled Markov Model

We model a network as a directed graph G = (V, E), where V = {1, . . . , |V |} is the vertex (node)
set and E ? V 2 is the set of edges (direct connections between vertices). An observation, y ? V ,
is a subset of vertices co-activated when a particular stimulus is applied to the network (e.g., collection of signaling proteins activated in response to an environmental stress). Given a set of T
(m)
(m)
observations, Y = {y(1), . . . , y(T ) }, each corresponding to a path, where y(m) = {y1 , . . . , yNm },
we say that a graph (V, E) is feasible w.r.t. Y if for each y(m) ? Y there is an ordered path
(m)
(m)
(m)
(m)
(m)
(m)
z(m) = (z1 , . . . , zNm ) and a permutation ? (m) = (?1 , . . . , ?Nm ) such that zt = y (m) , and
?t

(zt?1 , zt ) ? E, for t = 2, ..., Nm .

The (unobserved) ordered paths, Z = {z(1) , ..., z(T ) }, are modelled as T independent samples of
a first-order Markov chain with state set V . The Markov chain is parameterized by the initial state
distribution ? and the (stochastic) transition matrix A. We assume that the support of the transition
matrix is determined by the adjacency structure of the graph; i.e., Ai,j > 0 ? (i, j) ? E. Each
observation y(m) results from shuffling the elements of z(m) via an unobserved permutation ? (m) ,
(m)
(m)
drawn uniformly from SNm (the set of all permutations of Nm objects); i.e., zt
= y (m) , for
?t

t = 1, . . . , Nm . All the ? (m) are assumed mutually independent and independent of all the z(m) .
Under this model, the log-likelihood of the set of observations Y is
?
?
? ?
T
X
X
?log ?
log P [Y|A, ?] =
P [y(m) |? , A, ?]? ? log(Nm !)? .
(1)
m=1
? ?SNm
QN
where P [y|? , A, ?] = ?y?1 t=2 Ay?t?1 ,y?t , and network inference consists in computing the
maximum likelihood (ML) estimates (AML , ? ML ) = arg maxA,? log P [Y|A, ?]. With the ML
estimates in hand, we may determine the most likely permutation for each y(m) and obtain a feasible reconstruction from the ordered paths. In general, log P [Y|A, ?] is a non-concave function of
(A, ?), so finding (AML , ? ML ) is not easy. Next, we derive an EM algorithm for this purpose, by
treating the permutations as missing data.
2.2

EM Algorithm
(m)

(m)

(m)

Let w(m) = (w1 , ..., wNm ) be a binary representation of z(m) , defined by wt
(m)

(m)

..., wt,|V | ) ? {0, 1}|V | , with (wt,i

(m)

= 1) ? (zt

(m)

= (wt,1 ,

= i); let W = {w(1) , ..., w(T ) }. Let

X = {x(1) , . . . , x(T ) } be the binary representation for Y, defined in a similar way: x(m) =
(m)
(m)
(m)
(m)
(m)
(m)
(m)
(x1 , ..., xNm ), where xt
= (xt,1 , ..., xt,|V | ) ? {0, 1}|V | , with (xt,i = 1) ? (yt
= i).
Finally, let R = {r(1) , . . . , r(T ) } be the collection of permutation matrices corresponding to
(m)
(m)
= t0 ). With this notation in place, the comT = {? (1) , . . . , ? (T ) }; i.e., (rt,t0 = 1) ? (?t
plete log-likelihood can be written as log P [X , R|A, ?] = log P [X |R, A, ?] + log P [R], where
log P [X |R, A, ?]

=

T
X

log P [x(m) |r(m) , A, ?]

m=1

=

|V |
T
X
X

Nm
X

Nm
X

m=1 i,j=1 t0 ,t00 =1

(m) (m)
(m) (m)
rt,t0 rt?1,t00 xt00 ,i xt0 ,j

log Ai,j +

|V | Nm
T X
X
X

(m) (m)

r1,t0 xt0 ,i log ?i ,

(2)

m=1 i=1 t0 =1

t=2

and P [R] is the probability of the set of permutations, which is constant and thus dropped, since the
permutations are independent and equiprobable.

The
by (the E-step) computing Q A, ?; Ak , ? k
=
 EM algorithm proceeds

E log P [X , R|A, ?] X , Ak , ? k , the expected value of log P [X , R|A, ?] w.r.t. the missing
R, conditioned on the observations and on the current model estimate (Ak , ? k ). Examining
log P [X , R|A, ?] reveals that it is linear w.r.t. simple functions of R: (a) the first row of each
PNm (m) (m)
(m)
(m)
r(m) , i.e., r1,t0 ; (b) sums of transition indicators, i.e., ?t0 ,t00 ? t=2
rt,t0 rt?1,t00 . Consequently,
(m)

(m)

(m)

the E-step reduces to computing the conditional expectations of r1,t0 and ?t0 ,t00 , denoted r?1,t0
(m)

and ?
? t0 ,t00 , respectively, and plugging them into the complete log-likelihood (2), which yields

Q A, ?; Ak , ? k .
 (m)
Since the permutations are (a priori) equiprobable, we have P [r(m) ] = (Nm !)?1 , P r1,t0 = 1] =
(m)

(Nm ? 1)!/Nm ! = 1/Nm , and P [r(m) |r1,t0 = 1] = 1/(Nm ? 1)!. Using these facts, the mutual
independence among different observations, and Bayes law, it is not hard to show that
(m)

?0
(m)
r?1,t0 = PN t
m

(m)
t0 =1 ?t0

with

(m)

?t0

=

X
r: r1,t0 =1




P x(m) r, Ak , ? k ,

(3)




where each term P x(m) r, Ak , ? k is easily computed after using r to ?unshuffle? x(m) :
N
m
Y
 (m) 

 (m) 

k
k
k
k
k


P x
r, A , ? = P y
? , A , ? = ?y(m)
Aky(m)
?1

(m)

t=2

(m)
?t?1 ,y?t

(m)

.

(m) (m)

The computation of ?
? t0 ,t00 is similar to that of r?1,t0 ; the key observations are that P [rt,t0 rt?1,t00 =
(m) (m)

1] = (Nm ? 2)!/Nm ! and P [r(m) |rt,t0 rt?1,t00 = 1] = 1/(Nm ? 2)!, leading to
(m)

(m)
?
? t0 ,t00

?t0 ,t00

= PN
m

t0 =1

(m)

,

with

?t0

(m)
?t0 ,t00

=

X

(m)

P [x

k

k

|r, A , ? ]

r

Nm
X

rt,t0 rt?1,t00 .

(4)

t=2


(m)
(m)
Computing {?
r1,t0 } and {?
?t0 ,t00 } requires O Nm ! operations. For large Nm , this is a heavy load; in
Section 3, we describe a sampling approach for computing approximations to r?1,t0 and ?
? t0 ,t00 .

Maximization of Q A, ?; Ak , ? k w.r.t. A and ?, under the normalization constraints, leads to the
M-step:
PT PNm
PT
PNm (m) (m)
(m) (m) (m)
? t0 ,t00 xt00 ,i xt0 ,j
?1,t0 xt0 ,i
m=1
t0 ,t00 =1 ?
m=1
t0 =1 r
k+1
k+1
and ?i = P|S| PT
Ai,j = P|S| PT PN
PNm (m) (m) .
(m) (m) (m)
m
? t0 ,t00 xt00 ,i xt0 ,j
?1,t0 xt0 ,i
j=1
i=1
m=1
m=1
t0 ,t00 =1 ?
t0 =1 r
(5)
Standard convergence results for the EM algorithm due to Boyles and Wu [11,12] guarantee that the
sequence {(Ak , ? k )} converges monotonically to a local maximum of the likelihood.
2.3

Handling Known Endpoints

In some applications, (one or both of) the endpoints of each path are known and only the internal
nodes are shuffled. For example, in telecommunications problems, the origin and destination of
each transmission are known, but not the network connectivity. In estimating biological signal
transduction pathways, a physical stimulus (e.g., hypotonic shock) causes a sequence of protein
interactions, resulting in another observable physical response (e.g., a change in cell wall structure);
in this case, the stimulus and response act as fixed endpoints, the goal is to infer the order of the
sequence of protein interactions. Knowledge of the endpoints of each path imposes the constraints
(m)
(m)
r1,1 = 1 and rNm ,Nm = 1. Under the first constraint, estimates of the initial state probabilities
PT
(m)
are simply given by ?i = T1 m=1 x1,i . Thus, EM only needs to be used to estimate A. In this
setup, the E-step has a similar form as (4) but with sums over r replaced by sums over permutation
matrices satisfying r1,1 = 1 and rN,N = 1. The M-step update for Ak+1 remains unchanged.

3

Large Scale Inference via Importance Sampling

For long paths, the combinatorial nature of the exact E-step ? summing over all permutations of
each sequence in (3) and (4) ? may render exact computation intractable. This section presents a
Monte Carlo importance sampling (see, e.g., [13]) version of the E-step, along with finite sample
bounds guaranteeing that a polynomial complexity Monte Carlo EM algorithm retains desirable
convergence properties of the EM algorithm; i.e., monotonic convergence to a local maximum.
3.1

Monte Carlo E-Step by Importance Sampling

To lighten notation in this section we drop the superscripts from (Ak , ? k ), using simply (A, ?)
(m)
(m)
for the current parameter estimates. Moreover, since the statistics ?
? t0 ,t00 and r?1,t0 depend only
on the mth co-activation observation, y(m) , we focus on a particular length-N path observation
y = (y1 , y2 , . . . , yN ) and drop the superscript (m).
A na??ve Monte Carlo approximation would be based on random permutations sampled from the
uniform distribution on SN . However, the reason we resort to approximation techniques in the first

place is that SN is large, but typically only a small fraction of its elements have non-negligible
posterior probability, P [? |y, A, ?]. Although we would ideally sample directly from the posterior, this would require determining its value for all N ! permutations. Instead, we propose the
following sequential scheme for sampling a permutation using the current parameter estimates,
(A, ?). To ensure the same element is not sampled twice we introduce a vector of binary flags,
f = (f1 , f2 , . . . , f|V | ) ? {0, 1}|V | . Given a probability distribution p = (p1 , p2 , . . . , p|V | ) on the
vertex set, V , denote by p|f the restriction of p to those elements i ? V for which fi = 1; i.e.,
pi fi
(p|f )i = P|V |
,
j=1 pj fj

for i = 1, 2, . . . , |V |.

(6)

Our sampling scheme proceeds as follows:
Step 1: Initialize f so that fi = 1 if yt = i for some t = 1, . . . , N , and fi = 0 otherwise.
Sample an element v from V according to the distribution ?|f on V .
Find t such that yt = v. Set ?1 = t.
Set fv = 0 to prevent yt from being sampled again (ensure ? is a permutation). Set i = 2.
Step 2: Let Av denote the vth row of the transition matrix.
Sample an element v 0 from V according to the distribution Av |f on V .
Find t such that yt = v 0 . Set ?i = t. Set fv0 = 0.
Step 3: While i < N , update v ? v 0 and i ? i + 1 and repeat Step 2; otherwise, stop.
Repeating this sampling procedure L times yields a collection of iid permutations ? 1 , ? 2 , . . . , ? L ,
where the superscript now identifies the sample number; the corresponding permutation matrices
are r1 , r2 , . . . , rL . Samples generated according to the scheme described above are drawn from a
distribution R[? |x, A, ?] on SN which is different from the posterior P [? |x, A, ?]. Importance
sample estimates correct for this disparity and are given by the expressions
PL
PL
PN ` `
`
`=1 u` r1,t0
`=1 u`
t=2 rt,t0 rt?1,t00
0
00
0
rb1,t = PL
and ?
bt ,t =
,
(7)
PL
`=1 u`
`=1 u`
where the correction factor (or weight) for sample r` is given by
N

u` =

N

P [? ` |y, A, ?] Y X
P [r` |x, A, ?]
=
=
Ay? ` ,y? ` .
R[r` |x, A, ?]
R[? ` |y, A, ?] t=2 0
t?1
t0

(8)

t =t

A detailed derivation of the exact form of the induced distribution, R, and the correction factor, u` ,
based on the sequential nature of the sampling scheme, along with further discussion and comparison
with alternative sampling schemes can be found in the supplementary document [7]. In fact, terms
in the product (8) are readily available as a byproduct of Step 2 (denominator of Av |f ).
3.2

Monotonicity and Convergence

Standard EM convergence results directly apply when the exact E-step is used [11, 12]. Let
? k = (Ak , ? k ). By choosing ? k+1 according to (5) we have ? k+1 = arg max? Q(?; ? k ), and
the monotonicity property, Q(? k+1 ; ? k ) ? Q(? k ; ? k ), is satisfied. Together with the fact that the
marginal log-likelihood (1) is continuous in ? and bounded above, the monotonicity property guarantees that the exact EM iterates converge monotonically to a local maximum of log P [Y|?].
When the Monte Carlo E-step is used, we no longer have monotonicity since now the M-step solves
(m)
(m)
bk+1 = arg max Q(?;
bk ), where Q
b ?
b is defined analogously to Q but with ?
?
? t0 ,t00 and r?1,t0 replaced
?
(m)
(m)
bk+1 ; ?
bk ) ? Q(?
bk ; ?
bk ). To assure the Monte Carlo
by ?
bt0 ,t00 and rb1,t0 ; for monotonicity we need Q(?
EM algorithm (MCEM) converges, the number of importance samples, L, must be chosen carefully
b approximates Q well enough; otherwise the MCEM may be swamped with error.
so that Q
Recently, Caffo et al. [14] have proposed a method, based on central limit theorem-like arguments,
for automatically adapting the number of Monte Carlo samples used at each EM iteration. They

guarantee what we refer to as an (, ?)-probably approximately monotonic (PAM) update, stating
bk+1 ; ?
bk ) ? Q(?
bk ; ?
bk ) ? , with probability at least 1 ? ?.
that Q(?
Rather than resorting to asymptotic approximations, we take advantage of the specific form of Q
bk+1 ; ?
bk ) involves terms
b ?
in our problem to obtain the finite-sample PAM result below. Because Q(
bk and log ?
bk and ?
b does not
log A
bik , in practice we bound A
bik away from zero to ensure that Q
i,j
i,j
k
b
blow up. Specifically, we assume a small positive constant ?min so that A ? ?min and ?
bk ? ?min .
i,j

i

Theorem 1 Let , ? > 0 be given. There exist finite constants bm > 0, independent of Nm , so that
if


4
2
2b2m T 2 Nm
| log ?min |2
2Nm
Lm =
log
(9)
2
1 ? (1 ? ?)1/T
b
importance samples are used for the mth observation, then Q(?
probability greater than 1 ? ?.

k+1

k

k

k

b ) ? Q(?
b ;?
b ) ? , with
;?

The proof involves two key steps. First, we derive finite sample concentration-style bounds for
(m)
(m)
the importance sample estimates showing, e.g., that ?
bt0 ,t00 converges to ?
? t0 ,t00 at a rate which is
exponential in the number of importance samples used. These bounds are based on rather novel
concentration inequalities for importance sampling estimators, which may be of interest in their
own right (see the supplementary document [7] for details). Then, accounting for the explicit form
of Q in our problem, the result follows from application of the union bound and the assumptions that
k
bk , ?
A
i,j bi ? ?min . In fact, by making a slightly stronger assumption it can be shown that the MCEM
update is probably monotonic (i.e., (0, ?)-PAM, not approximately monotonic) if L0m importance
samples are used for the mth observation, where L0m also depends polynomially on Nm and T . See
the supplementary document [7] for further discussion and for the full proof of Theorem 1.
Recall that exact E-step computation requires Nm ! operations for the mth observation (enumerating
all permutations). The bound above stipulates that the number of importance samples required for a
2
4
. Generating one importance sample using the sequential
log Nm
PAM update is on the order of Nm
procedure described above requires Nm operations. In contrast to the (exponential complexity) exact
EM algorithm, this clearly demonstrates that the MCEM converges with high probability while only
having polynomial computational complexity, and, in this sense, the MCEM meaningfully breaks
the curse of dimensionality by using randomness to preserve the monotonic convergence property.

4

Experimental Results

The performance of our algorithm for network inference from co-occurrences (NICO, pronounced
?nee-koh?) has been evaluated on both simulated data and on a biological data set. In these experiments, network structure is inferred by first executing the EM algorithm to infer the parameters
(A, ?) of a Markov chain. Then, inserting edges in the inferred graph based on the most likely order
of each path according to (A, ?) ensures the resulting graph is feasible with respect to the observations. Because the EM algorithm is only guaranteed to converge to a local maximum, we rerun
the algorithm from multiple random initializations and chose the mostly likely of these solutions.
To gauge the performance of our algorithm we use the edge symmetric difference error: the total
number of false positives (edges in the inferred network which do not exist in the true network) plus
the number of false negatives (edges in the true network not appearing in the inferred network).
We simulate co-occurrence observations in the following fashion. A random graph on 50 vertices
is sampled. Disjoint sets of vertices are randomly chosen as path origins and destinations, paths
are generated between each origin-destination pair using the shortest path algorithm with either unit
weight per edge (?shortest path?) or a random weight on each edge (?random routing?), and then
co-occurrence observations are formed from each path. We keep the number of origins fixed at 5
and vary the number of destinations between 5 and 40 to see how the number of observations effects
performance. NICO performance is compared against the frequency method (FM) described in [4].
Figure 1 plots the edge error for synthetic data generated using (a) shortest path routing, and (b)
random routing. Each curve is the average performance over 100 different network and path real-

7

5
4
3
2
1
0
5

Freq. Method (Sparsest)
Freq. Method (Best)
NICO (ML)

6
Edge Symmetric Difference

6
Edge Symmetric Difference

7

Freq. Method (Sparsest)
Freq. Method (Best)
NICO (ML)

5
4
3
2
1

10

15

20
25
Num. Destinations

30

(a) Shortest path routes

35

40

0
5

10

15

20
25
Num. Destinations

30

35

40

(b) Random routes

Figure 1: Edge symmetric differences between inferred networks and the network one would obtain
using co-occurrence measurements arranged in the correct order. Performance is averaged over
100 different network realizations. For each configuration 10 NICO and FM solutions are obtained
via different initializations. We then choose the NICO solution yielding the largest likelihood, and
compare with both the sparsest (fewest edges) and clairvoyant best (lowest error) FM solution.

izations. For each network/path realization, the EM algorithm is executed with 10 random initializations. Exact E-step calculation is used for observations with Nm ? 12, and importance sampling
is used for longer paths. The longest observation in our data has Nm = 19. The FM uses simple
pairwise frequencies of co-occurrence to assign an order independently to each path observation. Of
the 10 NICO solutions (different random initializations), we use the one based on parameter estimates yielding the highest likelihood score which also always gives the best performance. Because
it is a heuristic, the FM does not provide a similar mechanism for ranking solutions from different
initializations. We plot FM performance for two schemes; one based on choosing the sparsest FM
solution (the one with the fewest edges), and one based on clairvoyantly choosing the FM solution
with lowest error. NICO consistently outperforms even the clairvoyant best FM solution.
Our method has also been applied to infer the stress-activated protein kinease (SAPK)/Jun N terminal kinase (JNK) and NF?B signal transduction pathways1 (biological networks). The clustering procedure described in [2] is applied to microarray data in order to identify 18 co-occurrences
arising from different environmental stresses or growth factors (path source) and terminating in the
production of SAPK/JNK or NF?B proteins. The reconstructed network (combined SAPK/JNK and
NF?B signal transduction pathways) is depicted in Figure 2. This structure agrees with the signalling
pathways identified using traditional experimental techniques which test individually for each possible edge (e.g., ?MAPK? and ?NF-?B Signaling? on http://www.cellsignal.com).

5

Conclusion

This paper describes a probabilistic model and statistical inference procedure for inferring network
structure from incomplete ?co-occurrence? measurements. Co-occurrences are modelled as samples
of a first-order Markov chain subjected to a random permutation. We describe exact and Monte Carlo
EM algorithms for calculating maximum likelihood estimates of the Markov chain parameters (initial state distribution and transition matrix), treating the random permutations as hidden variables.
Standard results for the EM algorithm guarantee convergence to a local maximum. Although our
exact EM algorithm has exponential computational complexity, we provide finite-sample bounds
guaranteeing convergence of the Monte Carlo EM variation to a local maximum with high probability and with only polynomial complexity. Our algorithm is easily extended to compute maximum a
posteriori estimates, applying a Dirichlet prior to the initial state distribution and to each row of the
Markov transition matrix.
1
NF?B proteins control genes regulating a broad range of biological processes including innate and adaptive
immunity, inflammation and B cell development. The NF?B pathway is a collection of paths activated by
various environmental stresses and growth factors, and terminating in the production of NF?B.

LT

Ag

NIK

PI3K
ArtCot

AgMHC

PLCgamma2

PKC

MALT1
TRAF6

RHO
CS2

TAK1
IKK

IL1

RAC
NFkappaBC1
dsRNA

GF

RAS

NFkappaBC2

NFKappaB

PKR

CS1

bTrCP

CDC42

MEKK

MKK

JNK

HPK
GCKs
UV
FAS
TNF

ASK1

OS

Figure 2: Inferred topology of the combined SAPK/JNK and NF?B signal transduction pathways.
Co-occurrences are obtained from gene expression data via the clustering algorithm described in [2],
and then network is inferred using NICO.
Acknowledgments
The authors of this paper would like to thank D. Zhu and A.O. Hero for providing the data and collaborating on the biological network experiment reported in Section 4. This work was supported in
part by the Portuguese Foundation for Science and Technology grant POSC/EEA-SRI/61924/2004,
the Directorate of National Intelligence, and National Science Foundation grants CCF-0353079 and
CCR-0350213.
References
[1] E. Klipp, R. Herwig, A. Kowald, C. Wierling, and H. Lehrach. Systems Biology in Practice: Concepts,
Implementation and Application. John Wiley & Sons, 2005.
[2] D. Zhu, A. O. Hero, H. Cheng, R. Khanna, and A. Swaroop. Network constrained clustering for gene
microarray data. Bioinformatics, 21(21):4014?4020, 2005.
[3] Y. Liu and H. Zhao. A computational approach for ordering signal transduction pathway components
from genomics and proteomics data. BMC Bioinformatics, 5(158), October 2004.
[4] M. G. Rabbat, J. R. Treichler, S. L. Wood, and M. G. Larimore. Understanding the topology of a telephone
network via internally-sensed network tomography. In Proc. IEEE International Conference on Acoustics,
Speech, and Signal Processing, 2005.
[5] O. Sporns and G. Tononi. Classes of network connectivity and dynamics. Complexity, 7(1):28?38, 2002.
[6] O. Sporns, D. R. Chialvo, M. Kaiser, and C. C. Hilgetag. Organization, development and function of
complex brain networks. Trends in Cognitive Science, 8(9), 2004.
[7] M.G. Rabbat, M.A.T. Figueiredo, and R.D. Nowak. Supplement to inferring network structure from
co-occurrences. Technical report, University of Wisconsin-Madison, October 2006.
[8] J. Kubica, A. Moore, D. Cohn, and J. Schneider. cGraph: A fast graph-based method for link analysis
and queries. In Proc. IJCAI Text-Mining and Link-Analysis Workshop, Acapulco, Mexico, August 2003.
[9] D. Heckerman, D. Geiger, and D. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20:197?243, 1995.
[10] N. Friedman and D. Koller. Being Bayesian about Bayesian network structure: A Bayesian approach to
structure discovery in Bayesian networks. Machine Learning, 50(1?2):95?125, 2003.
[11] R. A. Boyles. On the convergence of the EM algorithm. J. Royal Statistical Society B, 45(1):47?50, 1983.
[12] C. F. J. Wu. On the convergence properties of the EM algorithm. Ann. of Statistics, 11(1):95?103, 1983.
[13] C. Robert and G. Casella. Monte Carlo Statistical Methods. Springer Verlag, New York, 1999.
[14] B. S. Caffo, W. Jank, and G. L. Jones. Ascent-based Monte Carlo EM. J. Royal Statistical Society B,
67(2):235?252, 2005.

"
3189,"Lg DEPTH ESTIMATION AND RIPPLE FIRE
CHARACTERIZA TION USING
ARTIFICIAL NEURAL NETWORKS
John L. Perry and Douglas R. Baumgardt
ENSCO, Inc.
Signal Analysis and Systems Division
5400 Port Royal Road
Springfield, Virginia 22151
(703) 321-9000, perry@dewey.css.gov

Abstract
This srudy has demonstrated how artificial neural networks (ANNs) can
be used to characterize seismic sources using high-frequency regional
seismic data. We have taken the novel approach of using ANNs as a
research tool for obtaining seismic source information, specifically
depth of focus for earthquakes and ripple-fire characteristics for
economic blasts, rather than as just a feature classifier between
earthquake and explosion populations. Overall, we have found that
ANNs have potential applications to seismic event characterization and
identification, beyond just as a feature classifier. In future studies, these
techniques should be applied to actual data of regional seismic events
recorded at the new regional seismic arrays. The results of this study
indicates that an ANN should be evaluated as part of an operational
seismic event identification system.

1 INTRODUCTION
1.1 NEURAL NET,\VORKS FOR SEISl\UC SOURCE ANALYSIS
In this study, we have explored the application of artificial neural networks (ANNs) for
-the characterization of seismic sources for the purpose of distinguishing between
explosions and earthquakes. ANNs have usually been used as pattern matching
algorithms, and recent studies have applied ANNs to standard classification between
classes of earthquakes and explosions using wavefonn features (Dowla, et al, 1989),
(Dysart and Pulli, 1990). However, in considering the current state-of-the-art in seismic
event identification, we believe the most challenging problem is not to develop a superior
classification method, but rather, to have a better understanding of the physics of seismic
source and regional signal propagation.

544

Lg Depth Estimation and Ripple Fire Characterization
Our approach to the problem has been to use ANN technology as a research tool for
obtaining a better understanding of the phenomenology behind regional discrimination,
with emphasis on high-frequency regional array data, as well as using ANNs as a pattern
classifier. We have explored two applications of ANNs to seismic source
characterization: (1) the use of ANNs for depth characterization and (2) the recognition of
ripple-fIring effects in economic explosions.
In the fIrst study, we explored the possible use of the Lg cross-coherence matrix,
measured at a regional array, as a ""hidden discriminant"" for event depth of focus. In the
second study, we experimented with applying ANNs to the recognition of ripple-fIre
effects in the spectra of regional phases. Moreover, we also investigated how a small
(around 5 Kt yield) possibly decoupled nuclear explosion, detonated as part of a ripple-fIre
sequence, would affect the spectral modulations observed at regional distances and how
these effects could be identified by the ANN.

1.2

ANN DESCRIPTION

MLP Architecture:

The ANN that we used was a multilayer perceptron (MLP)
architecture with a backpropagation training algorithm (Rumelhart, et al, 1986). The
input layer is fully connected to the hidden layer, which is fully connected to the output
layer. There are no connections within an individual layer. Each node communicates
with another node through a weighted connection. Associated with each connection is a
weight connecting input node to hidden node, and a weight connecting hidden node to
output node. The output of ""activation level"" of a particular node is defined as the linear
weighted sum of all its inputs. For an MLP, a sigmoidal transformation is applied to
this weighted sum. Two layers of our network have activation levels.

MLP Training: The:w..,p uses a backpropagation training algorithm which employs
an iterating process where an output error signal is propagated back through the network
and used to modify weight values. Training involves presenting sweeps of input patterns
to the network and backpropagating the error until it is minimized. It is the weight
values that represent a trained network and which can be used in the
recognition/classification phase.

MLP Recognition: Recognition, on the other hand, involves presenting a pattern to
a trained network and propagating node activation levels uni-directionally from the input
layer, through the hidden layer(s), to the output layer, and then selecting the class
corresponding to the highest output (activation) signal.

2 Lg DEPTH ESTIMATION
In theory, the Lg phase, which is often the largest regional phase on the seismogram,
should provide depth information because Lg results from the superposition of numerous
normal modes in the crust, whose excitation is highly depth dependent. Some studies
have shown that Lg amplitudes do depend on depth (Der and Baumgardt, 1989). However,
the precise dependency of Lg amplitude on depth has been hard to establish because other
effects in the crustal model, such as anelastic attenuation, can also affect the Lg wave
amplitude.

545

546

Perry and Baumgardt
In this study, we have considered if the Lg coherence, measured across a regional array,
might show depth dependency. This idea is based on the fact that alI the normal modes
which comprise Lg propagate at different phase velocities. For multilayered media, the
normal modes will have frequency-dependent phase velocities because of dispersion. Our
method for studying this dependency is a neural network implementation of a technique,
called matchedjieldprocessing, which has been used in underwater acoustics for source
water-depth estimation (Bucker, 1976), (Baggeroer, et al, 1988). This method consists of
computing the spectral matrix of an emitted signal, in our case, Lg, and comparing it
against the same spectral matrix for master events at different depths. In the past, various
optimal methods have been developed for the matching process. In our study, we have
investigated using a neural network to accomplish the matching.

2.1 SPECTRAL MATRIX CALCULATION AND MATCHED FIELD
PROCESSING
The following is a description of how the spectral matrix is computed. First, the
synthetic seismograms for each of the nine elements of the hypothetical array are Fourier
transformed in some time window. If Si (co) is the Fourier transfonn of a time window
for the i the channel, then, the spectral matrix is written as, Hij (co) =S j (co) S j*(co), where
Si(co)=Ale 1[411 +41 / (41)), the indexjis the complex number, rpt is the phase angle, and
the * represents complex transpose. The elements, aik of the spectral matrix can be
.j [41, .(11)

written as a(k ( co)=AjAie
?
where the exponential phase shift term
t1>i (co) - <l\ (co) = __co_(x j -Xl) = - COT?l (co)
is
ell (co)
? en (co) represents the phase velocity for
mode n, which is a function of frequency because of dispersion, Xi - xk is the spatial
separation of the i th and k th channels of the array, and --ri k (m ) is the time shift of
mode n at frequency (J) across the two channels. The product of the synthetic
eigenfunctions, Ai ? and thus, the spectral matrix tenns, are functions of source depth and
model parameters.
The spectral matrix, H ij ( (J) ) , can be computed for an entire synthetic waveform or for a
window on a part of the waveform. The elements of the spectral matrix can be
normalized by inter- or intra- window nonnalization so that its values range from 0 to 1.

2.2 ANN ? MATCHED FIELD DEPTH ESTIMATION
Two different depth studies were performed during this effort. The flrst study evaluated
using the ANN to classify deep (greater than 4 kilometers) vs. shallow (less than 4
kilometers) seismic events. The number of input nodes equaled the number of points in
the spectral matrix which was 1620 (36 data points x 45 spectral elements after
smoothing). The number of output nodes was dependent on the type of classification we
wanted to perfonn. For the shallow-deep discrimination, we only required two output
nodes, one for each class. Training the ANN involved compiling a training (exemplar)
set of spectral matrices for various shallow and deep events and then presenting the
training set to the ANN.
In the second study, we investigated if the ANN could be used to classify seismic events
at different depths. Again, we used flve windows on the Lg phase and implemented the

Lg Depth Estimation and Ripple Fire Characterization
interwindow and intrawindow normalization procedure. The second network was trained
with a seven-element depth vector, whose elements represent the depths of 1, 3, 6, 9, 12,
16, and 20 kilometers.

3 RIPPLE-FIRE CHARACTERIZATION
In this study, we wanted to determine if spectral modulations could be recognized by the
neural network and if they could be attached to concepts relating to the source parameters
of ripple-flred events. Previous studies have demonstrated how such patterns could be
found by looking for time-independent spectral modulations (Baumgardt and Ziegler,
1989), (HedIin, et al, 1990). In this study, we assumed such a pattern has been found and
that the input to the ANN is one of the time-independent spectra. An additional issue we
considered was whether it would be possible to hide a nuclear explosion in the ripple-flre
pattern, and whether or not such a pattern might be recognizable by an ANN. In this
study, as in the previous depth characterization study, we relied entirely on simulated data
for training and test

3.1 ANN ? RIPPLE FIRE CHARACTERIZATION
We performed two ripple-fIred studies which were designed to extract different parameters
from the ripple-fIred events. The two studies characterized the following parameters: 1)
time delay (Experiment A), and 2) normal vs. anomalous (Experiment B). The purpose
of the time delay study was to estimate the time delay between explosions irrespective of
the number of explosions. The goal of the second study was to determine if the ANN
could extract a ""normal"" ripple-fIred explosion from a simulated nuclear explosion buried
in a ripple-fIred event
The input nodes to all three networks consisted of the elements of the seismic spectra
which had 256 data points, covering the frequency range of 0 to 20 Hz. All weights were
initialized to random values in the range of [-0.5, 0.5] and all networks had their
momentum term set to 0.9. The number of hidden units, learning rate, and number of
output nodes varied between each experiment
In Experiment A, the number of hidden units was 24, and we used a learning rate of 0.2.
We used seven output nodes to represent seven different classes with time delays of 37.5,
62.5,87.5, 112.5, 137.5, 162.5, and 187.5 ms. These delay times are the centers of the
following delay time bins: 25-50 ms, 50-75 ms, 75-100 ms, 100-125 ms, 125-150 ms,
150-175 ms, and 175-200 ms. We used five examples of each class for training derived
by varying the time delay by ?5 msec. Training involved presenting the flve exemplars
for each class to the ANN until the squared error approached zero, as we did in the depth
discrimination study. The ANN was trained to return a high activation level in the bin
closest to the delay time of the ripple-fIre.
In Experiment B, we wanted to determine if the ANN could discern between normal and
anomalous ripple-fIre patterns. The ANN was trained with 50 hidden units and a learning
rate of 0.1. There were 36 exemplars of each class, which resulted from all combinations
of six time delays of 5, 6, 7, 8, 9, and 10 ms between individual shots and six time
delays of 5, 6, 7, 8, 9, and 10 ms between rows of shots. Each time delay was also
varied ?1.0 ms to simulate the effect of errors in the blasting delays.

547

548

Perry and Baumgardt
Two output nodes were defined which represent anomalous or normal ripple-fire. The
normal ripple-fire class represented all the simulations done for the triangular pattern. We
assumed each shot had a yield of 1000 kg. The anomalous class were all the simulations
for when the last row of 10 shots was replaced with a single large explosion of 10,000
kg. The ANN was then trained to produce a high activation level for either of these
classes depending on which kind of event it was presented. The effect of the single large
explosion signal was to wash out the scalloping pattern produced by the ripple-fired
explosions. We trained the ANN with normal ripple-fired patterns, with no embedded
nuclear explosions, and anomalous patterns, with an embedded nuclear explosion.

4 RESULTS
4.1

RESUL TS OF DEPTH STUDY

In our first study, we wanted to determine if the network could learn the simple concepts
of shallow and deep from Lg synthetics when presented with only a small number of
exemplar patterns. We presented the ANN with four depths, 1, 2, 12, and 20 krn, and
trained the network to recognize the first two as shallow and the second two as deep. We
then presented the network with the rest of the synthetics, including synthetics for depths
the ANN had not seen in training.
The results of the shallow-deep discrimination study are shown in Table 1. The table
shows the results for both the interwindow and intrawindow normalization procedures.
The test set used to generate these results were also synthetically generated events that
were either less than 4 krn (shallow) or greater than 4 krn (deep). Our criteria for a correct
match was if the correct output node had an activation level that was 0.4 or more above
the other output node's activation. This is a very conservative threshold criteria, which is
evident from the number of undecided values. However, the results do indicate that the
percent of incorrect classifications was only 5.0% for the intrawindow case and 8.3% for
the interwindow case. The percent of correct classification (PCC) for the intrawindow
case was 50% and the PCC for the interwindow case was 58.3%. The network appeared
to be well trained, relative to their squared error values for this study. Using a less
conservative correct match criteria, where the correct output node only had to be larger
than the other output node's activation, the PCC was 88.3% for the intrawindow case and
93.3% for the interwindow case.
Inn? Window Ilnlcr?Window
Depd\
(bn.)

cmrca

o""~nttnOQ

3

1/3

4

liZ

S
6
7
8
9

10
11

IJlCOIRCt

""""""""'""

lCIl.:laoa 0?? cntenoa

515

3/3

113

.115
515

0/0

~/:'

4/S

1/0
0/0
1/0
0/0
1/2
0/0
0/0
0/0
315

3/2

31S

3n
liZ
-:.n.

515

~iS

SIS

~{3

IS

01./01.
3/2

TOQJ

30/35

531 ~6

I~

T~ble

I:

0/1

.1/3

SIS
SIS
41S

13

0/0
0/1

~
XdVCMJoa

0/0
0/2
0/0
0/0
1/0
0/0
0/0
0/0
1/2

0/0
0/0
0/0
';./4

I

undce,ded

...-..

Jcd~

0."" c:ntenoa

0/0
ZIO
1/0
0/0
0/0
0/0
1/0
0/0
0/0
0/0
0/0
1/0

3/2
4/1

3/2
III

ZI1
2/3

I/O

2/2

3/1
3/3
1/1
-:'/3
-:'71 :0

I

Resulls or ANN ror Shallow?Dtep Discrimination.

SIO

4.2

RESULTS OF THE RIPPLE?FIRED STUDY

Linear Shot Patterns (Experiment A)
Table 2 summarizes all the results for the time-delay ripple-fired classification study
performed during Experiment A. The table shows both two-shot training and a two- and
three-shot training cases. The test set for both cases were spectra that had time delays that
were in a ?5 ms range of the target time delay pattern. We set two criteria for PCC for
the two-shot case. The fIrst was that the activation level for the correct output node be
larger than the activation levels of the other output nodes. This produced a PCC of
77.7%, with a 22.2% error rate and no undecided responses. All of the errors resulted
from attempting to use the ANN to learn time delays from a three-shot pattern where the
network was only trained on two-shot events. The second criterion was more
conservative and required that the activation level of the correct output node be ~ 0.5 than
the other output nodes. This gave a PCC 68.2%, an error percentage of 4.5%. although
the number of undecided responses increased to 27.2 %. Again, all the errors resulted from
expecting the ANN to generalize to three-shot events from only being trained with twoshot patterns. Finally. the results for the two- and three-shot training case were much
more impressive. Using both threshold criteria, the ANN achieved a PCC of 100%.

=

I

Threshoid Cdlem

0.0 I O.S

TCSI Set?

:ncorrca

=
D.seA
D.seB
:; shots

I
II

Tocli

I

andt:cCe11

0/0
010

il6
8/1
3/2

.Ill

:S liS

.III

OIl
OIl
0/.1

I

0/6

? (Tr:lincd with a'z?shol pancml
Tiln:siloid Crirena

0.0 10.5

Tesl Sct'
~o=

:nccrreCl

C15eA
D.seB
:; shots

71i
3/8
iIi

010
010

Tccli

221::

010

illO

I

uncieciCed

010
0/0
010

I
I

0/0

? (Truncd with a ~. and. 3-shot pam:m)
Tanle 1:

Results 01 A.""IN lor Time-DelllY Ripple-Fired Discriminlllion

Triangular Shot Patterns? Normal Versus Anomalous (Experiment B)
Table 3 depicts the results of Experiment B for the normal vs. anomalous study. The
threshold criteria for the target output node compared to the other output nodes was 0.4.
Again_ the test set consisted of time delays that were within ?5 ms of the target time
delay pattern. The PCC was 69.4%. the error percentage was 2.7%, and the percentage of
undecided responses was 27.7%. As evident from the table, the majority of undecided
responses were generated from attempting to classify the anomalous event.
Threshold

Cruena

0.4

Tes SCI

,

COmet

incoaect

undecided

Nonn:1.l
Anom:1.lous

31
19

1
I

16

TOI.u

~O

2

:0

T:able J:

Results or A:""I:'oI row CIIormal

RiDDle-Fired

'50
Oi~crimin""li .. n

4

Anomalous

550

Perry and Baumgardt

5

CONCLUSIONS

This study has shown that ANNs can be used to characterize seismic waveform patterns
for the purpose of characterizing depth of focus, from Lg spectral matrices, and for
recognizing ripple-fIre patterns from spectral modulations. However, we were only able
to analyze the results for simulated input data. In future studies, we intend to use real data
as input.
We have demonstrated that events can be classed as shallow or deep on the basis of the Lg
spectral matrix and that the ANN provided a convenient and robust methodology for
matching spectral matrices. The fact that we obtained nearly the same recognition
performance for interwindow and intrawindow normalizations shows that the Lg spectral
matrix does in fact contain significant information about the depth of focus of a seismic
event, at least for theoretically derived synthetic cases.
The results for the ripple-fIre recognition study were very encouraging. We found that
neural networks could easily be trained to recognize many different ripple-fIre patterns.
For a given blasting region, a neural network could be trained to recognize the usual,
routine ripple-fIre patterns generally used in the region. We have shown that it should be
possible to identify unusual or anomalous ripple-fIre patterns due to attempts to include a
large decoupled nuclear explosion in with an ordinary ripple-fIre sequence.

References
Baggeroer, A.M., W.A. Kuperman, and H. Schmidt (1988). Matched field processing:
source localization in correlated noise as optimum parameter estimation, J. Acoust. Soc.
Am.,83, 571-587.
Baumgardt, D.R. and K.A. Ziegler (1989). Automatic recognition of economic and
underwater blasts using regional array data. Unpublished report to Science Applications
Incorporated, 11-880085-51.
Bucker, H.P. (1976). Use of calculated sound fields and matched-field detection to locate
sound sources in shallow water. J. Acoust. Soc. Am .? 59, 368-373.
Der, Z.A. and D.R. Baumgardt (1989). Effect of source depth on the Lg phase,
DARPA/AFTAC Research Review, November 1989.
Dowla, F.U., S.R. Taylor, and R.W. Anderson (1989). Seismic discrimination with
artificial neural networks: preliminary results with regional spectral data, UCRL-102310,
Lawrence Livermore National Laboratory, Livermore, CA.
Dysart, P.S. and J.J. Pulli (1990). Regional seismic event classification at the NORESS
array: seismological measurements and the use of trained neural networks, abstract in
Program, Symposium on Regional Seismic Arrays and Nuclear Test Ban Verification,
Oslo, Norway, 14-17 February 1990.
Hedlin, M.A.H., J.B. Minster, J.A. Orcutt (1990). An automatic means to discriminate
between earthquakes and quarry blasts, submitted to Bull. Seism. Soc. Am.
Rumelhart, D.E., Hinton, G.B., Williams, RJ. (1986). Learning internal representations
by error propagation"", in Parallel Distributed Processing, 1, MIT Press, Cambridge, MA.

"
4505,"A New Convex Relaxation for Tensor Completion

Bernardino Romera-Paredes
Department of Computer Science
and UCL Interactive Centre
University College London
Malet Place, London WC1E 6BT, UK
B.RomeraParedes@cs.ucl.ac.uk

Massimiliano Pontil
Department of Computer Science and
Centre for Computational Statistics
and Machine Learning
University College London
Malet Place, London WC1E 6BT, UK
m.pontil@cs.ucl.ac.uk

Abstract
We study the problem of learning a tensor from a set of linear measurements.
A prominent methodology for this problem is based on a generalization of trace
norm regularization, which has been used extensively for learning low rank matrices, to the tensor setting. In this paper, we highlight some limitations of this
approach and propose an alternative convex relaxation on the Euclidean ball. We
then describe a technique to solve the associated regularization problem, which
builds upon the alternating direction method of multipliers. Experiments on one
synthetic dataset and two real datasets indicate that the proposed method improves
significantly over tensor trace norm regularization in terms of estimation error,
while remaining computationally tractable.

1

Introduction

During the recent years, there has been a growing interest on the problem of learning a tensor from
a set of linear measurements, such as a subset of its entries, see [9, 17, 22, 23, 25, 26, 27] and
references therein. This methodology, which is also referred to as tensor completion, has been
applied to various fields, ranging from collaborative filtering [15], to computer vision [17], and
medical imaging [9], among others. In this paper, we propose a new method to tensor completion,
which is based on a convex regularizer which encourages low rank tensors and develop an algorithm
for solving the associated regularization problem.
Arguably the most widely used convex approach to tensor completion is based upon the extension
of trace norm regularization [24] to that context. This involves computing the average of the trace
norm of each matricization of the tensor [16]. A key insight behind using trace norm regularization
for matrix completion is that this norm provides a tight convex relaxation of the rank of a matrix
defined on the spectral unit ball [8]. Unfortunately, the extension of this methodology to the more
general tensor setting presents some difficulties. In particular, we shall prove in this paper that the
tensor trace norm is not a tight convex relaxation of the tensor rank.
The above negative result stems from the fact that the spectral norm, used to compute the convex
relaxation for the trace norm, is not an invariant property of the matricization of a tensor. This
observation leads us to take a different route and study afresh the convex relaxation of tensor rank on
the Euclidean ball. We show that this relaxation is tighter than the tensor trace norm, and we describe
a technique to solve the associated regularization problem. This method builds upon the alternating
direction method of multipliers and a subgradient method to compute the proximity operator of the
proposed regularizer. Furthermore, we present numerical experiments on one synthetic dataset and
two real-life datasets, which indicate that the proposed method improves significantly over tensor
trace norm regularization in terms of estimation error, while remaining computationally tractable.
1

The paper is organized in the following manner. In Section 2, we describe the tensor completion
framework. In Section 3, we highlight some limitations of the tensor trace norm regularizer and
present an alternative convex relaxation for the tensor rank. In Section 4, we describe a method to
solve the associated regularization problem. In Section 5, we report on our numerical experience
with the proposed method. Finally, in Section 6, we summarize the main contributions of this paper
and discuss future directions of research.

2

Preliminaries

In this section, we begin by introducing some notation and then proceed to describe the learning
problem. We denote by N the set of natural numbers and, for every k ? N, we define [k] =
{1, . . . , k}. Let N ? N and let1 p1 , . . . , pN ? 2. An N -order tensor W ? Rp1 ?????pN , is a
collection of real numbers (Wi1 ,...,iN : in ? [pn ], n ? [N ]). Boldface Euler scripts, e.g. W, will be
used to denote tensors of order higher than two. Vectors are 1-order tensors and will be denoted by
lower case letters, e.g. x or a; matrices are 2-order tensors and will be denoted by upper case letters,
e.g. W . If x ? Rd then for every r ? s ? d, we define xr:s := (xi : r ? i ? s). We also use the
notation pmin = min{p1 , . . . , pN } and pmax = max{p1 , . . . , pN }.

A mode-n fiber of a tensor W is a vector composed of the elements of W obtained by fixing all
indices but one, corresponding to the n-th mode. This notion is a higher order analogue of columns
(mode-1 fibers) and rows (mode-2 fibers) for matrices. The mode-n matricization (or unfolding) of
W, denoted by W(n) , is a matrix obtained by arranging the mode-n fibers of W so that each of
Q
them is a column of W(n) ? Rpn ?Jn , where Jn := k6=n pk . Note that the ordering of the columns
is not important as long as it is used consistently.
We are now ready to describe the learning problem. We choose a linear operator I : Rp1 ?????pN ?
Rm , representing a set of linear measurements obtained from a target tensor W 0 as y = I(W 0 )+?,
where ? is some disturbance noise. Tensor completion is an important example of this setting, in
this case the operator I returns the known elements of the tensor. That is, we have I(W 0 ) =
(W 0i1 (j),...,iN (j) : j ? [m]), where, for every j ? [m] and n ? [N ], the index in (j) is a prescribed
integer in the set [pn ]. Our aim is to recover the tensor W 0 from the data (I, y). To this end, we
solve the regularization problem
	

(1)
min ky ? I(W)k22 + ?R(W) : W ? Rp1 ?????pN
where ? is a positive parameter which may be chosen by cross validation. The role of the regularizer
R is to encourage solutions W which have a simple structure in the sense that they involve a small
number of ?degrees of freedom?. A natural choice is to consider the average of the rank of the
tensor?s matricizations. Specifically, we consider the combinatorial regularizer
N
1 X
R(W) =
rank(W(n) ).
(2)
N n=1

Finding a convex relaxation of this regularizer has been the subject of recent works [9, 17, 23]. They
all agree to use the sum of nuclear norms as a convex proxy of R. This is defined as the average of
the trace norm of each matricization of W, that is,
N
1 X
kWktr =
kW(n) ktr
(3)
N n=1

where kW(n) ktr is the trace (or nuclear) norm of matrix W(n) , namely the ?1 -norm of the vector of
singular values of matrix W(n) (see, e.g. [14]). Note that in the particular case of 2-order tensors,
functions (2) and (3) coincide with the usual notion of rank and trace norm of a matrix, respectively.
A rational behind the regularizer (3) is that the trace norm is the tightest convex lower bound to the
rank of a matrix on the spectral unit ball, see [8, Thm. 1]. This lower bound is given by the convex
envelope of the function

rank(W ), if kW k? ? 1
(4)
?(W ) =
+?,
otherwise
1
For simplicity we assume that pn ? 2 for every n ? [N ], otherwise we simply reduce the order of the
tensor without loss of information.

2

where k ? k? is the spectral norm, namely the largest singular value of W . The convex envelope can
be derived by computing the double conjugate of ?. This is defined as
	

(5)
??? (W ) = sup hW, Si ? ?? (W ) : S ? Rp1 ?p2

where ?? is the conjugate of ?, namely ?? (S) = sup {hW, Si ? ?(W ) : W ? Rp1 ?p2 }.

Note that ? is a spectral function, that is, ?(W ) = ?(?(W )) where ? : Rd+ ? R denotes the
associated symmetric gauge function. Using von Neumann?s trace theorem (see e.g. [14]) it is
easily seen that ?? (S) is also a spectral function. That is, ?? (S) = ? ? (?(S)), where

	
? ? (?) = sup h?, wi ? ?(w) : w ? Rd+ , with d := min(p1 , p2 ).

We refer to [8] for a detailed discussion of these ideas. We will use this equivalence between spectral
and gauge functions repeatedly in the paper.

3

Alternative Convex Relaxation

In this section, we show that the tensor trace norm is not a tight convex relaxation of the tensor rank
R in equation (2). We then propose an alternative convex relaxation for this function.
Note that due to the composite nature of the function R, computing its convex envelope is a challenging task and one needs to resort to approximations. In [22], the authors note that the tensor trace
norm k ? ktr in equation (3) is a convex lower bound to R on the set



	
G? := W ? Rp1 ?????pN : 
W(n) 
? ? 1, ?n ? [N ] .

The key insight behind this observation is summarized in Lemma 4, which we report in Appendix A.
However, the authors of [22] leave open the question of whether the tensor trace norm is the convex
envelope of R on the set G? . In the following, we will prove that this question has a negative answer
by showing that there exists a convex function ? 6= k ? ktr which underestimates the function R on
G? and such that for some tensor W ? G? it holds that ?(W) > kWktr .

To describe our observation we introduce the set

	
G2 := W ? Rp1 ?...?pN : kWk2 ? 1

where k ? k2 is the Euclidean norm for tensors, that is,
kWk22

:=

p1
X

i1 =1

???

pN
X

(Wi1 ,...,iN )2 .

iN =1

We will choose
?(W) = ?? (W) :=

N

1 X ??
?
? W(n)
N n=1 ?

(6)

where ???? is the ?
convex envelope of the cardinality of a vector on the ?2 -ball of radius ? and we
will choose ? = pmin . Note, by Lemma 4 stated in Appendix A, that for every ? > 0, function
?? is a convex lower bound of function R on the set ?G2 .
Below, for every vector s ? Rd we denote by s? the vector obtained by reordering the components
of s so that they are non increasing in absolute value, that is, |s?1 | ? ? ? ? ? |s?d |.

Lemma 1. Let ???? be the convex envelope of the cardinality on the ?2 -ball of radius ?. Then, for
every x ? Rd such that kxk2 = ?, it holds that ???? (x) = card (x).
This lemma is proved in Appendix B. The function ???? resembles the norm developed in [1], which
corresponds to the convex envelope of the indicator function of the cardinality of a vector in the ?2
ball. The extension of its application to tensors is not straighforward though, as it is required to
specify beforehand the rank of each matricization.
The next lemma provides, together with Lemma 1, a sufficient condition for the existence of a tensor
W ? G? at which the regularizer in equation (6) is strictly larger than the tensor trace norm.
3

Lemma 2. If N ? 3 and p1 , . . . , pN are?not all equal to each other, then there exists
W ? Rp1 ?????pN such that: (a) kWk2 = pmin , (b) W ? G? , (c) min rank(W(n) ) <
n?[N ]

max rank(W(n) ).

n?[N ]

The proof of this lemma is presented in Appendix C. We are now ready to formulate the main result
of this section.
Proposition 3. Let p1 , . . . , pN ? N, let k ? ktr
? be the tensor trace norm in equation (3) and let
?? be the function in equation (6) for ? = pmin . If pmin < pmax , then there are infinitely
many tensors W ? G? such that ?? (W) > kWktr . Moreover, for every W ? G2 , it holds that
?1 (W) ? kWktr .
Proof. By construction ?? (W) ? R(W) for every W ? ?G2 . Since G? ? ?G2 then ?? is a
convex lower bound for the tensor rank R on the set G? as well. The first claim now follows by
Lemmas 1 and 2. Indeed, all tensors obtained following the process described in the proof of Lemma
2 (in Appendix C) have the property that


N
q
1 X
1
2
pmin (N ? 1) + pmin + pmin
kWktr =
k?(W(n) )k1 =
N n=1
N
1
(pmin (N ? 1) + pmin + 1) = ?(W) = R(W).
N

<

Furthermore there are infinitely many such tensors which satisfy this claim (see Appendix C).
With respect to the second claim, given that ?1?? is the convex envelope of the cardinality card on
the Euclidean unit ball, then ?1?? (?) ? k?k1 for every vector ? such that k?k2 ? 1. Consequently,
?1 (W) =

1
N

PN

n=1

?1?? ? W(n)



?

1
N

PN

n=1

k?(W(n) )k1 = kWktr .

The above result stems from the fact that the spectral norm is not an invariant property of the matricization of a tensor, whereas the Euclidean (Frobenius) norm is. This observation leads us to further
study the function ?? .

4

Optimization Method

In this section, we explain how to solve the regularization problem associated with the regularizer
(6). For this purpose, we first recall the alternating direction method of multipliers (ADMM) [4],
which was conveniently applied to tensor trace norm regularization in [9, 22].
4.1

Alternating Direction Method of Multipliers (ADMM)

To explain ADMM we consider a more general problem comprising both tensor trace norm regularization and the regularizer we propose,
)
(
N
X

min E (W) + ?
? W(n)
(7)
W

n=1

where E(W) is an error term such as ky ? I(W)k22 and ? is a convex spectral function. It is
defined, for every matrix A, as
?(A) = ?(?(A))
where ? is a gauge function, namely a function which is symmetric and invariant under permutations. In particular, if ? is the ?1 norm then problem (7) corresponds to tensor trace norm regularization, whereas if ? = ???? it implements the proposed regularizer.

Problem (7) poses some difficulties because the terms under the summation are interdependent, due
to the different matricizations of W having the same elements rearranged in a different way. In
4

order to overcome this difficulty, the authors of [9, 22] proposed to use ADMM as a natural way to
decouple the regularization term appearing in problem (7). This strategy is based on the introduction
of N auxiliary tensors, B1 , . . . , BN ? Rp1 ?????pN , so that problem (7) can be reformulated as2
)
(
N
X

1
min
? Bn(n) : Bn = W, n ? [N ]
(8)
E (W) +
W,B1 ,...,BN
?
n=1
The corresponding augmented Lagrangian (see e.g. [4, 5]) is given by

N 
X

?
1
2
? Bn(n) ? hAn , W ? Bn i + kW ? Bn k2 ,
L (W, B, A) = E (W) +
?
2
n=1

(9)

where h?, ?i denotes the scalar product between tensors, ? is a positive parameter and A1 , . . . AN ?
Rp1 ?????pN are the set of Lagrange multipliers associated with the constraints in problem (8).
ADMM is based on the following iterative scheme
W [i+1]
Bn[i+1]
An[i+1]



? argmin L W, B[i] , A[i]
W


? argmin L W [i+1] , B, A[i]
Bn


[i]
? An ? ?W [i+1] ? Bn[i+1] .

(10)
(11)
(12)

Step (12) is straightforward, whereas step (10) is described in [9]. Here we focus on the step (11)
since this is the only problem which involves function ?. We restate it with more explanatory
notations as



2
 

 ?


argmin ? Bn(n) ? An(n) , W(n) ? Bn(n) +
W(n) ? Bn(n) 2 .
2
Bn(n)
By completing the square in the right hand side, the solution of this problem is given by





?n(n) = prox 1 ? (X) := argmin 1 ? Bn(n) + 1 
Bn(n) ? X 
2 ,
B
2
?
?
2
Bn(n)

where X = W(n) ? ?1 An(n) . By using properties of proximity operators (see e.g. [2, Prop. 3.1]) we
know that if ? is a gauge function then


prox ?1 ? (X) = UX diag prox ?1 ? (?(X)) VX? ,

where UX and VX are the orthogonal matrices formed by the left and right singular vectors of
X, respectively. If we choose ? = k?k1 the associated proximity operator is the well-known soft
thresholding operator, that is, prox ?1 k?k1 (?) = v, where the vector v has components


1
.
vi = sign (?i ) |?i | ?
?

On the other hand, if we choose ? = ???? , we need to compute prox ?1 ???? . In the next section, we
describe a method to accomplish this task.
4.2

Computation of the Proximity Operator

To compute the proximity operator of the function ?1 ???? we will use several properties of proximity
calculus. First, we use the formula (see e.g. [7]) proxg? (x) = x ? proxg (x) for g ? = ?1 ???? . Next
we use a property of conjugate functions from [21, 13], which states that g(?) = ?1 ??? (??). Finally,
by the scaling property of proximity operators [7], we have that proxg (x) = ?1 prox???? (?x).
2
The somewhat cumbersome notation Bn(n) denotes the mode-n matricization of tensor Bn , that is,
Bn(n) = (Bn )(n) .

5

Algorithm 1 Computation of prox???? (y)
Input: y ? Rd , ?, ? > 0.
Output: w
? ? Rd .
Initialization: initial step ?0 = 12 , initial and best found solution w0 = w
? = PS (y) ? Rd .
for t = 1, 2, . . . do
?0
???
t

	
t?1
Find k such that k 
? argmax
?r?d
 ?kw1:r k2? r : 0 
t?1
t?1
? ? w1:k
1 + w??
? y1:k
w
?1:k ? w1:k
t?1
k 1:k
k2

t?1
t?1
w
?k+1:d ? wk+1:d
? ? wk+1:d
? yk+1:d
t
?
w ? PS (w)
?
If h(wt ) < h(w)
? then w
? ? wt
If ?Stopping Condition = True? then terminate.
end for
It remains to compute the proximity operator of a multiple of the function ??? in equation (13), that
is, for any ? > 0, y ? S, we wish to compute
prox???? (y) = argmin {h (w) : w ? S}
w

d

where we have defined S := {w ? R : w1 ? ? ? ? ? wd ? 0} and
1
d
2
h (w) = kw ? yk2 + ? max {? kw1:r k2 ? r} .
r=0
2
In order to solve this problem we employ the projected subgradient method, see e.g. [6]. It consists
in applying two steps at each iteration. First, it advances along a negative subgradient of the current
solution; second, it projects the resultant point onto the feasible set S. In fact, according to [6], it
is sufficient to compute an approximate projection, a step which we describe in Appendix D. To
d

compute a subgradient of h at w, we first find any integer k such that k ? argmax {? kw1:r k2 ? r}.
r=0

Then, we calculate a subgradient g of the function h at w by the formula
( 

1 + kw??
wi ? yi , if i ? k,
k
1:k 2
gi =
w i ? yi ,
otherwise.

Now we have all the ingredients to apply the projected subgradient method, which is summarized
in Algorithm 1. In our implementation we stop the algorithm when an update of w
? is not made for
more than 102 iterations.

5

Experiments

We have conducted a set of experiments to assess whether there is any advantage of using the proposed regularizer over the tensor trace norm for tensor completion3 . First, we have designed a
synthetic experiment to evaluate the performance of both approaches under controlled conditions.
Then, we have tried both methods on two tensor completion real data problems. In all cases, we have
used a validation procedure to tune
	 the hyper-parameter ?, present in both approaches, among the
values 10j : j = ?7, ?6, . . . , 1 . In our proposed approach there is one further hyper-parameter,
?, to be specified. It should take the value of the Euclidean norm of the underlying tensor. Since
this is unknown, we propose to use the estimate
v
!N
#
u
u
Y
2
?
? = tkwk + (mean(w)2 + var(w))
p ?m ,
i

2

i=1

where m is the number of known entries and w ? Rm contains their values. This estimator assumes
that each value in the tensor is sampled from N (mean(w), var(w)), where mean(w) and var(w)
are the average and the variance of the elements in w.
3

The code is available at http://romera-paredes.com/code/tensor-completion

6

0.0115
0.011

3000
Tensor Trace Norm
Proposed Regularizer

2000
Seconds

RMSE

0.0105
0.01

1500

0.0095

1000

0.009

500

0.0085
?5

Tensor Trace Norm
Proposed Regularizer

2500

?4

?3
2
log ?

?2

0

?1

50

100

150

200

p

Figure 1: Synthetic dataset: (Left) Root Mean Squared Error (RMSE) of tensor trace norm and the
proposed regularizer. (Right) Running time execution for different sizes of the tensor.
5.1

Synthetic Dataset

We have generated a 3-order tensor W 0 ? R40?20?10 by the following procedure. First we generated a tensor W with ranks (12, 6, 3) using Tucker decomposition (see e.g. [16])
Wi1 ,i2 ,i3 =

6 X
3
12 X
X

j1 =1 j2 =1 j3 =1

(1)

(2)

(3)

Cj1 ,j2 ,j3 Mi1 ,j1 Mi2 ,j2 Mi3 ,j3 , (i1 , i2 , i3 ) ? [40] ? [20] ? [10]

where each entry of the Tucker decomposition components is sampled from the standard Gaussian
distribution N (0, 1). We then created the ground truth tensor W 0 by the equation
Wi01 ,i2 ,i3 =

Wi1 ,i2 ,i3 ? mean(W)
?
+ ?i1 ,i2 ,i3
N std(W)

where mean(W) and std(W) are the mean and standard deviation of the elements of W, N is
the total number of elements of W, and the ?i1 ,i2 ,i3 are i.i.d. Gaussian random variables with zero
mean and variance ? 2 . We have randomly sampled 10% of the elements of the tensor to compose
the training set, 45% for the validation set, and the remaining 45% for the test set. After repeating
this process 20 times, we report the average results in Figure 1 (Left). Having conducted a paired
t-test for each value of ? 2 , we conclude that the visible differences in the performances are highly
significant, obtaining always p-values less than 0.01 for ? 2 ? 10?2 .
Furthermore, we have conducted an experiment to test the running time of both approaches. We
have generated tensors W 0 ? Rp?p?p for different values of p ? {20, 40, . . . , 200}, following
the same procedure as outlined above. The results are reported in Figure 1 (Right). For low values
of p, the ratio between the running time of our approach and that of the trace norm regularization
method is quite high. For example in the lowest value tried for p in this experiment, p = 20, this
ratio is 22.661. However, as the volume of the tensor increases, the ratio quickly decreases. For
example, for p = 200, the running time ratio is 1.9113. These outcomes are expected because when
p is low, the most demanding routine in our
 method is the one described in Algorithm 1, where
each iteration is of order O (p) and O p2 in the best and worst case, respectively. However, as
p increases the singular value decomposition routine, which iscommon to both methods, becomes
the most demanding because it has a time complexity O p3 [10]. Therefore, we can conclude
that even though our approach is slower than the trace norm based method, this difference becomes
much smaller as the size of the tensor increases.
5.2

School Dataset

The first real dataset we have tried is the Inner London Education Authority (ILEA) dataset. It is
composed of examination marks ranging from 0 to 70, of 15362 students who are described by a set
of attributes such as school and ethnic group. Most of these attributes are categorical, thereby we can
think of exam mark prediction as a tensor completion problem where each of the modes corresponds
to a categorical attribute. In particular, we have used the following attributes: school (139), gender
(2), VR-band (3), ethnic (11), and year (3), leading to a 5-order tensor W ? R139?2?3?11?3 .
7

42
Tensor Trace Norm
Proposed Regularizer

11.6

Tensor Trace Norm
Proposed Regularizer

40
38

11.4
RMSE

RMSE

36
11.2
11

34
32
30

10.8

28
10.6
10.4

26
24
4000

6000
8000
10000
m (Training Set Size)

12000

2

4

6

8
10
12
m (Training Set Size)

14

16
4

x 10

Figure 2: Root Mean Squared Error (RMSE) of tensor trace norm and the proposed regularizer for
ILEA dataset (Left) and Ocean video (Right).

We have selected randomly 5% of the instances to make the test set and another 5% of the instances
for the validation set. From the remaining instances, we have randomly chosen m of them for several
values of m. This procedure has been repeated 20 times and the average performance is presented
in Figure 2 (Left). There is a distinguishable improvement of our approach with respect to tensor
trace norm regularization for values of m > 7000. To check whether this gap is significant, we have
conducted a set of paired t-tests in this regime. In all these cases we obtained a p-value below 0.01.
5.3

Video Completion

In the second real-data experiment we have performed a video completion test. Any video can be
treated as a 4-order tensor: ?width? ? ?height? ? ?RGB? ? ?video length?, so we can use tensor
completion algorithms to rebuild a video from a few inputs, a procedure that can be useful for
compression purposes. In our case, we have used the Ocean video, available at [17]. This video
sequence can be treated as a tensor W ? R160?112?3?32 . We have randomly sampled m tensors
elements as training data, 5% of them as validation data, and the remaining ones composed the test
set. After repeating this procedure 10 times, we present the average results in Figure 2 (Right). The
proposed approach is noticeably better than the tensor trace norm in this experiment. This apparent
outcome is strongly supported by the paired t-tests which we run for each value of m, obtaining
always p-values below 0.01, and for the cases m > 5 ? 104 , we obtained p-values below 10?6 .

6

Conclusion

In this paper, we proposed a convex relaxation for the average of the rank of the matricizations of
a tensor. We compared this relaxation to a commonly used convex relaxation used in the context
of tensor completion, which is based on the trace norm. We proved that this second relaxation is
not tight and argued that the proposed convex regularizer may be advantageous. Our numerical
experience indicates that our method consistently improves in terms of estimation error over tensor
trace norm regularization, while being computationally comparable on the range of problems we
considered. In the future it would be interesting to study methods to speed up the computation of the
proximity operator of our regularizer and investigate its utility in tensor learning problems beyond
tensor completion such as multilinear multitask learning [20].
Acknowledgements
We wish to thank Andreas Argyriou, Raphael Hauser, Charles Micchelli and Marco Signoretto for
useful comments. A valuable contribution was made by one of the anonymous referees. Part of this
work was supported by EPSRC Grant EP/H017178/1, EP/H027203/1 and Royal Society International Joint Project 2012/R2.
8

References
[1] A. Argyriou, R. Foygel and N. Srebro. Sparse Prediction with the k-Support Norm. Advances in Neural
Information Processing Systems 25, pages 1466?1474, 2012.
[2] A. Argyriou, C.A. Micchelli, M. Pontil, L. Shen and Y. Xu. Efficient first order methods for linear composite regularizers. arXiv:1104.1436, 2011.
[3] R. Bhatia. Matrix Analysis. Springer Verlag, 1997.
[4] D.P. Bertsekas, J.N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. Prentice-Hall,
1989.
[5] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning
via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1?
122, 2011.
[6] S. Boyd, L. Xiao, A. Mutapcic. Subgradient methods, Stanford University, 2003.
[7] P. L. Combettes and J.-C. Pesquet. Proximal splitting methods in signal processing. In Fixed-Point Algorithms for Inverse Problems in Science and Engineering (H. H. Bauschke et al. Eds), pages 185?212,
Springer, 2011.
[8] M. Fazel, H. Hindi, and S. Boyd. A rank minimization heuristic with application to minimum order
system approximation. Proc. American Control Conference, Vol. 6, pages 4734?4739, 2001.
[9] S. Gandy, B. Recht, I. Yamada. Tensor completion and low-n-rank tensor recovery via convex optimization. Inverse Problems, 27(2), 2011.
[10] G. H. Golub, C. F. Van Loan. Matrix Computations. 3rd Edition. Johns Hopkins University Press, 1996.
[11] Z. Harchaoui, M. Douze, M. Paulin, M. Dudik, J. Malick. Large-scale image classification with tracenorm regularization. IEEE Conference on Computer Vision & Pattern Recognition (CVPR), pages 3386?
3393, 2012.
[12] J-B. Hiriart-Urruty and C. Lemar?echal. Convex Analysis and Minimization Algorithms, Part I. Springer,
1996.
[13] J-B. Hiriart-Urruty and C. Lemar?echal. Convex Analysis and Minimization Algorithms, Part II. Springer,
1993.
[14] R.A. Horn and C.R. Johnson. Topics in Matrix Analysis. Cambridge University Press, 2005.
[15] A. Karatzoglou, X. Amatriain, L. Baltrunas, N. Oliver. Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering. Proc. 4th ACM Conference on Recommender
Systems, pages 79?86, 2010.
[16] T.G. Kolda and B.W. Bade. Tensor decompositions and applications. SIAM Review, 51(3):455?500,
2009.
[17] J. Liu, P. Musialski, P. Wonka, J. Ye. Tensor completion for estimating missing values in visual data.
Proc. 12th International Conference on Computer Vision (ICCV), pages 2114?2121, 2009.
[18] Y. Nesterov. Gradient methods for minimizing composite objective functions. ECORE Discussion Paper,
2007/96, 2007.
[19] B. Recht. A simpler approach to matrix completion. Journal of Machine Learning Research, 12:3413?
3430, 2009.
[20] B. Romera-Paredes, H. Aung, N. Bianchi-Berthouze and M. Pontil. Multilinear multitask learning. Proc.
30th International Conference on Machine Learning (ICML), pages 1444?1452, 2013.
[21] N. Z. Shor. Minimization Methods for Non-differentiable Functions. Springer, 1985.
[22] M. Signoretto, Q. Tran Dinh, L. De Lathauwer, J.A.K. Suykens. Learning with tensors: a framework
based on convex optimization and spectral regularization. Machine Learning, to appear.
[23] M. Signoretto, R. Van de Plas, B. De Moor, J.A.K. Suykens. Tensor versus matrix completion: a comparison with application to spectral data. IEEE Signal Processing Letters, 18(7):403?406, 2011.
[24] N. Srebro, J. Rennie and T. Jaakkola. Maximum margin matrix factorization. Advances in Neural Information Processing Systems (NIPS) 17, pages 1329?1336, 2005.
[25] R. Tomioka, K. Hayashi, H. Kashima, J.S.T. Presto. Estimation of low-rank tensors via convex optimization. arXiv:1010.0789, 2010.
[26] R. Tomioka and T. Suzuki. Convex tensor decomposition via structured Schatten norm regularization.
arXiv:1303.6370, 2013.
[27] R. Tomioka, T. Suzuki, K. Hayashi, H. Kashima. Statistical performance of convex tensor decomposition.
Advances in Neural Information Processing Systems (NIPS) 24, pages 972?980, 2013.

9

"
5321,"Regularization Path of
Cross-Validation Error Lower Bounds
Atsushi Shibagaki, Yoshiki Suzuki, Masayuki Karasuyama, and Ichiro Takeuchi
Nagoya Institute of Technology
Nagoya, 466-8555, Japan
{shibagaki.a.mllab.nit,suzuki.mllab.nit}@gmail.com
{karasuyama,takeuchi.ichiro}@nitech.ac.jp

Abstract
Careful tuning of a regularization parameter is indispensable in many machine
learning tasks because it has a significant impact on generalization performances.
Nevertheless, current practice of regularization parameter tuning is more of an art
than a science, e.g., it is hard to tell how many grid-points would be needed in
cross-validation (CV) for obtaining a solution with sufficiently small CV error. In
this paper we propose a novel framework for computing a lower bound of the CV
errors as a function of the regularization parameter, which we call regularization
path of CV error lower bounds. The proposed framework can be used for providing a theoretical approximation guarantee on a set of solutions in the sense that
how far the CV error of the current best solution could be away from best possible CV error in the entire range of the regularization parameters. Our numerical
experiments demonstrate that a theoretically guaranteed choice of a regularization
parameter in the above sense is possible with reasonable computational costs.

1

Introduction

Many machine learning tasks involve careful tuning of a regularization parameter that controls the
balance between an empirical loss term and a regularization term. A regularization parameter is
usually selected by comparing the cross-validation (CV) errors at several different regularization
parameters. Although its choice has a significant impact on the generalization performances, the
current practice is still more of an art than a science. For example, in commonly used grid-search, it
is hard to tell how many grid points we should search over for obtaining sufficiently small CV error.
In this paper we introduce a novel framework for a class of regularized binary classification problems
that can compute a regularization path of CV error lower bounds. For an ? ? [0, 1], we define ?approximate regularization parameters to be a set of regularization parameters such that the CV
error of the solution at the regularization parameter is guaranteed to be no greater by ? than the best
possible CV error in the entire range of regularization parameters. Given a set of solutions obtained,
for example, by grid-search, the proposed framework allows us to provide a theoretical guarantee
of the current best solution by explicitly quantifying its approximation level ? in the above sense.
Furthermore, when a desired approximation level ? is specified, the proposed framework can be used
for efficiently finding one of the ?-approximate regularization parameters.
The proposed framework is built on a novel CV error lower bound represented as a function of the
regularization parameter, and this is why we call it as a regularization path of CV error lower bounds.
Our CV error lower bound can be computed by only using a finite number of solutions obtained by
arbitrary algorithms. It is thus easy to apply our framework to common regularization parameter tuning strategies such as grid-search or Bayesian optimization. Furthermore, the proposed framework
can be used not only with exact optimal solutions but also with sufficiently good approximate solu1

Figure 1: An illustration of the proposed framework. One of our
algorithms presented in ?4 automatically selected 39 regularization parameter values in [10?3 , 103 ], and an upper bound of the
validation error for each of them is obtained by solving an optimization problem approximately. Among those 39 values, the
one with the smallest validation error upper bound (indicated as
? at C = 1.368) is guaranteed to be ?(= 0.1) approximate regularization parameter in the sense that the validation error for
the regularization parameter is no greater by ? than the smallest
possible validation error in the whole interval [10?3 , 103 ]. See
?5 for the setup (see also Figure 3 for the results with other options).
tions, which is computationally advantageous because completely solving an optimization problem
is often much more costly than obtaining a reasonably good approximate solution.
Our main contribution in this paper is to show that a theoretically guaranteed choice of a regularization parameter in the above sense is possible with reasonable computational costs. To the best
of our knowledge, there is no other existing methods for providing such a theoretical guarantee on
CV error that can be used as generally as ours. Figure 1 illustrates the behavior of the algorithm for
obtaining ? = 0.1 approximate regularization parameter (see ?5 for the setup).

Related works Optimal regularization parameter can be found if its exact regularization path can
be computed. Exact regularization path has been intensively studied [1, 2], but they are known to be
numerically unstable and do not scale well. Furthermore, exact regularization path can be computed
only for a limited class of problems whose solutions are written as piecewise-linear functions of the
regularization parameter [3]. Our framework is much more efficient and can be applied to wider
classes of problems whose exact regularization path cannot be computed. This work was motivated
by recent studies on approximate regularization path [4, 5, 6, 7]. These approximate regularization
paths have a property that the objective function value at each regularization parameter value is no
greater by ? than the optimal objective function value in the entire range of regularization parameters. Although these algorithms are much more stable and efficient than exact ones, for the task
of tuning a regularization parameter, our interest is not in objective function values but in CV errors. Our approach is more suitable for regularization parameter tuning tasks in the sense that the
approximation quality is guaranteed in terms of CV error.
As illustrated in Figure 1, we only compute a finite number of solutions, but still provide approximation guarantee in the whole interval of the regularization parameter. To ensure such a property, we
need a novel CV error lower bound that is sufficiently tight and represented as a monotonic function
of the regularization parameter. Although several CV error bounds (mostly for leave-one-out CV) of
SVM and other similar learning frameworks exist (e.g., [8, 9, 10, 11]), none of them satisfy the above
required properties. The idea of our CV error bound is inspired from recent studies on safe screening
[12, 13, 14, 15, 16] (see Appendix A for the detail). Furthermore, we emphasize that our contribution is not in presenting a new generalization error bound, but in introducing a practical framework
for providing a theoretical guarantee on the choice of a regularization parameter. Although generalization error bounds such as structural risk minimization [17] might be used for a rough tuning of
a regularization parameter, they are known to be too loose to use as an alternative to CV (see, e.g.,
?11 in [18]). We also note that our contribution is not in presenting new method for regularization
parameter tuning such as Bayesian optimization [19], random search [20] and gradient-based search
[21]. As we demonstrate in experiments, our approach can provide a theoretical approximation
guarantee of the regularization parameter selected by these existing methods.

2

Problem Setup

We consider linear binary classification problems. Let {(xi , yi ) ? Rd ?{?1, 1}}i?[n] be the training
set where n is the size of the training set, d is the input dimension, and [n] := {1, . . . , n}. An independent held-out validation set with size n? is denoted similarly as {(x?i , yi? ) ? Rd ? {?1, 1}}i?[n? ] .
A linear decision function is written as f (x) = w? x, where w ? Rd is a vector of coefficients,
and ? represents the transpose. We assume the availability of a held-out validation set only for simplifying the exposition. All the proposed methods presented in this paper can be straightforwardly
2

adapted to a cross-validation setup. Furthermore, the proposed methods can be kernelized if the loss
function satisfies a certain condition. In this paper we focus on the following class of regularized
convex loss minimization problems:
!
1
?
wC
:= arg min ?w?2 + C
?(yi , w? xi ),
(1)
w?Rd 2
i?[n]

where C > 0 is the regularization parameter, and ? ? ? is the Euclidean norm. The loss function is
denoted as ? : {?1, 1} ? R ? R. We assume that ?(?, ?) is convex and subdifferentiable in the 2nd
argument. Examples of such loss functions include logistic loss, hinge loss, Huber-hinge loss, etc.
For notational convenience, we denote the individual loss as ?i (w) := ?(yi , w? xi ) for all i ? [n].
?
The optimal solution for the regularization parameter C is explicitly denoted as wC
. We assume that
the regularization parameter is defined in a finite interval [C? , Cu ], e.g., C? = 10?3 and Cu = 103
as we did in the experiments.
For a solution w ? Rd , the validation error1 is defined as
1 !
Ev (w) := ?
I(yi? w? x?i < 0),
n
?

(2)

i?[n ]

where I(?) is the indicator function. In this paper, we consider two problem setups. The first prob?
?
lem setup is, given a set of (either optimal or approximate) solutions wC
, . . . , wC
at T different
1
T
regularization parameters C1 , . . . , CT ? [C? , Cu ], to compute the approximation level ? such that
?
min Ev (wC
)
t
Ct ?{C1 ,...,CT }

?
? Ev? ? ?, where Ev? := min Ev (wC
),
C?[Cl ,Cu ]

(3)

by which we can find how accurate our search (grid-search, typically) is in a sense of the deviation
of the achieved validation error from the true minimum in the range, i.e., Ev? . The second problem
setup is, given the approximation level ?, to find an ?-approximate regularization parameter within
an interval C ? [Cl , Cu ], which is defined as an element of the following set
#
""
$
#
?
C(?) := C ? [Cl , Cu ] # Ev (wC
) ? Ev? ? ? .
Our goal in this second setup is to derive an efficient exploration procedure which achieves the
specified validation approximation level ?. These two problem setups are both common scenarios in
practical data analysis, and can be solved by using our proposed framework for computing a path of
validation error lower bounds.

3

Validation error lower bounds as a function of regularization parameter

In this section, we derive a validation error lower bound which is represented as a function of the
regularization parameter C. Our basic idea is to compute a lower and an upper bound of the inner
?? ?
product score wC
xi for each validation input x?i , i ? [n? ], as a function of the regularization param?? ?
eter C. For computing the bounds of wC
xi , we use a solution (either optimal or approximate) for
a different regularization parameter C? ?= C.
3.1

Score bounds

?? ?
We first describe how to obtain a lower and an upper bound of inner product score wC
xi based on
?
an approximate solution w
?C? at a different regularization parameter C ?= C.
Lemma 1. Let w
?C? be an approximate solution of the problem (1) for a regularization parameter
?
value C and ?i (w
?C ) be a subgradient of ?i at w = w
?C such that a subgradient of the objective
function is
!
g(w
?C? ) := w
?C + C?
? i (w
?C ).
(4)
i?[n]

1
For simplicity, we regard a validation instance whose score is exactly zero, i.e., w? x?i = 0, is correctly
classified in (2). Hereafter, we assume that there are no validation instances whose input vector is completely
0, i.e., x?i = 0, because those instances are always correctly classified according to the definition in (2).

3

?? ?
Then, for any C > 0, the score wC
xi , i ? [n? ], satisfies
%
?
?(w
?C? , x?i ) ? C1? (?(w
?C? , x?i ) + ?(g(w
?C? ), x?i ))C, if C > C,
?? ?
?? ?
wC
xi ? LB(wC
x i |w
?C? ) :=
1
?
?
?
?
??(w
?C? , xi ) + C? (?(w
?C? , xi ) + ?(g(w
?C? ), xi ))C, if C < C,
%
?
??(w
?C? , x?i ) + C1? (?(w
?C? , x?i ) + ?(g(w
?C? ), x?i ))C, if C > C,
?? ?
?? ?
wC xi ? U B(wC xi |w
?C? ) :=
1
?
?
?
?
?(w
?C? , xi ) ? C? (?(w
?C? , xi ) + ?(g(w
?C? ), xi ))C, if C < C,

(5a)

(5b)

where

1
1
?
?
?? ?
(?wC
?C? ), x?i ) := (?g(w
?C? )??x?i ? + g(w
?C? )? x?i ) ? 0,
? ??xi ? + wC
? xi ) ? 0, ?(g(w
2
2
1
1
?
?
?
?
?? ?
?(wC
?C? ), x?i ) := (?g(w
?C? )??x?i ? ? g(wC? )? x?i ) ? 0.
? , xi ) := (?wC
? ??xi ? ? wC
? xi ) ? 0, ?(g(w
2
2
?
?
?(wC
? , xi ) :=

The proof is presented in Appendix A. Lemma 1 tells that we have a lower and an upper bound of the
?? ?
score wC
xi for each validation instance that linearly change with the regularization parameter C.
When w
?C? is optimal, it can be shown that (see Proposition B.24 in [22]) there exists a subgradient
such that g(w
?C? ) = 0, meaning that the bounds are tight because ?(g(w
?C? ), x?i ) = ?(g(w
?C? ), x?i ) = 0.
? the score w?? x? , i ? [n? ], for the regularization parameter value C?
Corollary 2. When C = C,
i
?
C
itself satisfies
?? ?
?? ?
? ?
?? ?
?? ?
? ?
wC
?C? ) = w
?C
?C? ), x?i ), wC
?C? ) = w
?C
?C? ), x?i ).
? xi ?LB(wC
? xi | w
? xi ??(g(w
? xi ? U B(wC
? xi |w
? xi +?(g(w

The results in Corollary 2 are obtained by simply substituting C = C? into (5a) and (5b).
3.2

Validation Error Bounds

Given a lower and an upper bound of the score of each validation instance, a lower bound of the
validation error can be computed by simply using the following facts:
?? ?
yi? = +1 and U B(wC
x i |w
?C? ) < 0 ? mis-classified,

yi?

(6a)

?? ?
LB(wC
x i |w
?C? )

= ?1 and
> 0 ? mis-classified.
(6b)
Furthermore, since the bounds in Lemma 1 linearly change with the regularization parameter C, we
can identify the interval of C within which the validation instance is guaranteed to be mis-classified.
Lemma 3. For a validation instance with yi? = +1, if
?(w
?C? , x?i )
?(w
?C? , x?i )
? or
?
C? < C <
C
C? < C < C,
?(w
?C? , x?i ) + ?(g(w
?C? ), x?i )
?(w
?C? , x?i ) + ?(g(w
?C? ), x?i )
then the validation instance (x?i , yi? ) is mis-classified. Similarly, for a validation instance with yi? =
?1, if
?(w
?C? , x?i )
?(w
?C? , x?i )
?
C? < C <
C? or
C? < C < C,
?
?
?
?(w
?C? , xi ) + ?(g(w
?C? ), xi )
?(w
?C? , xi ) + ?(g(w
?C? ), x?i )
then the validation instance (x?i , yi? ) is mis-classified.
This lemma can be easily shown by applying (5) to (6).
As a direct consequence of Lemma 3, the lower bound of the validation error is represented as a
function of the regularization parameter C in the following form.
? the validation
Theorem 4. Using an approximate solution w
?C? for a regularization parameter C,
?
error Ev (wC
) for any C > 0 satisfies
?
?
Ev (wC
) ? LB(Ev (wC
)|w
?C? ) :=
(7)
&
'
(
'
(
?
?
!
!
?(w
?C? , xi )
?(w
?C? , xi )
1
?
?
?
I C<C<
C? +
I
?
?
?
? ) C<C<C
?
n
?(
w
?
,
x
)+?(g(
w
?
),
x
)
?(
w
?
,
x
)+?(g(
w
?
),
x
?
?
?
?
i
i
i
i
C
C
C
C
yi? =+1
yi? =+1
'
(
'
()
?
?
!
!
?(
w
?
,
x
)
?(
w
?
,
x
)
?
?
i
i
C
C
?
?
+
I C<C<
C? +
I
C<C<
C? .
?(w
?C? , x?i )+?(g(w
?C? ), x?i )
?(w
?C? , x?i )+?(g(w
?C? ), x?i )
?
?
yi =?1

yi =?1

4

Algorithm 1: Computing the approximation level ? from the given set of solutions
Input: {(xi , yi )}i?[n] , {(x?i , yi? )}i?[n? ] , Cl , Cu , W := {wC?1 , . . . , wC?T }
?
1: Evbest ? minC?t ?{C?1 ,...,C?T } U B(Ev (wC
?t )
?t )|wC
*
+
2: LB(Ev? ) ? minc?[Cl ,Cu ] maxC?t ?{C?1 ,...,C?T } LB(Ev (wc? )|wC?t )
Output: ? = Evbest ? LB(Ev? )
The lower bound (7) is a staircase function of the regularization parameter C.
Remark 5. We note that our validation error lower bound is inspired from recent studies on safe
screening [12, 13, 14, 15, 16], which identifies sparsity of the optimal solutions before solving the
optimization problem. A key technique used in those studies is to bound Lagrange multipliers at the
optimal, and we utilize this technique to prove Lemma 1, which is a core of our framework.
? we can obtain a lower and an upper bound of the validation error for the reguBy setting C = C,
larization parameter C? itself, which are used in the algorithm as a stopping criteria for obtaining an
approximate solution w
?C? .
?
Corollary 6. Given an approximate solution w
?C? , the validation error Ev (wC
? ) satisfies
?
?
Ev (wC
?C? )
? ) ? LB(Ev (wC
? )|w
&
)
! ,
! ,
1
? ?
?
? ?
?
= ?
I w
?C? xi + ?(g(w
?C? ), xi ) < 0 +
I w
?C? xi ? ?(g(w
?C? ), xi ) > 0 ,
n
?
?
yi =+1

(8a)

yi =?1

?
?
Ev (wC
?C? )
? ) ? U B(Ev (wC
? )|w
&
)
! ,
! ,
1
? ?
?
? ?
?
=1? ?
I w
?C? xi ? ?(g(w
?C? ), xi ) ? 0 +
I w
?C? xi + ?(g(w
?C? ), xi ) ? 0 . (8b)
n
?
?
yi =+1

4

yi =?1

Algorithm

In this section we present two algorithms for each of the two problems discussed in ?2. Due to the
space limitation, we roughly describe the most fundamental forms of these algorithms. Details and
several extensions of the algorithms are presented in supplementary appendices B and C.
4.1

Problem setup 1: Computing the approximation level ? from a given set of solutions

Given a set of (either optimal or approximate) solutions w
?C?1 , . . . , w
?C?T , obtained e.g., by ordinary
grid-search, our first problem is to provide a theoretical approximation level ? in the sense of (3)2 .
This problem can be solved easily by using the validation error lower bounds developed in ?3.2. The
algorithm is presented in Algorithm 1, where we compute the current best validation error Evbest in
?
line 1, and a lower bound of the best possible validation error Ev? := minC?[C? ,Cu ] Ev (wC
) in line
2. Then, the approximation level ? can be simply obtained by subtracting the latter from the former.
We note that LB(Ev? ), the lower bound of Ev? , can be easily computed by using T evaluation error
?
lower bounds LB(Ev (wC
)|wC?t ), t = 1, . . . , T , because they are staircase functions of C.
4.2

Problem setup 2: Finding an ?-approximate regularization parameter

Given a desired approximation level ? such as ? = 0.01, our second problem is to find an ?approximate regularization parameter. To this end we develop an algorithm that produces a set of
optimal or approximate solutions w
?C?1 , . . . , w
?C?T such that, if we apply Algorithm 1 to this sequence,
then approximation level would be smaller than or equal to ?. Algorithm 2 is the pseudo-code of
this algorithm. It computes approximate solutions for an increasing sequence of regularization parameters in the main loop (lines 2-11).
2

When we only have approximate solutions w
?C?1 , . . . , w
?C?T , Eq. (3) is slightly incorrect. The first term of
the l.h.s. of (3) should be minC?t ?{C?1 ,...,C?T } U B(Ev (w
?C?t )|w
?C?t ).

5

Let us now consider tth iteration in the main
loop, where we have already computed t?1 apAlgorithm 2: Finding an ? approximate regularproximate solutions w
?C?1 , . . . , w
?C?t?1 for C?1 <
ization parameter with approximate solutions
?
Input: {(xi , yi )}i?[n] , {(x?i , yi? )}i?[n? ] , Cl , Cu , ? . . . < Ct?1 . At this point,
?t ? Cl , C best ? Cl , E best ? 1
?
1: t ? 1, C
C best := arg min
U B(Ev (wC
?C?? ),
v
?? )|w
?? ?{C
?1 ,...,C
?t?1 }
?t ? Cu do
C
2: while C
3: w
?C?t?solve (1) approximately for C=C?t
is the best (in worst-case) regularization param?
4:
Compute U B(Ev (wC
?C?t ) by (8b).
eter obtained so far and it is guaranteed to be an
?t )|w
?
best
?-approximate regularization parameter in the
5:
if U B(Ev (wC
)|
w
?
)
<
E
then
?
v
?t
Ct
interval [Cl ,C?t ] in the sense that the validation
best
?
6:
Ev ? U B(Ev (wC? )|w
?C?t )
t
error,
7:
C best ? C?t
?
Evbest :=
min
U B(Ev (wC
?C?? ),
8:
end if
?? )|w
?? ?{C
?1 ,...,C
?t?1 }
C
?
9:
Set Ct+1 by (10)
10:
t?t+1
is shown to be at most greater by ? than the
11: end while
smallest possible validation error in the interOutput: C best ? C(?).
val [Cl , C?t ]. However, we are not sure whether
C best can still keep ?-approximation property
for C > C?t . Thus, in line 3, we approximately solve the optimization problem (1) at C = C?t and obtain an approximate solution
w
?C?t . Note that the approximate solution w
?C?t must be sufficiently good enough in the sense that
?
?
U B(Ev (wC
?C?? ) ? LB(Ev (wC
?C?? ) is sufficiently smaller than ? (typically 0.1?). If the up?? )|w
?? )|w
?
per bound of the validation error U B(Ev (wC
?C?? ) is smaller than Evbest , we update Evbest and
?? )|w
C best (lines 5-8).
Our next task is to find C?t+1 in such a way that C best is an ?-approximate regularization parameter
in the interval [Cl , C?t+1 ]. Using the validation error lower bound in Theorem 4, the task is to find
the smallest C?t+1 > C?t that violates
?
Evbest ? LB(Ev (wC
)|w
?C?t ) ? ?, ?C ? [C?t , Cu ],

(9)

In order to formulate such a C?t+1 , let us define
?? ?
?? ?
P := {i ? [n? ]|yi? = +1, U B(wC
?C?t ) < 0}, N := {i ? [n? ]|yi? = ?1, LB(wC
?C?t ) > 0}.
?t xi |w
?t xi |w

Furthermore, let
""
$
""
$
?(w
?C?t , x?i )
?(w
?C?t , x?i )
?t
?t
? :=
C
?
C
,
?(w
?C?t , x?i ) + ?(g(w
?C?t ), x?i )
?(w
?C?t , x?i ) + ?(g(w
?C?t ), x?i )
i?P
i?N

and denote the k th -smallest element of ? as k th (?) for any natural number k. Then, the smallest
C?t+1 > C?t that violates (9) is given as
?
C?t+1 ? (?n? (LB(Ev (wC
?C?t )?Evbest +?)?+1)th (?).
?t )|w

5

(10)

Experiments

In this section we present experiments for illustrating the proposed methods. Table 2 summarizes
the datasets used in the experiments. They are taken from libsvm dataset repository [23]. All the
input features except D9 and D10 were standardized to [?1, 1]3 . For illustrative results, the instances were randomly divided into a training and a validation sets in roughly equal sizes. For
quantitative results, we used 10-fold CV. We used Huber hinge loss (e.g., [24]) which is convex
and subdifferentiable with respect to the second argument. The proposed methods are free from
the choice of optimization solvers. In the experiments, we used an optimization solver described in
[25], which is also implemented in well-known liblinear software [26]. Our slightly modified code
3

We use D9 and D10 as they are for exploiting sparsity.

6

liver-disorders (D2)

ionosphere (D3)

australian (D4)

Figure 2: Illustrations of Algorithm 1 on three benchmark datasets (D2, D3, D4). The plots indicate
how the approximation level ? improves as the number of solutions T increases in grid-search (red),
Bayesian optimization (blue) and our own method (green, see the main text).

(a) ? = 0.1 without tricks

(b) ? = 0.05 without tricks

(c) ? = 0.05 with tricks 1 and 2

Figure 3: Illustrations of Algorithm 2 on ionosphere (D3) dataset for (a) op2 with ? = 0.10, (b)
op2 with ? = 0.05 and (c) op3 with ? = 0.05, respectively. Figure 1 also shows the result for op3
with ? = 0.10.
(for adaptation to Huber hinge loss) is provided as a supplementary material, and is also available
on https://github.com/takeuchi-lab/RPCVELB. Whenever possible, we used warmstart approach, i.e., when we trained a new solution, we used the closest solutions trained so far
(either approximate or optimal ones) as the initial starting point of the optimizer. All the computations were conducted by using a single core of an HP workstation Z800 (Xeon(R) CPU X5675
(3.07GHz), 48GB MEM). In all the experiments, we set C? = 10?3 and Cu = 103 .
Results on problem 1 We applied Algorithm 1 in ?4 to a set of solutions obtained by 1) gridsearch, 2) Bayesian optimization (BO) with expected improvement acquisition function, and 3)
adaptive search with our framework which sequentially computes a solution whose validation lower
bound is smallest based on the information obtained so far. Figure 2 illustrates the results on three
datasets, where we see how the approximation level ? in the vertical axis changes as the number of
solutions (T in our notation) increases. In grid-search, as we increase the grid points, the approximation level ? tends to be improved. Since BO tends to focus on a small region of the regularization
parameter, it was difficult to tightly bound the approximation level. We see that the adaptive search,
using our framework straightforwardly, seems to offer slight improvement from grid-search.
Results on problem 2 We applied Algorithm 2 to benchmark datasets for demonstrating theoretically guaranteed choice of a regularization parameter is possible with reasonable computational
costs. Besides the algorithm presented in ?4, we also tested a variant described in supplementary
Appendix B. Specifically, we have three algorithm options. In the first option (op1), we used op?
timal solutions {wC
?t }t?[T ] for computing CV error lower bounds. In the second option (op2),
we instead used approximate solutions {w
?C?t }t?[T ] . In the last option (op3), we additionally used
speed-up tricks described in supplementary Appendix B. We considered four different choices of
? ? {0.1, 0.05, 0.01, 0}. Note that ? = 0 indicates the task of finding the exactly optimal regular7

Table 1: Computational costs. For each of the three options and ? ? {0.10, 0.05, 0.01, 0}, the
number of optimization problems solved (denoted as T ) and the total computational costs (denoted
as time) are listed. Note that, for op2, there are no results for ? = 0.
?

op1
(using w?? )
C
time
T
(sec)

op2
(using w
?C
?)
time
T
(sec)

op3
(using tricks)
time
T
(sec)

op1
(using w?? )
C
time
T
(sec)

op2
(using w
?C
?)
time
T
(sec)

op3
(using tricks)
time
T
(sec)

0.10
0.05
0.01
0

D1

30
68
234
442

0.068
0.124
0.428
0.697

32
70
324

0.031
0.061
0.194
N.A.

33
57
205
383

0.041
0.057
0.157
0.629

D6

92
207
1042
4276

1.916
4.099
16.31
57.57

93
209
1069

0.975
2.065
9.686
N.A.

62
123
728
2840

0.628
1.136
5.362
44.68

0.10
0.05
0.01
0

D2

221
534
1503
10939

0.177
0.385
0.916
6.387

223
540
2183

0.124
0.290
0.825
N.A.

131
367
1239
6275

0.084
0.218
0.623
3.805

D7

289
601
2532
67490

8.492
16.18
57.79
1135

293
605
2788

5.278
9.806
35.21
N.A.

167
379
1735
42135

3.319
6.604
24.04
760.8

0.10
0.05
0.01
0

D3

61
123
600
5412

0.617
1.073
4.776
26.39

62
129
778

0.266
0.468
0.716
N.A.

43
73
270
815

0.277
0.359
0.940
6.344

D8

72
192
1063
34920

0.761
1.687
8.257
218.4

74
195
1065

0.604
1.162
6.238
N.A.

66
110
614
15218

0.606
0.926
4.043
99.57

0.10
0.05
0.01
0

D4

27
64
167
342

0.169
0.342
0.786
1.317

27
65
181

0.088
0.173
0.418
N.A.

23
47
156
345

0.093
0.153
0.399
1.205

D9

134
317
1791
85427

360.2
569.9
2901
106937

136
323
1822

201.0
280.7
1345
N.A.

89
200
1164
63300

74.37
128.5
657.4
98631

0.10
0.05
0.01
0

D5

62
108
421
2330

0.236
0.417
1.201
4.540

63
109
440

0.108
0.171
0.631
N.A.

45
77
258
968

0.091
0.137
0.401
2.451

D10

Ev < 0.10
Ev < 0.05
157
81.75
258552
85610

Ev < 0.10
Ev < 0.05
162
31.02
N.A.

Ev < 0.10
Ev < 0.05
114
36.81
42040
23316

ization parameter. In some datasets, the smallest validation errors are less than 0.1 or 0.05, in which
cases we do not report the results (indicated as ?Ev < 0.05? etc.). In trick1, we initially computed
solutions at four different regularization parameter values evenly allocated in [10?3 , 103 ] in the logarithmic scale. In trick2, the next regularization parameter C?t+1 was set by replacing ? in (10) with
1.5? (see supplementary Appendix B). For the purpose of illustration, we plot examples of validation error curves in several setups. Figure 3 shows the validation error curves of ionosphere (D3)
dataset for several options and ?.
Table 1 shows the number of optimization problems solved in the algorithm (denoted as T ), and
the total computation time in CV setups. The computational costs mostly depend on T , which gets
smaller as ? increases. Two tricks in supplementary Appendix B was effective in most cases for
reducing T . In addition, we see the advantage of using approximate solutions by comparing the
computation times of op1 and op2 (though this strategy is only for ? ?= 0). Overall, the results
suggest that the proposed algorithm allows us to find theoretically guaranteed approximate regularization parameters with reasonable costs except for ? = 0 cases. For example, the algorithm found
an ? = 0.01 approximate regularization parameter within a minute in 10-fold CV for a dataset with
more than 50000 instances (see the results on D10 for ? = 0.01 with op2 and op3 in Table 1).
Table 2: Benchmark datasets used in the experiments.

D1
D2
D3
D4
D5

6

dataset name
heart
liver-disorders
ionosphere
australian
diabetes

sample size
270
345
351
690
768

input dimension
13
6
34
14
8

D6
D7
D8
D9
D10

dataset name
german.numer
svmguide3
svmguide1
a1a
w8a

sample size
1000
1284
7089
32561
64700

input dimension
24
21
4
123
300

Conclusions and future works

We presented a novel algorithmic framework for computing CV error lower bounds as a function
of the regularization parameter. The proposed framework can be used for a theoretically guaranteed
choice of a regularization parameter. Additional advantage of this framework is that we only need to
compute a set of sufficiently good approximate solutions for obtaining such a theoretical guarantee,
which is computationally advantageous. As demonstrated in the experiments, our algorithm is practical in the sense that the computational cost is reasonable as long as the approximation quality ? is
not too close to 0. An important future work is to extend the approach to multiple hyper-parameters
tuning setups.

8

References
[1] B. Efron, T. Hastie, I. Johnstone, and R. TIbshirani. Least angle regression. Annals of Statistics,
32(2):407?499, 2004.
[2] T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. The entire regularization path for the support vector
machine. Journal of Machine Learning Research, 5:1391?1415, 2004.
[3] S. Rosset and J. Zhu. Piecewise linear regularized solution paths. Annals of Statistics, 35:1012?1030,
2007.
[4] J. Giesen, J. Mueller, S. Laue, and S. Swiercy. Approximating Concavely Parameterized Optimization
Problems. In Advances in Neural Information Processing Systems, 2012.
[5] J. Giesen, M. Jaggi, and S. Laue. Approximating Parameterized Convex Optimization Problems. ACM
Transactions on Algorithms, 9, 2012.
[6] J. Giesen, S. Laue, and Wieschollek P. Robust and Efficient Kernel Hyperparameter Paths with Guarantees. In International Conference on Machine Learning, 2014.
[7] J. Mairal and B. Yu. Complexity analysis of the Lasso reguralization path. In International Conference
on Machine Learning, 2012.
[8] V. Vapnik and O. Chapelle. Bounds on Error Expectation for Support Vector Machines. Neural Computation, 12:2013?2036, 2000.
[9] T. Joachims. Estimating the generalization performance of a SVM efficiently. In International Conference
on Machine Learning, 2000.
[10] K. Chung, W. Kao, C. Sun, L. Wang, and C. Lin. Radius margin bounds for support vector machines with
the RBF kernel. Neural computation, 2003.
[11] M. Lee, S. Keerthi, C. Ong, and D. DeCoste. An efficient method for computing leave-one-out error in
support vector machines with Gaussian kernels. IEEE Transactions on Neural Networks, 15:750?7, 2004.
[12] L. El Ghaoui, V. Viallon, and T. Rabbani. Safe feature elimination in sparse supervised learning. Pacific
Journal of Optimization, 2012.
[13] Z. Xiang, H. Xu, and P. Ramadge. Learning sparse representations of high dimensional data on large
scale dictionaries. In Advances in Neural Information Processing Sysrtems, 2011.
[14] K. Ogawa, Y. Suzuki, and I. Takeuchi. Safe screening of non-support vectors in pathwise SVM computation. In International Conference on Machine Learning, 2013.
[15] J. Liu, Z. Zhao, J. Wang, and J. Ye. Safe Screening with Variational Inequalities and Its Application to
Lasso. In International Conference on Machine Learning, volume 32, 2014.
[16] J. Wang, J. Zhou, J. Liu, P. Wonka, and J. Ye. A Safe Screening Rule for Sparse Logistic Regression. In
Advances in Neural Information Processing Sysrtems, 2014.
[17] V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1996.
[18] S. Shalev-Shwartz and S. Ben-David. Understanding machine learning. Cambridge University Press,
2014.
[19] J. Snoek, H. Larochelle, and R. Adams. Practical Bayesian Optimization of Machine Learning Algorithms. In Advances in Neural Information Processing Sysrtems, 2012.
[20] J. Bergstra and Y. Bengio. Random Search for Hyper-Parameter Optimization. Journal of Machine
Learning Research, 13:281?305, 2012.
[21] O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector
machines. Machine Learning, 46:131?159, 2002.
[22] P D. Bertsekas. Nonlinear Programming. Athena Scientific, 1999.
[23] C. Chang and C. Lin. LIBSVM : A Library for Support Vector Machines. ACM Transactions on Intelligent
Systems and Technology, 2:1?39, 2011.
[24] O. Chapelle. Training a support vector machine in the primal. Neural computation, 19:1155?1178, 2007.
[25] C. Lin, R. Weng, and S. Keerthi. Trust Region Newton Method for Large-Scale Logistic Regression. The
Journal of Machine Learning Research, 9:627?650, 2008.
[26] R. Fan, K. Chang, and C. Hsieh. LIBLINEAR: A library for large linear classification. The Journal of
Machine Learning, 9:1871?1874, 2008.

9

"
2399,"Exponential Family
Predictive Representations of State

David Wingate
Computer Science and Engineering
University of Michigan
wingated@umich.edu

Satinder Singh
Computer Science and Engineering
University of Michigan
baveja@umich.edu

Abstract
In order to represent state in controlled, partially observable, stochastic dynamical
systems, some sort of sufficient statistic for history is necessary. Predictive representations of state (PSRs) capture state as statistics of the future. We introduce a
new model of such systems called the ?Exponential family PSR,? which defines
as state the time-varying parameters of an exponential family distribution which
models n sequential observations in the future. This choice of state representation
explicitly connects PSRs to state-of-the-art probabilistic modeling, which allows
us to take advantage of current efforts in high-dimensional density estimation, and
in particular, graphical models and maximum entropy models. We present a parameter learning algorithm based on maximum likelihood, and we show how a
variety of current approximate inference methods apply. We evaluate the quality of our model with reinforcement learning by directly evaluating the control
performance of the model.

1

Introduction

One of the basic problems in modeling controlled, partially observable, stochastic dynamical systems is representing and tracking state. In a reinforcement learning context, the state of the system
is important because it can be used to make predictions about the future, or to control the system
optimally. Often, state is viewed as an unobservable, latent variable, but models with predictive representations of state [4] propose an alternative: PSRs represent state as statistics about the future.
The original PSR models used the probability of specific, detailed futures called tests as the statistics
of interest. Recent work has introduced the more general notion of using parameters that model the
distribution of length n futures as the statistics of interest [8]. To clarify this, consider an agent
interacting with the system. It observes a series of observations o1 ...ot , which we call a history
ht (where subscripts denote time). Given any history, there is some distribution over the next n
observations: p(Ot+1 ...Ot+n |ht ) ? p(F n |ht ) (where Ot+i is the random variable representing
an observation i steps in the future, and F n is a mnemonic for future). We emphasize that this
distribution directly models observable quantities in the system.
Instead of capturing state with tests, the more general idea is to capture state by directly modeling
the distribution p(F n |ht ). Our central assumption is that the parameters describing p(F n |ht ) are
sufficient for history, and therefore constitute state (as the agent interacts with the system, p(F n |ht )
changes because ht changes; therefore the parameters and hence state change). As an example of
this, the Predictive Linear-Gaussian (PLG) model [8] assumes that p(F n |ht ) is jointly Gaussian;
state therefore becomes its mean and covariance. Nothing is lost by defining state in terms of observable quantities: Rudary et al [8] proved that the PLG is formally equivalent to the latent-variable
approach in linear dynamical systems. In fact, because the parameters are grounded, statistically
consistent parameter estimators are available for PLGs.
1

Thus, as part of capturing state in a dynamical system in our method, p(F n |ht ) must be estimated.
This is a density estimation problem. In systems with rich observations (say, camera images),
p(F n |ht ) may have high dimensionality. As in all high-dimensional density estimation problems,
structure must be exploited. It is therefore natural to connect to the large body of recent research
dealing with high-dimensional density estimation, and in particular, graphical models.
In this paper, we introduce the Exponential Family PSR (EFPSR) which assumes that p(F n |ht ) is
a standard exponential family distribution. By selecting the sufficient statistics of the distribution
carefully, we can impose graphical structure on p(F n |ht ), and therefore make explicit connections to
graphical models, maximum entropy modeling, and Boltzmann machines. The EFPSR inherits both
the advantages and disadvantages of graphical exponential family models: inference and parameter
learning in the model is generally hard, but all existing research on exponential family distributions
is applicable (in particular, work on approximate inference).
Selecting the form of p(F n |ht ) and estimating its parameters to capture state is only half of the problem. We must also model the dynamical component, which describes the way that the parameters
vary over time (that is, how the parameters of p(F n |ht ) and p(F n |ht+1 ) are related). We describe a
method called ?extend-and-condition,? which generalizes many state update mechanisms in PSRs.
Importantly, the EFPSR has no hidden variables, but can still capture state, which sets it apart from
other graphical models of sequential data. It is not directly comparable to latent-variable models
such as HMMs, CRFs [3], or Maximum-entropy Markov Models (MEMMs) [5], for example. In
particular, EM-based procedures used in the latent-variable models for parameter learning are unnecessary, and indeed, impossible. This is a consequence of the fact that the model is fully observed:
all statistics of interest are directly related to observable quantities.
We refer the reader to [11] for an extended version of this paper.

2

The Exponential Family PSR

We now present the Exponential Family PSR (EFPSR) model. The next sections discuss the specifics
of the central parts of the model: the state representation, and how we maintain that state.
2.1

Standard Exponential Family Distributions

We first discuss exponential family distributions, which we use because of their close connections
to maximum entropy modeling and graphical models. We refer the reader to Jaynes [2] for detailed
justification, but briefly, he states that the maximum entropy distribution ?agrees with everything
that is known, but carefully avoids assuming anything that is not known,? which ?is the fundamental
property which justifies its use for inference.? The standard exponential family distribution is the
form of the maximum entropy distribution under certain constraints.
For a random variable X, a standard exponential family distribution has the form p(X = x; s) =
exp{sT ?(x) ? Z(s)}, where s is the canonical (or natural) vector of parameters and ?(x) is a vector
of features of variable x. The vector ?(x) also forms the sufficient statistics of the distribution.
The term Z(s) is known as the log-partition function,R and is a normalizing constant which ensures
that p(X; s) defines a valid distribution: Z(s) = log exp{sT ?(x)}dx. By carefully selecting the
features ?(x), graphical structure may be imposed on the distribution.
2.2

State Representation and Dynamics

State. The EFPSR defines state as the parameters of an exponential family distribution modeling
p(F n |ht ). To emphasize that these parameters represent state, we will refer to them as st :

	
n
p(F n = f n |ht ; st ) = exp s?
(1)
t ?(f ) ? log Z(st ) ,
with both { ?(f n ), st } ? Rl?1 . We emphasize that st changes with history, but ?(f n ) does not.

Maintaining State. In addition to selecting the form of p(F n |ht ), there is a dynamical component:
given the parameters of p(F n |ht ), how can we incorporate a new observation to find the parameters
of p(F n |ht , ot+1 )? Our strategy is to extend and condition, as we now explain.
2

Extend. We assume that we have the parameters of p(F n |ht ), denoted st . We extend the distribution of F n |ht to include Ot+n+1 , which forms a new variable F n+1 |ht , and we assume it has the
distribution p(F n , Ot+n+1 |ht ) = p(F n+1 |ht ). This is a temporary distribution with (n + 1)d random variables. In order to add the new variable Ot+n+1 , we must add new features which describe
Ot+n+1 and its relationship to F n . We capture this with a new feature vector ?+ (f n+1 ) ? Rk?1 ,
k?1
and define the vector s+
to be the parameters associated with this feature vector. We thus
t ? R
have the following form for the extended distribution:
 +? + n+
	
p(F n+1 = f n+1 |ht ; s+
) ? log Z(s+
t ) = exp st ? (f
t ) .

To define the dynamics, we define a function which maps the current state vector to the parameters
of the extended distribution. We call this the extension function: s+
t = extend(st ; ?), where ? is a
vector of parameters controlling the extension function (and hence, the overall dynamics).
The extension function helps govern the kinds of dynamics that the model can capture. For example,
in the PLG family of work, a linear extension allows the model to capture linear dynamics [8], while
a non-linear extension allows the model to capture non-linear dynamics [11].
Condition. Once we have extended the distribution to model the n + 1?st observation in the future,
we then condition on the actual observation ot+1 , which results in the parameters of a distribution
over observations from t + 1 through t + n + 1: st+1 = condition(s+
t , ot+1 ), which are precisely
the statistics representing p(F n |ht+1 ), which is our state at time t + 1.
By extending and conditioning, we can maintain state for arbitrarily long periods. Furthermore, for
many choices of features and extension function, the overall extend-and-condition operation does
not involve any inference, mean that tracking state is computationally efficient.
There is only one restriction on the extension function: we must ensure that after extending and conditioning the distribution, the resulting distribution can be expressed as: p(F n = f n |ht+1 ; st+1 ) =
n
exp{s?
t+1 ?(f ) ? log Z(st+1 )}. This looks like exactly like Eq. 1, which is the point: the feature vector ? did not change between timesteps, which means the form of the distribution does not
change. For example, if p(F n |ht ) is a Gaussian, then p(F n |ht+1 ) will also be a Gaussian.
2.3

Representational Capacity

The EFPSR model is quite general. It has been shown that a number of popular models can be unified
under the umbrella of the general EFPSR: for example, every PSR can be represented as an EFPSR
(implying that every POMDP, MDP, and k-th order Markov model can also be represented as an
EFPSR); and every linear dynamical system (Kalman filter) and some nonlinear dynamical systems
can also be represented by an EFPSR. These different models are obtained with different choices of
the features ? and the extension function, and are possible because many popular distributions (such
as multinomials and Gaussians) are exponential family distributions [11].

3

The Linear-Linear EFPSR

We now choose specific features and extension function to generate an example model designed to
be analytically tractable. We select a linear extension function, and we carefully choose features
so that conditioning is always a linear operation. We restrict the model to domains in which the
observations are vectors of binary random variables. The result is named the Linear-Linear EFPSR.
Features. Recall that the features ?() and ?+ () do not depend on time. This is equivalent to saying
that the form of the distribution does not vary over time. If the features impose graphical structure
on the distribution, it is also equivalent to saying that the form of the graph does not change over
time. Because of this, we will now discuss how we can use a graph whose form is independent of
time to help define structure on our distributions.
We construct the feature vectors ?() and ?+ () as follows. Let each Ot ? {0, 1}d ; therefore, each
i
F n |ht ? {0, 1}nd . Let (F n ) be the i?th random variable in F n |ht . We assume that we have an
undirected graph G which we will use to create the features in the vector ?(), and that we have
another graph G+ which we will use to define the features in the vector ?+ (). Define G = (V, E)
i
where V = {1, ..., nd} are the nodes in the graph (one for each F n |ht ), and (i, j) ? E are the
3

G+

Observation features

G

t+1

t+2

t+n

Distribution of next n observations

p(F n |ht )

t+1

t+2

t+n

G

t+n+1

Extended distribution

p(F n , Ot+n+1 |ht )

t+1

t+2

t+n

t+n+1

Conditioned distribution

p(F n |ht , ot+1 )

Figure 1: An illustration of extending and conditioning the distribution.
edges. Similarly, we define G+ = (V +, E+) where V + = {1, ..., (n + 1)d} are the nodes in the
i
graph (one for each (F n+1 |ht ) ), and (i, j) ? E+ are the edges. Neither graph depends on time.
To use the graph to define our distribution, we will let entries in ? be conjunctions of atomic observation variables (like the standard Ising model): for i ? V , there will be some feature k in the vector
such that ?(ft )k = fti . We also create one feature for each edge: if (i, j) ? E, then there will be
some feature k in the vector such that ?(ft )k = fti ftj . Similarly, we use G+ to define ?+ ().
As discussed previously, neither G nor G+ (equivalently, ? and ?+ ) can be arbitrary. We must ensure
that after conditioning G+ , we recover G. To accomplish this, we ensure that both temporally shifted
copies and conditioned versions of each feature exist in the graphs (seen pictorially in Fig. 1).
Because all features are either atomic variables or conjunctions of variables, conditioning the distribution can be done with an operation which is linear in the state (this is true even if the random
variables are discrete or real-valued). We therefore define the linear conditioning operator G(ot+1 )
+
to be a matrix which transforms s+
t into st+1 : st+1 = G(ot+1 )st . See [11] for details.
Linear extension. In general, the function extend can take any form. We choose a linear extension:
s+
t = Ast + B
where A ? Rk?l and B ? Rk?1 are our model parameters. The combination of a linear extension
and a linear conditioning operator can be rolled together into a single operation. Without loss of
generality, we can permute the indices in our state vector such that st+1 = G(ot+1 ) (Ast + B).
Note that although this is linear in the state, it is nonlinear in the observation.

4

Model Learning

We have defined our concept of state, as well as our method for tracking that state. We now address
the question of learning the model from data. There are two things which can be learned in our
model: the structure of the graph, and the parameters governing the state update. We briefly address
each in the next two subsections. We assume we are given a sequence of T observations, [o1 ? ? ? oT ],
which we stack to create a sequence of samples from the F n |ht ?s: ft |ht = [ot+1 ? ? ? ot+n |ht ].
4.1

Structure Learning

To learn the graph structure, we make the approximation of ignoring the dynamical component of
the model. That is, we treat each ft as an observation, and try to estimate the density of the resulting unordered set, ignoring the t subscripts (we appeal to density estimation because many good
algorithms have been developed for structure induction). We therefore ignore temporal relationships
across samples, but we preserve temporal relationships within samples. For example, if observation
a is always followed by observation b, this fact will be captured within the ft ?s.
The problem therefore becomes one of inducing graphical structure for a non-sequential data set,
which is a problem that has already received considerable attention. In all of our experiments, we
used the method of Della Pietra et. al [7]. Their method iteratively evaluates a set of candidate
features and adds the one with highest expected gain in log-likelihood. To enforce the temporal
4

invariance property, whenever we add a feature, we also add all of the temporally shifted copies of
that feature, as well as the conditioned versions of that feature.
4.2

Maximum Likelihood Parameter Estimation

With the structure of the graph in place, we are left to learn the parameters A and B of the state extension. It is now useful that our state is defined in terms of observable quantities, for two reasons:
first, because everything in our model is observed, EM-style procedures for estimating the parameters of our model are not needed, simply because there are no unobserved variables over which to
take expectations. Second, when trying to learn a sequence of states (st ?s) given a long trajectory
of futures (ft ?s), each ft is a sample of information directly from the distribution we?re trying to
model. Given a parameter estimate, an initial state s0 , and a sequence of observations, the sequence
of st ?s is completely determined. This will be a key element to our proposed maximum-likelihood
learning algorithm.
Although the sequence of state vectors st are the parameters defining the distributions p(F n |ht ),
they are not the model parameters ? that is, we cannot freely select them. Instead, the model parameters are the parameters ? which govern the extension function. This is a significant difference
from standard maximum entropy models, and stems from the fact that our overall problem is that of
modeling a dynamical system, rather than just density estimation.
QT
The likelihood of the training data is p(o1 , o2 ...oT ) = t=1 p(ot |ht ). We will find it more conveQT
nient to measure the likelihood of the corresponding ft ?s: p(o1 , o2 ...oT ) ? n t=1 p(ft |ht ) (the
likelihoods are not the same because the likelihood of the ft ?s counts a single observation n times;
the approximate equality is because the first n and last n are counted fewer than n times).
The expected log-likelihood of the training ft ?s under the model defined in Eq. 1 is
!
T
1 X ?
LL =
?s ?(ft ) ? log Z(st )
T t=1 t

(2)

Our goal is to maximize this quantity. Any optimization method can be used to maximize the loglikelihood. Two popular choices are gradient ascent and quasi-Newton methods, such as (L-)BFGS.
We use both, for different problems (as discussed later). However, both methods require the gradient
of the likelihood with respect to the parameters, which we will now compute.
Using the chain rule of derivatives, we can compute the derivative with respect to the parameters A:
T
?LL X ?LL ? ?st
=
(3)
?A
?st ?A
t=1

First, we compute the derivative of the log-likelihood with respect to each state:

?LL
?  ?
=
?st ?(ft ) ? log Z(st ) = Est [?(F n |ht )] ? ?(ft ) ? ?t
(4)
?st
?st
where Est [?(F n |ht )] ? Rl?1 is the vector of expected sufficient statistics at time t. Computing
this is a standard inference problem in exponential family models, as discussed in Section 5. This
gradient tells us that we wish to adjust each state to make the expected features of the next n observations closer to the observed features however, we cannot adjust st directly; instead, we must
adjust it implicitly by adjusting the transition parameters A and B.
We now compute the gradients of the state with respect to each parameter:


?
?st?1
?st
=
G(ot+1 ) (Ast?1 + B) = G(ot+1 ) A
+ s?
?
I
.
t?1
?A
?A
?A
where ? is the Kronecker product, and I is an identity matrix the same size as A. The gradients of
the state with respect to B are given by


?st
?
?st?1
=
G(ot+1 ) (Ast?1 + B) = G(ot+1 ) A
+I
?B
?B
?B
These gradients are temporally recursive ? they implicitly depend on gradients from all previous
timesteps. It might seem prohibitive to compute them: must an algorithm examine all past t1 ? ? ? tt?1
data points to compute the gradient at time t? Fortunately, the answer is no: the necessary statistics
can be computed in a recursive fashion as the algorithm walks through the data.
5

Training LL
Testing LL
True LL
Naive LL

Log?likelihood

?1.4
?1.6

p

?2.07

1?p

A

q

B
1?q

?1.8
?2.08

0

10

20

?2
0

10

20

0

10

20 0

10

20

Iterations of optimization

(a)

(b)

Figure 2: Results on two-state POMDPs. The right shows the generic model used. By varying the
transition and observation probabilities, three different POMDPs were generated. The left shows
learning performance on the three models. Likelihoods for naive predictions are shown as a dotted
line near the bottom; likelihoods for optimal predictions are shown as a dash-dot line near the top.
Problem
Paint
Network
Tiger

# of
states
16
7
2

# of
obs.
2
2
2

# of
actions
4
4
3

Naive
LL
6.24
6.24
6.24

True
LL
4.66
4.49
5.23

Training set
LL
%
4.67 99.7
4.50 99.5
5.24 92.4

Test set
LL
%
4.66 99.9
4.52 98.0
5.25 86.0

Figure 3: Results on standard POMDPs. See text for explanation.

5

Inference

In order to compute the gradients needed for model learning, the expected sufficient statistics
E[?(F n |ht )] at each timestep must be computed
(see Eq. 4):
Z
E [?(F n |ht )] =

?(ft )p(F n |ht )dft = ?Z(s).

This quantity, also known as the mean parameters, is of central interest in standard exponential families, and has several interesting properties. For example, each possible set of canonical parameters
s induces one set of mean parameters; assuming that the features are linearly independent, each set
of valid mean parameters is uniquely determined by one set of canonical parameters [9].
Computing these marginals is an inference problem. This is repeated T times (the number of samples) in order to get one gradient, which is then used in an outer optimization loop; because inference
must be repeatedly performed in our model, computational efficiency is a more stringent requirement than accuracy. In terms of inference, our model inherits all of the properties of graphical
models, for better and for worse. Exact inference in our model is generally intractable, except in
the case of fully factorized or tree-structured graphs. However, many approximate algorithms exist: there are variational methods such as naive mean-field, tree-reweighted belief propagation, and
log-determinant relaxations [10]; other methods include Bethe-Kikuchi approximations, expectation
propagation, (loopy) belief propagation, MCMC methods, and contrastive divergence [1].

6

Experiments and Results

Two sets of experiments were conducted to evaluate the quality of our model and learning algorithm.
The first set tested whether the model could capture exact state, given the correct features and exact
inference. We evaluated the learned model using exact inference to compute the exact likelihood of
the data, and compared to the true likelihood. The second set tested larger models, for which exact
inference is not possible. For the second set, bounds can be provided for the likelihoods, but may be
so loose as to be uninformative. How can we assess the quality of the final model? One objective
gauge is control performance: if the model has a reward signal, reinforcement learning can be used
to determine an optimal policy. Evaluating the reward achieved becomes an objective measure of
model quality, even though approximate likelihood is the learning signal.
6

EFPSR/VMF

0.15

EFPSR/LBP

0.1

EFPSR/LDR
POMDP

0.05

Reactive

0

1

2
3
4
5
Steps of optimization

6

Random

Average Reward

Average Reward

0.2

0.15
0.1
0.05
0

1

2
3
4
5
Steps of optimization

6

Figure 4: Results on Cheesemaze (left) and Maze 4x3 (right) for different inference methods.
First set. We tested on three two-state problems, as well as three small, standard POMDPs. For
each problem, training and test sets were generated (using a uniformly random policy for controlled
systems). We used 10,000 samples, set n = 3 and used structure learning as explained in Section 4.1.
We used exact inference to compute the E[?(F n |ht )] term needed for the gradients. We optimized
the likelihood using BFGS. For each dataset, we computed the log-likelihood of the data under the
true model, as well as the log-likelihood of a ?naive? model, which assigns uniform probability
to every possible observation. We then learned the best model possible, and compared the final
log-likelihood under the learned and true models.
Figure 2 (a) shows results for three two-state POMDPs with binary observations. The left panel of
Fig. 2 (a) shows results for a two-state MDP. The likelihood of the learned model closely approaches
the likelihood of the true model (although it does not quite reach it; this is because the model
has trouble modeling deterministic observations, because the weights in the exponential need to be
infinitely large [or small] to generate a probability of one [or zero]). The middle panel shows results
for a moderately noisy POMDP; again, the learned model is almost perfect. The third panel shows
results for a very noisy POMDP, in which the naive and true LLs are very close; this indicates that
prediction is difficult, even with a perfect model.
Figure 3 shows results for three standard POMDPs, named Paint, Network and Tiger1 . The table conveys similar information to the graphs: naive and true log-likelihoods, as well as the loglikelihood of the learned models (on both training and test sets). To help interpret the results, we
also report a percentage (highlighted in bold), which indicates the amount of the likelihood gap (between the naive and true models) that was captured by the learned model. Higher is better; again we
see that the learned models are quite accurate, and generalize well.
Second set. We also tested on a two more complicated POMDPs called Cheesemaze and Maze
4x31 . For both problems, exact inference is intractable, and so we used approximate inference. We
experimented with loopy belief propagation (LBP) [12], naive mean field (or variational mean field,
VMF), and log-determinant relaxations (LDR) [10]. Since the VMF and LDR bounds on the loglikelihood were so loose (and LBP provides no bound), it was impossible to assess our model by an
appeal to likelihood. Instead, we opted to evaluate the models based on control performance.
We used the Natural Actor Critic (or NAC) algorithm [6] to test our model (see [11] for further
experiments). The NAC algorithm requires two things: a stochastic, parameterized policy which
operates as a function of state, and the gradients of the log probability of that policy. We used a
softmax function of a linear projection of the state: the probability of taking action ai from state st
 ? 	

	 P|A|
given the policy parameters ? is: p(ai ; st , ?) = exp s?
t ?i /
j=1 exp st ?j . The parameters
? are to be determined. For comparison, we also ran the NAC planner with the POMDP belief
state: we used the same stochastic policy and the same gradients, but we used the belief state of the
true POMDP in place of the EFPSR?s state (st ). We also tested NAC with the first-order Markov
assumption (or reactive policy) and a totally random policy.
Results. Figure 4 shows the results for Cheesemaze. The left panel shows the best control performance obtained (average reward per timestep) as a function of steps of optimization. The ?POMDP?
line shows the best reward obtained using the true belief state as computed under the true model,
the ?Random? line shows the reward obtained with a random policy, and the ?Reactive? line shows
the best reward obtained by using the observation as input to the NAC algorithm. The lines ?VMF,?
?LBP,? and ?LDR? correspond to the different inference methods.
1

From Tony Cassandra?s POMDP repository at http://www.cs.brown.edu/research/ai/pomdp/index.html

7

The EFPSR models all start out with performance equivalent to the random policy (average reward of
0.01), and quickly hop to of 0.176. This is close to the average reward of using the true POMDP state
at 0.187. The EFPSR policy closes about 94% of the gap between a random policy and the policy
obtained with the true model. Surprisingly, only a few iterations of optimization were necessary to
generate a usable state representation. Similar results hold for the Maze 4x3 domain, although the
improvement over the first order Markov model is not as strong: the EFPSR closes about 77.8% of
the gap between a random policy and the optimal policy. We conclude that the EFPSR has learned
a model which successfully incorporates information from history into the state representation, and
that it is this information which the NAC algorithm uses to obtain better-than-reactive performance.
This implies that the model and learning algorithm are useful even with approximate inference
methods, and even in cases where we cannot compare to the exact likelihood.

7

Conclusions

We have presented the Exponential Family PSR, a new model of controlled, stochastic dynamical
systems which provably unifies other models with predictively defined state. We have also discussed
a specific member of the EFPSR family, the Linear-Linear EFPSR, and a maximum likelihood learning algorithm. We were able to learn almost perfect models of several small POMDP systems, both
from a likelihood perspective and from a control perspective. The biggest drawback is computational: the repeated inference calls make the learning process very slow. Improving the learning
algorithm is an important direction for future research. While slow, the learning algorithm generates
models which can be accurate in terms of likelihood and useful in terms of control performance.

Acknowledgments
David Wingate was supported under a National Science Foundation Graduate Research Fellowship.
Satinder Singh was supported by NSF grant IIS-0413004. Any opinions, findings, and conclusions
or recommendations expressed in this material are those of the authors and do not necessarily reflect
the views of the NSF.

References
[1] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation,
14(8):1771?1800, 2002.
[2] E. T. Jaynes. Notes on present status and future prospects. In W. Grandy and L. Schick, editors, Maximum
Entropy and Bayesian Methods, pages 1?13, 1991.
[3] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for segmenting
and labeling sequence data. In International Conference on Machine Learning (ICML), 2001.
[4] M. L. Littman, R. S. Sutton, and S. Singh. Predictive representations of state. In Neural Information
Processing Systems (NIPS), pages 1555?1561, 2002.
[5] A. McCallum, D. Freitag, and F. Pereira. Maximum entropy Markov models for information extraction
and segmentation. In International Conference on Machine Learning (ICML), pages 591?598, 2000.
[6] J. Peters, S. Vijayakumar, and S. Schaal. Natural actor-critic. In European Conference on Machine
Learning (ECML), pages 280?291, 2005.
[7] S. D. Pietra, V. D. Pietra, and J. Lafferty. Inducing features of random fields. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 19(4):380?393, 1997.
[8] M. Rudary, S. Singh, and D. Wingate. Predictive linear-Gaussian models of stochastic dynamical systems.
In Uncertainty in Artificial Intelligence (UAI), pages 501?508, 2005.
[9] M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference.
Technical Report 649, UC Berkeley, 2003.
[10] M. J. Wainwright and M. I. Jordan. Log-determinant relaxation for approximate inference in discrete
Markov random fields. IEEE Transactions on Signal Processing, 54(6):2099?2109, 2006.
[11] D. Wingate. Exponential Family Predictive Representations of State. PhD thesis, University of Michigan,
2008.
[12] J. S. Yedida, W. T. Freeman, and Y. Weiss. Understanding belief propagation and its generalizations.
Technical Report TR-2001-22, Mitsubishi Electric Research Laboratories, 2001.

8

"
5180,"The Efficient Learning of Multiple Task
Sequences

Satinder P. Singh
Department of Computer Science
University of Massachusetts
Amherst, MA 01003

Abstract
I present a modular network architecture and a learning algorithm based
on incremental dynamic programming that allows a single learning agent
to learn to solve multiple Markovian decision tasks (MDTs) with significant transfer of learning across the tasks. I consider a class of MDTs,
called composite tasks, formed by temporally concatenating a number of
simpler, elemental MDTs. The architecture is trained on a set of composite and elemental MDTs. The temporal structure of a composite task is
assumed to be unknown and the architecture learns to produce a temporal decomposition. It is shown that under certain conditions the solution
of a composite MDT can be constructed by computationally inexpensive
modifications of the solutions of its constituent elemental MDTs.

1

INTRODUCTION

Most applications of domain independent learning algorithms have focussed on
learning single tasks. Building more sophisticated learning agents that operate in
complex environments will require handling multiple tasks/goals (Singh, 1992). Research effort on the scaling problem has concentrated on discovering faster learning
algorithms, and while that will certainly help, techniques that allow transfer of
learning across tasks will be indispensable for building autonomous learning agents
that have to learn to solve multiple tasks . In this paper I consider a learning agent
that interacts with an external, finite-state, discrete-time, stochastic dynamical environment and faces multiple sequences of Markovian decision tasks (MDTs).
251

252

Singh

Each MDT requires the agent to execute a sequence of actions to control the environment, either to bring it to a desired state or to traverse a desired state trajectory
over time. Let S be the finite set of states and A be the finite set of actions available
to the agent. l At each time step t, the agent observes the system's current state
Zt E S and executes action at E A. As a result, the agent receives a payoff with
expected value R(zt, at) E R and the system makes a transition to state Zt+l E S
with probability P:r:t:r:t+l (at). The agent's goal is to learn an optimal closed loop
control policy, i.e., a function assigning actions to states, that maximizes the agent's
objective. The objective used in this paper is J = E~o -yt R(zt, at), i.e., the sum
of the payoffs over an infinite horizon. The discount factor, 0 ~ ""Y ~ I, allows
future payoff to be weighted less than more immediate payoff. Throughout this
paper, I will assume that the learning agent does not have access to a model of the
environment. Reinforcement learning algorithms such as Sutton's (1988) temporal
difference algorithm and Watkins's (1989) Q-Iearning algorithm can be used to learn
to solve single MDTs (also see Barto et al., 1991).
I consider compositionally-structured MDTs because they allow the possibility of
sharing knowledge across the many tasks that have common subtasks. In general,
there may be n elemental MDTs labeled T I , T2 , ??? , Tn. Elemental MDTs cannot be
decomposed into simpler subtasks. Compo8ite MDTs, labeled G I , G2 , ??? , Gm , are
produced by temporally concatenating a number of elemental MDTs. For example,
G; = [T(j, I)T(j, 2) ... T(j, k)] is composite task j made up of k elemental tasks that
have to be performed in the order listed. For 1 $ i $ k, T(j, i) E {TI' T2 , ??? , Tn} is
the itk elemental task in the list for task G;. The sequence of elemental tasks in a
composite task will be referred to as the decompo8ition of the composite task; the
decomposition is assumed to be unknown to the learning agent.

Compo8itional learning involves solving a composite task by learning to compose
the solutions of the elemental tasks in its decomposition. It is to be emphasized that
given the short-term, evaluative nature of the payoff from the environment (often
the agent gets informative payoff only at the completion of the composite task),
the task of discovering the decomposition of a composite task is formidable. In this
paper I propose a compositional learning scheme in which separate modules learn
to solve the elemental tasks, and a task-sensitive gating module solves composite
tasks by learning to compose the appropriate elemental modules over time.

2

ELEMENTAL AND COMPOSITE TASKS

All elemental tasks are MDTs that share the the same state set S, action set A, and
have the same environment dynamics. The payoff function for each elemental task
11, 1 ~ i ~ n, is ~(z, a) = EYES P:r:y(a)ri(Y) - c(z, a), where ri(Y) is a positive
reward associated with the state Y resulting from executing action a in state Z for
task 11, and c(z, a) is the positive cost of executing action a in state z. I assume
that ri(z) = 0 if Z is not the desired final state for 11. Thus, the elemental tasks
share the same cost function but have their own reward functions.
A composite task is not itself an MDT because the payoff is a function of both
lThe extension to the case where different sets of actions are available in different states
is straightforward.

The Efficient Learning of Multiple Task Sequences

the state and the current elemental task, instead of the state alone. Formally, the
new state set 2 for a composite task, S', is formed by augmenting the elements of
set S by n bits, one for each elemental task. For each z, E S', the projected 3tate
z E S is defined as the state obtained by removing the augmenting bits from z'.
The environment dynamics and cost function, c, for a composite task is defined by
assigning to each z, E S' and a E A the transition probabilities and cost assigned
to the projected state z E S and a E A. The reward function for composite task
C j , rj, is defined as follows. rj( z') ;::: 0 if the following are all true: i) the projected
state z is the final state for some elemental task in the decomposition of Cj, say
task Ii, ii) the augmenting bits of z' corresponding to elemental tasks appearing
before and including sub task Ti in the decomposition of C j are one, and iii) the rest
of the augmenting bits are zero; rj(z') 0 everywhere else.

=

3

COMPOSITIONAL Q-LEARNING

Following Watkins (1989), I define the Q-value, Q(z,a), for z E S and a E A, as the
expected return on taking action a in state z under the condition that an optimal
policy is followed thereafter. Given the Q-values, a greedy policy that in each state
selects an action with the highest associated Q-value, is optimal. Q-Iearning works
as follows. On executing action a in state z at time t, the resulting payoff and next
state are used to update the estimate of the Q-value at time t, Qt(z, a):
(1.0 - Qt)Qt(z, a)

+ ae[R(z, a) + l' max
Qt(Y, a')],
a'EA

(1)

where Y is the state at time t + 1, and at is the value of a positive learning rate
parameter at time t. Watkins and Dayan (1992) prove that under certain conditions
on the sequence {at}, if every state-action pair is updated infinitely often using
Equation 1, Qt converges to the true Q-values asymptotically.
Compositional Q-Iearning (CQ-Iearning) is a method for constructing the Q-values
of a composite task from the Q-values of the elemental tasks in its decomposition.
Let QT.(z,a) be the Q-value of (z,a), z E S and a E A, for elemental task Ii,
and let Q~:(z',a) be the Q-value of (z', a), for z' E S' and a E A, for task Ii
when performed as part of the composite task Cj
[T(j, 1) ... T(j, k)]. Assume
Ii = T(j, I) . Note that the superscript on Q refers to the task and the subscript
refers to the elemental task currently being performed. The absence of a superscript
implies that the task is elemental.

=

=

1) MDTs that have compositional structure
Consider a set of undiscounted (1'
and satisfy the following conditions:
(AI) Each elemental task has a single desired final state.
(A2) For all elemental and composite tasks, the expected value of undiscounted
return for an optimal policy is bounded both from above and below for all states.
(A3) The cost associated with each state-action pair is independent of the task
being accomplished.
2The theory developed in this paper does not depend on the particular extension of S
chosen, as long as the appropriate connection between the new states and the elements of
S can be made.

253

254

Singh

(A4) For each elemental task 71, the reward function ri is zero for all states except
the desired final state for that task. For each composite task C j , the reward function
rj is zero for all states except pouibly the final states of the elemental tasks in its
decomposition (Section 2).
Then, for any elemental task Ii and for all composite tasks C j containing elemental
task 71, the following holds:

Q~:(z',a)

QT.(Z, a)

+ K(Cj,T(j, I?,

(2)

for all z' E S' and a E A, where z E S is the projected state, and K (Cj, T(j, I? is a
function of the composite task Cj and subtask T(j, I), where Ti T(j, I). Note that
K( Cj , T(j, I? is independent of the state and the action. Thus, given solutions of
the elemental tasks, learning the solution of a composite task with n elemental tasks
requires learning only the values of the function K for the n different subtasks. A
proof of Equation 2 is given in Singh (1992).

=

a

WIll.
NoIN

N(O.G)
Q
Networtc
1

Q

Q
? ?? Networtt
n

Figure 1: The CQ-Learning Architecture (CQ-L). This figure is adapted from Jacobs
et al. (1991). See text for details.
Equation 2 is based on the assumption that the decomposition of the composite
tasks is known. In the next Section, I present a modular architecture and learning
algorithm that simultaneously discovers the decomposition of a composite task and
implements Equation 2.

4

CQ-L: CQ-LEARNING ARCHITECTURE

Jacobs (1991) developed a modular connectionist architecture that performs task
decomposition. Jacobs's gating architecture consists of several expert networks and
a gating network that has an output for each expert network. The architecture
has been used to learn multiple non-sequential tasks within the supervised learning

The Efficient Learning of Multiple Task Sequences

Table 1: Tasks. Tasks Tl, T2, and T3 are elemental tasks; tasks G l , G2 , and G3
are composite tasks. The last column describes the compositional structure of the
tasks.
Label
'11

T2
T3

01
C2
C3

Command
000001
000010
000100
001000
010000
100000

De.eription
A
B
C

VlS1t
VlS1t
V1S1t
VlSlt
VlS1t
V1S1t

Deeompo.ition
Tl
T2
T3

A and then C

1113

B and then C
A, then B and then C

T2 T 3
T1 T2T3

paradigm. I extend the modular network architecture to a CQ-Learning architecture (Figure I), called CQ-L, that can learn multiple compositionally-structured
sequential tasks even when training information required for supervised learning is
not available. CQ-L combines CQ-learning and the gating architecture to achieve
transfer of learning by ""sharing"" the solutions of elemental tasks across multiple
composite tasks. Only a very brief description of the CQ-L is provided in this
paper; details are given in Singh (1992) .
In CQ-L the expert networks are Q-learning networks that learn to approximate
the Q-values for the elemental tasks. The Q-networks receive as input both the
current state and the current action. The gating and bias networks (Figure 1)
receive as input the augmenting bits and the task command used to encode the
current task being performed by the architecture. The stochastic switch in Figure 1
selects one Q-network at each time step. CQ-L's output, Q, is the output of the
selected Q-network added to the output of the bias network.
The learning rules used to train the network perform gradient descent in the log
likelihood, L(t), of generating the estimate of the desired Q-value at time t, denoted
D(t), and are given below:
8 log L(t)

qj(t) + oQ 8qj(t) ,
Si(t)

+ Og

8 log L(t)
8Si(t)

,and

b(t) + ob(D(t) - Q(t)),
where qj is the output of the jt"" Q-network, Si is the it"" output of the gating
network, b is the output of the bias network, and 0Q, Ob and Og are learning rate
parameters. The backpropagation algorithm ( e.g., Rumelhart et al., 1986) was
used to update the weights in the networks. See Singh (1992) for details.

5

NAVIGATION TASK

To illustrate the utility of CQ-L, I use a navigational test bed similar to the one used
by Bachrach (1991) that simulates a planar robot that can translate simultaneously

255

256

Singh

c

G

Figure 2: Navigation Testbed. See text for details.

and independently in both ~ and y directions. It can move one radius in any
direction on each time step. The robot has 8 distance sensors and 8 gray-scale
sensors evenly placed around its perimeter. These 16 values constitute the state
vector. Figure 2 shows a display created by the navigation simulator. The bottom
portion of the figure shows the robot's environment as seen from above. The upper
panel shows the robot's state vector. Three different goal locations, A, B, and C,
are marked on the test bed. The set of tasks on which the robot is trained are shown
in Table 1. The elemental tasks require the robot to go to the given goal location
from a random starting location in minimum time. The composite tasks require the
robot to go to a goal location via a designated sequence of subgoallocations.
Task commands were represented by standard unit basis vectors (Table 1), and thus
the architecture could not ""parse"" the task command to determine the decomposition of a composite task. Each Q-network was a feedforward connectionist network
with a single hidden layer containing 128 radial basis units. The bias and gating
networks were also feedforward nets with a single hidden layer containing sigmoid
units. For all ~ E S U Sf and a E A, c(~, a)
-0.05. ri(~)
1.0 only if ~ is the
desired final state of elemental task Ii, or if ~ E Sf is the final state of composite
task Cii ri(~) = 0.0 in all other states. Thus, for composite tasks no intermediate
payoff for successful completion of subtasks was provided.

=

6

=

SIMULATION RESULTS

In the simulation described below, the performance of CQ-L is compared to the
performance of a ""one-for-one"" architecture that implements the ""learn-each-taskseparately"" strategy. The one-for-one architecture has a pre-assigned distinct net-

The Efficient Learning of Multiple Task Sequences

work for each task, which prevents transfer of learning. Each network of the onefor-one architecture was provided with the augmented state.

-

,oo

..,.

?

1I ..

8.
I

t

t??

.-..

- --

'I

COA.

ON.?FOA-ONE

""
'

1I

,

0
0

f .... ,

'I

'~ ,~' I',

1oo

'; V\

'I

?

.'t .
, , ,1,1

...

...

'

...

,

,- ,

Trial Nurrber (for T..k [AB))

- - ---

t-

o',

.. I,' I

TrW NIJ1rioer (for T..k A)

...
...

COA.
ONE-FOA.oNE

,

?

1

0

-- -

1-

..
,

C<>L

Til.. Number (fer TMk [ABC))

Figure 3: Learning Curves for Multiple tasks.
Both CQ-L and the one-for-one architecture were separately trained on the six
tasks T 1 , T2, T3 , C lI C 2 , and C 3 until they could perform the six tasks optimally.
CQ-L contained three Q-networks, and the one-for-one architecture contained six
Q-networks. For each trial, the starting state of the robot and the task identity
were chosen randomly. A trial ended when the robot reached the desired final state
or when there was a time-out. The time-out period was 100 for the elemental tasks,
200 for C 1 and C 2 , and 500 for task C 3 ? The graphs in Figure 3 show the number
of actions executed per trial. Separate statistics were accumulated for each task.
The rightmost graph shows the performance of the two architectures on elemental
task TI. Not surprisingly, the one-for-one architecture performs better because
it does not have the overhead of figuring out which Q-network to train for task
T 1 . The middle graph shows the performance on task C I and shows that the CQL architecture is able to perform better than the one-for-one architecture for a
composite task containing just two elemental tasks. The leftmost graph shows the
results for composite task C 3 and illustrates the main point of this paper. The onefor-one architecture is unable to learn the task, in fact it is unable to perform the
task more than a couple of times due to the low probability of randomly performing
the correct task sequence.
This simulation shows that CQ-L is able to learn the decomposition of a composite
task and that compositional learning, due to transfer of training across tasks, can
be faster than learning each composite task separately. More importantly, CQ-L
is able to learn to solve composite tasks that cannot be solved using traditional
schemes.

7

DISCUSSION

Learning to solve MDTs with large state sets is difficult due to the sparseness of the
evaluative information and the low probability that a randomly selected sequence
of actions will be optimal. Learning the long sequences of actions required to solve
such tasks can be accelerated considerably if the agent has prior knowledge of useful
subsequences. Such subsequences can be learned through experience in learning to

257

258

Singh

solve other tasks. In this paper, I define a class of MOTs, called composite MOTs,
that are structured as the temporal concatenation of simpler MOTs, called elemental MOTs. I present CQ-L, an architecture that combines the Q-Iearning algorithm
of Watkins (1989) and the modular architecture of Jacobs et al. (1991) to achieve
transfer of learning by sharing the solutions of elemental tasks across multiple composite tasks. Given a set of composite and elemental MOTs, the sequence in which
the learning agent receives training experiences on the different tasks determines the
relative advantage of CQ-L over other architectures that learn the tasks separately.
The simulation reported in Section 6 demonstrates that it is possible to train CQ-L
on intermixed trials of elemental and composite tasks. Nevertheless, the ability of
CQ-L to scale well to complex sets of tasks will depend on the choice of the training
sequence.
Acknowledgements

This work was supported by the Air Force Office of Scientific Research, Bolling
AFB, under Grant AFOSR-89-0526 and by the National Science Foundation under
Grant ECS-8912623. I am very grateful to Andrew Barto for his extensive help in
formulating these ideas and preparing this paper.
References

J . R. Bachrach. (1991) A connectionist learning control architecture for navigation. In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, Adv4nce6 in
Neural Information Proceuing Sy6tem6 3, pages 457-463, San Mateo, CA. Morgan
Kaufmann.
A. G. Barto, S. J. Bradtke, and S. P. Singh. (1991) Real-time learning and control
using asynchronous dynamic programming. Technical Report 91-57, University of
Massachusetts, Amherst, MA. Submitted to AI Journal.
R. A. Jacobs. (1990) T46lc decomp06ition through competition in a modular connectioni6t architecture. PhD thesis, COINS dept, Univ. of Massachusetts, Amherst,
Mass. U.S.A.
R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. (1991) Adaptive
mixtures of local experts. Neural Computation, 3( 1).
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. (1986) Learning internal representations by error propagation. In D. E. Rumelhart and J. L. McClelland, editors,
Parallel Distributed Proceuing: E:cploration6 in the Micr06tructure of Cognition,
vol.1: Found4tion6. Bradford Books/MIT Press, Cambridge, MA.
S. P. Singh. (1992) Transfer of learning by composing solutions for elemental sequential tasks. Machine Learning.
R. S. Sutton. (1988) Learning to predict by the methods of temporal differences.
Machine Learning, 3:9-44.
C. J . C. H. Watkins. (1989) Learning from Delayed Rewards. PhD thesis, Cambridge Univ., Cambridge, England.
C. J. C. H. Watkins and P. Dayan. (1992) Q-learning. Machine Learning.

"
1643,"Subject-Independent Magnetoencephalographic
Source Localization by a Multilayer Perceptron

Sung C. Jun
Biological and Quantum Physics Group
MS-D454, Los Alamos National Laboratory
Los Alamos, NM 87545, USA
jschan@lanl.gov

Barak A. Pearlmutter
Hamilton Institute
NUI Maynooth
Maynooth, Co. Kildare, Ireland
barak@cs.may.ie

Abstract
We describe a system that localizes a single dipole to reasonable accuracy from noisy magnetoencephalographic (MEG) measurements in real
time. At its core is a multilayer perceptron (MLP) trained to map sensor signals and head position to dipole location. Including head position
overcomes the previous need to retrain the MLP for each subject and session. The training dataset was generated by mapping randomly chosen
dipoles and head positions through an analytic model and adding noise
from real MEG recordings. After training, a localization took 0.7 ms with
an average error of 0.90 cm. A few iterations of a Levenberg-Marquardt
routine using the MLP?s output as its initial guess took 15 ms and improved the accuracy to 0.53 cm, only slightly above the statistical limits
on accuracy imposed by the noise. We applied these methods to localize
single dipole sources from MEG components isolated by blind source
separation and compared the estimated locations to those generated by
standard manually-assisted commercial software.

1

Introduction

The goal of MEG/EEG localization is to identify and measure the signals emitted by electrically active brain regions. A number of methods are in widespread use, most assuming
dipolar sources (H?am?al?ainen et al., 1993). Recently MLPs (Rumelhart et al., 1986) have
become popular for building fast dipole localizers (Abeyratne et al., 1991; Kinouchi et al.,
1996). Since it is easy to use a forward model to create synthetic data consisting of dipole
locations and corresponding sensor signals, one can train a MLP on the inverse problem.
Hoey et al. (2000) took EEG measurements for both spherical and realistic head models
and trained MLPs on randomly generated noise-free datasets. Integrated approaches to the
EEG/MEG dipole source localization, in which the trained MLPs are used as initializers
for iterative methods, have also been studied (Jun et al., 2002) along with distributed output
representations (Jun et al., 2003). Interestingly, all work to date trained with a fixed head
model. However, for MEG, head movement relative to the fixed sensor array is very difficult to avoid, and even with heroic measures (bite bars) the position of the head relative
to the sensor array varies from subject to subject and session to session. This either results
in significant localization error (Kwon et al., 2002), or requires laborious retraining and

revalidation of the system.
We propose an augmented system which takes head position into account, yet remains
able to localize a single dipole to reasonable accuracy within a fraction of a millisecond
on a standard PC, even when the signals are contaminated by considerable noise. The
system uses a MLP trained on random dipoles and random head positions, which takes
as inputs both the coordinates of the center of a sphere fitted to the head and the sensor
measurements, uses two hidden layers, and generates the source location (in Cartesian
coordinates) as its output. Adding head position as an extra input overcomes the primary
practical limitation of previous MLP-based MEG localization systems: the need to retrain
the network for each new head position.
We use an analytical model of quasi-static
electromagnetic propagation through a
spherical head to map randomly chosen dipoles and head positions to superconducting quantum interference device
(SQUID) sensor activities according to the
sensor geometry of a 4D Neuroimaging
Neuromag-122 MEG system, and trained
a MLP to invert this mapping in the presence of real brain noise. To improve the localization accuracy we use a hybrid MLPstart-LM method, in which the MLP?s
output provides the starting point for a
Levenberg-Marquardt (LM) optimization
(Press et al., 1988). We use the MLP and
MLP-start-LM methods to localize singledipole sources from actual MEG signal
components isolated by a blind source separation (BSS) algorithm (Vig?ario et al.,
2000; Tang et al., 2002) and compare the
results with the output of standard interactive commercial localization software.

Sensor surface

11.343 cm
10.851 cm

13.594 cm

10.851cm

3.605 cm

z

z
x

y

Saggital View

Coronal View

Head Model B

10.5 cm
7.5 cm
Head Model A

7.5 cm
A

B
3 cm

Training Region

4 cm

Training Region and various Head Models

Figure 1: Sensor surface and training region.
The center of the spherical head model was
varied within the given region. Diamonds denote sensors.

Section 2 describes our synthetic data, the
forward model, the noise used to additively contaminate the training data, and the MLP
structure. Section 3 presents the localization performance of both the MLP and MLP-startLM, and compares them with various conventional LM methods. In Section 3.2, comparative localization results for our proposed methods and standard Neuromag commercial
software on actual BSS-separated MEG signals are presented.

2

Data and MLP structure

We constructed noisy data using the procedure of Jun et al. (2002), except that an additional
input was associated with each exemplar, namely the (x, y, z) coordinates of the center of
a sphere fitted to the head, and the forward model was modified to account for this offset.
Each exemplar thus consisted of the (x, y, z) coordinates of the center of a sphere fitted to
the head, sensor activations generated by a forward model, and the target dipole location.1
We made two datasets: one for training and another for testing. Centers of spherical head
1
Given the sensor activations and a dipole location, the minimum error dipole moment can be
calculated analytically (H?am?al?ainen et al., 1993). Therefore, although the dipoles used in generating
the dataset had both location and moment, the moments were not included in the datasets used for
training or testing.

models in the training set were drawn from a ball of radius 3 cm centered 4 cm above the
bottom of the training region,2 as shown in Figure 1. The dipoles in the training set were
drawn uniformly from a spherical region centered at the corresponding center, with a radius
of 7.5 cm, and truncated at the bottom. Their moments were drawn uniformly from vectors
of strength ?200 nAm. The corresponding sensor activations were calculated by adding
the results of a forward model and a noise model. To check the performance of the network
during training, a test set was generated in the same fashion as the training set. We used the
sensor geometry of a 4D Neuroimaging Neuromag-122 whole-head gradiometer (Ahonen
et al., 1993) and a standard analytic model of quasistatic electromagnetic propagation in a
spherical head (Jun et al., 2002).
This work could be easily extended to a more realistic head model. In that case the integral
equations are solved by the boundary element method (BEM) or the finite element method
(FEM) numerically (H?am?al?ainen et al., 1993). The human skull phantom study in Leahy
et al. (1998) shows that the fitted spherical head model for MEG localization is slightly
inferior in accuracy to the realistic head model numerically calculated by BEM. In forward
calculation, a spherical head model has some advantages: it is more easily implemented and
is much faster. Despite its inferiority in terms of localization accuracy, we use a spherical
head model in this work.
In order to properly compare the performance of various localizers, we need a dataset for
which we know the ground truth, but which contains the sorts of noise encountered in
actual MEG recordings. To this end, we measured real brain noise and used it to additively
contaminate synthetic sensor readings (Jun et al., 2002). This noise was taken, unaveraged,
from MEG recordings during periods in which the brain region of interest in the experiment
was quiescent, and therefore included all sources of noise present in actual data: brain
noise, external noise, sensor noise, etc. This had a RMS (square root of mean square)
magnitude of roughly P n = 50?200 fT/cm, where we measure the SNR of a dataset using
the ratios of the powers in the signal and noise, SNR (in dB) = 20 log10 P s /P n , where P s
and P n are the RMS sensor readings from the dipole and noise, respectively. The datasets
used for training and testing were made by adding the noise to synthetic sensor activations
generated by the forward model, and exemplars whose resulting SNR was below ?4 dB
were rejected.
The MLP charged with approximating the inverse mapping had an input layer of 125 units
consisting of the three Cartesian coordinate of the center of the sphere fitted to the head, and
the 122 sensor activations. It had two hidden layers with 320 and 30 units respectively, and
an output layer of three units representing the Cartesian coordinates of the fitted dipole. The
output units had linear activation functions, while the hidden unit had hyperbolic tangent
activation functions. Adjacent layers were fully connected, with no cut-through connections. The 122 sensor activation inputs were scaled to an RMS value of 0.5, and the target
outputs were scaled into [?1, +1]. The network weights were initialized with uniformly
distributed random values between ?0.1, and online stochastic gradient decent with no momentum and an empirically chosen constant of proportionality was used for optimization.

2
Fitted spheres from twelve subjects performing various tasks on a 4D Neuroimaging Neuromag122 MEG system were collected, and this distribution of head positions was chosen to include all
twelve cases. Just as the position of the center of the head varies from session to session and subject to
subject, so does head orientation and radius. Because a sphere is rotationally symmetric, our forward
model is insensitive to orientation, and similarly the external magnetic field caused by a dipole in a
homogeneous sphere is invariant to the sphere?s radius. On the other hand, the noise process would
not be invariant to orientation or radius, so we might expect a slight increase in performance if the
network had orientation and radius available as inputs, rather than just the position of the center.

14

14
Top

Top

12

12

10

10

8

0.58

8

0.59

6
0.85

0.85

1.04

4
0.58

0.6
0.79

Left

0

0.81

0.9

Right

0.7

1.07

1.37
0.89

Back

2
0

1.78

?2

1.08

1.52

Front

1.71

?2

0.81

?4

0.92

1.1

1.67

2.42

?4

?6

?15

0.82

0.87

4
2

0.92

6

?6

?10

?5

0

5

10

15

?15

?10

?5

0

5

10

15

Figure 2: Mean localization errors of the trained MLP as a function of correct dipole location, binned into regions. All units are in cm. Left: Coronal cross section. Right: Sagittal
cross section.

3.1

Results and discussion
Training and localization results

Datasets of 100,000 (training) and 25,000
(testing) patterns, all contaminated by real
brain noise, were constructed. As is typical, the incremental gains per epoch decrease exponentially with training. From
the training curves (not shown) it is
evident that additional training would
have further decreased the error, but we
nonetheless stopped after 1000 epochs,
which took about three days on 2.8 GHz
Intel Xeon CPU.

2.5

Mean Localization Error (cm)

3

fixed?4?start?LM
MLP
MLP?start?LM
optimal?start?LM

2

1.5

1

0.5

0

0

5

10

15

S/N (dB)

We investigated localization error distributions over various regions of interest. We Figure 3: Mean localization error vs. SNR.
considered two cross sections (coronal and MLP, MLP-start-LM, and optimal-start-LM
sagittal views) with width of 2 cm, and were tested on signals from 25,000 random
each of these was divided into 19 regions, dipoles, contaminated by real brain noise.
as shown in Figure 2. We extracted the noisy signals and the corresponding dipoles from
testing datasets. For each region 49?500 patterns were collected. A dipole localization was
performed using the trained MLP, and the average localization error for each region was
calculated. Figure 2 shows the localization error distribution over two cross sections. In
general, dipoles closer to the sensor surface were better localized.
We compared various automatic localization methods, most of which consist of LM used
in different ways:
? MLP-start-LM
LM was started with the trained MLP?s output.
? fixed-4-start-LM
LM was tuned for good performance using restarts at the four fixed initial points
(0, 0, 6), (?5, 2, ?1), (5, 2, ?1), and (0, ?5, ?1), in units of cm relative to the
center of the spherical head model. The best result among four results was chosen.

Table 1: Comparison of performance on real brain noise test set of Levenberg-Marquardt
source localizers with three LM restarts strategies, the trained MLP, and a hybrid system.
Each number is an average over 25,000 localizations, so the error bars are negligible.
Computation Localization
Algorithm
time (ms)
error (cm)
fixed-4-start-LM
random-20-start-LM
optimal-start-LM
MLP
MLP-start-LM

120
663
14
0.7
15

0.83
0.54
0.49
0.90
0.53

? random-n-start-LM
LM was restarted with n random (uniformly distributed) points within the spherical head model. We checked how many restarts were needed to match the accuracy
of the MLP-start-LM, yielding n = 20, which is the same as in Jun et al. (2002).
? optimal-start-LM
LM was started with the known exact dipole source location.
Figure 3 shows the localization performance as a function of SNR for fixed-4-start-LM,
optimal-start-LM, the trained MLP, and MLP-start-LM. Optimal-start-LM shows the best
localization performance across the whole range of SNRs, but the hybrid system shows
almost the same performance as optimal-start-LM except at very high SNRs, while the
trained MLP is more robust to noise than fixed-4-start-LM. In this experiment, most of the
sources with very high SNR were superficial, located around the upper neck or back of the
head. These sorts of sources are often very hard to localize well, as it is easy to become
trapped in a local minimum (Jun et al., 2002). It is expected that, under these conditions,
a better initial guess than the MLP output (which are 0.7 cm on average from the exact
source) would be required to obtain near-optimal performance from LM.
A grand summary, averaged across various SNR conditions, is shown in Table 1. The
trained MLP is fastest, and its hybrid system is about 40? faster than random-20-start-LM,
while the hybrid system is about 9? faster, yet more accurate than, fixed-4-start-LM. This
means that MLP-start-LM was about two times faster than might be naively expected.
3.2

Localization on real MEG signals and comparison with commercial software

The sensors in MEG systems have poor signal-to-noise ratios (SNRs) for single-trial data,
since MEG data is strongly contaminated by various noises. Blind source separation of
MEG data segregates noise from signal (Vig?ario et al., 2000; Tang et al., 2000a; Sander
et al., 2002), raising the SNR sufficiently to allow single-trial analysis (Tang et al., 2000b).
Even though the sensor attenuation vectors of the BSS-separated components can be well
localized to equivalent current dipoles (Vig?ario et al., 2000; Tang et al., 2002), the recovered field maps can be quite noisy. We applied the MLP and MLP-start-LM to localize
single dipolar sources from various actual BSS-separated MEG signals.3 The xfit program
3

Continuous 300 Hz MEG data for four right-handed subjects was collected using a cognitive
protocol developed by Michael P. Weisend, band-pass filtered at 0.03?100 Hz, separated using second order blind identification algorithm (SOBI), and scanned for neuronal sources of interest. The
following four visual reaction time tasks were performed by each subject: stimulus pre-exposure
task, trump card task, elemental discrimination task, and transverse patterning task. For each subject,
all four experiments were performed on the same day, but each in a separate session. Subjects were
permitted to move their heads between experiments.

15

20
MLP?start?LM
MLP
xfit

10

20
MLP?start?LM
MLP
xfit

15

5

MLP?start?LM
MLP
xfit

15

10

10
PV

0

PV

5

5
SV

SV

SV

PV

?5

0

0

?10

?5

?5

?15
?15

?10

?5

0

5

10

15

?10
?15

?10

?5

0

5

10

15

?10
?15

?10

?5

0

5

10

15

Figure 4: Dipole source localization results of Neuromag software (xfit), our MLP, MLPstart-LM for four BSS-separated primary visual and four secondary visual MEG signal
components of S01, over four sorts of tasks. PV and SVdenote primary visual source and
secondary visual source, respectively. Left: Axial view. Center: Coronal view. Right:
Sagittal view. The outer surface denotes the sensor surface, and diamonds on this surface
denote sensors. The inner surface denotes a spherical head model fit to the subject.
(standard commercial software bundled with the 4D Neuroimaging Neuromag-122 MEG
system) is compared with the methods developed here.
A field map of each component was scaled to an RMS of 0.5 and inputed to the trained
MLP. Their MLP?s outputs were scaled back to their dipole location vectors and were used
for initializing LM. Figure 4 shows the dipole locations estimated by the MLP, MLP-startLM, and Neuromag?s xfit software, for two sorts of sensory sources: primary visual sources
and secondary visual sources, respectively, over four tasks in subject S01. In Figure 5, the
estimated dipole locations are shown for somatosensory sources over three different subjects. Each figure consists of three viewpoints: axial (x-y plane), coronal (x-z plane), and
sagittal (y-z plane). The center of a fitted spherical head model (S01: trump card task) is
(0.335, 0.698, 3.157). All units are in cm. All dipole locations estimated by the MLP and
MLP-start-LM are clustered within about 3 cm, and about 0.7 cm, of xfit?s results, respectively. We see that the primary visual sources are more consistently localized, across all
four tasks, than the secondary visual sources. The secondary sources also had more variable stimulus-locked average time courses (Tang and Pearlmutter, 2003). It is noticeable
that somatosensory sources on the right hemisphere are localized poorly by the MLP, but
well localized by the hybrid method. Even though the auditory sources are the weakest (not
shown here), i.e. have the lowest SNRs, they are reasonably well localized.
While the MLP-estimated location is about 1.16 cm (|dx| ? 0.90, |dy| ? 0.57, |dz| ?
0.46) on average (N = 14) from those of xfit, the hybrid method?s result is about 0.35 cm
(|dx| ? 0.20, |dy| ? 0.22, |dz| ? 0.10) from xfit?s estimated location. Considering that
xfit had extra information, namely the identity of a subset of the sensors to use, this hybrid
method result is believed to be almost as good as the xfit result. The trained MLP and the
hybrid method are applicable to actual MEG signals, and seem to offer comparable and
perhaps superior localization relative to xfit, with clear advantages in both speed and in the
lack of required human interaction or subjective human input.
SOBI was performed on continuous 122-channel data collected during the entire period of the
experiment. It generated 122 components, each a one-dimensional time series with an associated field
map. Event triggered averages were calculated from their continuous single-trial time series for all
122 separated components. A dipole fitting method was applied to the identified neural components.
The input to the dipole fitting algorithm of xfit was the field map and the output was the location of
ECDs. From all separated components for four subjects and four sorts of tasks taken as in Tang et al.
(2002). only fourteen components were localized and compared. For further experimental details and
a detailed SOBI algorithm, see Tang et al. (2002).

15

20
MLP?start?LM
MLP
xfit

10

20
MLP?start?LM
MLP

15

5

10

10

0

5

5

?5

0

0

?10

?5

y

?15
?15

?10

?5

0

5

10

?5

z

x

MLP?start?LM
MLP
xfit

15

xfit

z

x

15

?10
?15

?10

?5

0

5

10

y

15

?10
?15

?10

?5

0

5

10

15

Figure 5: Dipole source localization results of Neuromag software (xfit), our MLP, MLPstart-LM for three real BSS-separated somatosensory MEG signal components from the
transverse patterning task over three different subjects (S01, S02, S03). Even the center of
a fitted spherical head model is varied over three subjects, the only fitted sphere of subject
S01 transverse patterning task, centered at (0.373, 0.642, 3.205), is depicted. Left: Axial
view. Center: Coronal view. Right: Sagittal view. The outer surface denotes the sensor
surface, and diamonds on this surface denote sensors. The inner surface denotes a spherical
head model fit to the subject.

4

Conclusion

We propose the inclusion of a head position input for MLP-based MEG dipole localizers.
This overcomes the limitation of previous MLP-based MEG localization systems, namely
the need to retrain the network for each session or subject. Experiments showed that the
trained MLP was far faster, albeit slightly less accurate, than fixed-4-start-LM. This motivated us to construct a hybrid system, MLP-start-LM, which improves the localization
accuracy while reducing the computational burden to less than one ninth than that of fixed4-start-LM. This hybrid method was comparable in accuracy to random-20-start-LM, at
1/40-th the computation burden, which is about two times faster than might be naively
expected. Over the whole range of SNRs, the hybrid system showed almost as good performance in accuracy and computation time as the hypothetical optimal-start-LM.
We applied the MLP and MLP-start-LM to localize single dipolar sources from actual BSSseparated MEG signals, and compared these with the results of the commercial Neuromag
program xfit. The MLP yielded dipole locations close to those of xfit, and MLP-start-LM
gave locations that were even closer to those of xfit.
In conclusion, our MLP can itself serve as a reasonably accurate real-time MEG dipole
localizer, even when the head position changes regularly. This MLP also constitutes an
excellent dipole guessor for LM. Because this MLP receives a head position input, the
need to retrain for various subjects or sessions has been eliminated without sacrificing the
many advantages of the universal approximator direct inverse approach to localization.
Acknowledgements
This work was supported by NSF CAREER award 97-02-311, the Mental Illness and Neuroscience Discovery Institute, a gift from the NEC Research Institute, NIH grant 2 R01
EB000310-05, and Science Foundation Ireland grant 00/PI.1/C067. We would like to thank
Guido Nolte for help with the forward model, Michael Weisend for allowing us to use his
data, and Michael Weisend, Akaysha Tang, and Natalie Malaszenko for providing experimental details.

References
Abeyratne, U. R., Kinouchi, Y., Oki, H., Okada, J., Shichijo, F., and Matsumoto, K. (1991).
Artificial neural networks for source localization in the human brain. Brain Topography,
4:3?21.
Ahonen, A. I., H?am?al?ainen, M. S., Knuutila, J. E. T., Kajola, M. J., Laine, P. P., Lounasmaa,
O. V., Parkkonen, L. T., Simola, J. T., and Tesche, C. D. (1993). 122-channel SQUID
instrument for investigating the magnetic signals from the human brain. Physica Scripta,
T49:198?205.
H?am?al?ainen, M., Hari, R., Ilmoniemi, R. J., Knuutila, J., and Lounasmaa, O. V. (1993).
Magnetoencephalography?theory, instrumentation, and applications to noninvasive
studies of the working human brain. Rev. Modern Physics, 65:413?497.
Hoey, G. V., Clercq, J. D., Vanrumste, B., de Walle, R. V., Lemahieu, I., D?Hav?e, M., and
Boon, P. (2000). EEG dipole source localization using artificial neural networks. Phys.
Med. Biol., 45:997?1011.
Jun, S. C., Pearlmutter, B. A., and Nolte, G. (2002). Fast accurate MEG source localization
using a multilayer perceptron trained with real brain noise. Physics in Medicine and
Biology, 47(14):2547?2560.
Jun, S. C., Pearlmutter, B. A., and Nolte, G. (2003). MEG source localization using a MLP
with a distributed output representation. IEEE Transactions on Biomedical Engineering,
50(6):786?789.
Kinouchi, Y., Ohara, G., Nagashino, H., Soga, T., Shichijo, F., and Matsumoto, K. (1996).
Dipole source localization of MEG by BP neural networks. Brain Topography, 8:317?
321.
Kwon, H., Lee, Y. H., Kim, J. M., Park, Y. K., and Kuriki, S. (2002). Localization accuracy
of single current dipoles from tangential components of auditory evoked fields. Phys.
Med. Biol., 47:4145?4154.
Leahy, R. M., Mosher, J. C., Spencer, M. E., Huang, M. X., and Lewine, J. D. (1998). A
study of dipole localization accuracy for MEG and EEG using a human skull phantom.
Electroencephalography and clinical neurophysiology, 107(2):159?173.
Press, W. H., Flannery, B. P., Teukolsky, S. A., and Verrerling, W. T. (1988). Numerical
Recipes in C. Cambridge University Press.
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning representations by
back?propagating errors. Nature, 323:533?536.
Sander, T. H., W?ubbeler, G., Lueschow, A., Curio, G., and Trahms, L. (2002). Cardiac artifact subspace identification and elimination in cognitive MEG data using time-delayed
decorrelation. IEEE Transactions on Biomedical Engineering, 49:345?354.
Tang, A. C. and Pearlmutter, B. A. (2003). Independent components of magnetoencephalography: Localization and single-trial response onset detection. In Lu, Z.-L. and
Kaufman, L., editors, Magnetic Source Imaging of the Human Brain, pages 159?201.
Lawrence Erlbaum Associates.
Tang, A. C., Pearlmutter, B. A., Malaszenko, N. A., Phung, D. B., and Reeb, B. C. (2002).
Independent components of magnetoencephalography: Localization. Neural Computation, 14(8):1827?1858.
Tang, A. C., Pearlmutter, B. A., Zibulevsky, M., and Carter, S. A. (2000a). Blind separation
of multichannel neuromagnetic responses. Neurocomputing, 32?33:1115?1120.
Tang, A. C., Pearlmutter, B. A., Zibulevsky, M., Hely, T. A., and Weisend, M. P. (2000b).
An MEG study of response latency and variability in the human visual system during
a visual-motor integration task. In Advances in Neural Information Processing Systems
12, pages 185?191. MIT Press.
Vig?ario, R., S?arel?a, J., Jousm?aki, V., H?am?al?ainen, M., and Oja, E. (2000). Independent
component approach to the analysis of EEG and MEG recordings. IEEE Transactions
on Biomedical Engineering, 47(5):589?593.

"
3821,"Decoding of Neuronal Signals in Visual Pattern
Recognition

Emad N Eskandar
Laboratory of Neuropsychology
National Institute of Mental Health
Bethesda MD 20892 USA

Barry J Richmond
Laboratory of Neuropsychology
National Institute of Mental Health
Bethesda MD 20892 USA

John A Hertz
NORDITA
B1egdamsvej 17
DK-2100 Copenhagen 0, Denmark

Lance M Optican
Laboratory of Sensorimotor Research
National Eye Institute
Bethesda MD 20892 USA

Troels Kjmr
NORDITA
B1egdamsvej 17
DK-2100 Copenhagen 0, Denmark

Abstract

We have investigated the properties of neurons in inferior temporal (IT)
cortex in monkeys performing a pattern matching task. Simple backpropagation networks were trained to discriminate the various stimulus
conditions on the basis of the measured neuronal signal. We also trained
networks to predict the neuronal response waveforms from the spatial patterns of the stimuli. The results indicate t.hat IT neurons convey temporally encoded information about both current and remembered patterns,
as well as about their behavioral context.

356

Decoding of Neuronal Signals in Visual Pattern Recognition

1

INTRODUCTION

Anatomical and neurophysiological studies suggest that there is a cortical pathway
specialized for visual object recognition, beginning in the primary visual cortex
and ending in the inferior temporal (IT) cortex (Ungerleider and Mishkin, 1982).
Studies of IT neurons in awake behaving monkeys have found that visually elicited
responses depend on the pattern of the stimulus and on the behavioral context of
the stimulus presentation (Richmond and Sato, 1987; Miller et aI, 1991). Until now,
however, no attempt had been made to quantify the temporal pattern of firing in
the context of a behaviorally complex task such as pattern recognition.
Our goal was to examine the information present in IT neurons about visual stimuli
and their behavioral context. We explicitly allowed for the possibility that this
information was encoded in the temporal pattern of the response. To decode the
responses, we used simple feed-forward networks trained by back propagation.
In work reported elsewhere (Eskandar et al, 1991) this information is calculated
another way, with similar results.

2

THE EXPERIMENT

Two monkeys were trained to perform a sequent.ial nonmatch to sample task using
a complete set of 32 black-and-white patterns based on 2-D Walsh functions. \\'hile
the monkey fixated and grasped a bar, a sample pattern appeared for 352 msecs;
after a pause of 500 msecs a test stimulus appeared for 352 msecs. The monkey
indicated whether the test stimulus failed to match the sample stimulus by releasing
the bar. (If the test matched the stimulus, the monkey waited for a third stimulus,
different from the sample, before releasing the bar; see Fig. 1.)

SAMPLE

MATCH

~
~----------~~----------~,

352 ms

550 ms

SAMPLE

352 ms

550 ms

-----------_ .
REWARD

NON MATCH

~
I - - - -_

INTER-TRIAL

_+_, _ _ _ _ _ _ _ _ _ _ _ _ ?

lNTER-STIMULUS

REWARD

Figure 1: The nonmatch-to-sample task.

357

358

Eskandar, Richmond, Hertz, Optican, and Kj<er

The type of trial (match or nonmatch) and t.he pairings of sample stimuli with
nonmatch stimuli were selected randomly. A single experiment usually contained
several thousand trials; thus each of the 32 patterns appeared repeatedly under the
three conditions (sample, match, and nonmatch). Single neuron recordings from IT
cortex were carried out while the monkeys were performing the task.

SAMPLE

A

MATCH

NONMATCH

IJ

,
""""

B

,""

""',

..

.....

Ji?
:,O,,,,,,

I I

?

I

?

?

""

I. , , ? I .

.. ..
"" . ... , ..
""'""

,.,

I

""

..

Figure 2: Responses produced by 2 stimuli under 3 behavioural condit.ions.
Fig. 2 shows the neuronal signals produced by two different stimulus patterns in
the three behavioural conditions: sample, match and nonmatch. The lower parts
of the figure show single-trial spike trains, while the upper parts show the effective
time-dependent firing probabilities, inferred from the spike trains by convolving

Decoding of Neuronal Signals in Visual Pattern Recognition

each spike with a Gaussian kernel, adding these up for each trial and averaging the
resulting continuous signals over trials. It is evident that for a given stimulus pattern
the average signals produced in different behavioural conditions are different. In
v,,-hat follows, we proceed further to show that there is information about behavioural
condition in the signal produced in a single trial. vVe will compute its average value
explicitly.

3

DECODING NETWORKS

To compute this information we trained networks to decode the measured signal.
The form of the network is shown in Fig. 3.

spike
trains

principal
components

hidden
units

output

Figure 3: Network to decode neuronal signals for information about behavioural
condition.
The first two layers of t he network shown preprocess the spike trains as follows: We
begin with the spikes measured in an interval starting 90 msec after the stimulus
onset and lasting 255 msec. First each spike is convolved with a Gaussian kernel
to produce a continuous signal. This signal is sampled at 4-msec intervals, giving a
54-dimensional input vector. In the second step this input vector is compressed by
throwing out all hut a small number of its principal components (PC's). The PC
basis was obtained by diagonalizing the 54 x 54 covariance matrix of the inputs
computed over all trials in the experiment. The remaining PC's are then the input
to the rest of the net work, which is a standard one with one further hidden layer.
Earlier work showed that the first five PC's transmit most of the pattern information
in a neuronal response (Richmond et aI, 1987). Furthermore, the first PC is highly
correlated with the spike count. Thus, our subsequent analysis was either on the
first PC alone, as a measure of spike count, or on the first five PC's, as a measure

359

360

Eskandar, Richmond, Hertz, Optican, and Kja::r

that incorporates temporal modulation.
We trained the networks to make pairwise discriminations between responses
measured under different conditions (sample-match, sample-non match , or matchnonmatch). Thus there is a single output unit, and the target is a 1 or 0 according
to the behavioural condition under which that spike train was measured.
The final two layers of the network were trained by standard backpropa.gation of
errors for the cross-entropy cost function

(1 )
where TIJ is the target and OIA the network output produced by the input vector
xiJ for training example J-l. The output of the network with the weights that result
from this training is then the optimal estimate (given the chosen architecture) of
the probability of a behavioural condition, given the measured neuronal signal used
as input. The number of hidden units was adjusted to minimize the generalization
error, which was computed on one quarter of the data that was reserved for this
purpose.
We then calculated the mean equivocation,
f

= -(O(x) log(O(x) + [1 -

O(x)] log[l - O(x)])x,

(2)

where O(x) is the value of the output unit for input x and the average is over all
inputs. (Vie calculated this by averagng over the test or training sets; the results
were not sensitive to which one we chose.) The equivocation is a measure of the
neuron's uncertainty with respect to a given discrimination. From it we can compute
the transmitted information
I

= Ia priori -

f

= 1-

f.

(3)

The last equality follows because in our data sets the two conditions always occur
equally often.
It is evident from Fig. 2 that if we already know that our signal is produced by a

particular st.imulus pattern, the discrimination of the behavioural condition will be
easier than if we do not possess this a priori knowledge. This is because the signal
varies with stimulus as well as behavioural condition (more strongly, in fact), and
the dependence on the latter has to be sorted out from that on the former. To
get an idea of the effect of this ""distraction"", we performed 4 separate calculations
for each of the 3 behavioural-condition discriminations, using 1, 4, 8, and all 32
stimulus patterns, respectively.
The results
about the 3
distraction,
information
the signal).

are summarized in Fig. 4, which shows the transmitted information
different behavioural-condition discriminations at the various levels of
averaged over 5 cells. It. also indicates how much of the tra.nsmitted
in each case is contained in the spike count alone (i.e. the first PC of

It is apparent that measurable information about behavioural condition is present

in a single neuronal response, even in the total absence of a priori information about
the stimulus pattern. It is also evident that most of this information is contained in

Decoding of Neuronal Signals in Visual Pattern Recognition

en

0.5

.0

0.4

~

0

c:

....
""'C

0.3

CI>

~

E
III
c:

......co

0.2
0.1

# patterns 1

4

8 32

samplenonmatch

1

4

8 32

samplematch

1

4

8 32

matchnonmatch

Figure 4: Transmitted information for the three behavioural discriminations with
different numbers of patterns. The lower white region on each bar shows the information transmit.ted in the first PC alone.
the time-dependence of the firing: the information cont.ained in the first PC of the
signal is significantly less (paired t-test p < 0.001) and was barely out of the noise.
A finite data set can lead to a biased estimate of the transmitted information (Optican et aI, 1991). In order to control for this we made a preliminary study of
the dependence of the calculated equivocation on training set size. We varied the
number of trials available to the network in a range (64 - 1024) for one pair of
discriminations (sample vs. nonmatch). The calculated apparent equivocation increased with the sample size N, indicating a small-sample bias. The best correlation
(Pearson r = -0.86) was obtained with a fit of the form:

= foo -

(c> 0).
(4)
This gives us a systematic way to estimate the small-sample bias and thus provide an
improved estimate foo of the true equivocation. Details will be reported elsewhere.
feN)

4

CN- 1 / 2

PREDICTING NEURONAL RESPONSES

In a second set of analyses, we examined the neuronal encoding of both current and
recalled patterns. The networks were trained to predict the neuronal response (as
represented by its first 5 PC's) from the spatial pattern of the current non match
stimulus, that of the immediately preceding sample stimulus, or both. The inputs
were the pixel values of the patterns.
The network is shown in Fig. 5. In order to avoid having different architectures for
predictions from one and two input patterns, we always used a number of input units

361

362

Eskandar, Richmond, Hertz, Optican, and Kja::r

equal to twice the number of pixels in the input. In the case where the prediction
was to be made on the basis of both previous and current patterns, each pattern
was fed into half the input units. For prediction from just one pattern (either the
current or previous one), the single input pixel array was loaded separately onto
both halves of the input array. As in the previous analyses, the number of hidden
units was fixed by testing on a quarter of the data held out of the training set for
this purpose.

/'

[]:
""/'

~:

--......

--....
~

~

~

""-

Figure 5: Network for predicting neuronal responses from the stimulus. The inputs
are pixel values of the stimuli (see text), and the targets are the first 5 PC's of the
measured response.
We performed this analysis on data from 6 neurons. Not surprisingly, the predicted
waveforms were better when the input was the current pattern (normalized mean
square error (mse) = 0.482) than when it was the previous pattern (mse = 0.589).
However, the best prediction was obtained when the input reflected both the current
and previous patterns (mse = 0.422) . Thus the neurons we analyzed conveyed
information about both remembered and current stimuli .

5

CONCLUSION

The results presented here demonstrate the utilit.y of connectionist networks in analyzing neuronal information processing. \Ve have shown that temporally modulated
responses in IT cortical neurons convey information about both spatial patterns and
behavioral context. The responses also convey information about the patterns of
remembered stimuli. Based on these results , we hypothesize that inferior temporal
neurons playa role in comparing visual patterns with those presented at an earlier
time.

Decoding of Neuronal Signals in Visual Pattern Recognition

Acknowledgements
This work was supported by NATO through Collaborative Research Grant CRG
900189. EE received support from the Howard Hughes Medical Institute as an NIH
Research Scholar.
References
E N Eskandar et al (1991): Inferior temporal neurons convey information about
stimulus patterns and their behavioral relevance, Soc Neurosci Abstr 17 443; Role
of inferior temporal neurons in visual memory, submitted to J Neurophysiol.
E K Miller et al (1991): A neural mechanism for working and recognition memory
in inferior temporal cortex, Science 253
L M Optican et al (1991) : Unbiased measures of transmitted information and channel capacity from multivariate neuronal data, Bioi Cybernetics 65 305-310.
B J Richmond and T Sato (1987): Enhancement of inferior temporal neurons during
visual discrimination , J NeurophysioL 56 1292-1306.
B J Richmond et al (1987): Temporal encoding of two-dimensional patterns by
single units in primate inferior temporal cortex, J Neurophysiol 57 132-178.
L G Ungerleider and M Mishkin (1982): Two cortical visual systems, in Analysis
of Visual Behavior, ed . D JIngle, M A Goodale and R J W Mansfield, pp 549-586.
Cambridge: MIT Press.

363

"
5486,"GP Kernels for Cross-Spectrum Analysis

1

Kyle Ulrich, 3 David E. Carlson, 2 Kafui Dzirasa, 1 Lawrence Carin
Department of Electrical and Computer Engineering, Duke University
2
Department of Psychiatry and Behavioral Sciences, Duke University
3
Department of Statistics, Columbia University
{kyle.ulrich, kafui.dzirasa, lcarin}@duke.edu
david.edwin.carlson@gmail.com

1

Abstract
Multi-output Gaussian processes provide a convenient framework for multi-task
problems. An illustrative and motivating example of a multi-task problem is
multi-region electrophysiological time-series data, where experimentalists are interested in both power and phase coherence between channels. Recently, Wilson
and Adams (2013) proposed the spectral mixture (SM) kernel to model the spectral density of a single task in a Gaussian process framework. In this paper, we
develop a novel covariance kernel for multiple outputs, called the cross-spectral
mixture (CSM) kernel. This new, flexible kernel represents both the power and
phase relationship between multiple observation channels. We demonstrate the
expressive capabilities of the CSM kernel through implementation of a Bayesian
hidden Markov model, where the emission distribution is a multi-output Gaussian process with a CSM covariance kernel. Results are presented for measured
multi-region electrophysiological data.

1

Introduction

Gaussian process (GP) models have become an important component of the machine learning literature. They have provided a basis for non-linear multivariate regression and classification tasks, and
have enjoyed much success in a wide variety of applications [16].
A GP places a prior distribution over latent functions, rather than model parameters. In the sense
that these functions are defined for any number of sample points and sample positions, as well
as any general functional form, GPs are nonparametric. The properties of the latent functions are
defined by a positive definite covariance kernel that controls the covariance between the function
at any two sample points. Recently, the spectral mixture (SM) kernel was proposed by Wilson and
Adams [24] to model a spectral density with a scale-location mixture of Gaussians. This flexible
and interpretable class of kernels is capable of recovering any composition of stationary kernels
[27, 9, 13]. The SM kernel has been used for GP regression of a scalar output (i.e., single function, or
observation ?task?), achieving impressive results in extrapolating atmospheric CO2 concentrations
[24]; image inpainting [25]; and feature extraction from electrophysiological signals [21].
However, the SM kernel is not defined for multiple outputs (multiple correlated functions). Multioutput GPs intersect with the field of multi-task learning [4], where solving similar problems jointly
allows for the transfer of statistical strength between problems, improving learning performance
when compared to learning all tasks individually. In this paper, we consider neuroscience applications where low-frequency (< 200 Hz) extracellular potentials are simultaneously recorded from
implanted electrodes in multiple brain regions of a mouse [6]. These signals are known as local field
potentials (LFPs) and are often highly correlated between channels. Inferring and understanding
that interdependence is biologically significant.
1

A multi-output GP can be thought of as a standard GP (all observations are jointly normal) where the
covariance kernel is a function of both the input space and the output space (see [2] and references
therein for a comprehensive review); here ?input space? means the points at which the functions are
sampled (e.g., time), and the ?output space? may correspond to different brain regions. A particular
positive definite form of this multi-output covariance kernel is the sum of separable (SoS) kernels,
or the linear model of coregionalization (LMC) in the geostatistics literature [10], where a separable
kernel is represented by the product of separate kernels for the input and output spaces.
While extending the SM kernel to the multi-output setting via the LMC framework (i.e., the SMLMC kernel) provides a powerful modeling framework, the SM-LMC kernel does not intuitively
represent the data. Specifically, the SM-LMC kernel encodes the cross-amplitude spectrum (square
root of the cross power spectral density) between every pair of channels, but provides no crossphase information. Together, the cross-amplitude and cross-phase spectra form the cross-spectrum,
defined as the Fourier transform of the cross-covariance between the pair of channels.
Motivated by the desire to encode the full cross-spectra into the covariance kernel, we design a novel
kernel termed the cross-spectral mixture (CSM) kernel, which provides an intuitive representation
of the power and phase dependencies between multiple outputs. The need for embedding the full
cross-spectrum into the covariance kernel is illustrated by a recent surge in neuroscience research
discovering that LFP interdependencies between regions exhibit phase synchrony patterns that are
dependent on frequency band [11, 17, 18].
The remainder of the paper is organized as follows. Section 2 provides a summary of GP regression
models for vector-valued data, and Section 3 introduces the SM, SM-LMC, and novel CSM covariance kernels. In Section 4, the CSM kernel is incorporated in a Bayesian hidden Markov model
(HMM) [14] with a GP emission distribution as a demonstration of its utility in hierarchical modeling. Section 5 provides details on inverting the Bayesian HMM with variational inference, as well
as details on a fast, novel GP fitting process that approximates the CSM kernel by its representation
in the spectral domain. Section 6 analyzes the performance of this approximation and presents results for the CSM kernel in the neuroscience application, considering measured multi-region LFP
data from the brain of a mouse. We conclude in Section 7 by discussing how this novel kernel can
trivially be extended to any time-series application where GPs and the cross-spectrum are of interest.

2

Review of Multi-Output Gaussian Process Regression

A multi-output regression task estimates samples from C output channels, y n = [yn1 , . . . , ynC ]T
corresponding to the n-th input point xn (e.g., the n-th temporal sample). An unobserved latent
function f (x) = [f1 (x), . . . , fC (x)]T is responsible for generating the observations, such that y n ?
N (f (xn ), H ?1 ), where H = diag(?1 , . . . , ?C ) is the precision of additive Gaussian noise.
A GP prior on the latent function is formalized by f (x) ? GP(m(x), K(x, x0 )) for arbitrary
input x, where the mean function m(x) ? RC is set to equal 0 without loss of generality, and
0
the covariance function (K(x, x0 ))c,c0 = k c,c (x, x0 ) = cov(fc (x), fc0 (x0 )) creates dependencies
0
between observations at input points x and x , as observed on channels c and c0 . In general, the input
space x could be vector valued, but for simplicity we here assume it to be scalar, consistent with our
motivating neuroscience application in which x corresponds to time.
A convenient representation for multi-output kernel functions is to separate the kernel into the product of a kernel for the input space and a kernel for the interactions between the outputs. This is
known as a separable kernel. A sum of separable kernels (SoS) representation [2] is given by
0

k c,c (x, x0 ) =

Q
X

bq (c, c0 )kq (x, x0 ),

or

q=1

K(x, x0 ) =

Q
X

B q kq (x, x0 ),

(1)

q=1

where kq (x, x0 ) is the input space kernel for component q, bq (c, c0 ) is the q-th output interaction
kernel, and B q ? RC?C is a positive semi-definite output kernel matrix. Note that we have a discrete set of C output spaces, c ? {1, . . . , C}, where the input space x is continuous, and discretely
sampled arbitrarily in experiments. The SoS formulation is also known as the linear model of coregionalization (LMC) [10] and B q is termed the coregionalization matrix. When Q = 1, the LMC
reduces to the intrinsic coregionalization model (ICM) [2], and when rank(B q ) is restricted to equal
1, the LMC reduces to the semiparametric latent factor model (SLFM) [19].
2

Any finite number of latent functional evaluations f = [f1 (x), . . . , fC (x)]T at locations x =
[x1 , . . . , xN ]T has a multivariate normal distribution N (f ; 0, K), such that K is formed through
the block partitioning
? 1,1
?
k (x, x) ? ? ? k 1,C (x, x)
Q
?
? X
..
..
..
=
B q ? kq (x, x),
(2)
K=?
?
.
.
.
k C,1 (x, x) ? ? ?

k C,C (x, x)

q=1

0

where each k c,c (x, x) is an N ? N matrix and ? symbolizes the Kronecker product.
A vector-valued dataset consists of observations y = vec([y 1 , . . . , y N ]T ) ? RCN at the respective
locations x = [x1 , . . . , xN ]T such that the first N elements of y are from channel 1 up to the last N
elements belonging to channel C. Since both the likelihood p(y|f , x) and distribution over latent
functions p(f |x) are Gaussian, the marginal likelihood is conveniently represented by
Z
p(y|x) = p(y|f , x)p(f |x)df = N (0, ?),
? = K + H ?1 ? I N ,
(3)
where all possible functions f have been marginalized out.
Each input-space covariance kernel is defined by a set of hyperparameters, ?. This conditioning was
removed for notational simplicity, but will henceforth be included in the notation. For example, if
the squared exponential kernel is used, then kSE (x, x0 ; ?) = exp(? 12 ||x ? x0 ||2 /`2 ), defined by a
single hyperparameter ? = {`}. To fit a GP to the dataset, the hyperparameters are typically chosen
to maximize the marginal likelihood in (3) via gradient ascent.

3

Expressive Kernels in the Spectral Domain

This section first introduces the spectral mixture (SM) kernel [24] as well as a multi-output extension
of the SM kernel within the LMC framework. While the SM-LMC model is capable of representing complex spectral relationships between channels, it does not intuitively model the cross-phase
spectrum between channels. We propose a novel kernel known as the cross-spectral mixture (CSM)
kernel that provides both the cross-amplitude and cross-phase spectra of multi-channel observations.
Detailed derivations of each of these kernels is found in the Supplemental Material.
3.1

The Spectral Mixture Kernel

A spectral Gaussian (SG) kernel is defined by an amplitude spectrum with a single Gaussian distribution reflected about the origin,
1
SSG (?; ?) = [N (?; ??, ?) + N (?; ?, ?)] ,
(4)
2
where ? = {?, ?} are the kernel hyperparameters, ? represents the peak frequency, and the variance
? is a scale parameter that controls the spread of the spectrum around ?. This spectrum is a function
of angular frequency. The Fourier transform of (4) results in the stationary, positive definite autocovariance function
1
kSG (? ; ?) = exp(? ?? 2 ) cos(?? ),
(5)
2
0
where stationarity implies dependence on input domain differences k(? ; ?) = k(x,
? x ; ?) with ? =
0
x ? x . The SG kernel may also be derived by considering a latent signal f (x) = 2 cos(?(x + ?))
with frequency uncertainty ? ? N (?, ?) and phase offset ??. The kernel is the auto-covariance
function for f (x), such that kSG (? ; ?) = cov(f (x), f (x+? )). When computing the auto-covariance,
the frequency ? is marginalized out, providing the kernel in (5) that includes all frequencies in the
spectral domain with probability 1.
A weighted, linear combination of SG kernels gives the spectral mixture (SM) kernel [24],
kSM (? ; ?) =

Q
X

aq kSG (? ; ? q ),

SSM (?; ?) =

q=1

Q
X

aq SSG (?; ? q ),

(6)

q=1

where ? q = {aq , ?q , ?q } and ? = {? q } has 3Q degrees of freedom. The SM kernel may be derived
as the Fourier transform of the spectral density SSM (?; ?) or as the auto-covariance of latent funcPQ p
tions f (x) = q=1 2aq cos(?q (x + ?q )) with uncertainty in angular frequency ?q ? N (?q , ?q ).
3

4

4

0

-2

-2

-4

-4
0.2

0.4

0.6

0.8

1

f 2 (x)

Amplitude

0

0

3

3.14

2

1.57

1

0

0

-1.57

f 1 (x)

2

f 2 (x)

0
0

0.2

Time

0.4

0.6

0.8

1

Phase

f 1 (x)

2

-3.14
3

3.5

4

Time

4.5

5

5.5

6

Frequency

Figure 1: Latent functions drawn for two channels f1 (x) (blue) and f2 (x) (red) using the CSM kernel (left)
and rank-1 SM-LMC kernel (center). The functions are comprised of two SG components centered at 4 and 5
Hz. For the CSM kernel, we set the phase shift ?c0 ,2 = ?. Right: the cross-amplitude (purple) and cross-phase
(green) spectra between f1 (x) and f2 (x) are shown for the CSM kernel (solid) and SM-LMC kernel (dashed).
The ability to tune phase relationships is beneficial for kernel design and interpretation.

The moniker for the SM kernel in (6) reflects the mixture of Gaussian components that define the
spectral density of the kernel. The SM kernel is able to represent any stationary covariance kernel
given large enough Q; to name a few, this includes any combination of squared exponential, Mat`ern,
rational quadratic, or periodic kernels [9, 16, 24].
3.2

The Cross-Spectral Mixture Kernel

A multi-output version of the SM kernel uses the SG kernel directly within the LMC framework:
Q
X
K SM-LMC (? ; ?) =
B q kSG (? ; ? q ),
(7)
q=1

where Q SG kernels are shared among the outputs via the coregionalization matrices {B q }Q
q=1 . A
generalized, non-stationary version of this SM-LMC kernel was proposed in [23] using the Gaussian
process regression network (GPRN) [26]. The marginal distribution for any single channel is simply
a Gaussian process with a SM covariance kernel. While this formulation is capable of providing
a full cross-amplitude spectrum between two channels, it contains no information
about a crossP
phase spectrum. Specifically, each channel is merely a weighted sum of q Rq latent functions
where Rq = rank(B q ). Whereas these functions are shared exactly across channels, our novel CSM
kernel shares phase-shifted versions of these latent functions across channels.
Definition 3.1. The cross-spectral mixture (CSM) kernel takes the form
Rq q
Q X
X

1
arcq arc0 q exp(? ?q ? 2 ) cos ?q ? + ?rc0 q ? ?rcq ,
(8)
2
q=1 r=1
PQ
Rq Q
}q=1 has 2Q + q=1 Rq (2C ? 1) degrees of freedom,
where ? = {?q , ?q , {arq , ?rq , ?r1q , 0}r=1
and arcq and ?rcq respectively represent the amplitude and shift in the input space for latent functions
associated with channel c. In the LMC framework, the CSM kernel is
( Q
)
Rq
X
X
K CSM (? ; ?) = Re
Bq e
kSG (? ; ? q ) ,
Bq =
? r (? r )? ,
0

c,c
kCSM
(? ; ?) =

q

q=1

q

r=1

p
1
r
r
e
kSG (? ; ? q ) = exp(? ?q ? 2 + j?q ? ),
?cq
= arcq exp(?j?cq
),
2
r
where e
kSG (?, ? q ) is phasor notation of the SG kernel, B q is rank-Rq , {?cq
} are complex scalar
r
r
coefficients encoding amplitude and phase, and ?cq , ?q ?cq is an alternative phase representation.
?
We use complex notation where j = ?1, Re{?} returns the real component of its argument, and
? ? represents the complex conjugate of ?.
Both the CSM and SM-LMC kernels force the marginal distribution of data from a single channel to be a Gaussian process with a SM covariance kernel. The CSM kernel is derived in the
Supplemental Material by considering functions represented by phase-shifted sinusoidal signals,
PQ PRq p r
iid
fc (x) =
2acq cos(?qr (x + ?rcq )), where each ?qr ? N (?q , ?q ). Computing the
r=1
q=1
cross-covariance function cov(fc (x), fc0 (x + ? )) provides the CSM kernel.
A comparison between draws from Gaussian processes with CSM and SM-LMC kernels is shown
in Figure 1. The utility of the CSM kernel is clearly illustrated by its ability to encode phase
4

information, as well as its powerful functional form of the full cross-spectrum (both amplitude
0 (?) are obtained by repand phase). The amplitude function Ac,c0 (?) and phase function ?c,cP
0
resenting the cross-spectrum in phasor notation, i.e., ?c,c0 (?; ?) =
q (B q )c,c SSG (?; ? q ) =
Ac,c0 (?) exp(j?c,c0 (?)). Interestingly, while the CSM and SM-LMC kernels have identical
marginal amplitude spectra for shared {?q , ?q , aq }, their cross-amplitude spectra differ due to the
inherent destructive interference of the CSM kernel (see Figure 1, right).

4

Multi-Channel HMM Analysis

Neuroscientists are interested in examining how the network structure of the brain changes as animals undergo a task, or various levels of arousal [15]. The LFP signal is a modality that allows
researchers to explore this network structure. In the model provided in this section, we cluster segments of the LFP signal into discrete ?brain states? [21]. Each brain state is represented by a unique
cross-spectrum provided by the CSM kernel. The use of the full cross-spectrum to define brain states
is supported by previous work discovering that 1) the power spectral density of LFP signals indicate
various levels of arousal states in mice [7, 21], and 2) frequency-dependent phase synchrony patterns
change as animals undergo different conditions in a task [11, 17, 18] (see Figure 2).
The vector-valued observations from C channels are segmented into W contiguous, non-overlapping
windows. The windows are common across channels, such that the C-channel data for window
w
w T
w
w ? {1, . . . , W } are represented by y w
n = [yn1 , . . . , ynC ] at sample location xn . Given data, each
window consists of Nw temporal samples, but the model is defined for any set of sample locations.
We model the observations {y w
n } as emissions from a hidden Markov model (HMM) with L hidden,
discrete states. State assignments are represented by latent variables ?w ? {1, . . . , L} for each window w ? {1, . . . , W }. In general, L is a set upper bound of the number of states (brain states [21],
or ?clusters?), but the model can shrink down and infer the number of states needed to fit the data.
This is achieved by defining the dynamics of the latent states according to a Bayesian HMM [14]:
?1 ? Categorical(?0 ),

?w ? Categorical(??w?1 ) ?w ? 2,

?0 , ?` ? Dirichlet(?),

where the initial state assignment is drawn from a categorical distribution with probability vector ?0
and all subsequent states assignments are drawn from the transition vector ??w?1 . Here, ?`h is the
probability of transitioning from state ` to state h. The vectors {?0 , ?1 , . . . , ?L } are independently
drawn from symmetric Dirichlet distributions centered around ? = [1/L, . . . , 1/L] to impose sparsity on transition probabilities. In effect, this allows the model to learn the number of states needed
for the data (i.e., fewer than L) [3].
Each cluster ` ? {1, . . . , L} is assigned GP parameters ? ` . The latent cluster assignment ?w for
window w indicates which set of GP parameters control the emission distribution of the HMM:
?1
w
yw
n ? N (f w (xn ), H ?w ),

f w (x) ? GP(0, K(x, x0 ; ? ?w )),

(9)

0

c,c
where (K(x, x0 ; ? ` ))c,c0 = kCSM
(x, x0 ; ? ` ) is the CSM kernel, and the cluster-dependent precision
H ?w = diag(? ?w ) generates independent Gaussian observation noise. In this way, each window w
is modeled as a stochastic process with a multi-channel cross-spectrum defined by ? ?w .
Raw LFP Data

Cross-Amplitude Spectrum

Cross-Phase Spectrum
1

BLA
IL Cortex

DELTA Waves

THETA Waves ALPHA Waves

BETA Waves

Lag (rad)

Potential

Amplitude

0.5
0
-0.5
-1
-1.5

0.1

0.2

0.3

0.4

Time (sec)

0.5

0.6

0.7

0.8

0

2

4

6

8

10

Frequency ( Hz)

12

14

16

0

2

4

6

8

10

12

14

16

Frequency ( Hz)

Figure 2: A short segment of LFP data recorded from the basolateral amygdala and infralimbic cortex is
shown on the left. The cross-amplitude and phase spectra are produced using Welch?s averaged periodogram
method [22] for several consecutive 5 second windows of LFP data. Frequency dependent phase synchrony lags
are consistently present in the cross-phase spectrum, motivating the CSM kernel. This frequency dependency
aligns with preconcieved notions of bands, or brain waves (e.g., 8-12 Hz alpha waves).

5

5

Inference

w T
A convenient notation vectorizes all observations within a window, y w = vec([y w
1 , . . . , y Nw ] ),
w
where vec(A) is the vectorization of matrix A; i.e., the first Nw elements of y are observations
from channel 1, up to the last Nw elements of y w belonging to channel C. Because samples are
obtained on an evenly spaced temporal grid, we fix Nw = N and align relative sample locations
within a window to an oracle xw = x = [x1 , . . . , xN ]T for all w.

The model in Section 4 generates the set of observations Y = {y w }W
w=1 at aligned sample locations
W
x given kernel hyperparameters ? = {? ` , ? ` }L
and
model
variables
? = {{?` }L
`=1
`=0 , {?w }w=1 }.
The latent variables ? are inverted using mean-field variational inference [3], obtaining an approxiQL
mate posterior distribution q(?) = q(?1:W ) `=0 Dir(?` ; ?` ). The approximate posterior is chosen
to minimize the KL divergence to the true posterior distribution p(?|Y , ?, x) using the standard
variational EM method detailed in Chapter 3 of [3].
During each iteration of the variational EM algorithm, the kernel hyperparameters ? are chosen to
PW PL
maximize the expected marginal log-likelihood Q = w=1 `=1 q(?w = `) log N (y w ; 0, ?` )via
gradient ascent, where q(?w = `) is the marginal posterior probability that window w is assigned
e ` } is the CSM kernel matrix for state ` with the complex form
to brain state `, and ?` = Re{?
P
`
e
e
?` = q B q ? kSG (x, x; ? ` ) + H ?1
` ? I N . Performing gradient ascent requires the derivatives
P
?1 w
?1 ??`
?Q
1
T
w,` tr((?`w ?`w ? ?` ) ??j ) where ?`w = ?` y [16]. A na??ve implementation of
??j = 2
this gradient requires the inversion of ?` , which has complexity O(N 3 C 3 ) and storage requirements
O(N 2 C 2 ) since a simple method to invert a sum of Kronecker products does not exist.
A common trick for GPs with evenly spaced samples (e.g., a temporal grid) is to use the discrete
Fourier transform (DFT) to approximate the inverse of ?` by viewing this as an approximately
circulant matrix [5, 12]. These methods can speed up inference because circulant matrices are diagonalizable by the DFT coefficient matrix. Adjusting these methods to the multi-output formulation,
we show how the DFT of the marginal covariance matrices retains the cross-spectrum information.
Proposition 5.1. Let y w ? N (0, ??w ) represent the marginal likelihood of circularly-symmetric
[8] real-valued observations in window w, and denote the concatenation of the DFT of each channel
as z w = (I C ? U )? y w where U is the N ? N unitary DFT matrix. Then, z w is shown in the
Supplemental Material to have the complex normal distribution [8]:
z w ? CN (0, 2S ?w ),

S ` = ? ?1

Q
X

B `q ? W `q + H ?1
` ? IN ,

(10)

q=1

where ? = xi+1 ? xi for all i = 2, . . . , N , and W `q ? diag([SSG (?; ? `q ), 0]) is approximately
diagonal. The spectral density SSG (?; ?) = [SSG (?1 ; ?), . . . , SSG (?b N +1 c ; ?)] is found via (4) at
2

 N 


2?
angular frequencies ? = N
, and 0 = [0, . . . , 0] is a row vector of N 2?1 zeros.
? 0, 1, . . . , 2
The hyperparameters of the CSM kernels ? may now be optimized from the expected marginal
log-likelihood of Z = {z w }W
w=1 instead of Y . Conceptually, the only difference during the fitting
process is that, with the latter, derivatives of the covariance kernel are used, while, with the former, derivatives of the power spectral density are used. Computationally, this method improves the
na??ve O(N 3 C 3 ) complexity of fitting the standard CSM kernel to O(N C 3 ) complexity. Memory
requirements are also reduced from O(N 2 C 2 ) to O(N C 2 ). The reason for this improvement is that
S ` is now represented as N independent C ? C blocks, reducing the inversion of S ` to inverting a
permuted block-diagonal matrix.

6

Experiments

Section 6.1 demonstrates the performance of the CSM kernel and the accuracy of the DFT approximation In Section 6.2, the DFT approximation for the CSM kernel is used in a Bayesian HMM
framework to cluster time-varying multi-channel LFP data based on the full cross-spectrum; the
HMM states here correspond to states of the brain during LFP recording.
6

Table 1: The mean and standard deviation of the difference between the
AIC value of a given model and the
AIC value of the rank-2 CSM model.
Lower values are better.

0.035

KL Divergence

0.03
0.025

7 = 0.5 Hz
7 = 1 Hz
7 = 3 Hz

0.02
0.015
0.01
0.005
0
0

1

2

3

4

5

6

7

8

Series Length (seconds)

Figure 3: Time-series data is drawn from a Gaussian process with
a known CSM covariance kernel, where the domain restricted to a
fixed number of seconds. A Gaussian process is then fitted to this
data using the DFT approximation. The KL-divergence of the fitted
marginal likelihood from the true marginal likelihood is shown.

6.1

Rank

Model

? AIC

1
1
1
2
2
2
3
3
3

SE-LMC
SM-LMC
CSM
SE-LMC
SM-LMC
CSM
SE-LMC
SM-LMC
CSM

4770 (993)
512 (190)
109 (110)
5180 (1120)
325 (167)
0 (0)
5550 (1240)
412 (184)
204 (71.7)

Performance and Inference Analysis

The performance of the CSM kernel is compared to the SM-LMC kernel and SE-LMC (squared
exponential) kernel. Each of these models allow Q=20, and the rank of the coregionalization matrices is varied from rank-1 to rank-3. For a given rank, the CSM kernel always obtains the largest
marginal likelihood for a window of LFP data, and the marginal likelihood always increases for
increasing rank. To penalize the number of kernel parameters (e.g., a rank-3, Q=20 CSM kernel for
7 channels has 827 free parameters to optimize), the Akaike information criterion (AIC) is used for
model selection [1]. For this reason, we do not test rank greater than 3. Table 1 shows that a rank-2
CSM kernel is selected using this criterion, followed by a rank-1 CSM kernel. To show the rank-2
CSM kernel is consistently selected as the preferred model we report means and standard deviations
of AIC value differences across 30 different randomly selected 3-second windows of LFP data.
Next, we provide numerical results for the conditions required when using the DFT approximation
in (10). This allows for one to define details of a particular application in order to determine if the
DFT approximation to the CSM kernel is appropriate. A CSM kernel is defined for two outputs
with a single Gaussian component, Q = 1. The mean frequency and variance for this component
are set to push the limits of the application. For example, with LFP data, low frequency content is
of interest, namely greater than 1 Hz; therefore, we test values of ?
e1 ? { 12 , 1, 3} Hz. We anticipate
2
variances at these frequencies to be around ?e1 = 1 Hz . A conversion to angular frequency gives
?1 = 2?e
?1 and ?1 = 4? 2 ?e1 . The covariance matrix ? in (3) is formed using these parameters, a
fixed noise variance, and N observations on a time grid with sampling rate of 200 Hz. Data y are
drawn from the marginal likelihood with covariance ?.
? The KL
A new CSM kernel is fit to y using the DFT approximation, providing an estimate ?.
divergence of the fitted marginal likelihood from the true marginal likelihood is
""
#
1
|?|
?1
?
? ,
KL(p(y|?)||p(y|?))
=
log
? N + tr(? ?)
?
2
|?|
where | ? | and tr(?) are the determinant and trace operators, respectively.
Computing
1
?
KL(p(y|
?)||p(y|?))
for
various
values
of
?
e
and
N
provides
the
results
in
Figure
3.
This plot
1
N
shows that the DFT approximation struggles to resolve low frequency components unless the series length is sufficiently long. Due to the approximation error, when using the DFT approximation
on LFP data we a priori filter out frequencies below 1.5 Hz and perform analyses with a series
length of 3 seconds. This ensures the DFT approximation represents the true covariance matrix. The
following application of the CSM kernel uses these settings.
6.2

Including the CSM Kernel in a Bayesian Hierarchical Model

We analyze 12 hours of LFP data of a mouse transitioning between different stages of sleep [7, 21].
Observations were recorded simultaneously from 4 channels [6], high-pass filtered at 1.5 Hz, and
subsampled to 200 Hz. Using 3 second windows provides N = 600 and W = 14, 400. The HMM
was implemented with the number of kernel components Q = 15 and the number of states L = 7.
7

1.57

2

Amplitude

0

1

Phase

3

?1.57

0

?3.14

6

Amplitude

2.5

3.14

BasalAmy

5

3.14

DLS

1.5

1

1.57

5
3

0

1

Phase

Amplitude

6

0.5

?1.57

0
0

0

15

3

5
3

0

1

Phase

1.57

2

1

Phase

?1.57
?3.14

6

DHipp

3.14
1.57

5
3

0

1

0

?1

Phase

Amplitude

10

Frequency (Hz )

3.14

DMS

0

Amplitude

5

?3.14

6

?2

?1.57

0
0

5

10

15 0

5

Frequency

10

15 0

Frequency

5

10

15 0

5

10

?3

?3.14
15

0

5

10

State 7
State 6
State 5
State 4
State 3
State 2
State 1

Dzirasa et al.
CSM Kernel
0

15

Frequency (Hz )

Frequency

Frequency

20

40

60

80

100

120

140

160

Minutes

Figure 4: A subset of results from the Bayesian HMM analysis of brain states. In the upper left, the full crossspectrum for an arbitrary state (state 7) is plotted. In the upper right, the amplitude (top) and phase (bottom)
functions for the cross-spectrum between the Dorsomedial Striatum (DMS) and Hippocampus (DHipp) are
shown for all seven states. On the bottom, the maximum likelihood state assignments are shown and compared
to the state assignments from [7]. The same colors between the CSM state assignments and the phase and
amplitude functions correspond to the same state. These colors are alligned to the [7] states, but there is no
explicit relationship between the colors of the two state sequences.

This was chosen because sleep staging tasks categorize as many as seven states: various levels of
rapid eye movement, slow wave sleep, and wake [20]. Although rigorous model selection on L
is necessary to draw scientific conclusions from the results, the purpose of this experiment is to
illustrate the utility of the CSM kernel in this application.
An illustrative subset of the results are shown in Figure 4. The full cross-spectrum is shown for
a single state (state 7), and the cross-spectrum between the Dorsomedial Striatum and the Dorsal
Hippocampus are shown for all states. Furthermore, we show the progression of these brain state
assignments over 3 hours and compare them to states from the method of [7], where statistics of the
Hippocampus spectral density were clustered in an ad hoc fashion. To the best of our knowledge,
this method represents the most relevant and accurate results for sleep staging from LFP signals in
the neuroscience literature. From these results, it is apparent that our clusters pick up sub-states of
[7]. For instance, states 3, 6, and 7 all appear with high probability when the method from [7] infers
state 3. Observing the cross-phase function of sub-state 7 reveals striking differences from other
states in the theta wave (4-7 Hz) and the alpha wave (8-15 Hz). This cross-phase function is nearly
identical for states 2 and 5, implying that significant differences in the cross-amplitude spectrum
may have played a role in identifying the difference between these two brain states.
Many more of these interesting details exist due to the expressive nature of the CSM kernel. As a
full interpretation of the cross-spectrum results is not the focus of this work, we contend that the
CSM kernel has the potential to have a tremendous impact in fields such as neuroscience, where the
dynamics of cross-spectrum relationships of LFP signals are of great interest.

7

Conclusion

This work introduces the cross-spectral mixture kernel as an expressive kernel capable of extracting
patterns for multi-channel observations. Combined with the powerful nonparametric representation
of a Gaussian process, the CSM kernel expresses a functional form for every pairwise cross-spectrum
between channels. This is a novel approach that merges Gaussian processes in the machine learning
community to standard signal processing techniques. We believe the CSM kernel has the potential
to impact a broad array of disciplines since the kernel can trivially be extended to any time-series
application where Gaussian processes and the cross-spectrum are of interest.
Acknowledgments
The research reported here was funded in part by ARO, DARPA, DOE, NGA and ONR.
8

References
[1] H. Akaike. A new look at the statistical model identification. IEEE Transactions on Automatic Control,
19(6):716?723, 1974.
[2] M. A. Alvarez, L. Rosasco, and N. D. Lawrence. Kernels for vector-valued functions: a review. Foundations and Trends in Machine Learning, 4(3):195?266, 2012.
[3] M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, University College
London.
[4] R. Caruana. Multitask learning. Machine Learning, 28(1):41?75, 1997.
[5] C. R. Dietrich and G. N. Newsam. Fast and exact simulation of stationary Gaussian processes through
circulant embedding of the covariance matrix. SIAM Journal on Scientific Computing, 18(4):1088?1107,
1997.
[6] K. Dzirasa, R. Fuentes, S. Kumar, J. M. Potes, and M. A. L. Nicolelis. Chronic in vivo multi-circuit
neurophysiological recordings in mice. Journal of Neuroscience Methods, 195(1):36?46, 2011.
[7] K. Dzirasa, S. Ribeiro, R. Costa, L. M. Santos, S. C. Lin, A. Grosmark, T. D. Sotnikova, R. R. Gainetdinov, M. G. Caron, and M. A. L. Nicolelis. Dopaminergic control of sleep?wake states. The Journal of
Neuroscience, 26(41):10577?10589, 2006.
[8] R. G. Gallager. Principles of digital communication. pages 229?232, 2008.
[9] M. G?onen and E. Alpaydn. Multiple kernel learning algorithms. JMLR, 12:2211?2268, 2011.
[10] P. Goovaerts. Geostatistics for Natural Resources Evaluation. Oxford University Press, 1997.
[11] G. G. Gregoriou, S. J. Gotts, H. Zhou, and R. Desimone. High-frequency, long-range coupling between
prefrontal and visual cortex during attention. Science, 324(5931):1207?1210, 2009.
[12] M. L?azaro-Gredilla, J. Qui?nonero Candela, C. E. Rasmussen, and A. R. Figueiras-Vidal. Sparse spectrum
Gaussian process regression. JMLR, (11):1865?1881, 2010.
[13] J. R. Lloyd, D. Duvenaud, R. Grosse, J. B. Tenenbaum, and Z. Ghahramani. Automatic construction and
natural-language description of nonparametric regression models. AAAI, 2014.
[14] D. J. C. MacKay. Ensemble learning for hidden Markov models. Technical report, 1997.
[15] D. Pfaff, A. Ribeiro, J. Matthews, and L. Kow. Concepts and mechanisms of generalized central nervous
system arousal. ANYAS, 2008.
[16] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. 2006.
[17] P. Sauseng and W. Klimesch. What does phase information of oscillatory brain activity tell us about
cognitive processes? Neuroscience and Biobehavioral Reviews, 32:1001?1013, 2008.
[18] C. M. Sweeney-Reed, T. Zaehle, J. Voges, F. C. Schmitt, L. Buentjen, K. Kopitzki, C. Esslinger, H. Hinrichs, H. J. Heinze, R. T. Knight, and A. Richardson-Klavehn. Corticothalamic phase synchrony and
cross-frequency coupling predict human memory formation. eLIFE, 2014.
[19] Y. W. Teh, M. Seeger, and M. I. Jordan. Semiparametric latent factor models. AISTATS, 10:333?340,
2005.
[20] M. A. Tucker, Y. Hirota, E. J. Wamsley, H. Lau, A. Chaklader, and W. Fishbein. A daytime nap containing
solely non-REM sleep enhances declarative but not procedural memory. Neurobiology of Learning and
Memory, 86(2):241?7, 2006.
[21] K. Ulrich, D. E. Carlson, W. Lian, J. S. Borg, K. Dzirasa, and L. Carin. Analysis of brain states from
multi-region LFP time-series. NIPS, 2014.
[22] P. D. Welch. The use of fast Fourier transform for the estimation of power spectra: A method based on
time averaging over short, modified periodograms. IEEE Transactions on Audio and Electroacoustics,
15(2):70?73, 1967.
[23] A. G. Wilson. Covariance kernels for fast automatic pattern discovery and extrapolation with Gaussian
processes. PhD thesis, University of Cambridge, 2014.
[24] A. G. Wilson and R. P. Adams. Gaussian process kernels for pattern discovery and extrapolation. ICML,
2013.
[25] A. G. Wilson, E. Gilboa, A. Nehorai, and J. P. Cunningham. Fast kernel learning for multidimensional
pattern extrapolation. NIPS, 2014.
[26] A. G. Wilson and D. A. Knowles. Gaussian process regression networks. ICML, 2012.
? la carte ? learning fast kernels. AISTATS, 2015.
[27] Z. Yang, A. J. Smola, L. Song, and A. G. Wilson. A

9

"
5625,"Backprop KF: Learning Discriminative Deterministic
State Estimators
Tuomas Haarnoja, Anurag Ajay, Sergey Levine, Pieter Abbeel
{haarnoja, anuragajay, svlevine, pabbeel}@berkeley.edu
Department of Computer Science, University of California, Berkeley

Abstract
Generative state estimators based on probabilistic filters and smoothers are one
of the most popular classes of state estimators for robots and autonomous vehicles. However, generative models have limited capacity to handle rich sensory
observations, such as camera images, since they must model the entire distribution
over sensor readings. Discriminative models do not suffer from this limitation,
but are typically more complex to train as latent variable models for state estimation. We present an alternative approach where the parameters of the latent state
distribution are directly optimized as a deterministic computation graph, resulting
in a simple and effective gradient descent algorithm for training discriminative
state estimators. We show that this procedure can be used to train state estimators
that use complex input, such as raw camera images, which must be processed
using expressive nonlinear function approximators such as convolutional neural
networks. Our model can be viewed as a type of recurrent neural network, and
the connection to probabilistic filtering allows us to design a network architecture
that is particularly well suited for state estimation. We evaluate our approach on
synthetic tracking task with raw image inputs and on the visual odometry task in
the KITTI dataset. The results show significant improvement over both standard
generative approaches and regular recurrent neural networks.

1

Introduction

State estimation is an important component of mobile robotic applications, including autonomous
driving and flight [22]. Generative state estimators based on probabilistic filters and smoothers are
one of the most popular classes of state estimators. However, generative models are limited in their
ability to handle rich observations, such as camera images, since they must model the full distribution
over sensor readings. This makes it difficult to directly incorporate images, depth maps, and other
high-dimensional observations. Instead, the most popular methods for vision-based state estimation
(such as SLAM [22]) are based on domain knowledge and geometric principles. Discriminative
models do not need to model the distribution over sensor readings, but are more complex to train
for state estimation. Discriminative models such as CRFs [16] typically do not use latent variables,
which means that training data must contain full state observations. Most real-world state estimation
problem settings only provide partial labels. For example, we might observe noisy position readings
from a GPS sensor and need to infer the corresponding velocities. While discriminative models can
be augmented with latent state [18], this typically makes them harder to train.
We propose an efficient and scalable method for discriminative training of state estimators. Instead of
performing inference in a probabilistic latent variable model, we instead construct a deterministic
computation graph with equivalent representational power. This computation graph can then be
optimized end-to-end with simple backpropagation and gradient descent methods. This corresponds
to a type of recurrent neural network model, where the architecture of the network is informed by the
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

structure of the probabilistic state estimator. Aside from the simplicity of the training procedure, one
of the key advantages of this approach is the ability to incorporate arbitrary nonlinear components
into the observation and transition functions. For example, we can condition the transitions on raw
camera images processed by multiple convolutional layers, which have been shown to be remarkably
effective for interpreting camera images. The entire network, including the observation and transition
functions, is trained end-to-end to optimize its performance on the state estimation task.
The main contribution of this work is to draw a connection between discriminative probabilistic state
estimators and recurrent computation graphs, and thereby derive a new discriminative, deterministic
state estimation method. From the point of view of probabilistic models, we propose a method for
training expressive discriminative state estimators by reframing them as representationally equivalent
deterministic models. From the point of view of recurrent neural networks, we propose an approach
for designing neural network architectures that are well suited for state estimation, informed by
successful probabilistic state estimation models. We evaluate our approach on a visual tracking
problem, which requires processing raw images and handling severe occlusion, and on estimating
vehicle pose from images in the KITTI dataset [8]. The results show significant improvement over
both standard generative methods and standard recurrent neural networks.

2

Related Work

Some of the most successful methods for state estimation have been probabilistic generative state space models (SSMs) based on filtering and smoothing (Figure 1).
Kalman filters are perhaps the best known state estimators,
and can be extended to the case of nonlinear dynamics
through linearization and the unscented transform. Nonparametric filtering methods, such as particle filtering, are
also often used for tasks with multimodal posteriors. For
a more complete review of state estimation, we refer the
reader to standard references on this topic [22].

xt?1

xt

xt+1

ot?1

ot

ot+1

Figure 1: A generative state space model
with hidden states xi and observation ot
generated by the model. ot are observed
at both training and test time.

Generative models aim to estimate the distribution over state observation sequences o1:T as originating
from some underlying hidden state x1:T , which is typically taken to be the state space of the system.
This becomes impractical when the observation space is extremely high dimensional, and when the
observation is a complex, highly nonlinear function of the state, as in the case of vision-based state
estimation, where ot corresponds to an image viewed from a robot?s on-board camera. The challenges
of generative state space estimation can be mitigated by using complex observation models [14] or
approximate inference [15], but building effective generative models of images remains a challenging
open problem.
As an alternative to generative models, discriminative models such as conditional random fields
(CRFs) can directly estimate p(xt |o1:t ) [16]. A number of CRFs and conditional state space models
(CSSMs) have been applied to state estimation [21, 20, 12, 17, 9], typically using a log-linear
representation. More recently, discriminative fine-tuning of generative models with nonlinear neural
network observations [6], as well as direct training of CRFs with neural network factors [7], have
allowed for training of nonlinear discriminative models. However, such models have not been
extensively applied to state estimation. Training CRFs and CSSMs typically requires access to
true state labels, while generative models only require observations, which often makes them more
convenient for physical systems where the true underlying state is unknown. Although CRFs have
also been combined with latent states [18], the difficulty of CRF inference makes latent state CRF
models difficult to train. Prior work has also proposed to optimize SSM parameters with respect to a
discriminative loss [1]. In contrast to this work, our approach incorporates rich sensory observations,
including images, and allows for training of highly expressive discriminative models.
Our method optimizes the state estimator as a deterministic computation graph, analogous to recurrent
neural network (RNN) training. The use of recurrent neural networks (RNNs) for state estimation
has been explored in several prior works [24, 4, 23, 19], but has generally been limited to simple
tasks without complex sensory inputs such as images. Part of the reason for this is the difficulty of
training general-purpose RNNs. Recently, innovative RNN architectures have been successful at
mitigating this problem, through models such as the long short-term memory (LSTM) [10] and the
2

ot?1

ot

ot+1

zt?1

zt

zt+1

xt?1

xt

st?2

ot?1

ot

ot+1

g?

g?

g?

?

zt?1
st?1

yt

?

st?1

xt+1
q

yt?1

zt

yt+1

?yt?1

(a)

zt+1
st

st+1

?
st+1

st
q

q

?yt

?yt+1

(b)

Figure 2: (a) Standard two-step engineering approach for filtering with high-dimensional observations.
The generative part has hidden state xt and two observations, yt and zt , where the latter observation
is actually the output of a second deterministic model zt = g? (ot ), denoted by dashed lines and
trained explicitly to predict zt . (b) Computation graph that jointly optimizes both models in (a),
consisting of the deterministic map g? and a deterministic filter that infers the hidden state given zt .
By viewing the entire model as a single deterministic computation graph, it can be trained end-to-end
using backpropagation as explained in Section 4.

gated recurrent unit (GRU) [5]. LSTMs have been combined with vision for perception tasks such as
activity recognition [3]. However, in the domain of state estimation, such black-box models ignore
the considerable domain knowledge that is available. By drawing a connection between filtering and
recurrent networks, we can design recurrent computation graphs that are particularly well suited to
state estimation and, as shown in our evaluation, can achieve improved performance over standard
LSTM models.

3

Preliminaries

Performing state estimation with a generative model directly using high-dimensional observations
ot , such as camera images, is very difficult, because these observations are typically produced by a
complex and highly nonlinear process. However, in practice, a low-dimensional vector, zt , which can
be extracted from ot , can fully capture the dependence of the observation on the underlying state
of the system. Let xt denote this state, and let yt denote some labeling of the states that we wish
to be able to infer from ot . For example, ot might correspond to pairs of images from a camera
on an automobile, zt to its velocity, and yt to the location of the vehicle. In that case, we can first
train a discriminative model g? (ot ) to predict zt from ot in feedforward manner, and then filter the
predictions to output the desired state labels y1:t . For example, a Kalman filter with hidden state
xt could be trained to use the predicted zt as observations, and then perform inference over xt and
yt at test time. This standard approach for state estimation with high-dimensional observations is
illustrated in Figure 2a.
While this method may be viewed as an engineering solution without a probabilistic interpretation, it
has the advantage that g? (ot ) is trained discriminatively, and the entire model is conditioned on ot ,
with xt acting as an internal latent variable. This is why the model does not need to represent the
distribution over observations explicitly. However, the function g? (ot ) that maps the raw observations
ot to low-dimensional predictions zt is not trained for optimal state estimation. Instead, it is trained
to predict an intermediate variable zt that can be readily integrated into the generative filter.

4

Discriminative Deterministic State Estimation

Our contribution is based on a generalized view of state estimation that subsumes the na?ve, piecewisetrained models discussed in the previous section and allows them to be trained end-to-end using
simple and scalable stochastic gradient descent methods. In the na?ve approach, the observation
function g? (ot ) is trained to directly predict zt , since a standard generative filter model does not
provide for a straightforward way to optimize g? (ot ) with respect to the accuracy of the filter on
the labels y1:T . However, the filter can be viewed as a computation graph unrolled through time, as
shown in Figure 2b. In this graph, the filter has an internal state defined by the posterior over xt . For
3

example, in a Kalman filter with Gaussian posteriors, we can represent the internal state with the
tuple st = (?xt , ?xt ). In general, we will use st to refer to the state of any filter. We also augment
this graph with an output function q(st ) = ?yt that outputs the parameters of a distribution over
labels yt . In the case of a Kalman filter, we would simply have q(st ) = (Cy ?xt , Cy ?xt CT
y ), where
the matrix Cy defines a linear observation function from xt to yt .
Viewing the filter as a computation graph in this way, g? (ot ) can be trained discriminatively
on the entire sequence, rather than individually on single time steps. Let l(?yt ) be a loss function on the output distribution of the computation graph, which might, for example, be given by
l(?yt ) = ? log p?yt (yP
t ), where p?yt is the distribution induced by the parameters ?yt , and yt is
the label. Let L(?) = t l(?yt ) be the loss on an entire sequence with respect to ?. Furthermore,
let ?(st , zt+1 ) denote the operation performed by the filter to compute st+1 based on st and zt+1 .
We can compute the gradient of l(?) with respect to the parameters ? by first recursively computing
the gradient of the loss with respect to the filter state st from the back to the front according to the
following recursion:
d?yt?1 dL
dst dL
dL
=
+
,
dst?1
dst d?yt?1
dst?1 dst

(1)

and then applying the chain rule to obtain
?? L(?) =

T
X
dzt dst dL
.
d? dzt dst
t=1

(2)

All of the derivatives in these equations can be obtained from g? (ot ), ?(st?1 , zt ), q(st ), and l(?yt ):
dst
dst
= ?st?1 ?(st?1 , zt ),
= ?zt ?(st?1 , zt ),
dst?1
dzt
dL
dzt
d?yt
= ?? g? (ot ).
= ??yt l(?yt ),
= ?st q(st ),
d?yt
dst
d?

(3)

The parameters ? can be optimized with gradient descent using these gradients. This is an instance of
backpropagation through time (BPTT), a well known algorithm for training recurrent neural networks.
Recognizing this connection between state-space models and recurrent neural networks allows us to
extend this generic filtering architecture and explore the continuum of models between filters with
a discriminatively trained observation model g? (ot ) all the way to fully general recurrent neural
networks. In our experimental evaluation, we use a standard Kalman filter update as ?(st , zt+1 ), but
we use a nonlinear convolutional neural network observation function g? (ot ). We found that this
provides a good trade-off between incorporating domain knowledge and end-to-end learning for the
task of visual tracking and odometry, but other variants of this model could be explored in future
work.

5

Experimental Evaluation

In this section, we compare our deterministic discriminatively trained state estimator with a set
of alternative methods, including simple feedforward convolutional networks, piecewise-trained
Kalman filter, and fully general LSTM models. We evaluate these models on two tasks that require
processing of raw image input: synthetic task of tracking a red disk in the presence of clutter and
severe occlusion; and the KITTI visual odometry task [8].
5.1

State Estimation Models

Our proposed model, which we call the ?backprop Kalman filter? (BKF), is a computation graph
made up of a Kalman filter (KF) and a feedforward convolutional neural network that distills the
observation ot into a low-dimensional signal zt , which serves as the observation for the KF. The
neural network outputs both a point observation zt and an observation covariance matrix Rt . Since
the network is trained together with the filter, it can learn to use the covariance matrix to communicate
the desired degree of uncertainty about the observation, so as to maximize the accuracy of the final
filter prediction.
4

?xt?1

fc

zt

A?xt?1
reshape
diag exp Lt

h4
fc

fc

h3

ReLU

fc

h2

ReLU

ReLU

max_pool

conv

h1

Kalman filter
resp_norm

ReLU

max_pool

conv

ot

resp_norm

Feedforward network

?t
L

Lt LT
t

?0xt + Kt zt ? Cz ?0xt

Rt
0
T
?0xt CT
z Cz ?xt Cz + Rt

T

A?xt?1 A +

Bw QBT
w

?0xt

yt

?0xt

?1


?xt

Loss
PN PT
i=1

1
t=1 2T N


2


(i)
(i) 

Cy ?xt ? yt 
2

Kt
(I ? Kt Cz ) ?0xt

?xt

?xt?1

Figure 3: Illustration of the computation graph for the BKF. The graph is composed of a feedforward
?t
part, which processes the raw images ot and outputs intermediate observations zt and a matrix L
that is used to form a positive definite observation covariance matrix Rt , and a recurrent part that
integrates zt through time to produce filtered state estimates. See Appendix A for details.
We compare the backprop KF to three alternative state estimators: the ?feedforward model?, the
?piecewise KF?, and the ?LSTM model?. The simplest of the models, the feedforward model, does
not consider the temporal structure in the task at all, and consists only of a feedforward convolutional
network that takes in the observations ot and outputs a point estimate y
?t of the label yt . This approach
is viable only if the label information can be directly inferred from ot , such as when tracking an
object. On the other hand, tasks that require long term memory, such as visual odometry, cannot
be solved with a plain feedforward network. The piecewise KF model corresponds to the simple
generative approach described in Section 3, which combines the feedforward network with a Kalman
? t . The
filter that filters the network predictions zt to produce a distribution over the state estimate x
piecewise model is based on the same computation graph as the BKF, but does not optimize the filter
and network together end-to-end, instead training the two pieces separately. The only difference
between the two graphs is that the piecewise KF does not implement the additional pathway for
propagating the uncertainty from the feedforward network into the filter, but instead, the filter needs
to learn to handle the uncertainty in zt independently. An example instantiation of BKF is depicted
in Figure 3. A detailed overview of the computational blocks shown in the figure is deferred to
Appendix A.
Finally, we compare to a recurrent neural network based on LSTM hidden units [10]. This model
resembles the backprop KF, except that the filter portion of the graph is replaced with a generic
LSTM layer. The LSTM model learns the dynamics from data, without incorporating the domain
knowledge present in the KF.
5.2

Neural Network Design

A special aspect of our network design is a novel response normalization layer that is applied to the
convolutional activations before applying the nonlinearity. The response normalization transforms
the activations such that the activations of layer i have always mean ?i and variance ?i2 regardless
of the input to the layer. The parameters ?i and ?i2 are learned along with other parameters. This
normalization is used in all of the convolutional networks in our evaluation, and resembles batch
normalization [11] in its behavior. However, we found this approach to be substantially more effective
for recurrent models that require backpropagation through time, compared to the more standard
batch normalization approach, which is known to require additional care when applied to recurrent
networks. It has been since proposed independently from our work in [2], which gives an in-depth
analysis of the method. The normalization is followed by a rectified linear unit (ReLU) and a max
pooling layer.
5.3

Synthetic Visual State Estimation Task

Our state estimation task is meant to reflect some of the typical challenges in visual state estimation:
the need for long-term tracking to handle occlusions, the presence of noise, and the need to process
raw pixel data. The task requires tracking a red disk from image observations, as shown in Figure
4. Distractor disks with random colors and radii are added into the scene to occlude the red disk,
and the trajectories of all disks follow linear-Gaussian dynamics, with a linear spring force that pulls
the disks toward the center of the frame and a drag force that prevents high velocities. The disks
can temporally leave the frame since contacts are not modeled. Gaussian noise is added to perturb
the motion. While these model parameters are assumed to be known in the design of the filter, it is
a straightforward to learn also the model parameters. The difficulty of the task can be adjusted by
increasing or decreasing the number of distractor disks, which affects the frequency of occlusions.
5

Figure 4: Illustration of six consecutive frames of two training sequences. The objective is to track
the red disk (circled in the the first frame for illustrative purposes) throughout the 100-frame sequence.
The distractor disks are sampled for each sequence at random and overlaid on top of the target disk.
The upper row illustrates an easy sequence (9 distractors), while the bottom row is a sequence of high
difficulty (99 distractors). Note that the target is very rarely visible in the hardest sequences.
Table 1: Benchmark Results
State Estimation Model # Parameters RMS test error ??
feedforward model
piecewise KF
LSTM model (64 units)
LSTM model (128 units)
BKF (ours)

0.2322 ? 0.1316
0.1160 ? 0.0330
0.1407 ? 0.1154
0.1423 ? 0.1352
0.0537 ? 0.1235

7394
7397
33506
92450
7493

The easiest variants of the task are solvable with a feedforward estimator, while the hardest variants
require long-term tracking through occlusion. To emphasize the sample efficiency of the models, we
trained them using 100 randomly sampled sequences.
The results in Table 1 show that the BKF outperforms both the standard probabilistic KF-based
estimators and the more powerful and expressive LSTM estimators. The tracking error of the simple
feedforward model is significantly larger due to the occlusions, and the model tends to predict the
mean coordinates when the target is occluded. The piecewise model performs better, but because
the observation covariance is not conditioned on ot , the Kalman filter learns to use a very large
observation covariance, which forces it to rely almost entirely on the dynamics model for predictions.
On the other hand, since the BKF learns to output the observation covariances conditioned on ot that
optimize the performance of the filter, it is able to find a compromise between the observations and
the dynamics model. Finally, although the LSTM model is the most general, it performs worse than
the BKF, since it does not incorporate prior knowledge about the structure of the state estimation
problem.

6

feedforward
piecewise
LSTM

10 0

RMS error

To test the robustness of the estimator to occlusions,
we trained each model on a training set of 1000 sequences of varying amounts of clutter and occlusions.
We then evaluated the models on several test sets,
each corresponding to a different level of occlusion
and clutter. The tracking error as the test set difficulty
is varied is shown Figure 5. Note that even in the
absence of distractors, BKF and LSTM models outperform the feedforward model, since the target occasionally leaves the field of view. The performance
of the piecewise KF does not change significantly as
the difficulty increases: due to the high amount of
clutter during training, the piecewise KF learns to
use a large observation covariance and rely primarily
on feedforward estimates for prediction. The BKF
achieves the lowest error in nearly all cases. At the
same time, the BKF also has dramatically fewer parameters than the LSTM models, since the transitions
correspond to simple Kalman filter updates.

10 -1

10 -2

10 -3

0

20

40

60

80

100

# distractors

Figure 5: The RMS error of various models
trained on a single training set that contained
sequences of varying difficulty. The models
were then evaluated on several test sets of
fixed difficulty.

Figure 6: Example image sequence from the KITTI dataset (top row) and the corresponding difference
image that is obtained by subtracting the RGB values of the previous image from the current image
(bottom row). The observation ot is formed by concatenating the two images into a six-channel
feature map which is then treated as an input to a convolutional neural network. The figure shows
every fifth sample from the original sequence for illustrative purpose.
Table 2: KITTI Visual Odometry Results
Test 100
# training trajectories
Translational Error [m/m]
piecewise KF
LSTM model (128 units)
LSTM model (256 units)
BKF (ours)
Rotational Error [deg/m]
piecewise KF
LSTM model (128 units)
LSTM model (256 units)
BKF (ours)

5.4

Test 100/200/400/800

3

6

10

3

6

10

0.3257
0.5022
0.5199
0.3089

0.2452
0.3456
0.3172
0.2346

0.2265
0.2769
0.2630
0.2062

0.3277
0.5491
0.5439
0.2982

0.2313
0.4732
0.4506
0.2031

0.2197
0.4352
0.4228
0.1804

0.1408
0.5484
0.4960
0.1207

0.1028
0.3681
0.3391
0.0901

0.0978
0.3767
0.2933
0.0801

0.1069
0.4123
0.3845
0.0888

0.0768
0. 3573
0.3566
0.0587

0.0754
0.3530
0.3221
0.0556

KITTI Visual Odometry Experiment

Next, we evaluated the state estimation models on visual odometry task in the KITTI dataset [8]
(Figure 6, top row). The publicly available training set contains 11 trajectories of ego-centric video
sequences of a passenger car driving in suburban scenes, along with ground truth position and
orientation. The dataset is challenging since it is relatively small for learning-based algorithms, and
the trajectories are visually very diverse. For training the Kalman filter variants, we used a simplified
state-space model with three of the state variables corresponding to the vehicle?s 2D pose (two spatial
coordinates and heading) and two for the forward and angular velocities. Because the dynamics
model is non-linear, we equipped our model-based state estimators with extended Kalman filters,
which is a straightforward addition to the BKF framework.
The objective of the task is to estimate the relative change in the pose during fixed-length subsequences. However, because inferring the pose requires integration over all past observations, a simple
feedforward model cannot be used directly. Instead, we trained a feedforward network, consisting of
four convolutional and two fully connected layers and having approximately half a million parameters,
to estimate the velocities from pairs of images at consecutive time steps. In practice, we found it better
to use a difference image, corresponding to the change in the pixel intensities between the images,
along with the current image as an input to the feedforward network (Figure 6). The ground truth
velocities, which were used to train the piecewise KF as well as to pretrain the other models, were
computed by finite differencing from the ground truth positions. The recurrent models?piecewise
KF, the BKF, and the LSTM model?were then fine-tuned to predict the vehicle?s pose. Additionally,
for the LSTM model, we found it crucial to pretrain the recurrent layer to predict the pose from the
velocities before fine-tuning.
We evaluated each model using 11-fold cross-validation, and we report the average errors of the
held-out trajectories over the folds. We trained the models by randomly sampling subsequences of
100 time steps. For each fold, we constructed two test sets using the held-out trajectory: the first set
contains all possible subsequences of 100 time steps, and the second all subsequences of lengths
100, 200, 400, and 800.1 We repeated each experiment using 3, 6, or all 10 of the sequences in each
training fold to evaluate the resilience of each method to overfitting.
1
The second test set aims to mimic the official (publicly unavailable) test protocol. Note, however, that
because the methods are not tested on the same sequences as the official test set, they are not directly comparable
to results on the official KITTI benchmark.

7

Table 2 lists the cross-validation results. As expected, the error decreases consistently as the number
of training sequences becomes larger. In each case, BKF outperforms the other variants in both
predicting the position and heading of the vehicle. Because both the piecewise KF and the BKF
incorporate domain knowledge, they are more data-efficient. Indeed, the performance of the LSTM
degrades faster as the number of training sequences is decreased. Although the models were trained
on subsequences of 100 time steps, they were also tested on a set containing a mixture of different
sequence lengths. The LSTM model generally failed to generalize to longer sequences, while the
Kalman filter variants perform slightly better on mixed sequence lengths.

6

Discussion

In this paper, we proposed a discriminative approach to state estimation that consists of reformulating
probabilistic generative state estimation as a deterministic computation graph. This makes it possible
to train our method end-to-end using simple backpropagation through time (BPTT) methods, analogously to a recurrent neural network. In our evaluation, we present an instance of this approach that
we refer to as the backprop KF (BKF), which corresponds to a (extended) Kalman filter combined
with a feedforward convolutional neural network that processes raw image observations. Our approach to state estimation has two key benefits. First, we avoid the need to construct generative state
space models over complex, high-dimensional observation spaces such as raw images. Second, by
reformulating the probabilistic state-estimator as a deterministic computation graph, we can apply
simple and effective backpropagation and stochastic gradient descent optimization methods to learn
the model parameters. This avoids the usual challenges associated with inference in continuous,
nonlinear conditional probabilistic models, while still preserving the same representational power as
the corresponding approximate probabilistic inference method, which in our experiments corresponds
to approximate Gaussian posteriors in a Kalman filter.
Our approach also can be viewed as an application of ideas from probabilistic state-space models to
the design of recurrent neural networks. Since we optimize the state estimator as a deterministic computation graph, it corresponds to a particular type of deterministic neural network model. However,
the architecture of this neural network is informed by principled and well-motivated probabilistic
filtering models, which provides us with a natural avenue for incorporating domain knowledge into
the system.
Our experimental results indicate that end-to-end training of a discriminative state estimators can
improve their performance substantially when compared to a standard piecewise approach, where
a discriminative model is trained to process the raw observations and produce intermediate lowdimensional observations that can then be integrated into a standard generative filter. The results also
indicate that, although the accuracy of the BKF can be matched by a recurrent LSTM network with
a large number of hidden units, BKF outperforms the general-purpose LSTM when the dataset is
limited in size. This is due to the fact that BKF incorporates domain knowledge about the structure of
probabilistic filters into the network architecture, providing it with a better inductive bias when the
training data is limited, which is the case in many real-world robotic applications.
In our experiments, we primarily focused on models based on the Kalman filter. However, our
approach to state estimation can equally well be applied to other probabilistic filters for which the
update equations (approximate or exact) can be written in closed form, including the information
filter, the unscented Kalman filter, and the particle filter, as well as deterministic filters such as state
observers or moving average processes. As long as the filter can be expressed as a differentiable mapping from the observation and previous state to the new state, we can construct and differentiate the
corresponding computation graph. An interesting direction for future work is to extend discriminative
state-estimators with complex nonlinear dynamics and larger latent state. For example, one could
explore the continuum of models that span the space between simple KF-style state estimators and
fully general recurrent networks. The trade-off between these two extremes is between generality and
domain knowledge, and striking the right balance for a given problem could produce substantially
improved results even with relative modest amounts of training data.
Acknowledgments
This research was funded in part by ONR through a Young Investigator Program award, by the Army
Research Office through the MAST program, and by the Berkeley DeepDrive Center.
8

References
[1] P. Abbeel, A. Coates, M. Montemerlo, A. Y. Ng, and S. Thrun. Discriminative training of kalman filters.
In Robotics: Science and Systems (R:SS), 2005.
[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
[3] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and A. Baskurt. Sequential deep learning for human
action recognition. In Second International Conference on Human Behavior Unterstanding, pages 29?39,
Berlin, Heidelberg, 2011. Springer-Verlag.
[4] O. Bobrowski, R. Meir, S. Shoham, and Y. C. Eldar. A neural network implementing optimal state
estimation based on dynamic spike train decoding. In Advances in Neural Information Processing Systems
(NIPS), 2007.
[5] K. Cho, B. van Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078,
2014.
[6] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for
large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on,
20(1):30?42, 2012.
[7] T. Do, T. Arti, et al. Neural conditional random fields. In International Conference on Artificial Intelligence
and Statistics, pages 177?184, 2010.
[8] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The KITTI dataset. International
Journal of Robotics Research (IJRR), 2013.
[9] R. Hess and A. Fern. Discriminatively trained particle filters for complex multi-object tracking. In
Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 240?247. IEEE,
2009.
[10] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735?1780, 1997.
[11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal
covariate shift. In International Conference on Machine Learning (ICML), 2015.
[12] M. Kim and V. Pavlovic. Conditional state space models for discriminative motion estimation. In Computer
Vision, 2007. ICCV 2007. IEEE 11th International Conference on, pages 1?8. IEEE, 2007.
[13] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
[14] J. Ko and D. Fox. GP-BayesFilters: Bayesian filtering using Gaussian process prediction and observation
models. Autonomous Robots, 27(1):75?90, 2009.
[15] R. G. Krishnan, U. Shalit, and D. Sontag. Deep kalman filters. arXiv preprint arXiv:1511.05121, 2015.
[16] J. Lafferty, A. McCallum, and F. C. Pereira. Conditional random fields: Probabilistic models for segmenting
and labeling sequence data. 2001.
[17] B. Limketkai, D. Fox, and L. Liao. CRF-filters: Discriminative particle filters for sequential state
estimation. In International Conference on Robotics and Automation (ICRA), 2007.
[18] L.-P. Morency, A. Quattoni, and T. Darrell. Latent-dynamic discriminative models for continuous gesture
recognition. In Computer Vision and Pattern Recognition, 2007. CVPR?07. IEEE Conference on, pages
1?8. IEEE, 2007.
[19] P. Ondruska and I. Posner. Deep tracking: Seeing beyond seeing using recurrent neural networks. arXiv
preprint arXiv:1602.00991, 2016.
[20] D. A. Ross, S. Osindero, and R. S. Zemel. Combining discriminative features to infer complex trajectories.
In Proceedings of the 23rd international conference on Machine learning, pages 761?768. ACM, 2006.
[21] C. Sminchisescu, A. Kanaujia, Z. Li, and D. Metaxas. Discriminative density propagation for 3d human
motion estimation. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer
Society Conference on, volume 1, pages 390?397. IEEE, 2005.
[22] S. Thrun, W. Burgard, and D. Fox. Probabilistic Robotics. The MIT Press, 2005.
[23] R. Wilson and L. Finkel. A neural implementation of the kalman filter. In Advances in neural information
processing systems, pages 2062?2070, 2009.
[24] N. Yadaiah and G. Sowmya. Neural network based state estimation of dynamical systems. In International
Joint Conference on Neural Networks (IJCNN), 2006.

9

"
4736,"A Unified Semantic Embedding:
Relating Taxonomies and Attributes
Sung Ju Hwang?
Disney Research
Pittsburgh, PA
sungju.hwang@disneyresearch.com

Leonid Sigal
Disney Research
Pittsburgh, PA
lsigal@disneyresearch.com

Abstract
We propose a method that learns a discriminative yet semantic space for object
categorization, where we also embed auxiliary semantic entities such as supercategories and attributes. Contrary to prior work, which only utilized them as side information, we explicitly embed these semantic entities into the same space where
we embed categories, which enables us to represent a category as their linear combination. By exploiting such a unified model for semantics, we enforce each category to be generated as a supercategory + a sparse combination of attributes, with
an additional exclusive regularization to learn discriminative composition. The
proposed reconstructive regularization guides the discriminative learning process
to learn a model with better generalization. This model also generates compact semantic description of each category, which enhances interoperability and enables
humans to analyze what has been learned.

1

Introduction

Object categorization is a challenging problem that requires drawing boundaries between groups of
objects in a seemingly continuous space. Semantic approaches have gained a lot of attention recently
as object categorization became more focused on large-scale and fine-grained recognition tasks and
datasets. Attributes [1, 2, 3, 4] and semantic taxonomies [5, 6, 7, 8] are two popular semantic sources
which impose certain relations between the category models, including a more recently introduced
analogies [9] that induce even higher-order relations between them. While many techniques have
been introduced to utilize each of the individual semantic sources for object categorization, no unified model has been proposed to relate them.
We propose a unified semantic model where we can learn to place categories, supercategories, and
attributes as points (or vectors) in a hypothetical common semantic space, and taxonomies provide
specific topological relationships between these semantic entities. Further, we propose a discriminative learning framework, based on dictionary learning and large margin embedding, to learn each
of these semantic entities to be well separated and pseudo-orthogonal, such that we can use them to
improve visual recognition tasks such as category or attribute recognition.
However, having semantic entities embedded into a common space is not enough to utilize the
vast number of relations that exist between the semantic entities. Thus, we impose a graph-based
regularization between the semantic embeddings, such that each semantic embedding is regularized
by sparse combination of auxiliary semantic embeddings. This additional requirement imposed on
the discriminative learning model would guide the learning such that we obtain not just the optimal
model for class discrimination, but to learn a semantically plausible model which has a potential to
be more robust and human-interpretable; we call this model Unified Semantic Embedding (USE).
?

Now at Ulsan National Institute of Science and Technology in Ulsan, South Korea

1

Figure 1: Concept: We regularize each category
to be represented by its supercategory + a sparse
combination of attributes, where the regularization
parameters are learned. The resulting embedding
model improves the generalization ability by the
specific relations between the semantic entities, and
also is able to compactly represent a novel category
in this manner. For example, given a novel category
tiger, our model can describe it as a striped feline.

The observation we make to draw the relation between the categories and attributes, is that a category
can be represented as the sum of its supercategory + the category-specific modifier, which in many
cases can be represented by a combination of attributes. Further, we want the representation to be
compact. Instead of describing a dalmatian as a domestic animal with a lean body, four legs, a
long tail, and spots, it is more efficient to say it is a spotted dog (Figure 1). It is also more exact
since the higher-level category dog contains all general properties of different dog breeds, including
indescribable dog-specific properties, such as the shape of the head, and its posture.
This exemplifies how a human would describe an object, to efficiently communicate and understand
the concept. Such decomposition of a category into attributes+supercategory can hold for categories
at any level. For example, supercategory feline can be described as a stalking carnivore.
With the addition of this new generative objective, our goal is to learn a discriminative model that
can be compactly represented as a combination of semantic entities, which helps learn a model that is
semantically more reasonable. We want to balance between these two discriminative and generative
objectives when learning a model for each object category. For object categories that have scarce
training examples, we can put more weight on the generative part of the model.
Contributions: Our contributions are threefold: (1) We show a multitask learning formulation for
object categorization that learns a unified semantic space for supercategories and attributes, while
drawing relations between them. (2) We propose a novel sparse-coding based regularization that
enforces the object category representation to be reconstructed as the sum of a supercategory and a
sparse combination of attributes. (3) We show from the experiments that the generative learning with
the sparse-coding based regularization helps improve object categorization performance, especially
in the one or few-shot learning case, by generating semantically plausible predictions.

2

Related Work

Semantic methods for object recognition. For many years, vision researchers have sought to
exploit external semantic knowledge about the object to incorporate semantics into learning of the
model. Taxonomies, or class hierarchies were the first to be explored by vision researchers [5, 6], and
were mostly used to efficiently rule out irrelevant category hypotheses leveraging class hierarchical
structure [8, 10]. Attributes are visual or semantic properties of an object that are common across
multiple categories, mostly regarded as describable mid-level representations. They have been used
to directly infer categories [1, 2], or as additional supervision to aid the main categorization problem
in the multitask learning framework [3]. While many methods have been proposed to leverage either
of these two popular types of semantic knowledge, little work has been done to relate the two, which
our paper aims to address.
Discriminative embedding for object categorization. Since the conventional kernel-based multiclass SVM does not scale due to its memory and computational requirements for today?s large-scale
classification tasks, embedding-based methods have gained recent popularity. Embedding-based
methods perform classification on a low dimensional shared space optimized for class discrimination. Most methods learn two linear projections, for data instances and class labels, to a common
lower-dimensional space optimized by ranking loss. Bengio et al. [10] solves the problem using
stochastic gradient, and also provides a way to learn a tree structure which enables one to efficiently
predict the class label at the test time. Mensink et al. [11] eliminated the need of class embedding by
replacing them with the class mean, which enabled generalization to new classes at near zero cost.
There are also efforts in incorporating semantic information into the learned embedding space.
Weinberger et al. [7] used the taxonomies to preserve the inter-class similarities in the learned space,
2

in terms of distance. Akata et al. [4] used attributes and taxonomy information as labels, replacing
the conventional unit-vector based class representation with more structured labels to improve on
zero-shot performance. One most recent work in this direction is DEVISE [12], which learns embeddings that maximize the ranking loss, as an additional layer on top of the deep network for both
images and labels. However, these models impose structure only on the output space, and structure
on the learned space is not explicitly enforced, which is our goal.
Recently, Hwang et al. [9] introduced one such model, which regularizes the category quadruplets,
that form an analogy, to form a parallelogram. Our goal is similar, but we explore a more general
compositional relationship, which we learn without any manual supervision.
Multitask learning. Our work can be viewed as a multitask learning method, since we relate
each model for different semantic entities by learning both the joint semantic space and enforcing
geometric constraints between them. Perhaps the most similar work is [13], where the parameter
of each model is regularized while fixing the parameter for its parent-level models. We use similar
strategy but instead of enforcing sharing between the models, we simply learn each model to be
close to its approximation obtained using higher-level (more abstract) concepts.
Sparse coding. Our method to approximate each category embedding as a sum of its direct supercategory plus a sparse combination of attributes, is similar to the objective of sparse coding. One
work that is specifically relevant to ours is Mairal et al. [14], where the learning objective is to reduce both the classification and reconstruction error, given class labels. In our model, however, the
dictionary atoms are also discriminatively learned with supervision, and are assembled to be a semantically meaningful combination of a supercategory + attributes, while [14] learns the dictionary
atoms in an unsupervised way.

3

Approach

We now explain our unified semantic embedding model, which learns a discriminative common
low-dimensional space to embed both the images and semantic concepts including object categories,
while enforcing relationships between them using semantic reconstruction.
Suppose that we have a d-dimensional image descriptor and m-dimensional vector describing labels
associated with the instances, including category labels at different semantic granularities and attributes. Our goal then is to embed both images and the labels onto a single unified semantic space,
where the images are associated with their corresponding semantic labels.
To formally state the problem, given a training set D that has N labeled examples, i.e. D =
d
{xi , yi }N
i=1 , where xi ? R denotes image descriptors and yi ? {1, . . . , m} are their labels associated with m unique concepts, we want to embed each xi as zi , and each label yi as uyi in the
de -dimensional space, such that the similarity between zi and uyi , S(zi , uyi ) is maximized.
One way to solve the above problem is to use regression, using S(zi , uyi ) = ?kzi ? uyi k22 . That is,
we estimate the data embedding zi as zi = W xi , and minimize their distances to the correct label
embeddings uyi ? Rm where the dimension for yi is set to 1 and every other dimension is set to 0:
m X
N
X
min
kW xi ? uyi k22 + ?kW k2F .
(1)
W

c=1 i=1

The above ridge regression will project each instance close to its correct embedding. However, it
does not guarantee that the resulting embeddings are well separated. Therefore, most embedding
methods for categorization add in discriminative constraints which ensure that the projected instances have higher similarity to their own category embedding than to others. One way to enforce
this is to use large-margin constraints on distance: kW xi ?uyi k22 +1 ? kW xi ?uc k22 +?ic , yi 6= c
which can be translated into to the following discriminative loss:
X
LC (W , U , xi , yi ) =
[1 + kW xi ? uyi k22 ? kW xi ? uc k22 ]+ , ?c 6= yi ,
(2)
c

where U is the columwise concatenation of each label embedding vector, such that uj denotes jth
column of U . After replacing the generative loss in the ridge regression formula with the discriminative loss, we get the following discriminative learning problem:
N
X
(3)
min
LC (W , U , xi , yi ) + ?kW k2F + ?kU k2F , yi ? {1, . . . , m},
W ,U

i

3

where ? regularizes W and U from shooting to infinity. This is one of the most common objective
used for learning discriminative category embeddings for multi-class classification [10, 7], while
ranking loss-based [15] models have been also explored for LC . Bilinear model on a single variable
W has been also used in Akata et al. [4], which uses structured labels (attributes) as uyi .
3.1

Embedding auxiliary semantic entities.

Now we describe how we embed the supercategories and attributes onto the learned shared space.
Supercategories. While our objective is to better categorize entry level categories, categories in
general can appear at different semantic granularities. For example, a zebra could be both an equus,
and an odd-toed ungulate. To learn the embeddings for the supercategories, we map each data
instance to be closer to its correct supercategory embedding than to its siblings: kW xi ?us k22 +1 ?
kW xi ? uc k22 + ?sc , ?s ? Pyi and c ? Ss where Pyi denotes the set of superclasses at all levels
for class s, and Ss is the set of its siblings. The constraints can be translated into the following loss
term:
X X
[1 + kW xi ? us k22 ? kW xi ? uc k22 ]+ .
(4)
LS (W , U , xi , yi ) =
s?Pyi c?Ss

Attributes. Attributes can be considered normalized basis vectors for the semantic space, whose
combination represents a category. Basically, we want to maximize the correlation between the
projected instance that possess the attribute, and its correct attribute embedding, as follows:
LA (W , U , xi , yi ) = 1 ?

X

(W xi )T yia ua , kua k2 ? 1, yia ? {0, 1}, ?a ? Ayi ,

(5)

a

where Ac is the set of all attributes for class c and ua is an embedding vector for an attribute a.
3.2

Relationship between the categories, supercategories, and attributes

Simply summing up all previously defined loss functions while adding {us } and {ua } as additional columns of U will result in a multi-task formulation that implicitly associate the semantic
entities, through the shared data embedding W . However, we want to further utilize the relationships between the semantic entities, to explicitly impose structural regularization on the semantic
embeddings U . One simple and intuitive relation is that an object class can be represented as the
combination of its parent level category plus a sparse combination of attributes, which translates into
the following constraint:
uc = up + U A ?c , c ? Cp , k?c k0  ?1 , ?c  0, ?c ? {1, . . . , C},

(6)

A

where U is the aggregation of all attribute embeddings {ua }, Cp is the set of children classes for
class p, ?1 is the sparsity parameter, and C is the number of categories. We require ? to be nonnegative, since it makes more sense and more efficient to describe an object with attributes that it
might have, rather than describing it by attributes that it might not have.
We rewrite Eq. 7 into a regularization term as follows, replacing the `0 -norm constraints with `1 norm regularizations for tractable optimization:
C
X
R(U , B) =
kuc ? up ? U A ?c k22 + ?2 k?c + ?o k22 ,
c
(7)
c ? Cp , o ? Pc ? Sc , 0  ?c  ?1 , ?c ? {1, . . . , C},
where B is the matrix whose jth column vector ?j is the reconstruction weight for class j, Sc is the
set of all sibling classes for class c, and ?2 is the parameters to enforce exclusivity.
The exclusive regularization term is used to prevent the semantic reconstruction ?c for class c from
fitting to the same attributes fitted by its parents and siblings. This is because attributes common
across parent and child, and between siblings, are less discriminative. This regularization is especially useful for discrimination between siblings, which belong to the same superclass and only
differ by the category-specific modifier. By generating unique semantic decomposition for each
class, we can better discriminate between any two categories using a semantic combination of discriminatively learned auxiliary entities.
4

With the sparsity regularization enforced by ?1 , the simple sum of the two weights will prevent the
two (super)categories from having high weight for a single attribute, which will let each category
embedding to fit to exclusive attribute set. This, in fact, is the exclusive lasso regularizer introduced
in [16], except for the nonnegativity constraint on ?c , which makes the problem easier to solve.
3.3

Unified semantic embeddings for object categorization

After augmenting the categorization objective in Eq. 3 with the superclass and attributes loss and the
sparse-coding based regularization in Eq. 7, we obtain the following multitask learning formulation
that jointly learns all the semantic entities along with the sparse-coding based regularization:
min

N
X

W ,U ,B

kwj k22

LC (W , U , xi , yi ) + ?1 (LS (W , U , xi , yi ) + LA (W , U , xi , yi )) + ?2 R(U , B);

i=1

?

?, kuk k22

(8)

? ?, 0  ?c  ?1 ?j ? {1, . . . , d}, ?k ? {1, . . . , m}, ?c ? {1, . . . , C + S},

where S is the number of supercategories, wj is W ?s jth column, and ?1 and ?2 are parameters to
balance between the main and auxiliary tasks, and discriminative and generative objective.
Eq. 8 could be also used for knowledge transfer when learning a model for a novel set of categories,
by replacing U A in R(U , B) with U S , learned on class set S to transfer the knowledge from.
3.4

Numerical optimization

Eq. 8 is not jointly convex in all variables, and has both discriminative and generative terms. This
problem is similar to the problem in [14], where the objective is to learn the dictionary, sparse
coefficients, and classifier parameters together, and can be optimized using a similar alternating
optimization, while each subproblem differs. We first describe how we optimize for each variable.
Learning of W and U . The optimization of both embedding models are similar, except for the
reconstructive regularization on U . and the main bottleneck lies in the minimization of the O(N m)
large-margin losses. Since the losses are non-differentiable, we solve the problems using stochastic
subgradient method. Specifically, we implement the proximal gradient algorithm in [17], handling
the `-2 norm constraints with proximal operators.
Learning B. This is similar to the sparse coding problem, but simpler. We use projected gradient
t+ 1
method, where at each iteration t, we project the solution of the objective ?c 2 for category c to `-1
norm ball and nonnegative orthant, to obtain ?ct that satisfies the constraints.
Alternating optimization. We decompose Eq. 8 to two convex problems: 1) Optimization of the
data embedding W and approximation parameter B (Since the two variable do not have direct link
between them) , and 2) Optimization of the category embedding U . We alternate the process of
optimizing each of the convex problems while fixing the remaining variables, until the convergence
criterion 1 is met, or the maximum number of iteration is reached.
Run-time complexity. Training: Optimization of W and U using proximal stochastic gradient [17], have time complexities of O(de d(k + 1)) and O(de (dk + C)) respectively. Both terms are
dominated by the gradient computation for k(k  N ) sampled constraints, that is O(de dk). Outer
loop for alternation converges within 5-10 iterations depending on . Test: Test time complexity is
exactly the same as in LME, which is O(de (C + d)).

4

Results

We validate our method for multiclass categorization performance on two different datasets generated from a public image collection, and also test for knowledge transfer on few-shot learning.
4.1

Datasets

We use Animals with Attributes dataset [1], which consists of 30, 475 images of 50 animal classes,
with 85 class-level attributes 2 . We use the Wordnet hierarchy to generate supercategories. Since
kW t+1 ? W t k2 + kU t+1 ? U t k2 + kB t+1 ? B t k2 < 
Attributes are defined on color (black, orange), texture (stripes, spots), parts (longneck, hooves), and other
high-level behavioral properties (slow, hibernate, domestic) of the animals
1

2

5

there is no fixed training/test split, we use {30,30,30} random split for training/validation/test. We
generate the following two datasets using the provided features. 1) AWA-PCA: We compose a 300dimensional feature vectors by performing PCA on each of 6 types of features provided, including
SIFT, rgSIFT, SURF, HoG, LSS, and CQ to have 50 dimensions per each feature type, and concatenating them. 2) AWA-DeCAF: For the second dataset, we use the provided 4096-D DeCAF features
[18] obtained from the layer just before the output layer of a deep convolutional neural network.
4.2

Baselines

We compare our proposed method against multiple existing embedding-based categorization approaches, that either do not use any semantic information, or use semantic information but do not
explicitly embed semantic entities. For non-semantics baselines, we use the following: 1)Ridge
Regression: A linear regression with `-2 norm (Eq. 1). 2) NCM: Nearest mean classifier from [11],
which uses the class mean as category embeddings (uc = x?c ). We use the code provided by the
authors3 . 3) LME: A base large-margin embedding (Eq. 3) solved using alternating optimization.
For implicit semantic baselines, we consider two different methods. 4) LMTE: Our implementation
of the Weinberger et al. [7], which enforces the semantic similarity between class embeddings as
distance constraints [7], where U is regularized to preserve the pairwise class similarities from a
given taxonomy. 5-7) ALE, HLE, AHLE: Our implementation of the attribute label embedding in
Akata et al. [4], which encodes the semantic information by representing each class with structured
labels that indicate the class? association with superclasses and attributes. We implement variants
that use attributes (ALE), leaf level + superclass labels (HLE), and both (AHLE) labels.
For our models, we implement multiple variants to analyze the impact of each semantic entity and
the proposed regularization. 1) LME-MTL-S: The multitask semantic embedding model learned
with supercategories. 2) LME-MTL-A: The multitask embedding model learned with attributes. 3)
USE-No Reg.: The unified semantic embedding model learned using both attributes and supercategories, without semantic regularization. 4) USE-Reg: USE with the sparse coding regularization.
For parameters, the projection dimension de = 50 for all our models. 4 For other parameters, we
find the optimal value by cross-validation on the validation set. We set ?1 = 1 that balances the main
and auxiliary task equally, and search for ?2 for discriminative/generative tradeoff, in the range of
{0.01, 0.1, 0.2 . . . , 1, 10}, and set `-2 norm regularization parameter ? = 1. For sparsity parameter
?1 , we set it to select on average several (3 or 4) attributes per class, and for disjoint parameter ?2 ,
we use 10?1 , without tuning for performance.

No
semantics
Implicit
semantics
Explicit
semantics
USE

Method
Ridge Regression
NCM [11]
LME
LMTE [7]
ALE [4]
HLE [4]
AHLE [4]
LME-MTL-S
LME-MTL-A
USE-No Reg.
USE-Reg.

1
19.31 ? 1.15
18.93 ? 1.71
19.87 ? 1.56
20.76 ? 1.64
15.72 ? 1.14
17.09 ? 1.09
16.65 ? 0.47
20.77 ? 1.41
20.65 ? 0.83
21.07 ? 1.53
21.64 ? 1.02

Flat hit @ k (%)
2
28.34 ? 1.53
29.75 ? 0.92
30.47 ? 1.56
30.71 ? 1.35
25.63 ? 1.44
27.52 ? 1.20
26.55 ? 0.77
32.09 ? 1.67
31.51 ? 0.72
31.59 ? 1.57
32.69 ? 0.83

5
44.17 ? 2.33
47.33 ? 1.60
48.07 ? 1.06
47.76 ? 2.25
43.42 ? 1.67
45.49 ? 0.61
43.05 ? 1.22
50.94 ? 1.21
49.40 ? 0.62
50.11 ? 1.51
52.04 ? 1.02

Hierarchical precision @ k (%)
2
5
28.95 ? 0.54
39.39 ? 0.17
30.81 ? 0.53
43.43 ? 0.53
30.98 ? 0.62
42.63 ? 0.56
31.05 ? 0.71
43.13 ? 0.29
29.26 ? 0.50
43.71 ? 0.34
30.51 ? 0.48
44.76 ? 0.20
29.49 ? 0.89
43.41 ? 0.65
33.71 ? 0.94
45.73 ? 0.71
31.69 ? 0.49
43.47 ? 0.23
33.67 ? 0.55
45.41 ? 0.43
33.37 ? 0.74
47.17 ? 0.91

Table 1: Multiclass classification performance on AWA-PCA dataset (300-D PCA features).
4.3

Multiclass categorization

We first evaluate the suggested multitask learning framework for categorization performance. We
report the average classification performance and standard error over 5 random training/test splits
in Table 1 and 2, using both flat hit@k, which is the accuracy for the top-k predictions made, and
hierarchical precision@k from [12], which is a precision the given label is correct at k, at all levels.
Non-semantic baselines, ridge regression and NCM, were outperformed by our most basic LME
model. For implicit semantic baselines, ALE-variants underperformed even the ridge regression
3
4

http://staff.science.uva.nl/?tmensink/code.php
Except for ALE variants where de =m, the number of semantic entities.

6

No
semantics
Implicit
semantics
Explicit
semantics
USE

Method
Ridge Regression
NCM [11]
LME
LMTE [7]
ALE [4]
HLE [4]
AHLE [4]
LME-MTL-S
LME-MTL-A
USE-No Reg.
USE-Reg.

1
38.39 ? 1.48
43.49 ? 1.23
44.76 ? 1.77
38.92 ? 1.12
36.40 ? 1.03
33.56 ? 1.64
38.01 ? 1.69
45.03 ? 1.32
45.55 ? 1.71
45.93 ? 1.76
46.42 ? 1.33

Flat hit @ k (%)
2
48.61 ? 1.29
57.45 ? 0.91
58.08 ? 2.05
49.97 ? 1.16
50.43 ? 1.92
45.93 ? 2.56
52.07 ? 1.19
57.73 ? 1.75
58.60 ? 1.76
59.37 ? 1.32
59.54 ? 0.73

5
62.12 ? 1.20
75.48 ? 0.58
75.11 ? 1.48
63.35 ? 1.38
70.25 ? 1.97
64.66 ? 1.77
71.53 ? 1.41
74.43 ? 1.26
74.97 ? 1.15
74.97 ? 1.15
76.62 ? 1.45

Hierarchical precision @ k (%)
2
5
38.51 ? 0.61
41.73 ? 0.54
45.25 ? 0.52
50.32 ? 0.47
44.84 ? 0.98
49.87 ? 0.39
38.67 ? 0.46
41.72 ? 0.45
42.52 ? 1.17
52.46 ? 0.37
46.11 ? 2.65
56.79 ? 2.05
44.43 ? 0.66
54.39 ? 0.55
46.05 ? 0.89
51.08 ? 0.36
44.23 ? 0.95
48.52 ? 0.29
47.13 ? 0.62
51.04 ? 0.46
47.39 ? 0.82
53.35 ? 0.30

Table 2: Multiclass classification performance on AWA-DeCAF dataset (4096-D DeCAF features).
baseline with regard to the top-1 classification accuracy 5 , while they improve upon the top-2 recognition accuracy and hierarchical precision. This shows that hard-encoding structures in the label
space do not necessarily improve the discrimination performance, while it helps to learn a more
semantic space. LMTE makes substantial improvement on 300-D features, but not on DeCAF features.
Explicit embedding of semantic entities using our method improved both the top-1 accuracy and
the hierarchical precision, with USE variants achieving the best performance in both. Specifically,
adding superclass embeddings as auxiliary entities improves the hierarchical precision, while using
attributes improves the flat top-k classification accuracy. USE-Reg, especially, made substantial
improvements on flat hit and hierarchical precision @ 5, which shows the proposed regularization?s
effectiveness in learning a semantic space that also discriminates well.
Category

Otter

Skunk

Deer

Moose
Equine
Primate

Ground-truth attributes

Supercategory + learned attributes

An animal that swims, fish, water, new world, small, flippers,
furry, black, brown, tail, . . .

A musteline mammal that is quadrapedal, flippers, furry,
ocean

An animal that is smelly, black, stripes, white, tail, furry,
ground, quadrapedal, new world, walks, . . .

A musteline mammal that has stripes

An animal that is brown, fast, horns, grazer, forest,
quadrapedal, vegetation, timid, hooves, walks, . . .

A deer that has spots, nestspot, longneck, yellow, hooves

An animal that has horns, brown, big, quadrapedal, new
world, vegetation, grazer, hooves, strong, ground,. . .

A deer that is arctic, stripes, black

N/A
N/A

An odd-toed ungulate, that is lean and active
An animal, that has hands and bipedal

Table 3: Semantic description generated using ground truth attributes labels and learned semantic decomposition of each categorys. For ground truth labels, we show top-10 ranked by their human-ranked relevance. For
our method, we rank the attributes by their learned weights. Incorrect attributes are colored in red.

4.3.1

Qualitative analysis

Besides learning a space that is both discriminative and generalizes well, our method?s main advantage, over existing methods, is its ability to generate compact, semantic descriptions for each
category it has learned. This is a great caveat, since in most models, including the state-of-the
art deep convolutional networks, humans cannot understand what has been learned; by generating
human-understandable explanation, our model can communicate with the human, allowing understanding of the rationale behind the categorization decisions, and to possibly allow feedback for
correction.
To show the effectiveness of using supercategory+attributes in the description, we report the learned
reconstruction for our model, compared against the description generated by its ground-truth attributes in Table 3. The results show that our method generates compact description of each category, focusing on its discriminative attributes. For example, our method select attributes such as
flippers for otter, and stripes for skunk, instead of attributes common and nondescriminative such as
tail. Note that some attributes that are ranked less relevant by humans were selected for their discriminativity, e.g., yellow for dear and black for moose, both of which human annotators regarded
5

We did extensive parameter search for the ALE variants.

7

placental

ungulate
horns
hooves
longneck
carnivore
pads
stalker
paws
aquatic
plankton
ocean
swims

rodent
plankton

g.ape

feline
plankton
yellow
orange

canine
longneck

whale
longleg
plains
fields

primate
plankton
hands
bipedal

even?toed
hunter

dog
strainteeth
toughskin
longneck

bear
musteln procyonid
pinnpd
dolphin
plankton strainteethlongneck
walks
planktonbaleen
longneck longneck toughskin
ground
longneck
strainteethplankton strainteeth
stalker

sheperd

cat
strainteeth
toughskin
hairless
big cat
toughskin
longneck
big

ruminant
plankton
meatteeth
hunter
bovid
hooves
horns
grazer

odd?toed ungulate
plankton
meatteeth
hunter

domestic
plankton
longneck
toughskin

equine
hunter
meatteeth
small

deer
muscle

bovine
small
meatteeth

s
es
irl
ha
s in
s
rn ou hsk ds
ho ulb ug pa e
b to ic iv
:
t ct
w h
co bus low es y a
: : s dom ilit s
g e
ox lo
h
a
v
ffa fis an oo
ck
e
bu p: le n h
s
gn
ee e: lea
im
on
sh lop ts e e l
sw
h
te po tiv ng
is
an : s ac ora ite h f
er : in w wh et
de se ello els inte ush
oo y nn tra b s
m e: tu : s eck lain
f
f
k
h
ra a us n p
et
gi we am long wn
te
o
g: ot s br s at
pi op ripe in usk me ws
k t
pp st s e er la
hi a: ugh ngl talk ds c
br to ju s a
ze e: s: tive tic p ts
rs ro ac es po
ho oce in om w s ly
in e: d llo el
rh es at: ye sm
am c ng us
Si ian tro lbo
rs : s bu h
Pe cat ert fis
er
b es ts e eth alk
bo : d spo ang atte y st y
r e ra ra
n :
o
lio ard up ic m g g tic g ss
op ro st le es rle
le r: g me long om hai
d g
e do :
tig e: erd eak gle
lli h w lon
co hep a: ts
u o
.s h p e
G ua : s scl h t w
h n u fis as lo
f
s
hi ia
C at tic m ge ts on
lm rc an po kt
da : a s or es s lan
p l
f
ol rn ip s ta
w ho str sk as
u
x: n: : t co r
fo oo da up aze
s
cc an gro gr e tic
ra t p ks ck oov arc s
n
e h ly
ai
an us n
l
gi r: t ong low me unt
te : l s s s o
ot sel orn ean ig m
ea h oc b
s
w k: r: ave
im
un ea c
sw
sk r b ar: l
c
la be tai ive esti n
a
po ly : ct
m
le
k
a
z
iz ac in do at th
gr pb le: ive me tee
s
m a ct th ck
nd
hu wh : a ttee r bu ks ha
l
n
ue hi a e a cts
bl lp me raz ll w se
do e: s g ma in
c. hal usk s s skin
w : t in h
r
k. us nta oug ous aze
r u t lb r c
g i
al o :
w : m ee l bu ns est
al nz da or m
se pa ipe : h do
im b ey up eth l
ch lla: nk gro tte da e
ri mo st ea ipe tiv
go er ore s m r b nac
id : f ld lke s i
sp se fie sta lk s
a
ou rce ds w ipe
m fie pa hes str
t: l: tc ds ss
ra irre pa pa irle
s ha
u r:
h
sq ste wim lys h bus
m : s f us s
ha er ton n b usk
av nk to n t
be pla ank kto els
t: pl an n
ba it: pl tun
:
bb nt on
ra ha nkt
ep la
el e: p
ol
m

Figure 2: Learned discriminative attributes association on the AWA-PCA dataset. Incorrect attributes are
colored in red.
AWA?DeCAF
70

40

60

35
30
No transfer
AHLE
USE
USE?Reg.

25
20
15

0

2
4
6
8
Number of training examples

10

Accuracy (%)

Accuracy (%)

AWA?PCA
45

Class
Humpback
whale
Leopard

50
40
No transfer
AHLE
USE
USE?Reg.

30
20

0

2
4
6
8
Number of training examples

10

Learned decomposition
A baleen whale, with plankton, flippers, blue, skimmer, arctic
A big cat that is orange, claws, black
An even-toed ungulate, that is gray,
Hippopotamus
bulbous, water, smelly, hands
A primate, that is mountains, strong,
Chimpanzee
stalker, black
A domestic cat, that is arctic, nestspot,
Persian Cat
fish, bush

Figure 3: Few-shot experiment result on the AWA dataset, and generated semantic decompositions.
as brown. Further, our method selects discriminative attributes for each supercategory, while there
is no provided attribute label for supercategories.
Figure 2 shows the discriminative attributes disjointly selected at each node on the class hierarchy.
We observe that coarser grained categories fit to attributes that are common throughout all its children (e.g. pads, stalker and paws for carnivore), while the finer grained categories fit to attributes
that help for finer-grained distinctions (e.g. orange for tiger, spots for leopard, and desert for lion).
4.4 One-shot/Few-shot learning
Our method is expected to be especially useful for few-shot learning, by generating a richer description than existing methods, that approximate the new input category using either trained categories
or attributes. For this experiment, we divide the 50 categories into predefined 40/10 training/test
split, and compare with the knowledge transfer using AHLE. We assume that no attribute label is
provided for test set. For AHLE, and USE, we regularize the learning of W with W S learned
on training class set S by adding ?2 kW ? W S k22 , to LME (Eq. 3). For USE-Reg we use the
reconstructive regularizer to regularize the model to generate semantic decomposition using U S .
Figure 3 shows the result, and the learned semantic decomposition of each novel category. While all
methods make improvements over the no-transfer baseline, USE-Reg achieves the most improvement, improving two-shot result on AWA-DeCAF from 38.93% to 49.87%, where USE comes in
second with 44.87%. Most learned reconstructions look reasonable, and fit to discriminative traits
that help to discriminate between the test classes, which in this case are colors; orange for leopard,
gray for hippopotamus, blue for humpback whale, and arctic (white) for Persian cat.

5

Conclusion

We propose a unified semantic space model that learns a discriminative space for object categorization, with the help of auxiliary semantic entities such as supercategories and attributes. The auxiliary
entities aid object categorization both indirectly, by sharing a common data embedding, and directly,
by a sparse-coding based regularizer that enforces the category to be generated by its supercategory
+ a sparse combination of attributes. Our USE model improves both the flat-hit accuracy and hierarchical precision on the AWA dataset, and also generates semantically meaningful decomposition
of categories, that provides human-interpretable rationale.
8

References
[1] Christoph Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to Detect Unseen Object Classes by Between-Class Attribute Transfer. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2009.
[2] Ali Farhadi, Ian Endres, Derek Hoiem, and David Forsyth. Describing Objects by their Attributes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.
[3] Sung Ju Hwang, Fei Sha, and Kristen Grauman. Sharing features between objects and their
attributes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
1761?1768, 2011.
[4] Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. Label-Embedding for
Attribute-Based Classification. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 819?826, 2013.
[5] Marcin Marszalek and Cordelia Schmid. Constructing category hierarchies for visual recognition. In European Conference on Computer Vision (ECCV), 2008.
[6] Gregory Griffin and Pietro Perona. Learning and using taxonomies for fast visual categorization. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1?8,
2008.
[7] Kilian Q. Weinberger and Olivier Chapelle. Large margin taxonomy embedding for document
categorization. In Neural Information Processing Systems (NIPS), pages 1737?1744, 2009.
[8] Tianshi Gao and Daphne Koller. Discriminative learning of relaxed hierarchy for large-scale
visual recognition. International Conference on Computer Vision (ICCV), pages 2072?2079,
2011.
[9] Sung Ju Hwang, Kristen Grauman, and Fei Sha. Analogy-preserving semantic embedding for
visual object categorization. In International Conference on Machine Learning (ICML), pages
639?647, 2013.
[10] Samy Bengio, Jason Weston, and David Grangier. Label Embedding Trees for Large MultiClass Task. In Neural Information Processing Systems (NIPS), 2010.
[11] Thomas Mensink, Jakov Verbeek, Florent Perronnin, and Gabriela Csurka. Distance-based
image classification: Generalizing to new classes at near zero cost. IEEE Transactions on
Pattern Analysis and Machine Intelligence (TPAMI), 35(11), 2013.
[12] Andrea Frome, Greg Corrado, Jon Shlens, Samy Bengio, Jeffrey Dean, Marc?Aurelio Ranzato,
and Tomas Mikolov. Devise: A deep visual-semantic embedding model. In Neural Information
Processing Systems (NIPS), 2013.
[13] Alon Zweig and Daphna Weinshall. Hierarchical regularization cascade for joint learning. In
International Conference on Machine Learning (ICML), volume 28, pages 37?45, 2013.
[14] Julien Mairal, Francis Bach, Jean Ponce, Guillermo Sapiro, and Andrew Zisserman. Supervised dictionary learning. In Neural Information Processing Systems (NIPS), pages 1033?
1040, 2008.
[15] Jason Weston, Samy Bengio, and Nicolas Usunier. Wsabie: Scaling up to large vocabulary
image annotation. In International Joint Conferences on Artificial Intelligence (IJCAI), 2011.
[16] Yang Zhou, Rong Jin, and Steven C. H. Hoi. Exclusive lasso for multi-task feature selection.
Journal of Machine Learning Research, 9:988?995, 2010.
[17] John Duchi and Yoram Singer. Efficient online and batch learning using forward backward
splitting. Journal of Machine Learning Research, 10, 2009.
[18] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor
Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In
International Conference on Machine Learning (ICML), 2014.

9

"
3564,"PAC-Bayesian Analysis of Contextual Bandits
Yevgeny Seldin1,4 Peter Auer2 Franc?ois Laviolette3 John Shawe-Taylor4 Ronald Ortner2
1
Max Planck Institute for Intelligent Systems, T?ubingen, Germany
2
Chair for Information Technology, Montanuniversit?at Leoben, Austria
3
D?epartement d?informatique, Universit?e Laval, Qu?ebec, Canada
4
Department of Computer Science, University College London, UK
seldin@tuebingen.mpg.de, {auer,ronald.ortner}@unileoben.ac.at,
francois.laviolette@ift.ulaval.ca, jst@cs.ucl.ac.uk

Abstract
We derive an instantaneous (per-round) data-dependent regret bound for stochastic multiarmed bandits with side information (also known as contextual bandits).
The scaling of our regret bound with the number of states (contexts) N goes as
p
N I?t (S; A), where I?t (S; A) is the mutual information between states and actions (the side information) used by the algorithm at round
p t. If the algorithm
uses all the side information, the regret bound scales as N ln K, where K is
the number of actions (arms). However, if the side information I?t (S; A) is not
fully used, the regret bound is significantly tighter. In the extreme case, when
I?t (S; A) = 0, the dependence on the number of states reduces from linear to
logarithmic. Our analysis allows to provide the algorithm large amount of side
information, let the algorithm to decide which side information is relevant for the
task, and penalize the algorithm only for the side information that it is using de
facto. We also present an algorithm for multiarmed bandits with side information
with O(K) computational complexity per game round.

1

Introduction

Multiarmed bandits with side information are an elegant mathematical model for many real-life
interactive systems, such as personalized online advertising, personalized medical treatment, and so
on. This model is also known as contextual bandits or associative bandits (Kaelbling, 1994, Strehl
et al., 2006, Langford and Zhang, 2007, Beygelzimer et al., 2011). In multiarmed bandits with side
information the learner repeatedly observes states (side information) {s1 , s2 , . . . } (for example,
symptoms of a patient) and has to perform actions (for example, prescribe drugs), such that the
expected regret is minimized. The regret is usually measured by the difference between the reward
that could be achieved by the best (unknown) fixed policy (for example, the number of patients that
would be cured if we knew the best drug for each set of symptoms) and the reward obtained by the
algorithm (the number of patients that were actually cured).
Most of the existing analyses of multiarmed bandits with side information has focused on the adversarial (worst-case) model, where the sequence of rewards associated with each state-action pair
is chosen by an adversary. However, many problems in real-life are not adversarial. We derive datadependent analysis for stochastic multiarmed bandits with side information. In the stochastic setting
the rewards for each state-action pair are drawn from a fixed unknown distribution. The sequence of
states is also drawn from a fixed unknown distribution. We restrict ourselves to problems with finite
number of states N and finite number of actions K and leave generalization to continuous state and
action spaces to future work. We also do not assume any structure of the state space. Thus, for us
a state is just a number between 1 and N . For example, in online advertising the state can be the
country from which a web page is accessed.
1

The result presented in this paper exhibits adaptive dependency on the side information (state identity) that is actually used by the algorithm. This allows us to provide the algorithm a large amount
of side information and let the algorithm decide, which of this side information is actually relevant
to the task. For example, in online advertising we can increase the state resolution and provide the
algorithm the town from which the web page was accessed, but if this refined state information is not
used by the algorithm the regret bound will not deteriorate. This can be opposed to existing analysis
of adversarial multiarmed bandits, where the regret bound depends on a predefined complexity of
the underlying expert class (Beygelzimer et al., 2011). Thus, the existing analysis of adversarial
multiarmed bandits would either become looser if we add more side information or a-priori limit the
usage of the side information through its internal structure. (We note that through the relation between PAC-Bayesian analysis and the analysis of adversarial online learning described in Banerjee
(2006) it might be possible to extend our analysis to adversarial setting, but we leave this research
direction to future work.)
The idea of regularization by relevant mutual information goes back to the Information Bottleneck
principle in supervised and unsupervised learning (Tishby et al., 1999). Tishby and Polani (2010)
further suggested to measure the complexity of a policy in reinforcement learning by the mutual
information between states and actions used by the policy. We note, however, that our starting point
is the regret bound and we derive the regularization term from our analysis without introducing it
a-priori. The analysis also provides time and data dependent weighting of the regularization term.
Our results are based on PAC-Bayesian analysis (Shawe-Taylor and Williamson, 1997, ShaweTaylor et al., 1998, McAllester, 1998, Seeger, 2002), which was developed for supervised learning
within the PAC (Probably Approximately Correct) learning framework (Valiant, 1984). In PACBayesian analysis the complexity of a model is defined by a user-selected prior over a hypothesis
space. Unlike in VC-dimension-based approaches and their successors, where the complexity is
defined for a hypothesis class, in PAC-Bayesian analysis the complexity is defined for individual
hypotheses. The analysis provides an explicit trade-off between individual model complexity and
its empirical performance and a high probability guarantee on the expected performance.
An important distinction between supervised learning and problems with limited feedback, such
as multiarmed bandits and reinforcement learning more generally, is the fact that in supervised
learning the training set is given, whereas in reinforcement learning the training set is generated by
the learner as it plays the game. In supervised learning every hypothesis in a hypothesis class can
be evaluated on all the samples, whereas in reinforcement learning rewards of one action cannot
be used to evaluate another action. Recently, Seldin et al. (2011b,a) generalized PAC-Bayesian
analysis to martingales and suggested a way to apply it under limited feedback. Here, we apply this
generalization to multiarmed bandits with side information.
The remainder of the paper is organized as follows. We start with definitions in Section 2 and provide
our main results in Section 3, which include an instantaneous regret bound and a new algorithm for
stochastic multiarmed bandits with side information. In Section 4 we present an experiment that
illustrates our theoretical results. Then, we dive into the proof of our main results in Section 5 and
discuss the paper in Section 6.

2

Definitions

In this section we provide all essential definitions for our main results in the following section. We
start with the definition of stochastic multiarmed bandits with side information. Let S be a set of
|S| = N states and let A be a set of |A| = K actions, such that any action can be performed in any
state. Let s 2 S denote the states and a 2 A denote the actions. Let R(a, s) be the expected reward
for performing action a in state s. At each round t of the game the learner is presented a state St
drawn i.i.d. according to an unknown distribution p(s). The learner draws an action At according to
his choice of a distribution (policy) ?t (a|s) and obtains a stochastic reward Rt with expected value
R(At , St ). Let {S1 , S2 , . . . } denote the sequence of observed states, {?1 , ?2 , . . . } the sequence of
policies played, {A1 , A2 , . . . } the sequence of actions played, and {R1 , R2 , . . . } the sequence of
observed rewards. Let Tt = {{S1 , . . . , St }, {?1 , . . . , ?t }, {A1 , . . . , At }, {R1 , . . . , Rt }} denote the
history of the game up to time t.
2

Assume that ?t (a|s) > 0 for all t, a, and s. For t
1, a 2 {1, . . . , K}, and the sequence of
observed states {S1 , . . . , St } define a set of random variables Rta,St :
?
1
a,St
?t (a|St ) Rt , if At = a
Rt
=
0,
otherwise.

(The variables Rta,s are defined only for the observed state s = St .) Note that whenever defined,
E[Rta,St |Tt 1 , St ] = R(a, St ). The definition of Rta,s is generally known as importance weighted
sampling (Sutton and Barto, 1998). Importance weighted sampling is required for application of
PAC-Bayesian analysis, as will be shown in the technical part of the paper.
Pt
Define nt (s) = ? =1 I{S? =s} as the number of times state s appeared up to time t (I is the indicator
function). We define the empirical rewards of state-action pairs as:
( P
a,s
{? =1,...,t:S? =s} R?
, if nt (s) > 0
? t (a, s) =
nt (s)
R
0,
otherwise.
? t (a, s) = R(a, s). For every state s we define the ?best?
Note that whenever nt (s) > 0 we have ER
action in that state as a? = arg maxa R(a, s) (if there are multiple ?best? actions, one of them is
chosen arbitrarily). We then define the expected and empirical regret for performing any other action
a in state s as:
(a, s) = R(a? (s), s)

? t (a, s) = R
? t (a? (s), s)

R(a, s),

? t (a, s).
R

Let p?t (s) = ntt(s) be the empirical distribution over states observed up to time t. For any policy ?(a|s) we define the empirical reward, empirical regret, and expected regret of the policy
P
P
?
? t (a, s), ? t (?) = P p?t (s) P ?(a|s) ? t (a, s), and (?) =
as:
?t (s) a ?(a|s)R
sp
s
a
P Rt (?)
P=
s p(s)
a ?(a|s) (a, s).

We define the marginal distribution
Pover actions that corresponds to a policy ?(a|s) and the uniform
distribution over S as ??(a) = N1 s ?(a|s) and the mutual information between actions and states
corresponding to the policy ?(a|s) and the uniform distribution over S as
I? (S; A) =

1 X
?(a|s)
?(a|s) ln
.
N s,a
??(a)

For the proof of our main result and also in order to explain the experiments we also have to define
a hypothesis space for our problem. This definition is not used in the statement of the main result.
Let H be a hypothesis space, such that each member h 2 H is a deterministic mapping from S to
A. Denote by a = h(s) the action assigned by hypothesis
Ph to state s. It is easy to see that the size
of the hypothesis space |H| = K N . Denote by R(h) = s2S p(s)R(h(s), s) the expected reward
of a hypothesis h. Define:
t
1 X h(S? ),S?
?
Rt (h) =
R
.
t ? =1 ?
? t (h) = R(h).
Note that ER

Let h? = arg maxh2H R(h) be the ?best? hypothesis (the one that chooses the ?best? action in each
state). (If there are multiple hypotheses achieving maximal reward pick any of them.) Define:
(h) = R(h? )

? t (h) = R
? t (h? )

R(h),

? t (h).
R

Any policy ?(a|s) defines a distribution over H: we can draw an action a for each state s according
to ?(a|s) and thus obtain a hypothesis h 2 H. We use ?(h) to denote the respective probability
of drawing h. For a policy ? we define (?) = E?(h) [ (h)] and ? t (?) = E?(h) [ ? t (h)]. By
marginalization these definitions are consistent with our preceding definitions of (?) and ? t (?).
PN
Finally, let nh (a) =
s=1
n Ih(s)=a
o be the number of states in which action a is played by the
nh (a)
h
hypothesis h. Let A =
be the normalized cardinality profile (histogram) over the
N
a2A

3

actions played by hypothesis h (with respect to the uniform distribution over S). Let H(Ah ) =
P nh (a) nh (a)
be the entropy of this cardinality profile. In other words, H(Ah ) is the entropy
a N ln N
of an action choice of hypothesis h (with respect to the uniform distribution over S). Note, that the
optimal policy ?? (a|s) (the one, that selects the ?best? action in each state) is deterministic and we
?
have I?? (S; A) = H(Ah ).

3

Main Results

Our main result is a data and complexity dependent regret bound for a general class of prediction
strategies of a smoothed exponential form. Let ?t (a) be an arbitrary distribution over actions, let
?

?exp
t (a|s) =
where Z(?exp
t , s) =

P

a

?t (a)e

?

t Rt (a,s)

exp

?t (a)e t Rt (a,s)
,
Z(?exp
t , s)

(1)

is a normalization factor, and let

??t (a|s) = (1

(2)

K""t+1 )?exp
t (a|s) + ""t+1

be a smoothed exponential policy. The following theorem provides a regret bound for playing ??t
at round t + 1 of the game. For generality, we assume that rounds 1, . . . , t were played according to
arbitrary policies ?1 , . . . , ?t .
Theorem 1. Assume that in game rounds 1, . . . , t policies {?1 , . . . , ?t } were played and assume
that mina,s ?t (a|s)
""t for an arbitrary ""t that is independent of Tt . Let ?t (a) be an arbitrary
distribution over A that can depend on Tt and satisfies mina ?t (a) ?t . Let c > 1 be an arbitrary
number that is independent of Tt . Then, with probability greater than 1
over Tt , simultaneously
for all policies ??exp
defined by (2) that satisfy
t
exp

N I?exp
(S; A) + K(ln N + ln K) + ln 2mt
""t
t
? 2
2(e 2)t
c
we have:

s

(3)

1
2)(N I?exp
(S; A) + K(ln N + ln K) + ln 2mt ) ln ?t+1
t
+
+ K""t+1 ,
t""t
t
(4)
?q
?
(e 2)t
exp
where mt = ln
/ ln(c), and for all ?t that do not satisfy (3), with the same probability:
ln 2

(?
?exp
t ) ? (1 + c)

exp

(?
?t

2(e

1
2(N I?exp
(S; A) + K(ln N + ln K) + ln 2mt ) ln ?t+1
t
)?
+
+ K""t+1 .
t""t
t

and not ??exp
Note that the mutual information in Theorem 1 is calculated with respect to ?exp
t
t .
Theorem 1 allows to tune the learning rate t based on the sample. It also provides an instantaneous
regret bound for any algorithm that plays the policies {?
?exp
?exp
1 ,?
2 , . . . } throughout the game. In
order to obtain such a bound we just have to take a decreasing sequence {""1 , ""2 , . . . } and substitute
in Theorem 1 with t = t(t+1) . Then, by the union bound, the result holds with probability greater
than 1
for all rounds of the game simultaneously. This leads to Algorithm 1 for stochastic
multiarmed bandits with side information. Note that each round of the algorithm takes O(K) time.
Theorem 1 is based on the following regret decomposition and the subsequent theorem and two
lemmas that bound the three terms in the decomposition.
exp
(?
?exp
t ) = [ (?t )

exp
? t (?exp
? t (?exp
t )] +
t ) + [R(?t )

R(?
?exp
t )].

(5)

Theorem 2. Under the conditions of Theorem 1 on {?1 , . . . , ?t } and c, simultaneously for all
policies ? that satisfy (3) with probability greater than 1
:
s
2(e 2)(N I?(S;A) + K(ln N + ln K) + ln 2mt )
(?) ? t (?) ? (1 + c)
,
(6)
t""t
4

Algorithm 1: Algorithm for stochastic contextual bandits. (See text for definitions of ""t and
Input: N, K
? s)
R(a,
0 for all a, s (These are cumulative [unnormalized] rewards)
1
?(a)
K for all a
n(s)
0 for all s
t
1
while not terminated do
Observe state St .
1
if ""t
K or (n(St ) = 0) then
?(a|St )
?(a) for all a
else
?(a|St )

(1

?
t R(a,St )/n(St )

K""t ) P ?(a)e
?(a0 )e
a0

? 0
t R(a ,St )/n(St )

t .)

+ ""t for all a

?(a)
+
for all a
Draw action At according to ?(a|St ) and play it.
Observe reward Rt .
n(St )
n(St ) + 1
?
? t , St ) + Rt
R(At , St )
R(A
?(At |St )
t
t+1
N 1
N ?(a)

1
N ?(a|St )

and for all ? that do not satisfy (3) with the same probability:
(?)

2mt
)
? t (?) ? 2(N I? (S; A) + K(ln N + ln K) + ln
.
t""t

Note that Theorem 2 holds for all possible ?-s, including those that do not have an exponential form.
Lemma 1. For any distribution ?exp
of the form (1), where ?t (a) ? for all a, we have:
t
ln 1?
? t (?exp
.
t )?
t

Lemma 2. Let ?? be an ""-smoothed version of a policy ?, such that ??(a|s) = (1
then
R(?) R(?
?) ? K"".

K"")?(a|s) + "",

Proof of Theorem 2 is provided in Section 5 and proofs of Lemmas 1 and 2 are provided in the
supplementary material.
Comments on Theorem 1. Theorem 1 exhibits what we were looking for: the regret of a policy
??exp
depends on the trade-off between its complexity, N I?exp
(S; A), and the empirical regret, which
t
t
1
is bounded by 1t ln ?t+1
. We note that 0 ? I?t (S; A) ? ln K, hence, the result is interesting when
N
K, since otherwise K ln K term in the bound neutralizes the advantage we get from having
small mutual information values. The assumption that N
K is reasonable for many applications.
We believe that the dependence of the first term of the regret bound (4) on ""t is an artifact of our
crude upper bound on the variance of the sampling process (given in Lemma 3 in the proof of Theorem 2) and that this term should not be in the bound. This is supported by an empirical study of
stochastic multiarmed bandits (Seldin et al., 2011a). With the current bound the best choice for ""t
is ""t = (Kt) 1/3 , which, by integration over the game rounds, yields O(K 1/3 t2/3 ) dependence of
the cumulative regret on the number of arms and game rounds. However, if we manage to derive a
tighter analysis and remove ""t from the first term in (4), the best choice of ""t will be ""t = (Kt) 1/2
and the dependence of the cumulative regret on the number of arms and time horizon will improve
to O((Kt)1/2 ). One way to achieve this is to apply EXP3.P-style updates (Auer et al., 2002b), however, Seldin et al. (2011a) empirically show that in stochastic environments EXP3 algorithm of Auer
et al. (2002b), which is closely related to Algorithm 1, has significantly better performance. Thus,
it is desirable to derive a better analysis for EXP3 algorithm in stochastic environments. We note
5

that although UCB algorithm for stochastic multiarmed bandits (Auer et al., 2002a) is asymptotically better than the EXP3 algorithm, it is not compatible with PAC-Bayesian analysis and we are
not aware of a way to derive a UCB-type algorithm and analysis for multiarmed bandits with side
information, whose dependence on the number of states would be better than O(N ln K). Seldin
et al. (2011a) also demonstrate that empirically it takes a large number of rounds until the asymptotic
advantage of UCB over EXP3 translates into a real advantage in practice.
It is not trivial to minimize (4) with respect to t analytically. Generally, higher values of t decrease
the second term of the bound, but also lead to more concentrated policies (conditional distributions)
exp
?exp
t (a|s) and thus higher mutual information values I?t (S; A). A simple way to address this
trade-off is to set t such that the contribution of the second term is as close to the contribution of
the first term as possible. This can be approximated by taking the value of mutual information from
the previous round (or approximation of the value of mutual information from the previous round).
More details on parameter setting for the algorithm are provided in the supplementary material.
Comments on Algorithm 1. By regret decomposition (5) and Theorem 2, regret at round t + 1 is
minimized by a policy ?t (a|s) that minimizes a certain trade-off between the mutual information
? t (?). This trade-off is analogical to rate-distortion trade-off in
I? (S; A) and the empirical regret R
information theory (Cover and Thomas, 1991). Minimization of rate-distortion trade-off is achieved
by iterative updates of the following form, which are known as Blahut-Arimoto (BA) algorithm:
?

t Rt (a,s)
?BA
t (a)e
?BA
,
t (a|s) = P
?
BA
t Rt (a,s)
a ?t (a)e

?BA
t (a) =

1 X BA
? (a|s).
N s t

Running a similar type of iterations in our case would be prohibitively expensive, since they require
iteration over all states s 2 S at each round of the game. We approximate these iterations by
approximating the marginal distribution over the actions by a running average:
??exp
t+1 (a) =

N

1
N

??exp
t (a) +

1 exp
?? (a|St ).
N t

(7)

Since ?exp
t (a|s) is bounded from zero by a decreasing sequence ""t+1 , the same automatically holds
for ??exp
t+1 (a) (meaning that in Theorem 1 ?t = ""t ). Note that Theorem 1 holds for any choice of
?t (a), including (7).
We point out an interesting fact: ?exp
t (a) propagates information between different states, but Theo1
rem 1 also holds for the uniform distribution ?(a) = K
, which corresponds to application of EXP3
algorithm in each state independently. If these independent multiarmed bandits independently converge to similar strategies, we still get a tighter regret bound. This happens because the corresponding subspace of the hypothesis space is significantly smaller than the total hypothesis space, which
enables us to put a higher prior on it (Seldin and Tishby, 2010). Nevertheless, propagation of information between states via the distribution ?exp
t (a) helps to achieve even faster convergence of the
regret, as we can see from the experiments in the next section.
Comparison with state-of-the-art. We are not aware of algorithms for stochastic multiarmed bandits with side information. The best known to us algorithm for adversarial multiarmed
bandits with
p
side information is EXP4.P by Beygelzimer et al. (2011). EXP4.P has O( Kt ln |H|) regret and
O(K|H|)
our case |H| = K N , which means that EXP4.P would
p complexity per game round.NIn
+1
have O( KtN ln K) regret and O(K
) computational complexity. For hard problems, where
all side information has to be used, our regret bound is inferior to the regret bound of Beygelzimer
et al. (2011) due to O(t2/3 ) dependence on the number of game rounds. However, we believe that
this can be improved by a more careful analysis of the existing algorithm. For simple problems
the dependence of our regret bound on the number of states is significantly
better, up to the point
p
that when the side information
is
irrelevant
for
the
task
we
can
get
O(
K
ln
N
) dependence on the
p
number of states versus O( N ln K) in EXP4.P. For N
K this leads to tighter regret bounds for
small t even despite the ?incorrect? dependence on t of our bound, and if we improve the analysis
it will lead to tighter regret bounds for all t. As we said it already, our algorithm is able to filter
relevant information from large amounts of side information automatically, whereas in EXP4.P the
usage of side information has to be restricted externally through the construction of the hypothesis
class.
6

5
0
0

1

2
t

(a)

3

4

6

H(Ah*)=0
H(Ah*)=1
H(Ah*)=2
H(Ah*)=3

2
1.5
1
0.5
0

H(Ah*)=0

2 H(Ah*)=1
1.5

H(Ah*)=2
H(Ah*)=3

1
0.5

1

2
t

x 10

(t)

2.5

t

Bound on ?(~?exp
)
t

?(t)

2.5

H(A )=0
H(Ah*)=1
h*
10 H(A )=2
H(Ah*)=3
Baseline

I? (S;A)

4

15 x 10 h*

(b) bound on

3

4

6

x 10

(?
?exp
)
t

0
0

1

2
t

3

4

6

x 10

(c) I?exp
(S; A)
t

Figure 1: Behavior of: (a) cumulative regret (t), (b) bound on instantaneous regret (?
?exp
t ),
exp
and (c) the approximation of mutual information I?t (S; A). ?Baseline? in the first graph corresponds to playing N independent multiarmed bandits, one in each state. Each line in the graphs
corresponds to an average over 10 repetitions of the experiment.
The second important advantage of our algorithm is the exponential improvement of computational
complexity. This is achieved by switching from the space of experts to the state-action space in all
our calculations.

4

Experiments

We present an experiment on synthetic data that illustrates our results. We take N = 100, K = 20, a
?
uniform distribution over states (p(s) = 0.01), and consider four settings, with H(Ah ) = ln(1) =
?
?
?
0, H(Ah ) = ln(3) ? 1, H(Ah ) = ln(7) ? 2, and H(Ah ) = ln(20) ? 3, respectively. In the
?
first case, the same action is the best in all states (and hence H(Ah ) = 0 for the optimal hypothesis
?
h ). In the second case, for the first 33 states the best action is number 1, for the next 33 states
the best action is number 2, and for the rest third of the states the best ?action is number 3 (thus,
depending on the state, one of the three actions is the ?best? and H(Ah ) = ln(3)). In the third
case, there are seven groups of 14 states and each group has its own best action. In the last case,
there are 20 groups of 5 states and each of K = 20 actions is the best in exactly one of the 20
groups. For all states, the reward of the best action in a state has Bernoulli distribution with bias 0.6
and the rewards of all other actions in that state have Bernoulli distribution with bias 0.5.
the
Pt We runexp
experiment for T = 4, 000, 000 rounds and calculate the cumulative regret (t) = ? =1 (?
?? )
and instantaneous regret bound given in (4). For computational efficiency, the mutual information
I?exp
(S; A) is approximated by a running average (see supplementary material for details).
t
As we can see from the graphs (see Figure 1), the algorithm exhibits sublinear cumulative regret
?
(put attention to the axes? scales). Furthermore, for simple problems (with small H(Ah )) the
regret grows slower than for complex problems. ?Baseline? in Figure 1.a shows the performance
of an algorithm with the same parameter values that runs N multiarmed bandits, one in each state
independently of other states. We see that for all problems except the hardest one our algorithm
performs better than the baseline and for the hardest problem it performs almost as good as the
baseline. The regret bound in Figure 1.b provides meaningful values for the simplest problem after
1 million rounds (which is on average 500 samples per state-action pair) and after 4 million rounds
for all the problems (the graph starts at t = 10, 000). Our estimates of the mutual information
?
?
I?exp
(S; A) reflect H(Ah ) for the corresponding problems (for H(Ah ) = 0 it converges to zero,
t
?
for H(Ah ) ? 1 it is approximately one, etc.).

5

Proof of Theorem 2

The proof of Theorem 2 is based on PAC-Bayes-Bernstein inequality for martingales (Seldin et al.,
2011b). Let KL(?k?) denote the KL-divergence between two distributions (Cover and Thomas,
1991). Let {Z1 (h), . . . , Zn (h) : h 2 H} be martingale difference sequences indexed by h with
respect to the filtration (U1 ), . . . , (Un ), where Ui = {Z1 (h), . . . , Zi (h) : h 2 H} is the subset
of martingale difference variables up to index i and (Ui ) is the -algebra generated by Ui . This
means that E[Zi (h)| (Ui 1 )] = 0, where Zi (h) may depend on Zj (h0 ) for all j < i and h0 2 H.
? i (h) = Pi Zj (h) be
There might also be interdependence between {Zi (h) : h 2 H}. Let M
j=1
7

Pi
the corresponding martingales. Let Vi (h) = j=1 E[Zj (h)2 | (Uj 1 )] be cumulative variances of
? i (h). For a distribution ? over H define M
? i (?) = E?(h) [M
? i (h)] and Vt (?) =
the martingales M
E?(h) [Vt (h)] as weighted averages of the martingales and their cumulative variances according to a
distribution ?.
Theorem 3 (PAC-Bayes-Bernstein Inequality). Assume that |Zi (h)| ? b for all h with probability
1. Fix a prior distribution ? over H. Pick an arbitrary number c > 1. Then with probability greater
than 1
over Un , simultaneously for all distributions ? over H that satisfy
s
KL(?k?) + ln 2m
1
?
(e 2)Vn (?)
cb
we have
s
?
?
? n (?)| ? (1 + c) (e 2)Vn (?) KL(?k?) + ln 2m ,
|M
?q
?
(e 2)n
where m = ln
/ ln(c), and for all other ?
2
ln
?
?
? n (?)| ? 2b KL(?k?) + ln 2m .
|M
Note that Mt (h) = t( (h) ? t (h)) are martingales and their cumulative variances are Vt (h) =
?
?
Pt
2
h? (S? ),S?
h(S ),S
R? ? ? ] [R(h? ) R(h)] T? 1 . In order to apply Theorem 3 we
? =1 E [R?
1
have to derive an upper bound on Vt (?exp
t ), a prior ?(h) over H, and calculate (or upper bound) the
exp
KL-divergence KL(?t k?). This is done in the following three lemmas.
Lemma 3. If {""1 , ""2 , . . . } is a decreasing sequence, such that ""t ? mina,s ?t (a|s), then for all h:
2t
Vt (h) ? .
""t
The proof of the lemma is provided in the supplementary material. Lemma 3 provides an imme2t
diate, but crude, uniform upper bound on Vt (h), which yields Vt (?exp
t ) ? ""t . Since our algorithm
concentrates on h-s with small (h), which, in turn, concentrate on the best action in each state, the
variance Vt (h) for the corresponding h-s is expected to be of the order of 2Kt and not ""2tt . However,
we were not able to prove yet that the probability ?exp
t (h) of the remaining hypotheses (those with
large (h)) gets sufficiently small (of order K""t ), so that the weighted cumulative variance would
be of order 2Kt. Nevertheless, this seems to hold in practice starting from relatively small values of
t (Seldin et al., 2011a). Improving the upper bound on Vt (?exp
t ) will improve the regret bound, but
2t
for the moment we present the regret bound based on the crude upper bound Vt (?exp
t ) ? ""t .
The remaining two lemmas, which define a prior ? over H and bound KL(?k?), are due to Seldin
and Tishby (2010).
Lemma 4. It is possible to define a distribution ? over H that satisfies:
h

?(h) e N H(A ) K ln N K ln K .
Lemma 5. For the distribution ? that satisfies (8) and any distribution ?(a|s):
KL(?k?) ? N I? (S; A) + K ln N + K ln K.

(8)

exp
Substitution of the upper bounds on Vt (?exp
t ) and KL(?t k?) into Theorem 3 yields Theorem 2.

6

Discussion

We presented PAC-Bayesian analysis of stochastic multiarmed bandits with side information. Our
analysis provides data-dependent algorithm and data-dependent regret analysis for this problem.
The selection of task-relevant side information is delegated from the user to the algorithm. We also
provide a general framework for deriving data-dependent algorithms and analyses for many other
stochastic problems with limited feedback. The analysis of the variance of our algorithm still waits
to be improved and will be addressed in future work.
1

Seldin et al. (2011b) show that Vn (?) can be replaced by an upper bound everywhere in Theorem 3.

8

Acknowledgments
We would like to thank all the people with whom we discussed this work and, in particular, Nicol`o
Cesa-Bianchi, G?abor Bart?ok, Elad Hazan, Csaba Szepesv?ari, Miroslav Dud??k, Robert Shapire, John
Langford, and the anonymous reviewers, whose comments helped us to improve the final version of
this manuscript. This work was supported in part by the IST Programme of the European Community, under the PASCAL2 Network of Excellence, IST-2007-216886, and by the European Community?s Seventh Framework Programme (FP7/2007-2013), under grant agreement N o 231495. This
publication only reflects the authors? views.

References
Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine Learning, 47, 2002a.
Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit
problem. SIAM Journal of Computing, 32(1), 2002b.
Arindam Banerjee. On Bayesian bounds. In Proceedings of the International Conference on Machine Learning
(ICML), 2006.
Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings on the International Conference on Artificial
Intelligence and Statistics (AISTATS), 2011.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. John Wiley & Sons, 1991.
Leslie Pack Kaelbling. Associative reinforcement learning: Functions in k-DNF. Machine Learning, 15, 1994.
John Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. In Advances
in Neural Information Processing Systems (NIPS), 2007.
David McAllester. Some PAC-Bayesian theorems. In Proceedings of the International Conference on Computational Learning Theory (COLT), 1998.
Matthias Seeger. PAC-Bayesian generalization error bounds for Gaussian process classification. Journal of
Machine Learning Research, 2002.
Yevgeny Seldin and Naftali Tishby. PAC-Bayesian analysis of co-clustering and beyond. Journal of Machine
Learning Research, 11, 2010.
Yevgeny Seldin, Nicol`o Cesa-Bianchi, Peter Auer, Franc?ois Laviolette, and John Shawe-Taylor. PAC-BayesBernstein inequality for martingales and its application to multiarmed bandits. 2011a. In review. Preprint
available at http://arxiv.org/abs/1110.6755.
Yevgeny Seldin, Franc?ois Laviolette, Nicol`o Cesa-Bianchi, John Shawe-Taylor, and Peter Auer. PAC-Bayesian
inequalities for martingales. 2011b. In review. Preprint available at http://arxiv.org/abs/1110.6886.
John Shawe-Taylor and Robert C. Williamson. A PAC analysis of a Bayesian estimator. In Proceedings of the
International Conference on Computational Learning Theory (COLT), 1997.
John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. Structural risk minimization
over data-dependent hierarchies. IEEE Transactions on Information Theory, 44(5), 1998.
Alexander L. Strehl, Chris Mesterharm, Michael L. Littman, and Haym Hirsh. Experience-efficient learning in
associative bandit problems. In Proceedings of the International Conference on Machine Learning (ICML),
2006.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.
Naftali Tishby and Daniel Polani. Information theory of decisions and actions. In Vassilis Cutsuridis, Amir
Hussain, John G. Taylor, and Daniel Polani, editors, Perception-Reason-Action Cycle: Models, Algorithms
and Systems. Springer, 2010.
Naftali Tishby, Fernando Pereira, and William Bialek. The information bottleneck method. In Allerton Conference on Communication, Control and Computation, 1999.
Leslie G. Valiant. A theory of the learnable. Communications of the Association for Computing Machinery, 27
(11), 1984.

9

"
5984,"Showing versus Doing: Teaching by Demonstration

Mark K Ho
Department of Cognitive, Linguistic, and Psychological Sciences
Brown University
Providence, RI 02912
mark_ho@brown.edu
Michael L. Littman
Department of Computer Science
Brown University
Providence, RI 02912
mlittman@cs.brown.edu
Fiery Cushman
Department of Psychology
Harvard University
Cambridge, MA 02138
cushman@fas.harvard.edu

James MacGlashan
Department of Computer Science
Brown University
Providence, RI 02912
james_macglashan@brown.edu
Joseph L. Austerweil
Department of Psychology
University of Wisconsin-Madison
Madison, WI 53706
austerweil@wisc.edu

Abstract
People often learn from others? demonstrations, and inverse reinforcement learning
(IRL) techniques have realized this capacity in machines. In contrast, teaching
by demonstration has been less well studied computationally. Here, we develop
a Bayesian model for teaching by demonstration. Stark differences arise when
demonstrators are intentionally teaching (i.e. showing) a task versus simply performing (i.e. doing) a task. In two experiments, we show that human participants
modify their teaching behavior consistent with the predictions of our model. Further, we show that even standard IRL algorithms benefit when learning from
showing versus doing.

1

Introduction

Is there a difference between doing something and showing someone else how to do something?
Consider cooking a chicken. To cook one for dinner, you would do it in the most efficient way
possible while avoiding contaminating other foods. But, what if you wanted to teach a completely
na?ve observer how to prepare poultry? In that case, you might take pains to emphasize certain
aspects of the process. For example, by ensuring the observer sees you wash your hands thoroughly
after handling the uncooked chicken, you signal that it is undesirable (and perhaps even dangerous)
for other ingredients to come in contact with raw meat. More broadly, how could an agent show
another agent how to do a task, and, in doing so, teach about its underlying reward structure?
To model showing, we draw on psychological research on learning and teaching concepts by example.
People are good at this. For instance, when a teacher signals their pedagogical intentions, children
more frequently imitate actions and learn abstract functional representations [7, 8]. Recent work
has formalized concept teaching as a form of recursive social inference, where a teacher chooses
an example that best conveys a concept to a learner, who assumes that the teacher is choosing in
this manner [13]. The key insight from these models is that helpful teachers do not merely select
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

probable examples of a concept, but rather choose examples that best disambiguate a concept from
other candidate concepts. This approach allows for more effective, and more efficient, teaching and
learning of concepts from examples.
Although previous research has explored formal models of teaching concepts, less computational
work has examined teaching reward functions by demonstration. Closely related work has looked at
how physical motion can be made more legible to an observer for better human-machine coordination
[9], but, for the most part, machine learning researchers have focused on learning from expert
demonstrations. In particular, Inverse Reinforcement Learning (IRL), in which an observer attempts
to infer the reward function that an expert (human or artificial) is maximizing, has been successfully
applied to domains ranging from simple grid games to complex control problems like flying a model
helicopter [1]. Nonetheless, in the IRL setting, experts are usually only doing the task and not
showing it, and IRL algorithms implicitly assume the same. This fact raises two related questions:
First, how might showing how to do a task differ from just doing it? And second, can IRL algorithms
reap the benefits of intentional teaching and showing?
In this paper, we begin to investigate these questions. To do so, we formulate a computational model
of showing that applies Bayesian models of teaching by example to the reward function learning
setting. We contrast this pedagogical model with a model of doing: standard optimal planning in
Markov Decision Processes. The pedagogical model predicts several systematic differences from
the standard planning model, and we test whether human participants reproduce these distinctive
patterns. For instance, the pedagogical model chooses paths to a goal that best disambiguates which
goal is being pursued (Experiment 1). Similarly, when teaching feature-based reward functions, the
model will prioritize trajectories that better signal the reward value of state features or even perform
trajectories that would be inefficient for an agent simply doing the task (Experiment 2). Finally, to
determine whether showing is indeed better than doing even for a na?ve learner, we train a standard
IRL algorithm with our model trajectories and human trajectories.

2

A Bayesian Model of Teaching by Demonstration

Our model draws on two approaches: IRL [3] and Bayesian models of teaching by example [13].
The first of these, IRL and the related concept of inverse planning, have been used to model people?s
theory of mind, or the capacity to infer another agent?s unobservable beliefs and/or desires through
their observed behavior [6]. The second, Bayesian models of pedagogy, prescribe how a teacher
should use examples to communicate a concept to an ideal learner. Our model of teaching by
demonstration, called Pedagogical Inverse Reinforcement Learning, merges these two approaches
together by treating a teacher?s demonstration trajectories as communicative acts that signal the
reward function that an observer should learn.
2.1
2.1.1

Learning from an Expert?s Actions
Markov Decision Processes

An agent that plans to maximize a reward function can be modeled as the solution to a Markov
Decision Process (MDP). An MDP is defined by the tuple < S, A, T, R, ? >: a set of states in the
world S; a set of actions for each state A(s); a transition function that maps states and actions to
next states, T : S ? A ? S (in this work we assume all transitions are deterministic, but this can
be generalized to probabilistic transitions); a reward function that maps states to scalar rewards,
R : S ? R; and a discount factor ? ? [0, 1]. Solutions to an MDP are stochastic policies that
map states to distributions over actions, ? : S ? P (A(s)). Given a policy, we define the expected
cumulative discounted reward, or value, V ? (s), at each state associated with following that policy:
V ? (s) = E?

?
hX

i
? k rt+k+1 | st = s .

(1)

k=0

In particular, the optimal policy for an MDP yields the optimal value function, V ? , which is the value
function that has the maximal value for every state (V ? (s) = max? V ? (s), ?s ? S). The optimal
policy also defines an optimal state-action value function, Q? (s, a) = E? [rt+1 + ?V ? (st+1 ) | st =
s, at = a].
2

Algorithm 1 Pedagogical Trajectory Algorithm
Require: starting states s, reward functions {R1 , R2 , ..., RN }, transition function T , maximum
showing trajectory depth lmax , minimum hypothetical doing probability pmin , teacher maximization parameter ?, discount factor ?.
1: ? ? ?
2: for i = 1 to N do
3:
Qi = calculateActionValues(s, Ri , T , ?)
4:
?i = softmax(Qi , ?)
5:
?.add(?i )
Q
6: Calculate j = {j : s1 ? s, length(j) ? lmax , and ?? ? ? s.t. (si ,ai )?j ?(ai | si ) > pmin }.
7: Construct hypothetical doing probability distribution PDoing (j | R) as an N x M array.
P
(j|R)P (R)
8: PObserving (R | j) = P 0 Doing
PDoing (j|R0 )P (R0 )
R

9: PShowing (j | R) =

?

P PObserving (R|j) 0 ?
j 0 PObserving (R|j )

10: return PShowing (j | R)

2.1.2

Inverse Reinforcement Learning (IRL)

In the Reinforcement Learning setting, an agent takes actions in an MDP and receives rewards, which
allow it to eventually learn the optimal policy [14]. We thus assume that an expert who knows the
reward function and is doing a task selects an action at in a state st according to a Boltzmann policy,
which is a standard soft-maximization of the action-values:
exp{Q? (si , ai )/?}
.
?
0
a0 ?A(si ) exp{Q (si , a )/?}

PDoing (at | st , R) = P

(2)

? > 0 is an inverse temperature parameter (as ? ? 0, the expert selects the optimal action with
probability 1; as ? ? ?, the expert selects actions uniformly randomly).
In the IRL setting, an observer sees a trajectory of an expert executing an optimal policy,
j = {(s1 , a1 ), (s2 , a2 ), ..., (sk , ak )}, and infers the reward function R that the expert is maximizing. Given that an agent?s policy is stationary and Markovian, the probability of the trajectory
given
a reward function is just the product of the individual action probabilities, PDoing (j | R) =
Q
t PDoing (at | st , R). From a Bayesian perspective [12], the observer is computing a posterior
probability over possible reward functions R:
PDoing (j | R)P (R)
.
0
0
R0 PDoing (j | R )P (R )

PObserving (R | j) = P

(3)

Here, we always assume that P (R) is uniform.
2.2

Bayesian Pedagogy

IRL typically assumes that the demonstrator is executing the stochastic optimal policy for a reward
function. But is this the best way to teach a reward function? Bayesian models of pedagogy and
communicative intent have shown that choosing an example to teach a concept differs from simply
sampling from that concept [13, 10]. These models all treat the teacher?s choice of a datum, d, as
maximizing the probability a learner will infer a target concept, h:
PLearner (h | d)?
.
0 ?
d0 PLearner (h | d )

PTeacher (d | h) = P

(4)

? is the teacher?s softmax parameter. As ? ? 0, the teacher chooses uniformly randomly; as ? ? ?,
the teacher chooses d that maximally causes the learner to infer a target concept h; when ? = 1, the
teacher is ?probability matching?.
The teaching distribution describes how examples can be effectively chosen to teach a concept. For
instance, consider teaching the concept of ?even numbers?. The sets {2, 2, 2} and {2, 18, 202} are
both examples of even numbers. Indeed, given finite options with replacement, they both have the
same probability of being randomly chosen as sets of examples. But {2, 18, 202} is clearly better
3

for helpful teaching since a na?ve learner shown {2, 2, 2} would probably infer that ?even numbers?
means ?the number 2?. This illustrates an important aspect of successful teaching by example: that
examples should not only be consistent with the concept being taught, but should also maximally
disambiguate the concept being taught from other possible concepts.
2.3

Pedagogical Inverse Reinforcement Learning

To define a model of teaching by demonstration, we treat the teacher?s trajectories in a reinforcementlearning problem as a ?communicative act? for the learner?s benefit. Thus, an effective teacher will
modify its demonstrations when showing and not simply doing a task. As in Equation 4, we can
define a teacher that selects trajectories that best convey the reward function:
PObserving (R | j)?
.
0 ?
j 0 PObserving (R | j )

PShowing (j | R) = P

(5)

In other words, showing depends on a demonstrator?s inferences about an observer?s inferences about
doing.
This model provides quantitative and qualitative predictions for how agents will show and teach how
to do a task given they know its true reward function. Since humans are the paradigm teachers and a
potential source of expert knowledge for artificial agents, we tested how well our model describes
human teaching. In Experiment 1, we had people teach simple goal-based reward functions in a
discrete MDP. Even though in these cases entering a goal is already highly diagnostic, different paths
of different lengths are better for showing, which is reflected in human behavior. In Experiment
2, people taught more complex feature-based reward functions by demonstration. In both studies,
people?s behavior matched the qualitative predictions of our models.

3

Experiment 1: Teaching Goal-based Reward Functions

Consider a grid with three possible terminal goals as shown in Figure 1. If an agent?s goal is &, it
could take a number of routes. For instance, it could move all the way right and then move upwards
towards the & (right-then-up) or first move upwards and then towards the right (up-then-right). But,
what if the agent is not just doing the task, but also attempting to show it to an observer trying to learn
the goal location?
When the goal is &, our pedagogical model predicts that up-then-right is the more probable trajectory
because it is more disambiguating. Up-then-right better indicates that the intended goal is & than
right-then-up because right-then-up has more actions consistent with the goal being #. We have
included an analytic proof of why this is the case for a simpler setting in the supplementary materials.
Additionally, our pedagogical model makes the prediction that when trajectory length costs are
negligible, agents will engage in repetitive, inefficient behaviors that gesture towards one goal
location over others. This ?looping? behavior results when an agent can return to a state with an
action that has high signaling value by taking actions that have a low signaling ?cost? (i.e. they do not
signal something other than the true goal). Figure 1d shows an example of such a looping trajectory.
In Experiment 1, we tested whether people?s showing behavior reflected the pedagogical model when
reward functions are goal-based. If so, this would indicate that people choose the disambiguating
path to a goal when showing.
3.1

Experimental Design

Sixty Amazon Mechanical Turk participants performed the task in Figure 1. One was excluded
due to missing data. All participants completed a learning block in which they had to find the
reward location without being told. Afterwards, they were either placed in a Do condition or a Show
condition. Participants in Do were told they would win a bonus based on the number of rewards
(correct goals) they reached and were shown the text, ?The reward is at location X?, where X was
one of the three symbols %, #, or &. Those in Show were told they would win a bonus based on how
well a randomly matched partner who was shown their responses (and did not know the location of
the reward) did on the task. On each round of Show, participants were shown text saying ?Show your
partner that the reward is at location X?. All participants were given the same sequence of trials in
which the reward locations were <%, &, #, &, %, #, %, #, &>.
4

Figure 1: Experiment 1: Model predictions and participant trajectories for 3 trials when the goal is
(a) &, (b) %, and (c) #. Model trajectories are the two with the highest probability (? = 2, ? = 1.0,
pmin = 10?6 , lmax = 4). Yellow numbers are counts of trajectories with the labeled tile as the
penultimate state. (d) An example of looping behavior predicted by the model when % is the goal.
3.2

Results

As predicted, Show participants tended to choose paths that disambiguated their goal as compared to
Do participants. We coded the number of responses on & and % trials that were ?showing? trajectories
based on how they entered the goal (i.e. out of 3 for each goal). On & trials, entering from the left,
and on % trials, entering from above were coded as ?showing?. We ran a 2x2 ANOVA with Show vs
Do as a between-subjects factor and goal (% vs &) as a repeated measure. There was a main effect
of condition (F (1, 57) = 16.17, p < .001; Show: M = 1.82, S.E. 0.17; Do: M = 1.05, S.E. 0.17) as
well as a main effect of goal (F (1, 57) = 4.77, p < .05; %-goal: M = 1.73, S.E. = 0.18; &-goal: M
= 1.15, S.E. = 0.16). There was no interaction (F (1, 57) = 0.98, p = 0.32).
The model does not predict any difference between conditions for the # (lower right) goal. However,
a visual analysis suggested that more participants took a ?swerving? path to reach #. This observation
was confirmed by looking at trials where # was the goal and comparing the number of swerving
trials, which was defined as making more than one change in direction (Show: M = 0.83, Do: M =
0.26; two-sided t-test: t(44.2) = 2.18, p = 0.03). Although not predicted by the model, participants
may swerve to better signal their intention to move ?directly? towards the goal.
3.3

Discussion

Reaching a goal is sufficient to indicate its location, but participants still chose paths that better
disambiguated their intended goal. Overall, these results indicate that people are sensitive to the
distinction between doing and showing, consistent with our computational framework.

4

Experiment 2: Teaching Feature-based Reward Functions

Experiment 1 showed that people choose disambiguating plans even when entering the goal makes
this seemingly unnecessary. However, one might expect richer showing behavior when teaching more
complex reward functions. Thus, for Experiment 2, we developed a paradigm in which showing
how to do a task, as opposed to merely doing a task, makes a difference for how well the underlying
reward function is learned. In particular, we focused on teaching feature-based reward functions that
allow an agent to generalize what it has learned in one situation to a new situation. People often
use feature-based representations for generalization [4], and feature-based reward functions have
been used extensively in reinforcement learning (e.g. [2]). We used a colored-tile grid task shown in
5

Figure 2: Experiment 2 results. (a) Column labels are reward function codes. They refer to which
tiles were safe (o) and which were dangerous (x) with the ordering <orange, purple, cyan>. Row 1:
Underlying reward functions that participants either did or showed; Row 2: Do participant trajectories
with visible tile colors; Row 3: Show participant trajectories; Row 4: Mean reward function learned
from Do trajectories by Maximum-Likelihood Inverse Reinforcement Learning (MLIRL) [5, 11];
Row 5: Mean reward function learned from Show trajectories by MLIRL. (b) Mean distance between
learned and true reward function weights for human-trained and model-trained MLIRL. For the
models, MLIRL results for the top two ranked demonstration trajectories are shown.

Figure 2 to study teaching feature-based reward functions. White tiles are always ?safe? (reward of 0),
while yellow tiles are always terminal states that reward 10 points. The remaining 3 tile types?orange,
purple, and cyan?are each either ?safe? or ?dangerous? (reward of ?2). The rewards associated with
the three tile types are independent, and nothing about the tiles themselves signal that they are safe or
dangerous.
A standard planning algorithm will reach the terminal state in the most efficient and optimal manner.
Our pedagogical model, however, predicts that an agent who is showing the task will engage in
specific behaviors that best disambiguate the true reward function. For instance, the pedagogical
model is more likely to take a roundabout path that leads through all the safe tile types, choose
to remain on a safe colored tile rather than go on the white tiles, or even loop repeatedly between
multiple safe tile-types. All of these types of behaviors send strong signals to the learner about which
tiles are safe as well as which tiles are dangerous.
4.1

Experimental Design

Sixty participants did a feature-based reward teaching task; two were excluded due to missing data.
In the first phase, all participants were given a learning-applying task. In the learning rounds, they
interacted with the grid shown in Figure 2 while receiving feedback on which tiles won or lost points.
6

Figure 3: Experiment 2 normalized median model fits.

Safe tiles were worth 0 points, dangerous tiles were worth -2 points, and the terminal goal tile was
worth 5 points. They also won an additional 5 points for each round completed for a total of 10
points. Each point was worth 2 cents of bonus. After each learning round, an applying round occurred
in which they applied what they just learned about the tiles without receiving feedback in a new
grid configuration. They all played 8 pairs of learning and applying rounds corresponding to the
8 possible assignments of ?safe? and ?dangerous? to the 3 tile types, and order was randomized
between participants.
As in Experiment 1, participants were then split into Do or Show conditions with no feedback.
Do participants were told which colors were safe and won points for performing the task. Show
participants still won points and were told which types were safe. They were also told that their
behavior would be shown to another person who would apply what they learned from watching the
participant?s behavior to a separate grid. The points won would be added to the demonstrator?s bonus.
4.2

Results

Responses matched model predictions. Do participants simply took efficient routes, whereas Show
participants took paths that signaled tile reward values. In particular, Show participants took paths
that led through multiple safe tile types, remained on safe colored tiles when safe non-colored tiles
were available, and looped at the boundaries of differently colored safe tiles.
4.2.1

Model-based Analysis

To determine how well the two models predicted human behaviors globally, we fit separate models for
each reward function and condition combination. We found parameters that had the highest median
likelihood out of the set of participant trajectories in a given reward function-condition combination.
Since some participants used extremely large trajectories (e.g. >25 steps) and we wanted to include
an analysis of all the data, we calculated best-fitting state-action policies. For the standard-planner, it
is straightforward to calculate a Boltzmann policy for a reward function given ?.
For the pedagogical model, we first need to specify an initial model of doing and distribution
over a finite set of trajectories. We determine this initial set of trajectories and their probabilities
using three parameters: ?, the softmax parameter for a hypothetical ?doing? agent that the model
assumes the learner believes it is observing; lmax , the maximum trajectory length; and pmin , the
minimum probability for a trajectory under the hypothetical doing agent. The pedagogical model
then uses an ? parameter that determines the degree to which the teacher is maximizing. State-action
probabilities are calculated from a distribution over trajectories using the equation P (a | s, R) =
P
|{(s,a):s=st ,a=at ?(st ,at )?j}|
.
j P (a | s, j)P (j | R), where P (a | s, j) =
|{(s,a):s=st ?(st ,at )?j}|
We fit parameter values that produced the maximum median likelihood for each model for each
reward function and condition combination. These parameters are reported in the supplementary
materials. The normalized median fit for each of these models is plotted in Figure 3. As shown
in the figure, the standard planning model better captures behavior in the Do condition, while the
pedagogical model better captures behavior in the Show condition. Importantly, even when the
standard planning model could have a high ? and behave more randomly, the pedagogical model
better fits the Show condition. This indicates that showing is not simply random behavior.
7

4.2.2

Behavioral Analyses

We additionally analyzed specific behavioral differences between the Do and Show conditions
predicted by the models. When showing a task, people visit a greater variety of safe tiles, visit tile
types that the learner has uncertainty about (i.e. the colored tiles), and more frequently revisit states
or ?loop? in a manner that leads to better signaling. We found that all three of these behaviors were
more likely to occur in the Show condition than in the Do condition.
To measure the variety of tiles visited, we calculated the entropy of the frequency distribution over
colored-tile visits by round by participant. Average entropy was higher for Show (Show: M = 0.50,
SE = 0.03; Do: M = 0.39, SE = 0.03; two-sided t-test: t(54.9) = ?3.27, p < 0.01). When analyzing
time spent on colored as opposed to un-colored tiles, we calculated the proportion of visits to colored
tiles after the first colored tile had been visited. Again, this measure was higher for Show (Show:
M = 0.87, SE = 0.01; Do: M = 0.82, SE = 0.01; two-sided t-test: t(55.6) = ?3.14, p < .01).
Finally, we calculated the number of times states were revisited in the two conditions?an indicator of
?looping??and found that participants revisited states more in Show compared to Do (Show: M = 1.38,
SE = 0.22; Do: M = 0.10, SE = 0.03; two-sided t-test: t(28.3) = ?2.82, p < .01). There was no
difference between conditions in the total rewards won (two-sided t-test: t(46.2) = .026, p = 0.80).
4.3

Teaching Maximum-Likelihood IRL

One reason to investigate showing is its potential for training artificial agents. Our pedagogical model
makes assumptions about the learner, but it may be that pedagogical trajectories are better even
for training off-the-shelf IRL algorithms. For instance, Maximum Likelihood IRL (MLIRL) is a
state-of-the-art IRL algorithm for inferring feature-based reward functions [5, 11]. Importantly, unlike
the discrete reward function space our showing model assumes, MLIRL estimates the maximum
likelihood reward function over a space of continuous feature weights using gradient ascent.
To test this, we input human and model trajectories into MLIRL. We constrained non-goal feature
weights to be non-positive. Overall, the algorithm was able to learn the true reward function better
from showing than doing trajectories produced by either the models or participants (Figure 2).
4.3.1

Discussion

When learning a feature-based reward function from demonstration, it matters if the demonstrator
is showing or doing. In this experiment, we showed that our model of pedagogical reasoning over
trajectories captures how people show how to do a task. When showing as opposed to simply doing,
demonstrators are more likely to visit a variety of states to show that they are safe, stay on otherwise
ambiguously safe tiles, and also engage in ?looping? behavior to signal information about the tiles.
Moreover, this type of teaching is even better at training standard IRL algorithms like MLIRL.

5

General Discussion

We have presented a model of showing as Bayesian teaching. Our model makes accurate quantitative
and qualitative predictions about human showing behavior, as demonstrated in two experiments.
Experiment 1 showed that people modify their behavior to signal information about goals, while
Experiment 2 investigated how people teach feature-based reward functions. Finally, we showed that
even standard IRL algorithms benefit from showing as opposed to merely doing.
This provides a basis for future study into intentional teaching by demonstration. Future research
must explore showing in settings with even richer state features and whether more savvy observers
can leverage a showing agent?s pedagogical intent for even better learning.
Acknowledgments
MKH was supported by the NSF GRFP under Grant No. DGE-1058262. JLA and MLL were
supported by DARPA SIMPLEX program Grant No. 14-46-FP-097. FC was supported by grant
N00014-14-1-0800 from the Office of Naval Research.
8

References
[1] P. Abbeel, A. Coates, M. Quigley, and A. Y. Ng. An Application of Reinforcement Learning to
Aerobatic Helicopter Flight. In B. Sch?lkopf, J. C. Platt, and T. Hofmann, editors, Advances
in Neural Information Processing Systems, volume 19, pages 1?8, Cambridge, Massachusetts,
2007. The MIT Press.
[2] P. Abbeel and A. Y. Ng. Apprenticeship Learning via Inverse Reinforcement Learning. In
Proceedings of the Twenty-first International Conference on Machine Learning, ICML ?04,
pages 1?, New York, NY, USA, 2004. ACM.
[3] B. D. Argall, S. Chernova, M. Veloso, and B. Browning. A survey of robot learning from
demonstration. Robotics and Autonomous Systems, 57(5):469?483, May 2009.
[4] J. L. Austerweil and T. L. Griffiths. A nonparametric Bayesian framework for constructing
flexible feature representations. Psychological Review, 120(4):817?851, 2013.
[5] M. Babes, V. Marivate, K. Subramanian, and M. L. Littman. Apprenticeship learning about
multiple intentions. In Proceedings of the 28th International Conference on Machine Learning
(ICML-11), pages 897?904, 2011.
[6] C. L. Baker, R. Saxe, and J. B. Tenenbaum. Action understanding as inverse planning. Cognition,
113(3):329?349, Dec. 2009.
[7] D. Buchsbaum, A. Gopnik, T. L. Griffiths, and P. Shafto. Children?s imitation of causal action
sequences is influenced by statistical and pedagogical evidence. Cognition, 120(3):331?340,
Sept. 2011.
[8] L. P. Butler and E. M. Markman. Preschoolers use pedagogical cues to guide radical reorganization of category knowledge. Cognition, 130(1):116?127, Jan. 2014.
[9] A. D. Dragan, K. C. T. Lee, and S. S. Srinivasa. Legibility and predictability of robot motion.
In 2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI), pages
301?308, Mar. 2013.
[10] M. C. Frank and N. D. Goodman. Predicting Pragmatic Reasoning in Language Games. Science,
336(6084):998?998, May 2012.
[11] J. MacGlashan and M. L. Littman. Between imitation and intention learning. In Proceedings
of the 24th International Conference on Artificial Intelligence, pages 3692?3698. AAAI Press,
2015.
[12] D. Ramachandran and E. Amir. Bayesian Inverse Reinforcement Learning. In Proceedings of
the 20th International Joint Conference on Artifical Intelligence, IJCAI?07, pages 2586?2591,
San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc.
[13] P. Shafto, N. D. Goodman, and T. L. Griffiths. A rational account of pedagogical reasoning:
Teaching by, and learning from, examples. Cognitive Psychology, 71:55?89, June 2014.
[14] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.

9

"
5351,"Adaptive Online Learning
Dylan J. Foster ?
Cornell University

Alexander Rakhlin ?
University of Pennsylvania

Karthik Sridharan ?
Cornell University

Abstract
We propose a general framework for studying adaptive regret bounds in the online
learning setting, subsuming model selection and data-dependent bounds. Given a
data- or model-dependent bound we ask, ?Does there exist some algorithm achieving this bound?? We show that modifications to recently introduced sequential
complexity measures can be used to answer this question by providing sufficient
conditions under which adaptive rates can be achieved. In particular each adaptive
rate induces a set of so-called offset complexity measures, and obtaining small
upper bounds on these quantities is sufficient to demonstrate achievability. A
cornerstone of our analysis technique is the use of one-sided tail inequalities to
bound suprema of offset random processes.
Our framework recovers and improves a wide variety of adaptive bounds including
quantile bounds, second order data-dependent bounds, and small loss bounds. In
addition we derive a new type of adaptive bound for online linear optimization
based on the spectral norm, as well as a new online PAC-Bayes theorem.

1

Introduction

Some of the recent progress on the theoretical foundations of online learning has been motivated by
the parallel developments in the realm of statistical learning. In particular, this motivation has led to
martingale extensions of empirical process theory, which were shown to be the ?right? notions for
online learnability. Two topics, however, have remained elusive thus far: obtaining data-dependent
bounds and establishing model selection (or, oracle-type) inequalities for online learning problems.
In this paper we develop new techniques for addressing both these questions.
Oracle inequalities and model selection have been topics of intense research in statistics in the last
two decades [1, 2, 3]. Given a sequence of models M1 , M2 , . . . whose union is M, one aims to
derive a procedure that selects, given an i.i.d. sample of size n, an estimator f? from a model Mm
?
that trades off bias and variance. Roughly speaking the desired oracle bound takes the form
err(f?) ? inf ? inf err(f ) + penn (m)? ,
m

f ?Mm

where penn (m) is a penalty for the model m. Such oracle inequalities are attractive because they
can be shown to hold even if the overall model M is too large. A central idea in the proofs of such
statements (and an idea that will appear throughout the present paper) is that penn (m) should be
?slightly larger? than the fluctuations of the empirical process for the model m. It is therefore not
surprising that concentration inequalities?and particularly Talagrand?s celebrated inequality for the
supremum of the empirical process?have played an important role in attaining oracle bounds. In
order to select a good model in a data-driven manner, one establishes non-asymptotic data-dependent
bounds on the fluctuations of an empirical process indexed by elements in each model [4].
?
?

Deptartment of Computer Science
Deptartment of Statistics

1

Lifting the ideas of oracle inequalities and data-dependent bounds from statistical to online learning
is not an obvious task. For one, there is no concentration inequality available, even for the simple
case of sequential Rademacher complexity. (For the reader already familiar with this complexity:
a change of the value of one Rademacher variable results in a change of the remaining path, and
hence an attempt to use a version of a bounded difference inequality grossly fails). Luckily, as we
show in this paper, the concentration machinery is not needed and one only requires a one-sided tail
inequality. This realization is motivated by the recent work of [5, 6, 7]. At a high level, our approach
will be to develop one-sided inequalities for the suprema of certain offset processes [7], where the
offset is chosen to be ?slightly larger? than the complexity of the corresponding model. We then show
that these offset processes determine which data-dependent adaptive rates are achievable for online
learning problems, drawing strong connections to the ideas of statistical learning described earlier.
1.1

Framework

Let X be the set of observations, D the space of decisions, and Y the set of outcomes. Let (S)
denote the set of distributions on a set S. Let ` ? D ? Y ? R be a loss function. The online learning
framework is defined by the following process: For t = 1, . . . , n, Nature provides input instance
xt ? X ; Learner selects prediction distribution qt ? (D); Nature provides label yt ? Y, while the
learner draws prediction y?t ? qt and suffers loss `(?
yt , yt ).

Two important settings are supervised learning (Y ? R, D ? R) and online linear optimization
(X = {0} is a singleton set, Y and D are balls in dual Banach spaces and `(?
y , y) = ??
y , y?). For a
class F ? DX , we define the learner?s cumulative regret to F as
n

n

yt , yt ) ? inf ? `(f (xt ), yt ).
? `(?
f ?F t=1

t=1

A uniform regret bound Bn is achievable if there exists a randomized algorithm selecting y?t such that
n

n

E?? `(?
yt , yt ) ? inf ? `(f (xt ), yt )? ? Bn
f ?F t=1

t=1

?x1?n , y1?n ,

(1)

where a1?n stands for {a1 , . . . , an }. Achievable rates Bn depend on complexity of the function class
F. For example, sequential Rademacher complexity of F is one of the tightest achievable uniform
rates for a variety of loss functions [8, 7].
An adaptive regret bound has the form Bn (f ; x1?n , y1?n ) and is said to be achievable if there exists a
randomized algorithm for selecting y?t such that
n

n

t=1

t=1

E?? `(?
yt , yt ) ? ? `(f (xt ), yt )? ? Bn (f ; x1?n , y1?n ) ?x1?n , y1?n , ?f ? F.

(2)

We distinguish three types of adaptive bounds, according to whether Bn (f ; x1?n , y1?n ) depends only
on f , only on (x1?n , y1?n ), or on both quantities. Whenever Bn depends on f , an adaptive regret can
be viewed as an oracle inequality which penalizes each f according to a measure of its complexity
(e.g. the complexity of the smallest model to which it belongs). As in statistical learning, an oracle
inequality (2) may be proved for certain functions Bn (f ; x1?n , y1?n ) even if a uniform bound (1)
cannot hold for any nontrivial Bn .
1.2

Related Work

The case when Bn (f ; x1?n , y1?n ) = Bn (x1?n , y1?n ) does not depend on f has received most of the
attention in the literature. The focus is on bounds that can be tighter for ?nice sequences,? yet maintain
near-optimal worst-case guarantees. An incomplete list of prior work includes [9, 10, 11, 12], couched
in the setting of online linear/convex optimization, and [13] in the experts setting.

A bound of type Bn (f ) was studied in [14], which presented an algorithm that competes with all
experts simultaneously, but with varied regret with respect to each of them depending on the quantile
of the expert. Another bound of this type was given by [15], who consider online linear optimization
with an unbounded set and provide oracle inequalities with an appropriately chosen function Bn (f ).
Finally, the third category of adaptive bounds are those that depend on both the hypothesis f ? F
and the data. The bounds that depend on the loss of the best function (so-called ?small-loss? bounds,
2

[16, Sec. 2.4], [17, 13]) fall in this category trivially, since one may overbound the loss of the best
function by the performance of f . We draw attention to the recent result of [18] who show an adaptive
bound in terms of both the loss of comparator and the KL divergence between the comparator and
some pre-fixed prior distribution over experts. An MDL-style bound in terms of the variance of the
loss of the comparator (under the distribution induced by the algorithm) was recently given in [19].
Our study was also partly inspired by Cover [20] who characterized necessary and sufficient conditions
for achievable bounds in prediction of binary sequences. The methods in [20], however, rely on the
structure of the binary prediction problem and do not readily generalize to other settings.
The framework we propose recovers the vast majority of known adaptive rates in literature, including
variance bounds, quantile bounds, localization-based bounds, and fast rates for small losses. It should
be noted that while existing literature on adaptive online learning has focused on simple hypothesis
classes such as finite experts and finite-dimensional p-norm balls, our results extend to general
hypothesis classes, including large nonparametric ones discussed in [7].

2

Adaptive Rates and Achievability: General Setup

The first step in building a general theory for adaptive online learning is to identify what adaptive
regret bounds are possible to achieve. Recall that an adaptive regret bound of Bn ? F ? X n ? Y n ? R
is said to be achievable if there exists an online learning algorithm such that, (2) holds.

In the rest of this work, we use the notation ?. . .?t=1 to denote the interleaved application of the
operators inside the brackets, repeated over t = 1, . . . , n rounds (see [21]). Achievability of an
adaptive rate can be formalized by the following minimax quantity.
Definition 1. Given an adaptive rate Bn we define the offset minimax value:
n

An (F , Bn ) ? ? sup

n

inf

sup E ?

?t ?qt
xt ?X qt ? (D) yt ?Y y

?? `(?
yt , yt ) ? inf ?? `(f (xt ), yt ) + Bn (f ; x1?n , y1?n )??.
n

n

f ?F

t=1 t=1

t=1

An (F, Bn ) quantifies how ?nt=1 `(?
yt , yt ) ? inf f ?F {?nt=1 `(f (xt ), yt ) + Bn (f ; x1?n , y1?n )} behaves
when the optimal learning algorithm that minimizes this difference is used against Nature trying to
maximize it. Directly from this definition,
An adaptive rate Bn is achievable if and only if An (F, Bn ) ? 0.

If Bn is a uniform rate, i.e., Bn (f ; x1?n , y1?n ) = Bn , achievability reduces to the minimax analysis
explored in [8]. The uniform rate Bn is achievable if and only if Bn ? Vn (F), where Vn (F) is the
minimax value of the online learning game.

We now focus on understanding the minimax value An (F, Bn ) for general adaptive rates. We
first show that the minimax value is bounded by an offset version of the sequential Rademacher
complexity studied in [8]. The symmetrization Lemma 1 below provides us with the first step towards
a probabilistic analysis of achievable rates. Before stating the lemma, we need to define the notion of
a tree and the notion of sequential Rademacher complexity.

Given a set Z, a Z-valued tree z of depth n is a sequence (zt )nt=1 of functions zt ? {?1}t?1 ? Z.
One may view z as a complete binary tree decorated by elements of Z. Let ? = (?t )nt=1 be a sequence
of independent Rademacher random variables. Then (zt (?)) may be viewed as a predictable process
with respect to the filtration S t = (?1 , . . . , ?t ). For a tree z, the sequential Rademacher complexity
of a function class G ? RZ on z is defined as
n

Rn (G, z) ? E? sup ? ?t g(zt (?))

and

g?G t=1

Rn (G) ? sup Rn (G, z) .
z

Lemma 1. For any lower semi-continuous loss `, and any adaptive rate Bn that only depends on
outcomes (i.e. Bn (f ; x1?n , y1?n ) = Bn (y1?n )), we have that
n

An ? sup E? ?sup ?2 ? ?t `(f (xt (?)), yt (?))? ? Bn (y1?n (?))? .
x,y

f ?F

t=1

3

(3)

Further, for any general adaptive rate Bn ,
n

?
An ? sup E? ?sup ?2 ? ?t `(f (xt (?)), yt (?)) ? Bn (f ; x1?n (?), y2?n+1
(?))?? .
f ?F

x,y,y?

(4)

t=1

Finally, if one considers the supervised learning problem where F ? X ? R, Y ? R and ` ? R ? R ? R
is a loss that is convex and L-Lipschitz in its first argument, then for any adaptive rate Bn ,
n

An ? sup E? ?sup ?2L ? ?t f (xt (?)) ? Bn (f ; x1?n (?), y1?n (?))?? .
x,y

f ?F

(5)

t=1

The above lemma tells us that to check whether an adaptive rate is achievable, it is sufficient to check
that the corresponding adaptive sequential complexity measures are non-positive. We remark that if
the above complexities are bounded by some positive quantity of a smaller order, one can form a new
achievable rate Bn? by adding the positive quantity to Bn .

3

Probabilistic Tools

As mentioned in the introduction, our technique rests on certain one-sided probabilistic inequalities.
We now state the first building block: a rather straightforward maximal inequality.
Proposition 2. Let I = {1, . . . , N }, N ? ?, be a set of indices and let (Xi )i?I be a sequence of
random variables satisfying the following tail condition: for any ? > 0,
P (Xi ? Bi > ? ) ? C1 exp ??? 2 ?(2

2
i )? + C2 exp (?? si )

(6)

for some positive sequence (Bi ), nonnegative sequence ( i ) and nonnegative sequence (si ) of
numbers, and for constants C1 , C2 ? 0. Then for any ? ? 1 , s? ? s1 , and
?
i
?i = max ?
2 log( i ?? ) + 4 log(i), (Bi si )?1 log ?i2 (?
s?si )?? + 1,
Bi
it holds that

E sup {Xi ? Bi ?i } ? 3C1 ? + 2C2 (?
s)?1 .

(7)

i?I

We remark that Bi need not be the expected value of Xi , as we are not interested in two-sided
deviations around the mean.
One of the approaches to obtaining oracle-type inequalities is to split a large class into smaller
ones according to a ?complexity radius? and control a certain stochastic process separately on each
subset (also known as the peeling technique). In the applications below, Xi will often stand for the
(random) supremum of this process on subset i, and Bi will be an upper bound on its typical size.
Given deviation bounds for Xi above Bi , the dilated size Bi ?i then allows one to pass to maximal
inequalities (7) and thus verify achievability in Lemma 1. The same strategy works for obtaining
data-dependent bounds, where we first prove tail bounds for the given size of the data-dependent
quantity, then appeal to (7).
A simple yet powerful example for the control of the supremum of a stochastic process is an inequality
due to Pinelis [22] for the norm (which is a supremum over the dual ball) of a martingale in a 2-smooth
Banach space. Here we state a version of this result that can be found in [23, Appendix A].
Lemma 3. Let Z be a unit ball in a separable (2, D)-smooth Banach space H. For any Z-valued
tree z, and any n > ? ?4D2
P ??? ?t zt (?)? ? ? ? ? 2 exp ??
n

t=1

?2
?
8D2 n

When the class of functions is not linear, we may no longer appeal to the above lemma. Instead, we
make use of a result from [24] that extends Lemma 3 at a price of a poly-logarithmic factor. Before
stating this lemma, we briefly define the relevant complexity measures (see [24] for more details).
First, a set V of R-valued trees is called an ?-cover of G ? RZ on z with respect to `p if
n

?g ? G, ?? ? {?1}n , ?v ? V s.t. ?(g(zt (?)) ? vt (?))p ? n?p .
t=1

4

The size of the smallest ?-cover is denoted by Np (G, ?, z), and Np (G, ?, n) ? supz Np (G, ?, z).
The set V is an ?-cover of G on z with respect to `? if

?g ? G, ?? ? {?1}, ?v ? V s.t. ?g(zt (?)) ? vt (?)? ? ?

?t ? [n].

We let N? (G, ?, z) be the smallest such cover and set N? (G, ?, n) = supz N? (G, ?, z).

Lemma 4 ([24]). Let G ? [?1, 1]Z . Suppose Rn (G)?n ? 0 with n ? ? and that the following
?1
mild assumptions hold: Rn (G) ? 1?n, N? (G,
?2 , n) ? 4, and there exists a constant such that
?
?j
?1
? ?j=1 N? (G, 2 , n) . Then for any ? > 12?n, for any Z-valued tree z of depth n,
?
n
P ?sup ?? ?t g(zt (?))? > 8 ?1 + ? 8n log3 (en2 )? ? Rn (G)?
g?G t=1

n

? P ?sup ?? ?t g(zt (?))? > n inf ?4? + 6? ?

?

?>0

g?G t=1

1?

log N? (G, , n)d ?? ? 2 e?

n? 2
4

.

The above lemma yields a one-sided control on the size of the supremum of the sequential Rademacher
process, as required for our oracle-type inequalities.
Next, we turn our attention to an offset Rademacher process, where the supremum is taken over a
collection of negative-mean random variables. The behavior of this offset process was shown to
govern the optimal rates of convergence for online nonparametric regression [7]. Such a one-sided
control of the supremum will be necessary for some of the data-dependent upper bounds we develop.
Lemma 5. Let z be a Z-valued tree of depth n, and let G ? RZ . For any
P ?sup ? ??t g(zt (?)) ? 2?g 2 (zt (?))? ?
n

g?G t=1

where

?

exp ??

?2
??
? + exp ?? ? ,
2 2
2

log (2n )

? ?j=12

? 1?n and ? > 0,

?
?
log N2 (G, , z)
? 12 2 ?
n log N2 (G, , z)d ? 1 > ? ?
?
1?n

N2 (G, 2?j , z)?2 and

= 12 ? 1

n

?
n log N2 (G, , z)d .

We observe that the probability of deviation has both subgaussian and subexponential components.
Using the above result and Proposition 2 leads to useful bounds on the quantities in Lemma 1 for
specific types of adaptive rates. Given a tree z, we obtain a bound on the expected size of the
sequential Rademacher process when we subtract off the data-dependent `2 -norm of the function on
the tree z, adjusted by logarithmic terms.
Corollary 6. Suppose G ? [?1, 1]Z , and let z be any Z-valued tree of depth n. Assume
log N2 (G, , n) ? ?p for some p < 2. Then
?
?
n
?
?
?n
E sup ?? ?t g(zt (?)) ? 4?2(log n) log N2 (G, ?2, z) ?? g 2 (zt (?)) + 1?
g?G, ?
?
t=1
?t=1
?
?
?
?
?
?24 2 log n ?
n log N2 (G, , z)d ? ? 7 + 2 log n .
?
1?n
?
?

The next corollary yields slightly faster rates than Corollary 6 when ?G? < ?.

Corollary 7. Suppose G ? [?1, 1]Z with ?G? = N , and let z be any Z-valued tree of depth n. Then

4

?
?
?
n
n
?
?
?
?n
?
2
E sup?? ?t g(zt (?)) ? 2 log?log N ? g (z(?)) + e??32?log N ? g 2 (z(?)) + e?? ? 1.
?
?
g?G ?t=1
?
t=1
t=1
?
?

Achievable Bounds

In this section we use Lemma 1 along with the probabilistic tools from the previous section to obtain
an array of achievable adaptive bounds for various online learning problems. We subdivide the section
into one subsection for each category of adaptive bound described in Section 1.1.
5

4.1

Adapting to Data

Here we consider adaptive rates of the form Bn (x1?n , y1?n ), uniform over f ? F. We show the power
of the developed tools on the following example.

Example 4.1 (Online Linear Optimization in Rd ). Consider the problem of online linear optimization where F = {f ? Rd ? ?f ?2 ? 1}, Y = {y ? ?y?2 ? 1}, X = {0}, and `(?
y , y) = ??
y , y?. The
following adaptive rate is achievable:
?
?
? 1?2
Bn (y1?n ) = 16 d log(n) ???n
? + 16 d log(n),
t=1 yt yt ?

where ??? is the spectral norm. Let us deduce this result from Corollary 6. First, observe that
?
???n
t=1 yt yt ?

1?2

? =

?
sup ???n
t=1 yt yt ?

1?2

f ??f ?2 ?1

f? =

sup

f ??f ?2 ?1

? n
?
?
f ? ?n
?t=1 `2 (f, yt ).
t=1 yt yt f = sup
f ?F

The linear function class F can be covered point-wise at any scale with (3? )d balls and thus
N (` ? F, 1?(2n), z) ? (6n)d for any Y-valued tree z. We apply Corollary 6 with = 1?n and the
integral term in the corollary vanishes, yielding the claimed statement.
4.2

Model Adaptation

In this subsection we focus on achievable rates for oracle inequalities and model selection, but
without dependence on data. The form of the rate is therefore Bn (f ). Assume we have a class
F = ?R?1 F(R), with the property that F(R) ? F(R? ) for any R ? R? . If we are told by an
oracle that regret will be measured with respect to those hypotheses f ? F with R(f ) ? inf{R ?
f ? F(R)} ? R? , then using the minimax algorithm one can guarantee a regret bound of at most
the sequential Rademacher complexity Rn (F(R? )). On the other hand, given the optimality of the
sequential Rademacher complexity for online learning problems for commonly encountered losses,
we can argue that for any f ? F chosen in hindsight, one cannot expect a regret better than order
Rn (F(R(f ))). In this section we show that simultaneously for all f ? F, one can attain an adaptive
?
upper bound of O ?Rn (F(R(f ))) log (Rn (F(R(f )))) log3?2 n?. That is, we may predict as if
we knew the optimal radius, at the price of a logarithmic factor. This is the price of adaptation.
Corollary 8. For any class of predictors F with F(1) non-empty, if one considers the supervised
learning problem with 1-Lipschitz loss `, the following rate is achievable:
Bn (f ) = log

3?2

?
?
?
?
?
log(2R(f )) ? Rn (F (2R(f ))) ?
n ?K1 Rn (F (2R(f ))) ?1 + ?log ?
?? + K2 Rn (F (1))? ,
Rn (F (1))
?
?
?
?

for absolute constants K1 , K2 , and

defined in Lemma 4.

In fact, this statement is true more generally with F(2R(f )) replaced by ` ? F(2R(f )). It is
tempting to attempt to prove the above statement with the exponential weights algorithm running as
an aggregation procedure over the solutions for each R. In general, this approach will fail for two
reasons. First, if function values grow with R, the exponential
? weights bound will scale linearly with
this value. Second, an experts bound yields only a slower n rate.

As a special case of the above lemma, we obtain an online PAC-Bayesian theorem. We postpone
this example to the next sub-section where we get a data-dependent version of this result. We now
provide a bound for online linear optimization in 2-smooth Banach spaces that automatically adapts
to the norm of the comparator. To prove it, we use the concentration bound from [22] (Lemma 3)
within the proof of the above corollary to remove the extra logarithmic factors.
Example 4.2 (Unconstrained Linear Optimization). Consider linear optimization with Y being
the unit ball of some reflexive Banach space with norm ???? . Let F = D be the dual space and the
loss `(?
y , y) = ??
y , y? (where we are using ??, ?? to represent the linear functional in the first argument
to the second argument). Define F(R) = {f ? ?f ? ? R} where ??? is the norm dual to ???? . If the unit
ball of Y is (2, D)-smooth, then the following rate is achievable for all f with ?f ? ? 1:
?
?
B(f ) = D n?8?f ??1 + log(2?f ?) + log log(2?f ?)? + 12?.

For the case of a Hilbert space, the above bound was achieved by [15].
6

4.3

Adapting to Data and Model Simultaneously

We now study achievable bounds that perform online model selection in a data-adaptive way. Of
specific interest is our online optimistic PAC-Bayesian bound. This bound should be compared
to [18, 19], with the reader noting that it is independent of the number of experts, is algorithmindependent, and depends quadratically on the expected loss of the expert we compare against.
Example 4.3 (Generalized Predictable Sequences (Supervised Learning)). Consider an online
supervised learning problem with a convex 1-Lipschitz loss. Let (Mt )t?1 be any predictable sequence
that the learner can compute at round t based on information provided so far, including xt (One can
think of the predictable sequence Mt as a prior guess for the hypothesis we would compare with in
hindsight). Then the following adaptive rate is achievable:
?
?
n
?
? ?
Bn (f ; x1?n ) = inf ?K1 ?log n ? log N2 (F , ?2, n) ? ?? (f (xt ) ? Mt )2 + 1?
?
?
t=1
?
?
?
?
?
+K2 log n ?
n log N2 (F , , n)d + 2 log n+7?,
?
1?n
?
?

?
?
for constants K1 = 4 2, K2 = 24 2 from Corollary 6. The achievability is a direct consequence
of Eq. (5) in Lemma 1, followed by Corollary 6 (one can include any predictable sequence in
the Rademacher average part because ?t Mt ?t is zero mean). Particularly, if we assume that the
sequential covering of class F grows as log N2 (F, ?, n) ? ??p for some p < 2, we get that
p
?
1? 2 ?
p?2
2
? ?? ?n
? n? ? .
Bn (f ) = O
t=1 (f (xt ) ? Mt ) + 1?

As p gets closer to 0, we get full adaptivity and replace n by ?nt=1 (f (xt ) ? Mt ) + 1. On the other
hand, as p gets closer to 2 (i.e. more complex function classes), we do not adapt and get a uniform
bound in terms of n. For p ? (0, 2), we attain a natural interpolation.
Example 4.4 (Regret to Fixed Vs Regret to Best (Supervised Learning)). Consider an online
supervised learning problem with a convex 1-Lipschitz loss and let ?F? = N . Let f ? ? F be a fixed
expert chosen in advance. The following bound is achievable:
2

?
n
?
Bn (f, x1?n ) = 4 log?log N ?(f (xt ) ? f (xt )) + e??32?log N ?(f (xt ) ? f ? (xt ))2 + e? + 2.
n

t=1

?

2

t=1

?
?
In particular, against
? f we have Bn (f , x1?n ) = O(1), and against an arbitrary expert we have
Bn (f, x1?n ) = O? n log N (log (n ? log N ))?. This bound follows from Eq. (5) in Lemma 1 followed
by Corollary 7. This extends the study of [25] to supervised learning and general class of experts F.
Example 4.5 (Optimistic PAC-Bayes). Assume that we have a countable set of experts and that the
loss for each expert on any round is non-negative and bounded by 1. The function class F is the set
of all distributions over these experts, and X = {0}. This setting can be formulated as online linear
optimization where the loss of mixture f over experts, given instance y, is ?f, y?, the expected loss
under the mixture. The following adaptive bound is achievable:

?
n
?
Bn (f ; y1?n ) = ?50 (KL(f ??) + log(n)) ? Ei?f ?ei , yt ?2 + 50 (KL(f ??) + log(n)) + 10.
t=1

This adaptive bound is an online PAC-Bayesian bound. The rate adapts not only to the KL di2
vergence of f with fixed prior ? but also replaces n with ?nt=1 Ei?f ?ei , yt ? . Note that we have
2
n
n
?t=1 Ei?f ?ei , yt ? ? ?t=1 ?f, yt ?, yielding the small-loss type bound described earlier. This is an
improvement over the bound in [18] in that the bound is independent of number of experts, and so
holds even for countably infinite sets of experts. The KL term in our bound may be compared to the
MDL-style term in the bound of [19]. If we have a large (but finite) number of experts and take ? to
be uniform, the above bound provides an improvement over both [14]1 and [18].
Evaluating the above bound with a distribution f that places all its weight on any one expert appears
to address the open question posed by [13] of obtaining algorithm-independent oracle-type variance
bounds for experts. The proof of achievability of the above rate is shown in the appendix because it
requires a slight variation on the symmetrization lemma specific to the problem.
1

See [18] for a comparison of KL-based bounds and quantile bounds.

7

5

Relaxations for Adaptive Learning

To design algorithms for achievable rates, we extend the framework of online relaxations from [26].
A relaxation Reln ? ?nt=0 X t ? Y t ? R that satisfies the initial condition,
Reln (x1?n , y1?n ) ? ? inf ?? `(f (xt ), yt ) + Bn (f ; x1?n , y1?n )?,
n

f ?F

and the recursive condition,

Reln (x1?t?1 , y1?t?1 ) ? sup

(8)

t=1

sup Ey??qt [`(?
yt , yt ) + Reln (x1?t , y1?t )],

inf

xt ?X qt ? (D) yt ?Y

(9)

is said to be admissible for the adaptive rate Bn . The relaxation?s corresponding strategy is
q?t = arg minqt ? (D) supyt ?Y Ey??qt [`(?
yt , yt ) + Reln (x1?t , y1?t )], which enjoys the adaptive bound
yt , yt ) ? inf ?? `(f (xt ), yt ) + Bn (f ; x1?n , y1?n )? ? Reln (?) ?x1?n , y1?n .
? `(?
n

t=1

n

f ?F

t=1

It follows immediately that the strategy achieves the rate Bn (f ; x1?n , y1?n ) + Reln (?). Our goal is
then to find relaxations for which the strategy is computationally tractable and Reln (?) ? 0 or at least
has smaller order than Bn . Similar to [26], conditional versions of the offset minimax values An
yield admissible relaxations, but solving these relaxations may not be computationally tractable.
Example 5.1 (Online PAC-Bayes). Consider the experts setting in Example 4.5 with:
?
?
Bn (f ) = 3 2n max{KL(f ? ?), 1} + 4 n.
?
Let Ri = 2i?1 and let qtR (y) denote the exponential weights distribution with learning rate R?n:
?
q R (y1?t )k ? ?k exp?? R?n(?ts=1 yt )k ?. The following is an admissible relaxation achieving Bn :
Reln (y1?t ) = inf ?
>0

1

log?? exp?? ? ? ?q Ri (y1?s?1 ), ys ? +
t

i

s=1

?
nRi ??? + 2 (n ? t)?.

Ri
Let qt? be a distribution with (qt? )i ? exp?? ?1n ??t?1
s=1 ?q (y1?s?1 ), ys ? ?

?
nRi ??. We predict by

drawing i according to qt? , then drawing an expert according to q Ri (y1?t?1 ).

While in general the problem of obtaining an efficient adaptive relaxation might be hard, one can ask
the question, ?If and efficient relaxation RelR
n is available for each F(R), can one obtain an adaptive
model selection algorithm for all of F??. To this end for supervised learning problem with convex
Lipschitz loss we delineate a meta approach which utilizes existing relaxations for each F(R).
Lemma 9. Let qtR (y1 , . . . , yt?1 ) be the randomized strategy corresponding to RelR
n , obtained after
observing outcomes y1 , . . . , yt?1 , and let ? ? R ? R be nonnegative. The following relaxation is
R
admissible for the rate Bn (R) = RelR
n (?)?(Reln (?)):
Adan (x1?t , y1?t ) =

R
R
sup E?t+1?n sup?RelR
`(?
ys , ys (?))?.
?
R (y
n (x1?t , y1?t ) ? Reln (?)?(Reln (?)) + 2 ? ?s Ey
?s ?qs
1?t ,yt+1?s?1 (?))

x,y,y?

n

R?1

s=t+1

Playing according to the strategy for Adan will guarantee a regret bound of Bn (R) + Adan (?), and
Adan (?) can be bounded using Proposition 2 when the form of ? is as in that proposition.

We remark that the above strategy is not necessarily obtained by running a high-level experts algorithm
over the discretized values of R. It is an interesting question to determine the cases when such a
strategy is optimal. More generally, when the adaptive rate Bn depends on data, it is not possible to
obtain the rates we show non-constructively in this paper using the exponential weights algorithm
with meta-experts as the required weighting over experts would be data dependent (and hence is not a
prior over experts). Further, the bounds from exponential-weights-type algorithms are akin to having
sub-exponential tails in Proposition 2, but for many problems we may have sub-gaussian tails.
Obtaining computationally efficient methods from the proposed framework is an interesting research
direction. Proposition 2 provides a useful non-constructive tool to establish achievable adaptive
bounds, and a natural question to ask is if one can obtain a constructive counterpart for the proposition.
8

References
[1] Lucien Birg?e, Pascal Massart, et al. Minimum contrast estimators on sieves: exponential bounds and rates
of convergence. Bernoulli, 4(3):329?375, 1998.
[2] G?abor Lugosi and Andrew B Nobel. Adaptive model selection using empirical complexities. Annals of
Statistics, pages 1830?1864, 1999.
[3] Peter L. Bartlett, St?ephane Boucheron, and G?abor Lugosi. Model selection and error estimation. Machine
Learning, 48(1-3):85?113, 2002.
[4] Pascal Massart. Concentration inequalities and model selection, volume 10. Springer, 2007.
[5] Shahar Mendelson. Learning without Concentration. In Conference on Learning Theory, 2014.
[6] Tengyuan Liang, Alexander Rakhlin, and Karthik Sridharan. Learning with square loss: Localization
through offset rademacher complexity. Proceedings of The 28th Conference on Learning Theory, 2015.
[7] Alexander Rakhlin and Karthik Sridharan. Online nonparametric regression. Proceedings of The 27th
Conference on Learning Theory, 2014.
[8] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Random averages, combinatorial parameters, and learnability. In Advances in Neural Information Processing Systems 23. 2010.
[9] Elad Hazan and Satyen Kale. Extracting certainty from uncertainty: Regret bounded by variation in costs.
Machine learning, 80(2):165?188, 2010.
[10] Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo
Zhu. Online optimization with gradual variations. In COLT, 2012.
[11] Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Proceedings of
the 26th Annual Conference on Learning Theory (COLT), 2013.
[12] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. The Journal of Machine Learning Research, 12:2121?2159, 2011.
[13] Nicolo Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for prediction
with expert advice. Machine Learning, 66(2-3):321?352, 2007.
[14] Kamalika Chaudhuri, Yoav Freund, and Daniel J Hsu. A parameter-free hedging algorithm. In Advances
in neural information processing systems, pages 297?305, 2009.
[15] H. Brendan McMahan and Francesco Orabona. Unconstrained online linear learning in hilbert spaces:
Minimax algorithms and normal approximations. Proceedings of The 27th Conference on Learning Theory,
2014.
[16] Nicolo Cesa-Bianchi and G?abor Lugosi. Prediction, Learning, and Games. Cambridge University Press,
2006.
[17] Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates. In Advances
in neural information processing systems, pages 2199?2207, 2010.
[18] Haipeng Luo and Robert E. Schapire. Achieving all with no parameters: Adaptive normalhedge. CoRR,
abs/1502.05934, 2015.
[19] Wouter M. Koolen and Tim van Erven. Second-order quantile methods for experts and combinatorial
games. In Proceedings of the 28th Annual Conference on Learning Theory (COLT), pages 1155?1175,
2015.
[20] Thomas M. Cover. Behavior of sequential predictors of binary sequences. In in Trans. 4th Prague
Conference on Information Theory, Statistical Decision Functions, Random Processes, pages 263?272.
Publishing House of the Czechoslovak Academy of Sciences, 1967.
[21] Alexander Rakhlin and Karthik Sridharan. Statistical learning theory and sequential prediction, 2012.
Available at http://stat.wharton.upenn.edu/?rakhlin/book_draft.pdf.
[22] Iosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals of
Probability, 22(4):1679?1706, 10 1994.
[23] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Beyond regret. arXiv preprint
arXiv:1011.3168, 2010.
[24] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Sequential complexities and uniform martingale
laws of large numbers. Probability Theory and Related Fields, 2014.
[25] Eyal Even-Dar, Michael Kearns, Yishay Mansour, and Jennifer Wortman. Regret to the best vs. regret to
the average. Machine Learning, 72(1-2):21?37, 2008.
[26] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Relax and randomize: From value to algorithms.
Advances in Neural Information Processing Systems 25, pages 2150?2158, 2012.

9

"
4486,"Principles of Risk Minimization
for Learning Theory

V. Vapnik
AT &T Bell Laboratories
Holmdel, NJ 07733, USA

Abstract
Learning is posed as a problem of function estimation, for which two principles of solution are considered: empirical risk minimization and structural
risk minimization. These two principles are applied to two different statements of the function estimation problem: global and local. Systematic
improvements in prediction power are illustrated in application to zip-code
recognition.

1

INTRODUCTION

The structure of the theory of learning differs from that of most other theories for
applied problems. The search for a solution to an applied problem usually requires
the three following steps:

1. State the problem in mathematical terms.
2. Formulate a general principle to look for a solution to the problem.
3. Develop an algorithm based on such general principle.
The first two steps of this procedure offer in general no major difficulties; the
third step requires most efforts, in developing computational algorithms to solve
the problem at hand.
In the case of learning theory, however, many algorithms have been developed, but
we still lack a clear understanding of the mathematical statement needed to describe
the learning procedure, and of the general principle on which the search for solutions
831

832

Vapnik

should be based. This paper is devoted to these first two steps, the statement of
the problem and the general principle of solution.
The paper is organized as follows. First, the problem of function estimation is
stated, and two principles of solution are discussed: the principle of empirical risk
minimization and the principle of structural risk minimization. A new statement
is then given: that of local estimation of function, to which the same principles are
applied. An application to zip-code recognition is used to illustrate these ideas.

2

FUNCTION ESTIMATION MODEL

The learning process is described through three components:
1. A generator of random vectors x, drawn independently from a fixed but unknown
distribution P(x).
2. A supervisor which returns an output vector y to every input vector x, according
to a conditional distribution function P(ylx), also fixed but unknown.
3. A learning machine capable of implementing a set of functions !(x, w), wE W.

The problem of learning is that of choosing from the given set of functions the one
which approximates best the supervisor's response. The selection is based on a
training set of e independent observations:
(1)

The formulation given above implies that learning corresponds to the problem of
function approximation.

3

PROBLEM OF RISK MINIMIZATION

In order to choose the best available approximation to the supervisor's response,
we measure the loss or discrepancy L(y, !(x, w? between the response y of the
supervisor to a given input x and the response !(x, w) provided by the learning
machine. Consider the expected value of the loss, given by the risk functional
R(w) =

J

L(y, !(x, w?dP(x,y).

(2)

The goal is to minimize the risk functional R( w) over the class of functions
!(x, w), w E W. But the joint probability distribution P(x, y) = P(ylx )P(x)
is unknown and the only available information is contained in the training set (1).

4

EMPIRICAL RISK MINIMIZATION

In order to solve this problem, the following induction principle is proposed: the
risk functional R( w) is replaced by the empirical risk functional

1

E(w)

=i

l

LL(Yi,!(Xi'W?
i=l

(3)

Principles of Risk Minimization for Learning Theory

constructed on the basis of the training set (1). The induction principle of empirical
risk minimization (ERM) assumes that the function I(x, wi) ,which minimizes E(w)
over the set w E W, results in a risk R( wi) which is close to its minimum.
This induction principle is quite general; many classical methods such as least square
or maximum likelihood are realizations of the ERM principle.
The evaluation of the soundness of the ERM principle requires answers to the following two questions:
1. Is the principle consistent? (Does R( wi) converge to its minimum value on the
set wE W when f - oo?)
2. How fast is the convergence as f increases?
The answers to these two questions have been shown (Vapnik et al., 1989) to be
equivalent to the answers to the following two questions:
1. Does the empirical risk E( w) converge uniformly to the actual risk R( w) over
the full set I(x, w), wE W? Uniform convergence is defined as
0
as
f - 00.
(4)
Prob{ sup IR(w) - E(w)1 > ?} wEW

2. What is the rate of convergence?
It is important to stress that uniform convergence (4) for the full set of functions is
a necessary and sufficient condition for the consistency of the ERM principle.

5

VC-DIMENSION OF THE SET OF FUNCTIONS

The theory of uniform convergence of empirical risk to actual risk developed in
the 70's and SO's, includes a description of necessary and sufficient conditions as
well as bounds for the rate of convergence (Vapnik, 19S2). These bounds, which
are independent of the distribution function P(x,y), are based on a quantitative
measure of the capacity of the set offunctions implemented by the learning machine:
the VC-dimension of the set.
For simplicity, these bounds will be discussed here only for the case of binary pattern recognition, for which y E {O, 1} and I(x, w), wE W is the class of indicator
functions. The loss function takes only two values L(y, I(x, w))
0 if y I(x, w)
and L(y, I(x, w)) = 1 otherwise. In this case, the risk functional (2) is the probability of error, denoted by pew). The empirical risk functional (3), denoted by
v(w), is the frequency of error in the training set.

=

=

The VC-dimension of a set of indicator functions is the maximum number h of
vectors which can be shattered in all possible 2h ways using functions in the set.
For instance, h = n + 1 for linear decision rules in n-dimensional space, since they
can shatter at most n + 1 points.

6

RATES OF UNIFORM CONVERGENCE

The notion of VC-dimension provides a bound to the rate of uniform convergence.
For a set of indicator functions with VC-dimension h, the following inequality holds:

833

834

Vapnik

Prob{ SUp IP(w) - v(w)1 > c}

2fe

h

2

< (-h) exp{-e fl?

(5)

wEW

It then follows that with probability 1 - T}, simultaneously for all w E W,

pew) < v(w)

+ Co(f/h, T}),

(6)

with confidence interval

C (f/h
o

) _ . Ih(1n 21/h + 1) - In T}
,T}-V
f
.

(7)

This important result provides a bound to the actual risk P( w) for all w E W,
including the w? which minimizes the empirical risk v(w).
The deviation IP(w) - v(w)1 in (5) is expected to be maximum for pew) close
to 1/2, since it is this value of pew) which maximizes the error variance u(w) =
P( w)( 1 - P( w)). The worst case bound for the confidence interval (7) is thus
likely be controlled by the worst decision rule. The bound (6) is achieved for the
worst case pew) = 1/2, but not for small pew), which is the case of interest. A
uniformly good approximation to P( w) follows from considering

J

Prob{ sup
wEW

pew) - v(w)
> e}.
(j(w)

(8)

The variance of the relative deviation (P( w) - v( w))/ (j( w) is now independent of w.
A bound for the probability (8), if available, would yield a uniformly good bound
for actual risks for all P( w).
Such a bound has not yet been established. But for pew) ?
(j(w) ~ JP(w) is true, and the following inequality holds:
Prob{ sup
wEW

1, the approximation

pew) - v(w)
2le h
e2 f
> e} < (-) exp{--}.
JP(w)
h
4

(9)

It then follows that with probability 1 - T}, simultaneously for all w E W,

pew) < v(w)

+ CI(f/h, v(w), T}),

(10)

with confidence interval

CI(l/h,v(w),T}) =2 (h(ln2f/h;l)-lnT})

(1+ 1+

v(w)f
)
h(1n 2f/h + 1) -In T} .
(11)
Note that the confidence interval now depends on v( w), and that for v( w) = 0 it
reduces to
CI(f/ h, 0, T}) = 2C'5(f/ h, T}),
which provides a more precise bound for real case learning.

7

STRUCTURAL RISK MINIMIZATION

The method of ERM can be theoretically justified by considering the inequalities
(6) or (10). When l/h is large, the confidence intervals Co or C 1 become small, and

Principles of Risk Minimization for Learning Theory

can be neglected . The actual risk is then bound by only the empirical risk, and the
probability of error on the test set can be expected to be small when the frequency
of error in the training set is small.
However, if ljh is small, the confidence interval cannot be neglected, and even
v( w) = 0 does not guarantee a small probability of error. In this case the minimization of P( w) requires a new principle, based on the simultaneous minimization of
v( w) and the confidence interval. It is then necessary to control the VC-dimension
of the learning machine.
To do this, we introduce a nested structure of subsets Sp
that
SlCS2C ... CSn

= {lex, w), wE Wp}, such

.

The corresponding VC-dimensions of the subsets satisfy
hl

< h2 < ... < h n .

The principle of structure risk minimization (SRM) requires a two-step process: the
empirical risk has to be minimized for each element of the structure. The optimal
element S* is then selected to minimize the guaranteed risk, defined as the sum
of the empirical risk and the confidence interval. This process involves a trade-off:
as h increases the minimum empirical risk decreases, but the confidence interval
mcreases.

8

EXAMPLES OF STRUCTURES FOR NEURAL NETS

The general principle of SRM can be implemented in many different ways . Here
we consider three different examples of structures built for the set of functions
implemented by a neural network .
1. Structure given by the architecture of the neural network. Consider an
ensemble of fully connected neural networks in which the number of units in one of
the hidden layers is monotonically increased. The set of implement able functions
makes a structure as the number of hidden units is increased.

2. Structure given by the learning procedure. Consider the set of functions
S = {lex, w), w E W} implementable by a neural net of fixed architecture. The
parameters {w} are the weights of the neural network. A structure is introduced
through Sp = {lex, w), Ilwll < Cp } and C l < C2 < ... < Cn. For a convex
loss function, the minimization of the empirical risk within the element Sp of the
structure is achieved through the minimizat.ion of

1

E(w""P)

=l

l

LL(Yi,!(Xi'W? +'P llwI1 2
i=l

with appropriately chosen Lagrange multipliers II > 12 > ... > In' The well-known
""weight decay"" procedure refers to the minimization of this functional.
3. Structure given by preprocessing. Consider a neural net with fixed arK(x, 13),
chitecture. The input representation is modified by a transformation z
where the parameter f3 controls the degree of the degeneracy introduced by this
transformation (for instance f3 could be the width of a smoothing kernel).

=

835

836

Vapnik

A structure is introduced in the set of functions S
through 13 > CP1 and C l > C2 > ... > Cn?

9

= {!(I?x, 13), w),

w E W}

PROBLEM OF LOCAL FUNCTION ESTIMATION

The problem of learning has been formulated as the problem of selecting from the
class of functions !(x, w), w E W that which provides the best available approximation to the response of the supervisor. Such a statement of the learning problem
implies that a unique function !( x, w?) will be used for prediction over the full input
space X. This is not necessarily a good strategy: the set !(x, w), w E W might
not contain a good predictor for the full input space, but might contain functions
capable of good prediction on specified regions of input space.
In order to formulate the learning problem as a problem of local function approximation, consider a kernel I?x - Xo, b) ~ 0 which selects a region of input space of
width b, centered at xo. For example, consider the rectangular kernel,

I< (x _ x b) = { 1 if Ix - ~o I < b
,.
0,
0 otherwIse
and a more general general continuous kernel, such as the gaussian
r

/ig(x-xo,b)=exp-{

(x-xO)2
b2
}.

The goal is to minimize the local risk functional

R(w, b, xo) =

J

L(y, !(x, w?

K(x - Xo, b)
K(xo, b) dP(x, V)?

(12)

The normalization is defined by

K(xo, b) =

J

K(x - Xo, b) dP(x).

(13)

The local risk functional (12) is to be minimized over the class of functions
!(x, w), w E Wand over all possible neighborhoods b E (0,00) centered at xo.
As before, the joint probability distribution P( x, y) is unknown, and the only available information is contained in the training set (1).

10

EMPIRICAL RISK MINIMIZATION FOR LOCAL
ESTIMATION

In order to solve this problem, the following induction principle is proposed: for
fixed b, the local risk functional (12) is replaced by the empirical risk functional

E(w,b,xo)

K(Xi - Xo, b)
= l1 ~
L..tL(Yj,!(Xj,w? 1?
b) ,
Xo,
i=l

(14)

Principles of Risk Minimization for Learning Theory

constructed on the basis of the training set. The empirical risk functional (14) is
to be minimized over w E W. In the simplest case, the class of functions is that of
constant functions, I(x, w) = C( w). Consider the following examples:
1. K-Nearest Neighbors Method: For the case of binary pattern recognition, the class of constant indicator functions contains only two functions: either
I(x, w)
for all x, or I(x, w)
1 for all x. The minimization of the empirical
risk functional (14) with the rectangular kernel Kr(x-xo,b) leads to the K-nearest
neighbors algorithm.

=

?

=

2. Watson-Nadaraya Method: For the case y E R, the class of constant functions contains an infinite number of elements, I(x,w) = C(w), C(w) E R. The
minimization of the empirical risk functional (14) for general kernel and a quadratic
loss function L(y, I(x, w)) = (y - I(x, w))2 leads to the estimator
l

I( Xo ) -- """".
~YI
i=1

K(Xi - Xo, b)
,
L;=I/\ (x; - xo, b)
l.

which defines the Watson-Nadaraya algorithm.
These classical methods minimize (14) with a fixed b over the class of constant
functions. The supervisor's response in the vicinity of Xo is thus approximated by a
constant, and the characteristic size b of the neighborhood is kept fixed, independent
of Xo.
A truly local algorithm would adjust the parameter b to the characteristics of the
region in input space centered at Xo . Further improvement is possible by allowing
for a richer class of predictor functions I(x, w) within the selected neighborhood.
The SRM principle for local estimation provides a tool for incorporating these two
features .

11

STRUCTURAL RISK MINIMIZATION FOR LOCAL
ESTIMATION

The arguments that lead to the inequality (6) for the risk functional (2) can be
extended to the local risk functional (12), to obtain the following result: with
probability 1 - T}, and simultaneously for all w E Wand all b E (0,00)

R(w,b,xo) < E(w,b,xo) + C 2(flh, b, T}).

(15)

The confidence interval C2(flh, b, T}) reduces to Co(llh, T}) in the b -+ 00 limit.
As before, a nested structure is introduced in the class of functions, and the empirical
risk (14) is minimized with respect to both w E Wand bE (0,00) for each element
of the structure. The optimal element is then selected to minimize the guaranteed
risk, defined as the sum of the empirical risk and the confidence interval. For fixed
b this process involves an already discussed trade-off: as h increases, the empirical
risk decreases but the confidence interval increases. A new trade-off appears by
varying b at fixed h: as b increases the empirical risk increases, but the confidence
interval decreases. The use of b as an additional free parameter allows us to find
deeper minima of the guaranteed risk.

837

838

Vapnik

12

APPLICATION TO ZIP-CODE RECOGNITION

We now discuss results for the recognition of the hand written and printed digits in
the US Postal database, containing 9709 training examples and 2007 testing examples. Human recognition of this task results in an approximately 2.5% prediction
error (Sackinger et al., 1991).
The learning machine considered here is a five-layer neural network with shared
weights and limited receptive fields. When trained with a back-propagation algorithm for the minimization of the empirical risk, the network achieves 5.1% prediction error (Le Cun et al., 1990).
Further performance improvement with the same network architecture has required
the introduction a new induction principle. Methods based on SRM have achieved
prediction errors of 4.1% (training based on a double-back-propagation algorithm
which incorporates a special form of weight decay (Drucker, 1991? and 3.95% (using
a smoothing transformation in input space (Simard, 1991?.
The best result achieved so far, of 3.3% prediction error, is based on the use of the
SRM for local estimation of the predictor function (Bottou, 1991).
It is obvious from these results that dramatic gains cannot be achieved through

minor algorithmic modifications, but require the introduction of new principles.
Acknowledgements

I thank the members of the Neural Networks research group at Bell Labs, Holmdel,
for supportive and useful discussions. Sara Solla, Leon Bottou, and Larry Jackel
provided invaluable help to render my presentation more clear and accessible to the
neural networks community.
References

V. N. Vapnik (1982), Estimation of Dependencies Based on Empirical Data,
Springer-Verlag (New York).
V. N. Vapnik and A. J a. Chervonenkis (1989) 'Necessary and sufficient conditions
for consistency of the method of empirical risk minimization' [in Russian], Yearbook of the Academy of Sciences of the USSR on Recognition, Classification, and
Forecasting, 2, 217-249, Nauka (Moscow) (English translation in preparation).
E. Sackinger and J. Bromley (1991), private communication.
Y. Le Cun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard and
L. D. Jackel (1990) 'Handwritten digit recognition with a back-propagation network', Neural Information Processing Systems 2, 396-404, ed. by D. S. Touretzky,
Morgan Kaufmann (California).
H. Drucker (1991), private communication.
P. Simard (1991), private communication.
L. Bottou (1991), private communication.

"
1579,"Efficient Multiscale Sampling from
Products of Gaussian Mixtures
Alexander T. Ihler, Erik B. Sudderth, William T. Freeman, and Alan S. Willsky
Department of Electrical Engineering and Computer Science
Massachusetts Institute of Technology
ihler@mit.edu, esuddert@mit.edu, billf@ai.mit.edu, willsky@mit.edu

Abstract
The problem of approximating the product of several Gaussian mixture
distributions arises in a number of contexts, including the nonparametric
belief propagation (NBP) inference algorithm and the training of product of experts models. This paper develops two multiscale algorithms
for sampling from a product of Gaussian mixtures, and compares their
performance to existing methods. The first is a multiscale variant of previously proposed Monte Carlo techniques, with comparable theoretical
guarantees but improved empirical convergence rates. The second makes
use of approximate kernel density evaluation methods to construct a fast
approximate sampler, which is guaranteed to sample points to within a
tunable parameter  of their true probability. We compare both multiscale samplers on a set of computational examples motivated by NBP,
demonstrating significant improvements over existing methods.

1

Introduction

Gaussian mixture densities are widely used to model complex, multimodal relationships.
Although they are most commonly associated with parameter estimation procedures like
the EM algorithm, kernel or Parzen window nonparametric density estimates [1] also take
this form for Gaussian kernel functions. Products of Gaussian mixtures naturally arise
whenever multiple sources of statistical information, each of which is individually modeled by a mixture density, are combined. For example, given two independent observations y1 , y2 of an unknown variable x, the joint likelihood p(y1 , y2 |x) ? p(y1 |x)p(y2 |x) is
equal to the product of the marginal likelihoods. In a recently proposed nonparametric belief propagation (NBP) [2, 3] inference algorithm for graphical models, Gaussian mixture
products are the mechanism by which nodes fuse information from different parts of the
graph. Product densities also arise in the product of experts (PoE) [4] framework, in which
complex densities are modeled as the product of many ?local? constraint densities.
The primary difficulty associated with products of Gaussian mixtures is computational. The
product of d mixtures of N Gaussians is itself a Gaussian mixture with N d components.
In many practical applications, it is infeasible to explicitly construct these components,
and therefore intractable to build a smaller approximating mixture using the EM algorithm.
Mixture products are thus typically approximated by drawing samples from the product
density. These samples can be used to either form a Monte Carlo estimate of a desired
expectation [4], or construct a kernel density estimate approximating the true product [2].

Although exact sampling requires exponential cost, Gibbs sampling algorithms may often
be used to produce good approximate samples [2, 4].
When accurate approximations are required, existing methods for sampling from products
of Gaussian mixtures often require a large computational cost. In particular, sampling is
the primary computational burden for both NBP and PoE. This paper develops a pair of
new sampling algorithms which use multiscale, KD-Tree [5] representations to improve
accuracy and reduce computation. The first is a multiscale variant of existing Gibbs samplers [2, 4] with improved empirical convergence rate. The second makes use of approximate kernel density evaluation methods [6] to construct a fast -exact sampler which, in
contrast with existing methods, is guaranteed to sample points to within a tunable parameter  of their true probability. Following our presentation of the algorithms, we demonstrate
their performance on a set of computational examples motivated by NBP and PoE.

2

Products of Gaussian Mixtures

Let {p1 (x), . . . , pd (x)} denote a set of d mixtures of N Gaussian densities, where
X
pi (x) =
wli N (x; ?li , ?i )

(1)

li

Here, li are a set of labels for the N mixture components in pi (x), wli are the normalized
component weights, and N (x; ?li , ?i ) denotes a normalized Gaussian density with mean
?li and diagonal covariance ?i . For simplicity, we assume that all mixtures are of equal
size N , and that the variances ?i are uniform within each mixture, although the algorithms
which follow may be readily extended to problems where this is not the case. Our goal is
Qd
to efficiently sample from the N d component mixture density p(x) ? i=1 pi (x).
2.1

Exact Sampling

Sampling from the product density can be decomposed into two steps: randomly select one
of the product density?s N d components, and then draw a sample from the corresponding
Gaussian. Let each product density component be labeled as L = [l1 , . . . , ld ], where li
labels one of the N components of pi (x).1 The relative weight of component L is given by
Qd
d
d
X
X
wl N (x; ?li , ?i )
??1
wL = i=1 i
??1
??1
??1
i ?li (2)
i
L ?L =
L =
N (x; ?L , ?L )
i=1
i=1
where ?L , ?L are the mean and variance of product component L, and this equation may be
evaluated at any x (the value x = ?L may be numerically convenient). To form
P the product
density, these weights are normalized by the weight partition function Z , L wL .
Determining Z exactly takes O(N d ) time, and given this constant we can draw N samples
from the distribution in O(N d ) time and O(N ) storage. This is done by drawing and sorting N uniform random variables on the interval [0, 1], and then computing the cumulative
distribution of p(L) = wL /Z to determine which, if any, samples are drawn from each L.
2.2

Importance Sampling

Importance sampling is a Monte Carlo method for approximately sampling from (or computing expectations of) an intractable distribution p(x), using a proposal distribution q(x)
for which sampling is feasible [7]. To draw N samples from p(x), an importance sampler
draws M ? N samples xi ? q(x), and assigns
the ith sample weight wi ? p(xi )/q(xi ).
P
The weights are then normalized by Z = i wi , and N samples are drawn (with replacement) from the discrete distribution p?(xi ) = wi /Z.
1
Throughout this paper, we use lowercase letters (li ) to label input density components, and capital letters (L = [l1 , . . . , ld ]) to label the corresponding product density components.

...

Mix 2

Mix 1

Sequential Gibbs Sampler

Mix 1

...

Parallel Gibbs Sampler

Mix 2

X

X
Figure 1: Two possible Gibbs samplers for a product of 2 mixtures of 5 Gaussians. Arrows show the
weights assigned to each label. Top left: At each iteration, one label is sampled conditioned on the
other density?s current label. Bottom left: Alternate between sampling a data point X conditioned on
the current labels, and resampling all labels in parallel. Right: After ? iterations, both Gibbs samplers
identify mixture labels corresponding to a single kernel (solid) in the product density (dashed).

For products of Gaussian mixtures, we consider two different proposal distributions. The
first, which we refer to as mixture importance sampling, draws each sample by randomly
selecting one of the d input mixtures, and sampling from its N components (q(x)
Q = p i (x)).
The remaining d ? 1 mixtures then provide the importance weight (wi = j6=i pj (xi )).
This is similar to the method used to combine density trees in [8]. Alternatively, we can
approximate
Q each input mixture pi (x) by a single Gaussian density qi (x), and choose
q(x) ? i qi (x). We call this procedure Gaussian importance sampling.
2.3

Gibbs Sampling

Sampling from Gaussian mixture products is difficult because the joint distribution over
product density labels, as defined by equation (2), is complicated. However, conditioned
on the labels of all but one mixture, we can compute the conditional distribution over the
remaining label in O(N ) operations, and easily sample from it. Thus, we may use a Gibbs
sampler [9] to draw asymptotically unbiased samples, as illustrated in Figure 1. At each
iteration, the labels {lj }j6=i for d ? 1 of the input mixtures are fixed, and the ith label is
sampled from the corresponding conditional density. The newly chosen li is then fixed,
and another label is updated. After a fixed number of iterations ?, a single sample is drawn
from the product mixture component identified by the final labels. To draw N samples, the
Gibbs sampler requires O(d?N 2 ) operations; see [2] for further details.
The previously described sequential Gibbs sampler defines an iteration over the labels of
the input mixtures. Another possibility uses the fact that, given a data point x
? in the product
density space, the d input mixture labels are conditionally independent [4]. Thus, one can
define a parallel Gibbs sampler which alternates between sampling a data point conditioned
on the current input mixture labels, and parallel sampling of the mixture labels given the
current data point (see Figure 1). The complexity of this sampler is also O(d?N 2 ).

3

KD?Trees

A KD-tree is a hierarchical representation of a point set which caches statistics of subsets
of the data, thereby making later computations more efficient [5]. KD-trees are typically
binary trees constructed by successively splitting the data along cardinal axes, grouping
points by spatial location. We use the variable l to denote the label of a leaf node (the index
of a single point), and l to denote a set of leaf labels summarized at a node of the KD-tree.

{1,2,3,4,5,6,7,8}

x xx

xx x

{1,2,3,4}

x xx
{1,2}

x xx

x x

x xx

xx x

x x

x x

x xx

xx x

x x

x xx

xx x

x x

{5,6,7,8}

xx x
{3,4}

{5,6}

{7,8}

xx x

x x

(a)
(b)
Figure 2: Two KD-tree representations of the same one-dim. point set. (a) Each node maintains a
bounding box (label sets l are shown in braces). (b) Each node maintains mean and variance statistics.

Figure 2 illustrates one-dimensional KD-trees which cache different sets of statistics. The
first (Figure 2(a)) maintains bounding boxes around the data, allowing efficient computation of distances; similar trees are used in Section 4.2. Also shown in this figure are the
label sets l for each node. The second (Figure 2(b)) precomputes means and variances of
point clusters, providing a multi-scale Gaussian mixture representation used in Section 4.1.
3.1

Dual Tree Evaluation

Multiscale representations have been effectively
applied to kernel density estimation problems.
Given a mixture of N Gaussians with means {?i },
we would like to evaluate
X
wi N (xj ; ?i , ?)
(3)
p(xj ) =
i

at a given set of M points {xj }. By representing
the means {?i } and evaluation points {xj } with
two different KD-trees, it is possible to define a
dual?tree recursion [6] which is much faster than
direct evaluation of all N M kernel?point pairs.

x xx

xx x
Dmax

oooo

x x
Dmin

o ooo

Figure 3: Two KD-tree representations
may be combined to efficiently bound
the maximum (Dmax ) and minimum
(Dmin ) pairwise distances between subsets of the summarized points (bold).

The dual-tree algorithm uses bounding box statistics (as in Figure 2(a)) to approximately
evaluate subsets of the data. For any set of labels in the density tree l? and location tree lx ,
one may use pairwise distance bounds (see Figure 3) to find upper and lower bounds on
X
wi N (xj ; ?i , ?) for any j ? lx
(4)
i?l?

When the distance bounds are sufficiently tight, the sum in equation (4) may be approximated by a constant, asymptotically allowing evaluation in O(N ) operations [6].

4
4.1

Sampling using Multiscale Representations
Gibbs Sampling on KD-Trees

Although the pair of Gibbs samplers discussed in Section 2.3 are often effective, they sometimes require a very large number of iterations to produce accurate samples. The most difficult densities are those for which there are multiple widely separated modes, each of which
is associated with disjoint subsets of the input mixture labels. In this case, conditioned
on a set of labels corresponding to one mode, it is very unlikely that a label or data point
corresponding to a different mode will be sampled, leading to slow convergence.
Similar problems have been observed with Gibbs samplers on Markov random fields [9].
In these cases, convergence can often be accelerated by constructing a series of ?coarser

scale? approximate models in which the Gibbs sampler can move between modes more easily [10]. The primary challenge in developing these algorithms is to determine procedures
for constructing accurate coarse scale approximations. For Gaussian mixture products,
KD-trees provide a simple, intuitive, and easily constructed set of coarser scale models.
As in Figure 2(b), each level of the KD-tree stores the mean and variance (biased by kernel
size) of the summarized leaf nodes. We start at the same coarse scale for all input mixtures,
and perform standard Gibbs sampling on that scale?s summary Gaussians. After several
iterations, we condition on a data sample (as in the parallel Gibbs sampler of Section 2.3)
to infer labels at the next finest scale. Intuitively, by gradually moving from coarse to fine
scales, multiscale sampling can better explore all of the product density?s important modes.
As the number of sampling iterations approaches infinity, multiscale samplers have the
same asymptotic properties as standard Gibbs samplers. Unfortunately, there is no guarantee that multiscale sampling will improve performance. However, our simulation results
indicate that it is usually very effective (see Section 5).
4.2

Epsilon-Exact Sampling using KD-Trees

In this section, we use KD-trees to efficiently compute an approximation to the partition
function Z, in a manner similar to the dual tree evaluation algorithm of [6] (see Section 3.1).
This leads to an -exact sampler for which a label L = [l1 , . . . , ld ], with true probability
pL , is guaranteed to be sampled with some probability p?L ? [pL ? , pL + ]. We denote
subsets of labels in the input densities with lowercase script (li ), and sets of labels in the
product density by L = l1 ? ? ? ? ? ld . The approximate sampling procedure is similar to
the exact sampler of Section 2.1. We first construct KD-tree representations of each input
density (as in Figure 2(a)), and use a multi?tree recursion to approximate the partition
P
function Z? =
w
?L by summarizing sets of labels L where possible. Then, we compute
?
the cumulative distribution of the sets of labels, giving each label set L probability w
? L /Z.
4.2.1 Approximate Evaluation of the Weight Partition Function
We first note that the weight function (equation (2)) can be rewritten using terms which
involve only pairwise distances (the quotient is computed elementwise):
d
Y
Y

?i ?j
wL =
w lj ?
N (?li ; ?lj , ?(i,j) )
where
?(i,j) =
(5)
?L
j=1
(li ,lj>i )
Qd
This equation may be divided into two parts: a weight contribution i=1 wli , and a distance
contribution (which we denote by KL ) expressed in terms of the pairwise distances between
kernel centers. We use the KD-trees? distance bounds to compute bounds on each of these
pairwise distance terms for a collection of labels L = l1 ?? ? ??ld . The product of the upper
(lower) pairwise bounds is itself an upper (lower) bound on the total distance contribution
for any label L within the set; denote these bounds by KL+ and KL? , respectively.2

By using the mean KL? = 12 KL+ + KL? to approximate KL , we incur a maximum error

+
?
1
2 KL ? KL for any label L ? L. If this error is less than Z? (which we ensure by
comparing to a running lower bound Zmin on Z), we treat it as constant over the set L and
approximate the contribution to Z by
X
X Y
Y X
w
?L = KL?
( wli ) = KL?
(
w li )
(6)
L?L

L?L

i

i

li ?li

This is easily calculated using cached statistics of the weight contained in each set. If the
error is larger than Z?, we need to refine at least one of the label sets; we use a heuristic
to make this choice. This procedure is summarized in Algorithm 1. Note that all of the
2
We can also use multipole methods such as the Fast Gauss Transform [11] to efficiently compute
alternate, potentially tighter bounds on the pairwise values.

MultiTree([l1 , . . . , ld ])
1. For each pair of distributions (i, j > i), use their bounding boxes to compute
(i,j)
(a) Kmax ? maxli ?li ,lj ?lj N (xli ? xlj ; 0, ?(i,j) )
(i,j)
(b) Kmin ? minli ?li ,lj ?lj N (xli ? xlj ; 0, ?(i,j) )
Q
Q
(i,j)
(i,j)
2. Find Kmax = (i,j>i) Kmax and Kmin = (i,j>i) Kmin
1
3. If 2 (Kmax ? Kmin ) ? Zmin ?, approximate this combination of label sets:
Q
P
(a) w
?L = 21 (Kmax + Kmin Q
) ( wli ), where wli = li ?li wli is cached by the KD-trees
(b) Zmin = Zmin + Kmin ( wli )
(c) Z? = Z? + w
?L
4. Otherwise, refine one of the label sets:
(i,j)
(i,j)
(a) Find arg max(i,j) Kmax /Kmin such that range(li ) ? range(lj ).
(b) Call recursively:
i. MultiTree([l1 , . . . , Nearer(Left(li ), Right(li ), lj ), . . . , ld ])
ii. MultiTree([l1 , . . . , Farther(Left(li ), Right(li ), lj ), . . . , ld ])
where Nearer(Farther) returns the nearer (farther) of the first two arguments to the third.

Algorithm 1: Recursive multi-tree algorithm for approximately evaluating the partition function Z
of the product of d Gaussian mixture densities represented by KD?trees. Z min denotes a running
lower bound on the partition function, while Z? is the current estimate. Initialize Zmin = Z? = 0.
? repeat Algorithm 1 with the following modifications:
Given the final partition function estimate Z,
? j < c? + w
3. (c) If c? ? Zu
?L for any j, draw L ? L by sampling li ? li with weight wli /wli
3. (d) c? = c? + w
?L

Algorithm 2: Recursive multi-tree algorithm for approximate sampling. c? denotes the cumulative
sum of weights w
?L . Initialize by sorting N uniform [0, 1] samples {uj }, and set Zmin = c? = 0.

quantities required by this algorithm may be stored within the KD?trees, avoiding searches
over the sets li . At the algorithm?s termination, the total error is bounded by
X
X
XY
Y
1
? ?
K+ ? K?
wl ? Z?
|Z ? Z|
|wL ? w
?L | ?
wl ? Z? (7)
2

L

L

L

i

L

i

L

where the last inequality follows because each input mixture?s weights are normalized.
This guarantees that our estimate Z? is within a fractional tolerance ? of its true value.
4.2.2

Approximate Sampling from the Cumulative Distribution
To use the partition function estimate Z? for approximate sampling, we repeat the approximation process in a manner similar to the exact sampler: draw N sorted uniform random
variables, and then locate these samples in the cumulative distribution. We do not explicitly
construct the cumulative distribution, but instead use the same approximate partial weight
sums used to determine Z? (see equation (6)) to find the block of labels L = l1 ? ? ? ? ? ld
associated with each sample. Since all labels L ? L within this block have approximately
equal distance contribution KL ? KL? , we independently sample a label li within each set
li proportionally to the weight wli .
This procedure is shown in Algorithm 2. Note that, to be consistent about when approxima? we repeat the procedure
tions are made and thus produce weights w
?L which still sum to Z,
for computing Z? exactly, including recomputing the running lower bound Zmin . This algorithm is guaranteed to sample each label L with probability p?L ? [pL ? , pL + ]:


w
wL 
2?
?L

?
?
,
(8)
|?
pL ? pL | = 

?
Z
1??
Z
? Q
Q
|KL ?KL
|
wli ? ?( wli ) ? ? and
Z
1+?
?. Thus, the estimated probability
1??
w
?L
2?
|
+
| w?ZL ? w?Z?L | ? 1??
.
Z

Proof: From our bounds on the error of KL? , | wZL ? w?ZL | =
1
1
?
| w?ZL ? w?Z?L | = w?ZL |1 ? Z/Z
| ? w?ZL |1 ? 1??
| ? w?ZL 1??
?
?
of choosing label L has at most error | wZL ?

w
?L
? |
Z

? | wZL ?

5

Computational Examples

5.1 Products of One?Dimensional Gaussian Mixtures
In this section, we compare the sampling methods discussed in this paper on three challenging one?dimensional examples, each involving products of mixtures of 100 Gaussians
(see Figure 4). We measure performance by drawing 100 samples, constructing a kernel
density estimate using likelihood cross?validation [1], and calculating the KL divergence
from the true product density. We repeat this test 250 times for each of a range of parameter
settings of each algorithm, and plot the average KL divergence versus computation time.
For the product of three mixtures in Figure 4(a), the multiscale (MS) Gibbs samplers dramatically outperform standard Gibbs sampling. In addition, we see that sequential Gibbs
sampling is more accurate than parallel. Both of these differences can be attributed to the
bimodal product density. However, the most effective algorithm is the ?exact sampler,
which matches exact sampling?s performance in far less time (0.05 versus 2.75 seconds).
For a product of five densities (Figure 4(b)), the cost of exact sampling increases to 7.6
hours, but the ?exact sampler matches its performance in less than one minute. Even
faster, however, is the sequential MS Gibbs sampler, which takes only 0.3 seconds.
For the previous two examples, mixture importance sampling (IS) is nearly as accurate
as the best multiscale methods (Gaussian IS seems ineffective). However, in cases where
all of the input densities have little overlap with the product density, mixture IS performs
very poorly (see Figure 4(c)). In contrast, multiscale samplers perform very well in such
situations, because they can discard large numbers of low weight product density kernels.
5.2 Tracking an Object using Nonparametric Belief Propagation
NBP [2] solves inference problems on non?Gaussian graphical models by propagating the
results of local sampling computations. Using our multiscale samplers, we applied NBP
to a simple tracking problem in which we observe a slowly moving object in a sea of randomly shifting clutter. Figure 5 compares the posterior distributions of different samplers
two time steps after an observation containing only clutter. ?exact sampling matches the
performance of exact sampling, but takes half as long. In contrast, a standard particle
filter [7], allowed ten times more computation, loses track. As in the previous section,
multiscale Gibbs sampling is much more accurate than standard Gibbs sampling.

6

Discussion

For products of a few mixtures, the ?exact sampler is extremely fast, and is guaranteed to
give good performance. As the number of mixtures grow, ?exact sampling may become
overly costly, but the sequential multiscale Gibbs sampler typically produces accurate samples with only a few iterations. We are currently investigating the performance of these
algorithms on large?scale nonparametric belief propagation applications.

References
[1] B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman & Hall, 1986.
[2] E. B. Sudderth, A. T. Ihler, W. T. Freeman, and A. S. Willsky. Nonparametric belief propagation.
In CVPR, 2003.
[3] M. Isard. PAMPAS: Real?valued graphical models for computer vision. In CVPR, 2003.
[4] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Technical
Report 2000-004, Gatsby Computational Neuroscience Unit, 2000.
[5] K. Deng and A. W. Moore. Multiresolution instance-based learning. In IJCAI, 1995.
[6] A. G. Gray and A. W. Moore. Very fast multivariate kernel density estimation. In JSM, 2003.
[7] A. Doucet, N. de Freitas, and N. Gordon, editors. Sequential Monte Carlo Methods in Practice.
Springer-Verlag, New York, 2001.
[8] S. Thrun, J. Langford, and D. Fox. Monte Carlo HMMs. In ICML, pages 415?424, 1999.
[9] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Trans. PAMI, 6(6):721?741, November 1984.
[10] J. S. Liu and C. Sabatti. Generalised Gibbs sampler and multigrid Monte Carlo for Bayesian
computation. Biometrika, 87(2):353?369, 2000.
[11] J. Strain. The fast Gauss transform with variable scales. SIAM J. SSC, 12(5):1131?1139, 1991.

Exact
MS ??Exact
MS Seq. Gibbs
MS Par. Gibbs
Seq. Gibbs
Par. Gibbs
Gaussian IS
Mixture IS

(a)

Input Mixtures

KL Divergence

0.5

0.4

0.3

0.2

0.1

0
0

Product Mixture

0.1

0.2

0.3

0.4

0.5

0.6

Computation Time (sec)
2
Exact
MS ??Exact
MS Seq. Gibbs
MS Par. Gibbs
Seq. Gibbs
Par. Gibbs
Gaussian IS
Mixture IS

1.8

(b)

Input Mixtures

KL Divergence

1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
0

Product Mixture

0.5

1

1.5

2

2.5

Computation Time (sec)
Exact
MS ??Exact
MS Seq. Gibbs
MS Par. Gibbs
Seq. Gibbs
Par. Gibbs
Gaussian IS
Mixture IS

(c)

Input Mixtures

KL Divergence

0.5

0.4

0.3

0.2

0.1

0
0

Product Mixture

0.1

0.2

0.3

0.4

0.5

0.6

Computation Time (sec)

Figure 4: Comparison of average sampling accuracy versus computation time for different algorithms (see text). (a) Product of 3 mixtures (exact requires 2.75 sec). (b) Product of 5 mixtures (exact
requires 7.6 hours). (c) Product of 2 mixtures (exact requires 0.02 sec).
Target Location
Observations
Exact NBP

(a)

Target Location
??Exact NBP
Particle Filter

(b)

Target Location
MS Seq. Gibbs NBP
Seq. Gibbs NBP

(c)

Figure 5: Object tracking using NBP. Plots show the posterior distributions two time steps after an
observation containing only clutter. The particle filter and Gibbs samplers are allowed equal computation. (a) Latest observations, and exact sampling posterior. (b) ?exact sampling is very accurate,
while a particle filter loses track. (c) Multiscale Gibbs sampling leads to improved performance.

"
1016,"High-temperature expansions for learning
models of nonnegative data

Oliver B. Downs
Dept. of Mathematics
Princeton University
Princeton, NJ 08544
ob do wn s@ p r in c et o n.edu

Abstract
Recent work has exploited boundedness of data in the unsupervised
learning of new types of generative model. For nonnegative data it was
recently shown that the maximum-entropy generative model is a Nonnegative Boltzmann Distribution not a Gaussian distribution, when the
model is constrained to match the first and second order statistics of the
data. Learning for practical sized problems is made difficult by the need
to compute expectations under the model distribution. The computational cost of Markov chain Monte Carlo methods and low fidelity of
naive mean field techniques has led to increasing interest in advanced
mean field theories and variational methods. Here I present a secondorder mean-field approximation for the Nonnegative Boltzmann Machine
model, obtained using a ""high-temperature"" expansion. The theory is
tested on learning a bimodal 2-dimensional model, a high-dimensional
translationally invariant distribution, and a generative model for handwritten digits.

1 Introduction
Unsupervised learning of generative and feature-extracting models for continuous nonnegative data has recently been proposed [1], [2] . In [1], it was pointed out that the maximum
entropy distribution (matching Ist- and 2nd-order statistics) for continuous nonnegative
data is not Gaussian, and indeed that a Gaussian is not in general a good approximation
to that distribution. The true maximum entropy distribution is known as the Nonnegative Boltzmann Distribution (NNBD), (previously the rectified Gaussian distribution [3]) ,
which has the functional form

p(x)

= {o~exp[-E(X)]

if Xi ~ OVi,
if any Xi < 0,

(1)

where the energy function E(x) and normalisation constant Z are:

E(x)
Z

(3x T Ax - bT X,

= (

10;""20

dx exp[-E(x)].

(2)
(3)

In contrast to the Gaussian distribution, the NNBD can be multimodal in which case its
modes are confined to the boundaries of the nonnegative orthant.
The Nonnegative Boltzmann Machine (NNBM) has been proposed as a method for learning
the maximum likelihood parameters for this maximum entropy model from data. Without
hidden units, it has the stochastic-EM learning rule:

(XiXj)f - (XiXj)c
(Xi)c - (Xi)r,

(4)
(5)

where the subscript ""c"" denotes a ""clamped"" average over the data, and the subscript ""f""
denotes a ""free"" average over the NNBD:

(f(x))c

=

1 M
M
f(x(I'))

L

(6)

1'=1

(f(X))f

=

1

dxp(x)f(x).

(7)

x~O

This learning rule has hitherto been extremely computationally costly to implement, since
naive variationaVmean-field approximations for (XXT)r are found empirically to be poor,
leading to the need to use Markov chain Monte Carlo methods. This has made the NNBM
impractical for application to high-dimensional data.
While the NNBD is generally skewed and hence has moments of order greater than 2, the
maximum-likelihood learning rule suggests that the distribution can be described solely in
terms of the Ist- and 2nd-order statistics of the data. With that in mind, I have pursued
advanced approximate models for the NNBM.
In the following section I derive a second-order approximation for (XiXj)r analogous to the
TAP-On sager correction for the mean-field Ising Model, using a high temperature expansion, [4]. This produces an analytic approximation for the parameters A ij , bi in terms of
the mean and cross-correlation matrix of the training data.

2 Learning approximate NNBM parameters using high-temperature
expansion
Here I use Taylor expansion of a ""free energy"" directly related to the partition function
of the distribution, Z in the fJ = 0 limit, to derive a second-order approximation for the
NNBM model parameters. In this free energy we embody the constraint that Eq. 5 is
satisfied:

where fJ is an ""inverse temperature"". There is a direct relationship between the ""free energy"", G and the normalisation, Z of the NNBD, Eq. 3.

-In Z

= G(fJ, m) + Constant(b, m)

(9)

Thus,
(10)

The Lagrange multipliers, Ai embody the constraint that (Xi)f match the mean field of the
patterns, mi = (x)c. This effectively forces tl.b = 0 in Eq. 5, with bi = -Ai((3).
Since the Lagrange constraint is enforced for all temperatures, we can solve for the specific
case (3 = O.

= (Xi)fl.8-o =

mi

TIk Ixoo=0 Xi exp (- L:l Al(O)(XI - ml)) dXk

hOO

-

1

= --

(11)

TIk IXh=o exp (- L:l Al (0) (Xl - ml)) dXk
Ai(O)
Note that this embodies the unboundedness of Xk in the nonnegative orthant, as compared
to the equivalent term of Georges & Yedidia for the Ising model, mi = tanh(Ai(O)).
We consider Taylor expansion of Eq. 8 about the ""high temperature"" limit, (3 = O.

G((3, m)

= G(O, m) + (3

8G
8(3 I
.8=0

2
88(32
GI
+ ...
.8=0

(32
+ 2'

(12)

Since the integrand becomes factorable in Xi in this limit, the infinite temperature values of
G and its derivatives are analytically calculable.

G((3,m)I.8=o

= - Lin
k

{OO_ exp (- LAi(O)(Xi -mi)) dXk

}Xh-O

(13)

i

using Eq. 11;

G((3,m)I.8=o

=-

~ln (Ak~O) exp (~Ai(O)mi)) =N+ Llnmk

(14)

k

The first derivative is then as follows

8GI
8(3 .8=0

TIk 1000 (L:i .j -AijXiXj - L:i(Xi - mi)

?t) exp (- L:l Am(O)(XI -

ml)) dXk

TIk 10 exp (- L:l Am(O)(XI - ml)) dXk
00

(15)

(16)

i,j
This term is exactly the result of applying naive mean-field theory to this system, as in [1].
Likewise we obtain the second derivative

~~~ Ip~o ~ - ( (~A';X'X;) ')

+

(pi

+

O';)A,;m,m;) ,

.8=0

+

(~AijXiXj L ~; (Xk t,}

=-

(17)

mk))

k

.8=0

L L Qijkl Aij Aklmimjmkml
i,j k,l

(18)

Where Qijkl contains the integer coefficients arising from integration by parts in the first
and second terms and (1 + Oij) in the second term of Eq. 17.
This expansion is to the same order as the TAP-Onsager correction term for the Ising model,
which can be derived by an analogous approach to the equivalent free-energy [4]. Substituting these results into Eq. 10, we obtain

(3(Xi Xj)f

R!

(3(1 + Oij)mimj -

(32

2' L
kl

QijklAklmimjmkml

(19)

We arrive at an analytic approximation for Aij as a function of the 1st and 2nd moments of
the data, using Eq. 19 in the learning rule, Eq. 4, setting ~Aij = 0 and solving the linear
equation for A.
We can obtain an equivalent expansion for Ai ((3) and hence bi . To first order in (3 (equivalent to the order of (3 in the approximation for A), we have

Ai((3) ~ Ai(O) + (3

8A?1
8;
P

+ . ..

(20)

/3 =0

Using Eqs. 11 & 15

(21)

(22)

= - 2:(1 + c5ij )Aijmj

(23)

j

Hence
(24)

The approach presented here makes an explicit approximation of the statistics required
for the NNBM learning rule (xxT}f' which can be substituted in the fixed-point equation
Eq. 4, and yields a linear equation in A to be solved. This is in contrast to the linear
response theory approach of Kappen & Rodriguez [6] to the Boltzmann Machine, which
exploits the relationship
8 2 1nZ
8b i 8b j = (XiXj) - (Xi) (Xj) = Xij
(25)
between the free energy and the covariance matrix X of the model. In the learning problem,
this produces a quadratic equation in A, the solution of which is non-trivial. Computationally efficient solutions of the linear response theory are then obtained by secondary
approximation of the 2nd-order term, compromising the fidelity of the model.

3 Learning a 'Competitive' Nonnegative Boltzmann Distribution
A visualisable test problem is that of learning a bimodal NNBD in 2 dimensions. MonteCarlo slice sampling (See [1] & [5]) was used to generate 200 samples from a NNBD as
shown in Fig. l(a). The high temperature expansion was then used to learn approximate
parameters for the NNBM model of this data. A surface plot of the resulting model distribution is shown in Fig. l(b), it is clearly a valid candidate generative distribution for the
data. This is in strong contrast with a naive mean field ((3 = 0) model, which by construction would be unable to produce a multiple-peaked approximation, as previously described,
[1] .

4

Orientation Tuning in Visual Cortex - a translationally invariant
model

The neural network model of Ben-Yishai et. al [7] for orientation-tuning in visual cortex
has the property that its dynamics exhibit a continuum of stable states which are trans-

(a)

8
~

15

-

6
><.-

>-

'iii
c

4 ~

~10

>-

:c
==co 5
.c

2to
0

0
0

o

oo~

2

Jiil..

4

x2

6

-""""

o

...

0
Q.

8

Figure 1: (a) Training data, generated from 2-dimensional 'competitive' NNBD, (b)
Learned model distribution, under the high temperature expansion.
lationally invariant across the network. The energy function of the network model is a
translationally invariant function of the angles of maximal response, Bi , of the N neurons,
and can be mapped directly onto the energy of the NNBM, as described in [1].

Aii=1'(c5ii + ~-

~COS(~li-jl)),bi=1'

(26)

We can generate training data for the NNBM by sampling from the neural network model
with known parameters. It is easily shown that Aii has 2 equal negative eigenvalues, the
remainder being positive and equal in value. The corresponding pair of eigenvectors of A
are sinusoids of period equal to the width of the stable activation bumps of the network,
with a small relative phase.
Here, the NNBM parameters have been solved using the high-temperature expansion for
training data generated by Monte Carlo slice-sampling [5] from a lO-neuron model with
parameters to = 4, I' = 100 in Eq. 26. Fig. 2 illustrates modal activity patterns of the learned
NNBM model distribution, found using gradient ascent of the log-likelihood function from
a random initialisation of the variables.
~x ex [-Ax + bj+
(27)
where the superscript + denotes rectification.
These modes of the approximate NNBM model are highly similar to the training patterns,
also the eigenvectors and eigenvalues of A exhibit similar properties between their learned
and training forms. This gives evidence that the approximation is successful in learning a
high-dimensional translationally invariant NNBM model.

5 Generative Model for Handwritten Digits
In figure 3, I show the results of applying the high-temperature NNBM to learning a generative model for the feature coactivations of the Nonnegative Matrix Factorization [2]

6
6

~

Q)

rn4

0:

0:

OJ
c::
.;::

OJ
c::
.;::

u:

U:2

0

1 2

4
2

O~

3 4 5 6 7 8 9 10
Neuron Number

1 2

3 4 5 6 7 8 9 10
Neuron Number

(b)

(a)

0.4
Q)

0.2

~

rn

0:

OJ

c::
.;::

0:

0

u: -0.2
2

4
6
8
Neuron Number

10

2

4
6
8
Neuron Number

10

Figure 2: Upper: 2 modal states of the NNBM model density, located by gradient-ascent
of the log-likelihood from different random initialisations, Lower: The two negativeeigenvalue eigenvectors of A - a) in the learned model, and b) as used to generate the
training data.

decomposition of a database of the handwritten digits, 0-9. This problem contains none of
the space-filling symmetry of the visual cortex model, and hence requires a more strongly
multimodal generative model distribution to generate distinct digits. Here performance is
poor, although superior to uniformly-sampled feature activitations.

6 Discussion
In this work, an approximate technique has been derived for directly determining the
NNBM parameters A, b in terms of the Ist- and 2nd-order statistics of the data, using
the method of high-temperature expansion. To second order this produces corrections to
the naive mean field approximation of the system analogous to the TAP term for the Ising
Model/Boltzmann Machine. The efficacy of this approximation has been demonstrated
in the pathological case of learning the 'competitive' NNBD, learning the translationally
invariant model in 10 dimensions, and a generative model for handwritten digits.
These results demonstrate an improvement in approximation to models in this class over
a naive mean field ((3 = 0) approach, without reversion to secondary assumptions such as
those made in the linear response theory for the Boltzmann Machine.
There is strong current interest in the relationship between TAP-like mean field theory,
variational approximation and belief-propagation in graphical models with loops. All of
these can be interpreted in terms of minimising an effective free energy of the system [8].
The distinction in the work presented here lies in choosing optimal approximate statistics
to learn the true model, under the assumption that satisfaction of the fixed-point equations
of the true model optimises the free energy. This compares favourably with variational

a)

b)

Figure 3: Digit images generated with feature activations sampled from a) a uniform distribution, and b) a high-temperature NNBM model for the digits.
approaches which directly optimise an approximate model distribution.
Methods of this type fail when they add spurious fixed points to the learning dynamics.
Future work will focus on understanding the origins of such fixed points, and the regimes
in which they lead to a poor approximation of the model parameters.

7 Acknowledgements
This work was inspired by the NIPS 1999 Workshop on Advanced Mean Field Methods.
The author is especially grateful to David MacKay and Gayle Wittenberg for comments on
early versions of this manuscript. I also acknowledge guidance from John Hopfield and
David Heckerman, detailed discussion with Bert Kappen, Daniel Lee and David Barber
and encouragement from Kim Midwood.

References
[1] Downs, DB, MacKay, DJC, & Lee, DD (2000). The Nonnegative Boltzmann Machine. Advances in Neural Information Processing Systems 12, 428-434.
[2] Lee, DD, and Seung, HS (1999) Learning the parts of objects by non-negative matrix factorization. Nature 401,788-791.
[3] Socci, ND, Lee, DD, and Seung, HS (1998). The rectified Gaussian distribution. Advances in
Neural Information Processing Systems 10, 350-356.
[4] Georges, A, & Yedidia, JS (1991). How to expand around mean-field theory using hightemperature expansions. Journal of Physics A 24, 2173- 2192.
[5] Neal, RM (1997). Markov chain Monte Carlo methods based on 'slicing' the density function.
Technical Report 9722, Dept. of Statistics, University of Toronto.
[6] Kappen, HJ & Rodriguez, FB (1998). Efficient learning in Boltzmann Machines using linear
response theory. Neural Computation 10, 1137-1156.
[7] Ben-Yishai, R, Bar-Or, RL, & Sompolinsky, H (1995). Theory of orientation tuning in visual
cortex. Proc. Nat. Acad. Sci. USA,92(9):3844-3848.
[8] Yedidia, JS , Freeman, WT, & Weiss, Y (2000). Generalized Belief Propagation. Mitsubishi
Electric Research Laboratory Technical Report, TR-2000-26.

"
4849,"Optimal Neural Codes for Control and Estimation

Alex Susemihl1 , Manfred Opper
Methods of Artificial Intelligence
Technische Universit?at Berlin
1
Current affiliation: Google

Ron Meir
Department of Electrical Engineering
Technion - Haifa

Abstract
Agents acting in the natural world aim at selecting appropriate actions based on
noisy and partial sensory observations. Many behaviors leading to decision making and action selection in a closed loop setting are naturally phrased within a
control theoretic framework. Within the framework of optimal Control Theory,
one is usually given a cost function which is minimized by selecting a control
law based on the observations. While in standard control settings the sensors are
assumed fixed, biological systems often gain from the extra flexibility of optimizing the sensors themselves. However, this sensory adaptation is geared towards
control rather than perception, as is often assumed. In this work we show that sensory adaptation for control differs from sensory adaptation for perception, even for
simple control setups. This implies, consistently with recent experimental results,
that when studying sensory adaptation, it is essential to account for the task being
performed.

1

Introduction

Biological systems face the difficult task of devising effective control strategies based on partial information communicated between sensors and actuators across multiple distributed networks. While
the theory of Optimal Control (OC) has become widely used as a framework for studying motor control, the standard framework of OC neglects many essential attributes of biological control [1, 2, 3].
The classic formulation of closed loop OC considers a dynamical system (plant) observed through
sensors which transmit their output to a controller, which in turn selects a control law that drives
actuators to steer the plant. This standard view, however, ignores the fact that sensors, controllers
and actuators are often distributed across multiple sub-systems, and disregards the communication
channels between these sub-systems. While the importance of jointly considering control and communication within a unified framework was already clear to the pioneers of the field of Cybernetics
(e.g., Wiener and Ashby), it is only in recent years that increasing effort is being devoted to the
formulation of a rigorous systems-theoretic framework for control and communication (e.g., [4]).
Since the ultimate objective of an agent is to select appropriate actions, it is clear that sensation and
communication must subserve effective control, and should be gauged by their contribution to action
selection. In fact, given the communication constraints that plague biological systems (and many
current distributed systems, e.g., cellular networks, sensor arrays, power grids, etc.), a major concern
of a control design is the optimization of sensory information gathering and communication (consistently with theories of active perception). For example, recent theoretical work demonstrated a sharp
communication bandwidth threshold below which control (or even stabilization) cannot be achieved
(for a summary of such results see [4]). Moreover, when informational constraints exists within a
control setting, even simple (linear and Gaussian) problems become nonlinear and intractable, as
exemplified in the famous Witsenhausen counter-example [5].
The inter-dependence between sensation, communication and control is often overlooked both in
control theory and in computational neuroscience, where one assumes that the overall solution to
the control problem consists of first estimating the state of the controlled system (without reference
1

to the control task), followed by constructing a controller based on the estimated state. This idea,
referred to as the separation principle in Control Theory, while optimal in certain restricted settings
(e.g., Linear Quadratic Gaussian (LQG) control) is, in general, sub-optimal [6]. Unfortunately, it is
in general very difficult to provide optimal solutions in cases where separation fails. A special case
of the separation principle, referred to as Certainty Equivalence (CE), occurs when the controller
treats the estimated state as the true state, and forms a controller assuming full state information. It
is generally overlooked, however, that although the optimal control policy does not depend directly
on the observation model at hand, the expected future costs do depend on the specifics of that model
[7]. In this sense, even when CE holds, costs still arise from uncertain estimates of the state and one
can optimise the sensory observation model to minimise these costs, leading to sensory adaptation.
At first glance, it might seem that the observation model that will minimise the expected future cost
will be the observation model that minimises the estimation error. We will show, however, that this
is not generally the case.
A great deal of the work in computational neuroscience has dealt independently with the problem
of sensory adaptation and control, while, as stated above, these two issues are part and parcel of
the same problem. In fact, it is becoming increasingly clear that biological sensory adaptation is
task-dependent [8, 9]. For example, [9] demonstrates that task-dependent sensory adaptation takes
place in purely motor tasks, explaining after-effect phenomena seen in experiments. In [10], the
authors show that specific changes occur in sensory regions, implying sensory plasticity in motor
learning. In this work we consider a simple setting for control based on spike time sensory coding,
and study the optimal coding of sensory information required in order to perform a well-defined
motor task. We show that even if CE holds, the optimal encoder strategy, minimising the control cost,
differs from the optimal encoder required for state estimation. This result demonstrates, consistently
with experiments, that neural encoding must be tailored to the task at hand. In other words, when
analyzing sensory neural data, one must pay careful care to the task being performed. Interestingly,
work within the distributed control community dealing with optimal assignment and selection of
sensors, leads to similar conclusions and to specific schemes for sensory adaptation.
The interplay between information theory and optimal control is a central pillar of modern control
theory, and we believe it must be accounted for in the computational neuroscience community.
Though statistical estimation theory has become central in neural coding issues, often through the
Cram?er-Rao bound, there have been few studies bridging the gap between partially observed control
and neural coding. We hope to narrow this gap by presenting a simple example where control
and estimation yield different conclusions. The remainder of the paper is organised as follows:
In section 1.1 we introduce the notation and concepts; In section 2 we derive expressions for the
cost-to-go of a linear-quadratic control system observed through spikes from a dense populations of
neurons; in section 3 we present the results and compare optimal codes for control and estimation
with point-process filtering, Kalman filtering and LQG control; in section 4 we discuss the results
and their implications.
1.1

Optimal Codes for Estimation and Control

We will deal throughout this paper with a dynamic system with state Xt , observed through noisy
sensory observations Zt , whose conditional distribution can be parametrised by a set of parameters
?, e.g., the widths and locations of the tuning curves of a population of neurons or the noise properties of the observation process. The conditional distribution is then given by P? (Zt |Xt = x).
Zt could stand for a diffusion process dependent on Xt (denoted Yt ) or a set of doubly-stochastic
Poisson processes dependent on Xt (denoted Ntm ). In that sense, the optimal Bayesian encoder for
an estimation problem, based on the Mean Squared Error (MSE) criterion, can be written as



2 
??e = argmin E z E Xt Xt ? X?t (Zt ) Zt = z ,
?
? t (Zt ) = E [Xt |Zt ] is the posterior mean, computable, in the linear Gaussian case, by the
where X
Kalman filter. We will throughout this paper consider the MMSE in the equilibrium, that is, the
error in estimating Xt from long sequences of observations Z[0,t] . Similarly, considering a control
problem with a cost given by
Z T
C(X 0 , U 0 ) =
c(Xs , Us , s)ds + cT (XT ),
0

2

where X t = {Xs |s ? [t, T ]}, U t = {Us |s ? [t, T ]}, and so forth. We can define
??c = argmin E z min [E X t [C(X 0 , U 0 )|Z t = z]] .
Ut

?

The certainty equivalence principle states that given a control policy ? ? : X ? U which minimises
the cost C,
? ? = argmin C(X 0 , ?(X 0 )),
?

the optimal control policy for the partially observed problem given by noisy observations Z 0 of X 0
is given by
?CE (Z t ) = ? ? (E [X 0 |Z t ]) .
Note that we have used the notation ?(X 0 ) = {?(Xs ), s ? [0, T ]}.

2

Stochastic Optimal Control

In stochastic optimal control we seek to minimize the expected future cost incurred by a system
with respect to a control variable applied to that system. We will consider linear stochastic systems
governed by the SDE
dXt = (AXt + BUt ) dt + D1/2 dWt ,
(1a)
with a cost given by
Z T

C(X t , U t , t) =
Xs> QXs + Us> RUs ds + XT> QT XT .
(1b)
t

From Bellman?s optimality principle or variational analysis [11], it is well known that the optimal
control is given by Ut? = ?R?1 B > St Xt , where St is the solution of the Riccati equation
?S? t = Q + ASt + St A> ? St B > R?1 BSt ,

(2)

with boundary condition ST = QT . The expected future cost at time t and state x under the optimal
control is then given by
Z T
1 >
J(x, t) = min E [C(X t , U t , t)|Xt = x] = x St x +
Tr (DSs ) ds.
Ut
2
t
This is usually called the optimal cost-to-go. However, the system?s state is not always directly
accessible and we are often left with noisy observations of it. For a class of systems e.g. LQG
control, CE holds and the optimal control policy for the indirectly observed control problem is
simply the optimal control policy for the original control problem applied to the Bayesian estimate of
the system?s state. In that sense, if the CE were to hold for the system above observed through noisy
observations Yt of the state at time t, the optimal control would be given simply by the observationdependent control Ut? = ?R?1 B > St E [Xt |Yt ] [7].
Though CE, when applicable, gives us a simple way to determine the optimal control, when considering neural systems we are often interested in finding the optimal encoder, or the optimal observation model for a given system. That is equivalent to finding the optimal tuning function for a
given neuron model. Since CE neatly separates the estimation and control steps, it would be tempting to assume the optimal codes obtained for an estimation problem would also be optimal for an
associated control problem. We will show here that this is not the case.
As an illustration, let us consider the case of LQG with incomplete state information. One could,
for example, take the observations to be a secondary process Yt , which itself is a solution to
dYt = F Xt dt + G1/2 dVt ,
the optimal cost-to-go would then be given by [11]



J(y, t) = min E C(X t , U t , t)Y[0,t] = y

(3)

Ut

=?t> St ?t + Tr (Kt St ) +

Z

T

Z
Tr (DSs ) ds +

t

t

3

T


Tr Ss BR?1 B > Ss Ks ds,

where we have defined Y[0,t] = {Ys , s ? [0, t]}, ?t = E[Xt |Y[0,t] ] and Kt = cov[Xt |Y[0,t] ]. We
give a demonstration of these results in the SI, but for a thorough review see [11]. Note that through
the last term in equation (3) the cost-to-go now depends on the parameters of the Yt process. More
precisely, the variance of the distribution of Xs given Yt , for s > t obeys the ODE
K? t = AKt + Kt A> + D ? Kt F > G?1 F Kt .

(4)

One could then choose the matrices F and G in such a way as to minimise the contribution of
the rightmost term in equation (3). Note that in the LQG case this is not particularly interesting,
as the conclusion is simply that we should strive to make Kt as small as possible, by making the
term F > G?1 F as large as possible. This translates to choosing an observation process with very
strong steering from the unobserved process (large F ) and a very small noise (small G). One case
that provides some more interesting situations is if we consider a two-dimensional system, where
we are restricted to a noise covariance with constant determinant. That means the hypervolume
spanned by the eigenvectors of the covariance matrix is constant. We will compare this case with
the Poisson-coded case below.
2.1

LQG Control with Dense Gauss-Poisson Codes

Let us now consider the case of the system given by equation (1a), but instead of observing the
system directly we observe a set of doubly-stochastic Poisson processes {Ntm } with rates given by


1
>
?m (x) = ? exp ? (x ? ?m ) P ? (x ? ?m ) .
(5)
2
To clarify, the process Ntm is a counting process which counts how many spikes the neuron m
has fired up to time t. In that sense, the differential of the counting process dNtm will give the
spike train process, a sum of Dirac delta functions placed at the times of spikes fired by neuron
m. Here P ? denotes the pseudo-inverse of P , which is used to allow for tuning functions that
do not depend on certain coordinates of the stimulus x. Furthermore, we will assume that the
tuning centre ?m are such that the probability of observing a spike of any neuron at a given time
? = P ?m (x) is independent of the specific value of the world state x. This can be a consequence
?
m
of either a dense packing of the tuning centres ?m along a given dimension of x, or of an absolute
insensitivity to that aspect of x through a null element in the diagonal of P ? . This is often called
the dense coding hypothesis [12]. It can be readily be shown that the filtering distribution is given
by P (Xt |{N[0,t) }) = N (?t , ?t ), where the mean and covariance are solutions to the stochastic
differential equations (see [13])
X
?1 ?
d?t = (A?t + BUt ) dt +
?t I + P ? ?t
P (?m ? ?t ) dNtm ,
(6a)
m


?1
d?t = A?t + ?t A> + D dt ? ?t P ? ?t I + P ? ?t
dNt ,

(6b)

m
m
where we have defined ?t = E[Xt |{N[0,t]
}] and ?t = cov[Xt |{N[0,t]
}]. Note that we have also
P
m
m
m
defined N[0,t] = {Ns |s ? [0, t]}, the history of the process Ns up to time t, and Nt = m Ntm .
Using Lemma 7.1 from [11] provides a simple connection between the cost function and the solution
of the associated Ricatti equation for a stochastic process. We have
Z T
 >

C(X t , U t , t) = XT> QT XT +
Xs QXs + Us> RUs ds

=Xt> St Xt

Z
+

t
T

(Us + R?1 B > Ss Xs )> R(Us + R?1 B > Ss Xs )ds

t

Z
+

T

Z
Tr(DSs )ds +

t

T

dWs> D>/2 Ss Xs ds

t

Z
+

T

Xs> Ss D1/2 dWs .

t

We can average over P (X t , N t |{N[0,t) }) to obtain the expected future cost. That gives us

""Z
# Z

T
T

>
?1 >
>
?1 >
?t St ?t +Tr(?t St )+E
(Us + R B Ss Xs ) R(Us + R B Ss Xs )ds{N[0,t) } +
Tr(DSs )ds

t
t
4

m
We can evaluate the average over P (X t , {N m
t }|{N[0,t) }) in two steps, by first averaging over the
m
Gaussian densities P (Xs |{N[0,s] }) and then over P ({N[0,s] }|{N[0,t) }). The average gives

Z
E
t

T


h
i 
(Us + R?1 B > Ss ?s )> R(Us + R?1 B > Ss ?s ) + Tr Ss BR?1 B > Ss ?s ({N[0,s] }) ds{N[0,t) } ,

where ?s and ?s are the mean and variance associated with the distribution P (Xs |{N[0,s) }). Note
that choosing Us = ?R?1 B > Ss ?s will minimise the expression above, consistently with CE. The
optimal cost-to-go is therefore given by
J({N[0,t) }, t) =?>
t St ?t + Tr(?t St )
Z
Z T
Tr (DSs ) ds +
+

T



Tr Ss BR?1 B > Ss E ?s ({N[0,s] })|{N[0,t) } ds

t

t

(7)
Note that the only term in the cost-to-go function that depends on the parameters of the encoders is
the rightmost term and it depends on it only through the average over future paths of the filtering
variance ?s . The average of the future covariance matrix is precisely the MMSE for the filtering
problem conditioned on the belief state at time t [13]. We can therefore analyse the quality of an
encoder for a control task by looking at the values of the term on the right for different encoding
parameters. Furthermore,
since the

 dynamics of ?t given by equation (6b) is Markovian, we can
write the average E ?s |{N[0,t) } as E [?s |?t ]. We will define then the function f (?, t) which
gives us the uncertainty-related expected future cost for the control problem as
Z T

f (?, t) =
Tr Ss BR?1 B > Ss E [?s |?t = ?] ds.
(8)
t

2.2

Mutual Information

Many results in information theory are formulated in terms of the mutual information of the communication channel P? (Y |X). For example, the maximum cost reduction achievable with R bits of
information about an unobserved variable X has been shown to be a function of the rate-distortion
function with the cost as the distortion function [14]. More recently there has also been a lot of
interest in the so-called I-MMSE relations, which provide connections between the mutual information of a channel and the minimal mean squared error of the Bayes estimator derived from the
same channel [15, 16]. The mutual information for the cases we are considering is not particularly
complex, as all distributions are Gaussians. Let us denote by ?0t the covariance of of the unobserved
process Xt conditioned on some initial Gaussian distribution P0 = N (?0 , ?0 ) at time 0. We can
then consider the Mutual Information between the stimulus at time t, Xt , and the observations up to
time t, Y[0,t] or N[0,t] . For the LQG/Kalman case we have simply
Z
I(Xt ; Y[0,t] |P0 ) = dx dyP (x, y) [log P (x|y) ? log P (x)] = log |?0t | ? log |?t |,
where ?t is a solution of equation (4). For the Dense Gauss-Poisson code, we can also write
Z


I(Xt ; Nt |P0 ) = dx dn P (x, n) [log P (x|n) ? log P (x)] = log |?0t | ? E N[0,t] log |?t (N[0,t] )| ,
where ?t (N[0,t] ) is a solution to the stochastic differential equation (6b) for the given value of N[0,t] .

3

Optimal Neural Codes for Estimation and Control

What could be the reasons for an optimal code for an estimation problem to be sub-optimal for a
control problem? We present examples that show two possible reasons for different optimal coding
strategies in estimation and control. First, one should note that control problems are often defined
over a finite time horizon. One set of classical experiments involves reaching for a target under
time constraints [3]. If we take the maximal firing rate of the neurons (?) to be constant while
varying the width of the tuning functions, this will lead the number of observed spikes to be inversely
proportional to the precision of those spikes, forcing a trade-off between the number of observations
5

and their quality. This trade-off can be tilted to either side in the case of control depending on the
information available at the start of the problem. If we are given complete information on the system
state at the initial time 0, the encoder needs fewer spikes to reliably estimate the system?s state
throughout the duration of the control experiment, and the optimal encoder will be tilted towards
a lower number of spikes with higher precision. Conversely, if at the beginning of the experiment
we have very little information about the system?s state, reflected in a very broad distribution, the
encoder will be forced towards lower precision spikes with higher frequency. These results are
discussed in section 3.1.
Secondly, one should note that the optimal encoder for estimation does not take into account the
differential weighting of different dimensions of the system?s state. When considering a multidimensional estimation problem, the optimal encoder will generally allocate all its resources equally
between the dimensions of the system?s state. In the framework presented we can think of the dimensions as the singular vectors of the tuning matrix P and the resources allocated to it are the singular
values. In this sense, we will consider a set of coding strategies defined by matrices P of constant
determinant in section 3.2. This constrains the overall firing rate of the population of neurons to be
constant, and we can then consider how the population will best allocate its observations between
these dimensions. Clearly, if we have an anisotropic control problem, which places a higher importance in controlling one dimension, the optimal encoder for the control problem will be expected to
allocate more resources to that dimension. This is indeed shown to be the case for the Poisson codes
considered, as well as for a simple LQG problem when we constrain the noise covariance to have
the same structure.
We do not mean our analysis to be exhaustive as to the factors leading to different optimal codes
in estimation and control settings, as the general problem is intractable, and indeed, is not even
separable. We intend this to be a proof of concept showing two cases in which the analogy between
control and estimation breaks down.
3.1

The Trade-off Between Precision and Frequency of Observations

In this section we consider populations of neurons with tuning functions as given by equation (5)
with tuning centers ?m distributed along a one- dimensional line. In the case of the OrnsteinUhlenbeck process these will be simply one-dimensional values ?m whereas in the case of the
stochastic oscillator, we will consider tuning centres of the form ?m = (?m , 0)> , filling only the
first dimension of the stimulus space. Note
? that in both cases the (dense) population firing rate
? = P ?m (x) will be given by ?
? = 2?p?/|??|, where ?? is the separation between neigh?
m
bouring tuning centres ?m .
The Ornstein-Uhlenbeck (OU) process controlled by a process Ut is given by the SDE
dXt = (bUt ? ?Xt )dt + D1/2 dWt .
Equation (7) can then be solved by simulating the dynamics of ?s . This has been considered extensively in [13] and we refer to the results therein. Specifically, it has been found that the dynamics of
the average can be approximated in a mean-field approach yielding surprisingly good results. The
evolution of the average posterior variance is given by the average of equation (6b), which involves
nonlinear averages over the covariances. These are intractable, but a simple mean-field approach
yields the approximate equation for the evolution of the average h?s i = E [?s |?0 ]

d h?s i
>
? h?s i P ? h?s i I + P ? h?s i ?1 .
= A h?s i + h?s i A> + D ? ?
ds
The alternative is to simulate the stochastic dynamics of ?t for a large number of samples and
compute numerical averages. These results can be directly employed to evaluate the optimal costto-go in the control problem f (?, t).
Alternatively, we can look at a system with more complex dynamics, and we take as an example the
stochastic damped harmonic oscillator given by the system of equations

X? t = Vt , dVt = bUt ? ?Vt ? ? 2 Xt dt + ? 1/2 dWt .
(9)
Furthermore, we assume that the tuning functions only depend on the position of the oscillator,
therefore not giving us any information about the velocity. The controller in turn seeks to keep the
6

0.40
0.35
0.30

0.12

M M SE

M M SE

0.18
0.16
0.14

0.10
0.08

0.25
0.20

0.06

0.15

0.04

0.10

0.02

0.05

0.007

1.35

0.006

1.30
1.25
0 ,t0 )

0.003

1.20

f(

0 ,0)

f(

0.005
0.004

b)

1.10

1.15

0.002

1.05

0.001

1.00
0.95
0.0

0.000
0

1

2

3

4

5

0.2

0.4

0.6
p

0.8

1.0

1.2

Figure 1: The trade-off between the precision and the frequency of spikes is illustrated for the OU process
(a) and the stochastic oscillator (b). In both figures, the initial condition has a very uncertain estimate of the
system?s state, biasing the optimal tuning width towards higher values. This forces the encoder to amass the
maximum number of observations within the duration of the control experiment. Parameters for figure (a) were:
T = 2, ? = 1.0, ? = 0.6, b = 0.2, ? = 0.1, ?? = 0.05, Q = 0.1, QT = 0.001, R = 0.1. Parameters for
figure (b) were T = 5, ? = 0.4, ? = 0.8, ? = 0.4, r = 0.4, q = 0.4, QT = 0, ? = 0.5, ?? = 0.1.

oscillator close to the origin while steering only the velocity. This can be achieved by the choice of
matrices A = (0, 1; ?? 2 , ??), B = (0, 0; 0, b), D = (0, 0; 0, ? 2 ), R = (0, 0; 0, r), Q = (q, 0; 0, 0)
and P = (p2 , 0; 0, 0).
In figure 1 we provide the uncertainty-dependent costs for LQG control, for the Poisson observed
control, as well as the MMSE for the Poisson filtering problem and for a Kalman-Bucy filter with the
same noise covariance matrix P . This illustrates nicely the difference between Kalman filtering and
the Gauss-Poisson filtering considered here. The Kalman filter MSE has a simple, monotonically
increasing dependence on the noise covariance, and one should simply strive to design sensors with
the highest possible precision (p = 0) to minimise the MMSE and control costs. The Poisson
case leads to optimal performance at a non-zero value of p. Importantly the optimal values of p
for estimation and control differ. Furthermore, in view of section 2.2, we also plotted the mutual
information between the process Xt and the observation process Nt , to illustrate that informationbased arguments would lead to the same optimal encoder as MMSE-based arguments.
3.2

Allocating Observation Resources in Anisotropic Control Problems

A second factor that could lead to different optimal encoders in estimation and control is the structure of the cost function C. Specifically, if the cost functions depends more strongly on a certain
coordinate of the system?s state, uncertainty in that particular coordinate will have a higher impact
on expected future costs than uncertainty in other coordinates. We will here consider two simple
linear control systems observed by a population of neurons restricted to a certain firing rate. This
can be thought of as a metabolic constraint, since the regeneration of membrane potential necessary
for action potential generation is one of the most significant metabolic expenditures for neurons
[17]. This will lead to a trade-off, where an increase in precision in one coordinate will result in a
decrease in precision in the other coordinate.
We consider a population of neurons whose tuning functions cover a two-dimensional space. Taking
a two-dimensional isotropic OU system with state Xt = (X1,t , X2,t )> where both dimensions are
uncoupled, we can consider a population with tuning centres ?m = (?1m , ?2m )> densely covering
the stimulus space. To consider a smoother class of stochastic systems we will also consider a
two-dimensional stochastic oscillator with state Xt = (X1,t , V1,t , X2,t , V2,t )> , where again, both
dimensions are uncoupled, and the tuning centres of the form ?m = (?1m , 0, ?2m , 0)> , covering
densely the position space, but not the velocity space.
Since we are interested in the case of limited resources, we will restrict ourselves to populations with a tuning matrix P yielding a constant population firing rate. We can parametrise
these simply as POU (?) = p2 Diag(tan(?), cotan(?)), for the OU case and POsc (?) =
7

estimation
kalman filter
mean field
stochastic
LQG control

0.80

1.2

0.75

1.0
0.8

0.70

0.6

0.65

0.4

0.60

0.2

0.300
0.295
0.290
0.285
0.280
0.275
0.270
0.265
0.260
0.255
0.0

0.15
0.14
0.13
0.12
0.11
0.10
0.09
0.08
0.07
0.06
0.0

f (?0 , t0 )

f (?0 , t0 )

MMSE

0.85

0.2

0.4

0.6

0.8
?

1.0

1.2

1.4

Poisson MMSE
Kalman MMSE
Mean Field f
Stochastic f
LQG f

1.4

MMSE

0.90

1.6

b)

0.2

0.4

0.6

0.8
?

1.0

1.2

1.4

1.6

Figure 2: The differential allocation of resources in control and estimation for the OU process (left) and the
stochastic oscillator (right). Even though the estimation MMSE leads to a symmetric optimal encoder both in
the Poisson and in the Kalman filtering problem, the optimal encoders for the control problem are asymmetric,
allocating more resources to the first coordinate of the stimulus.

p2 Diag(tan(?), 0, cotan(?), 0) for the stochastic oscillator, where ? ? (0, ?/2). Note that this
? = 2?p?/(??)2 , independent of the specifics of the matrix P .
will yield the firing rate ?
We can then compare the performance of all observers with the same firing rate in both control and
estimation tasks. As mentioned, we are interested in control problems where the cost functions are
anisotropic, that is, one dimension of the system?s state vector contributes more heavily to the cost
function. To study this case we consider cost functions of the type
2
2
2
2
c(Xt , Ut ) = Q1 X1,t
+ Q2 X2,t
+ R1 U1,t
+ R2 U2,t
.

This again, can be readily cast into the formalism introduced above, with a suitable choice of matrices Q and R for both the OU process as for the stochastic oscillator. We will also consider the case
where the first dimension of Xt contributes more strongly to the state costs (i.e., Q1 > Q2 ).
The filtering error can be obtained from the formalism developed in [13] in the case of Poisson
observations and directly from the Kalman-Bucy equations in the case of Kalman filtering [18]. For
LQG control, one can simply solve the control problem for the system mentioned using the standard
methods (see e.g. [11]). The Poisson-coded version of the control problem can be solved using
either direct simulation of the dynamics of ?s or by a mean-field approach which has been shown
to yield excellent results for the system at hand. These results are summarised in figure 2, with
similar notation to that in figure 1. Note the extreme example of the stochastic oscillator, where the
optimal encoder is concentrating all the resources in one dimension, essentially ignoring the second
dimension.

4

Conclusion and Discussion

We have here shown that the optimal encoding strategies for a partially observed control problem is
not the same as the optimal encoding strategy for the associated state estimation problem. Note that
this is a natural consequence of considering noise covariances with a constant determinant in the
case of Kalman filtering and LQG control, but it is by no means trivial in the case of Poisson-coded
processes. For a class of stochastic processes for which the certainty equivalence principle holds we
have provided an exact expression for the optimal cost-to-go and have shown that minimising this
cost provides us with an encoder that in fact minimises the incurred cost in the control problem.
Optimality arguments are central to many parts of computational neuroscience, but it seems that
partial observability and the importance of combining adaptive state estimation and control have
rarely been considered in this literature, although supported by recent experiments. We believe the
present work, while treating only a small subset of the formalisms used in neuroscience, provides a
first insight into the differences between estimation and control. Much emphasis has been placed on
tracing the parallels between the two (see [19, 20], for example), but one must not forget to take into
account the differences as well.
8

References
[1] Jun Izawa and Reza Shadmehr. On-line processing of uncertain information in visuomotor
control. The Journal of neuroscience : the official journal of the Society for Neuroscience,
28(44):11360?8, October 2008.
[2] Emanuel Todorov and Michael I Jordan. Optimal feedback control as a theory of motor coordination. Nature neuroscience, 5(11):1226?35, November 2002.
[3] Peter W Battaglia and Paul R Schrater. Humans trade off viewing time and movement duration
to improve visuomotor accuracy in a fast reaching task. The Journal of neuroscience : the
official journal of the Society for Neuroscience, 27(26):6984?94, June 2007.
[4] Boris Rostislavovich Andrievsky, Aleksei Serafimovich Matveev, and Aleksandr L?vovich
Fradkov. Control and estimation under information constraints: Toward a unified theory of
control, computation and communications. Automation and Remote Control, 71(4):572?633,
2010.
[5] Hans S Witsenhausen. A counterexample in stochastic optimum control. SIAM Journal on
Control, 6(1):131?147, 1968.
[6] Edison Tse and Yaakov Bar-Shalom. An actively adaptive control for linear systems with
random parameters via the dual control approach. Automatic Control, IEEE Transactions on,
18(2):109?117, 1973.
[7] Yaakov Bar-Shalom and Edison Tse. Dual Effect, Certainty Equivalence, and Separation in
Stochastic Control. IEEE Transactions on Automatic Control, (5), 1974.
[8] D. Huber, D. A. Gutnisky, S. Peron, D. H. O?Connor, J. S. Wiegert, L. Tian, T. G. Oertner,
L. L. Looger, and K. Svoboda. Multiple dynamic representations in the motor cortex during
sensorimotor learning. Nature, 484(7395):473?478, Apr 2012. n2123 (unprinted).
[9] AA Mattar, Mohammad Darainy, David J Ostry, et al. Motor learning and its sensory effects:
time course of perceptual change and its presence with gradual introduction of load. J Neurophysiol, 109(3):782?791, 2013.
[10] S. Vahdat, M. Darainy, T.E. Milner, and D.J. Ostry. Functionally specific changes in restingstate sensorimotor networks after motor learning. J Neurosci, 31(47):16907?16915, 2011.
? om. Introdution to Stochastic Control Theory. Courier Dover Publications, Mine[11] Karl J. Astr?
ola, NY, 1st edition, 2006.
[12] Steve Yaeli and Ron Meir. Error-based analysis of optimal tuning functions explains phenomena observed in sensory neurons. Frontiers in computational neuroscience, 4(October):16,
2010.
[13] Alex Susemihl, Ron Meir, and Manfred Opper. Dynamic state estimation based on Poisson
spike trains - towards a theory of optimal encoding. Journal of Statistical Mechanics: Theory
and Experiment, 2013(03):P03009, March 2013.
[14] Fumio Kanaya and Kenji Nakagawa. On the practical implication of mutual information for
statistical decisionmaking. IEEE transactions on information theory, 37(4):1151?1156, 1991.
[15] N Merhav. Optimum estimation via gradients of partition functions and information measures:
a statistical-mechanical perspective. Information Theory, IEEE Transactions on, 57(6):3887?
3898, 2011.
[16] Dongning Guo, Shlomo Shamai, and Sergio Verd?u. Mutual information and minimum meansquare error in gaussian channels. Information Theory, IEEE Transactions on, 51(4):1261?
1282, 2005.
[17] David Attwell and Simon B Laughlin. An energy budget for signaling in the grey matter of the
brain. Journal of Cerebral Blood Flow & Metabolism, 21(10):1133?1145, 2001.
[18] R. S. Bucy. Nonlinear filtering theory. Automatic Control, IEEE Transactions, 10(2):198,
1965.
[19] Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. Journal of
basic Engineering, 82(1):35?45, 1960.
[20] Emanuel Todorov. General duality between optimal control and estimation. 2008 47th IEEE
Conference on Decision and Control, (5):4286?4292, 2008.

9

"
3477,"Large Margin Learning of Upstream
Scene Understanding Models

?

Jun Zhu?
Li-Jia Li?
Fei-Fei Li?
Eric P. Xing?
?
{junzhu,epxing}@cs.cmu.edu
{lijiali,feifeili}@cs.stanford.edu
?
School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213
?
Department of Computer Science, Stanford University, Stanford, CA 94305

Abstract
Upstream supervised topic models have been widely used for complicated
scene understanding. However, existing maximum likelihood estimation (MLE)
schemes can make the prediction model learning independent of latent topic discovery and result in an imbalanced prediction rule for scene classification. This
paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization
problem is efficiently solved with a variational EM procedure, which iteratively
solves an online loss-augmented SVM. We demonstrate the advantages of the
large-margin approach on both an 8-category sports dataset and the 67-class MIT
indoor scene dataset for scene categorization.

1 Introduction
Probabilistic topic models like the latent Dirichlet allocation (LDA) [5] have recently been applied
to a number of computer vision tasks such as objection annotation and scene classification due
to their ability to capture latent semantic compositions of natural images [22, 23, 9, 13]. One of
the advocated advantages of such models is that they do not require ?supervision? during training,
which is arguably preferred over supervised learning that would necessitate extra cost. But with the
increasing availability of free on-line information such as image tags, user ratings, etc., various forms
of ?side-information? that can potentially offer ?free? supervision have led to a need for new models
and training schemes that can make effective use of such information to achieve better results, such
as more discriminative topic representations of image contents, and more accurate image classifiers.
The standard unsupervised LDA ignores the commonly available supervision information, and thus
can discover a sub-optimal topic representation for prediction tasks. Extensions to supervised topic
models which can explore side information for discovering predictive topic representations have
been proposed, such as the sLDA [4, 25] and MedLDA [27]. A common characteristic of these
models is that they are downstream, that is, the supervised response variables are generated from
topic assignment variables. Another type of supervised topic models are the so-called upstream
models, of which the response variables directly or indirectly generate latent topic variables. In
contrast to downstream supervised topic models (dSTM), which are mainly designed by machine
learning researchers, upstream supervised topic models (uSTM) are well-motivated from human
vision and psychology research [18, 10] and have been widely used for scene understanding tasks.
For example, in the recently developed scene understanding models [23, 13, 14, 8], complex scene
images are modeled as a hierarchy of semantic concepts where the most top level corresponds to a
scene, which can be represented as a set of latent objects likely to be found in a given scene. To
learn an upstream scene model, maximum likelihood estimation (MLE) is the most common choice.
However, MLE can make the prediction model estimation independent of latent topic discovery and
result in an imbalanced prediction rule for scene classification, as we explain in Section 3.
1

In this paper, our goal is to address the weakness of MLE for learning upstream supervised topic
models. Our approach is based on the max-margin principle for supervised learning which has
shown great promise in many machine learning tasks, such as classification [21] and structured output prediction [24]. For the dSTM, max-margin training has been developed in MedLDA [27], which
has achieved better prediction performance than MLE. In such downstream models, latent topic assignments are sufficient statistics for the prediction model and it is easy to define the max-margin
constraints based on existing max-margin methods (e.g., SVM). However, for upstream supervised
topic models, the discriminant function for prediction involves an intractable computation of posterior distributions, which makes the max-margin training more delicate.
Specifically, we present a joint max-margin and max-likelihood estimation method for learning upstream scene understanding models. By using a variational approximation to the posterior distribution of supervised variables (e.g., scene categories), our max-margin learning approach iterates
between posterior probabilistic inference and max-margin parameter learning. The parameter learning solves an online loss-augmented SVM, which closely couples the prediction model estimation
and latent topic discovery, and this close interplay results in a well-balanced prediction rule for scene
categorization. Finally, we demonstrate the advantages of our max-margin approach on both the 8category sports [13] and the 67-class MIT indoor scene [20] datasets. Empirical results show that
max-margin learning can significantly improve the scene classification accuracy.
The paper is structured as follows. Sec. 2 presents a generic scene understanding model we will
work on. Sec. 3 discusses the weakness of MLE in learning upstream models. Sec. 4 presents the
max-margin learning approach. Sec. 5 presents empirical results and Sec. 6 concludes.

2

Joint Scene and Object Model: a Generic Running Example

In this section, we present a generic joint scene categorization and object annotation model, which
will be used to demonstrate the large margin learning of upstream scene understanding models.
2.1

Image Representation

How should we represent a scene image? Friedman [10] pointed out that object recognition is
critical in the recognition of a scene. While individual objects contribute to the recognition of visual
scenes, human vision researchers Navon [18] and Biederman [2] also showed that people perform
rapid global scene analysis before conducting more detailed local object analysis when recognizing
scene images. To obtain a generic model, we represent a scene by using its global scene features
and objects within it. We first segment an image I into a set of local regions {r1 , ? ? ? , rN }. Each
region is represented by three region features R (i.e., color, location and texture) and a set of image
patches X. These region features are represented as visual codewords. To describe detailed local
information of objects, we partition each region into patches. For each patch, we extract the SIFT
[16] features, which are insensitive to view-point and illumination changes. To model the global
scene representation, we extract a set of global features G [19]. In our dataset, we represent an
image as a tuple (r, x, g), where r denotes an instance of R, and likewise for x and g.
2.2

The Joint Scene and Object Model

The model is shown in Fig. 1 (a). S is the scene random variable, taking values from a finite set
S = {s1 , ? ? ? , sMs }. For an image, the distribution over scene categories depends on its global
representation features G. Each scene is represented as a mixture over latent objects O and the
mixing weights are defined with a generalized linear model (GLM) parameterized by ?. By using
a normal prior on ?, the scene model can capture the mutual correlations between different objects,
similar to the correlated topic models (CTMs) [3]. Here, we assume that for different scenes, the
objects have different distributions and correlations. Let f denote the vector of real-valued feature
functions of S and G, the generating procedure of an image is as follows:
1. Sample a scene category from a conditional scene model: p(s|g, ?) =
2. Sample the parameters ?|s, ?, ? ? N (?s , ?s ).
3. For each region n
k)
(a) sample an object from: p(on = k|?) = ?exp(?
.
exp(?j )

?

exp(? ? f (g,s)
.
exp(? ? f (g,s? ))

s?

j

(b) sample Mr (i.e., 3: color, location and texture) region features: rnm |on , ? ? Multi(?mon ).
(c) sample Mx image patches xnm |on , ? ? Multi(?on ).

2

Log?likelihood Ratio

R
S

Mr

O
X

Mx

G

N

x 10

?3

MLE
8

6

6

4

4

2

2

0

0

x 10

Max?Margin

0.8
0.75

Scene Classification Accuracy

?3

8

Ms

0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35

D

1 2 3 4 5 6 7 8

0.3

1 2 3 4 5 6 7 8

1
MLE

2
Max?Margin

(a)
(b)
(c)
Figure 1: (a) a joint scene categorization and object annotation model with global features G; (b) average
log-likelihood ratio log p(s|g, ?)/L?? under MLE and max-margin estimations, where the first bar is for true
categories and the rest are for categories sorted based on their difference from the first one; (c) scene classification accuracy by using (Blue) L?? , (Green) log p(s|g, ?), and (Red) L?? + log p(s|g, ?) for prediction. Group
1 is for MLE and group 2 is for max-margin training.

The generative model defines a joint distribution
p(s, ?, o, r, x|g, ?) = p(s|?, g)p(?|?s , ?s )

Mr
Mx
N
?
?
?
(
)
p(on |?)
p(rnm |on , ?)
p(xnm |on , ?) ,

n=1

m=1

m=1

where we have used ? to denote all the unknown parameters (?, ?, ?, ?, ?). From the joint distribution, we can make two types of predictions, namely scene classification and object annotation. For
scene classification, we infer the maximum a posteriori prediction
s? , arg max p(s|g, r, x) = arg max log p(s, r, x|g).
s

s

(1)

For object annotation, we can use the inferred latent representation of regions based on p(o|g, r, x)
and build a classifier to categorize regions into object classes, when some training examples with
manually annotated objects are provided. Since collecting fully labeled images with annotated objects is difficult, upstream scene models are usually learned with partially labeled images for scene
categorization, where only scene categories are provided and objects are treated as latent topics
or themes [9]. In this paper, we focus on scene classification. Some empirical results on object
annotation will be reported when labeled objects are available.
We use this joint model as a running example to demonstrate the basic principe of performing maxmargin learning for the widely applied upstream scene understanding models because it is wellmotivated, very generic and covers many other existing scene understanding models. For example,
if we do not incorporate the global scene representation G, the joint model will be reduced to a
model similar as [14, 6, 23]. Moreover, the generic joint model provides a good framework for
studying the relative contributions of local object modeling and global scene representation, which
has been shown to be useful for scene classification [20] and object detection [17] tasks.

3

Weak Coupling of MLE in Learning Upstream Scene Models

To learn an upstream scene model, the most commonly used method is the maximum likelihood
estimation (MLE), such as in [23, 6, 14]. In this section, we discuss the weakness of MLE for
learning upstream scene models and motivate the max-margin approach.
Let D = {(Id , sd )}D
The standard MLE obtains
d=1 denote a set of partially labeled training images.
?D
the optimum model parameters by maximizing the log-likelihood1 d=1 log p(sd , rd , xd |gd , ?).
By using the factorization of p(s, ?, o,?
r, x|g, ?), MLE solves the following equivalent problem
max

? ?

?,???

d

(

)
log p(sd |gd , ?) + Lsd ,?? ,

(2)

where Lsd ,?? , log ? o p(?, o, rd , xd |sd , ?) = log p(rd , xd |sd , ?) is the log-likelihood of image features given the scene class, and ??? denotes all the parameters except ?.
Since Ls,?? does not depend on ?, the MLE estimation of the conditional scene model is to solve
max
?

?
d

log p(sd |gd , ?),

(3)

which does not depend on the latent object model. This is inconsistent with the prediction rule (1)
which does depend on both the conditional scene model (i.e., p(s|g, ?)) and the local object model.
1
The conditional likelihood estimation can avoid this problem to some extend, but it has not been studied,
to the best of our knowledge.

3

This decoupling will result in an imbalanced combination between the conditional scene and object
models for prediction, as we explain below.
We first present some details of the MLE method. For ?, the problem (3) is an MLE estimation of a
GLM, and it can be efficiently solved with gradient descent methods, such as quasi-Newton methods
[15]. For ??? , since the likelihood Ls,?? is intractable to compute, we apply variational methods
to obtain an approximation. By introducing a variational distribution qs (?, o) to approximate the
posterior p(?, o|s, r, x, ?) and using the Jensen?s inequality, we can derive a lower bound
Ls,?? ? Eqs [log p(?, o, r, x|s, ?)] + H(qs ) , L?? (qs , ?),

(4)

where H(q) = ?Eq [q] is the entropy. Then, the intractable prediction rule (1) can be approximated
with the variational prediction rule
(
)
s? , arg max log p(s|g, ?) + L?? (qs , ?) .
s,qs

?

(5)

Maximizing d L?? (qsd , ?) will lead to a closed form solution of ??? . See Appendix for the
inference of qs as involved in the prediction rule (5) and the estimation of ??? .

Now, we examine the effects of the conditional scene model p(s|g, ?) in making a prediction via the
prediction rule (5). Fig. 1 (b-left) shows the relative importance of log p(s|g, ?) in the joint decision
rule (5) on the sports dataset [13]. We can see that in MLE the conditional scene model plays a very
weak role in making a prediction when it is combined with the object model, i.e., L?? . Therefore,
as shown in Fig. 1 (c), although a simple logistic regression with global features (i.e., the green bar)
can achieve a good accuracy, the accuracy of the prediction rule (5) that uses the joint likelihood
bound (i.e, the red bar) is decreased due to the strong effect of the potentially bad prediction rule
based on L?? (i.e., the blue bar), which only considers local image features.

In contrast, as shown in Fig. 1 (b-right), in the max-margin approach to be presented, the conditional
scene model plays a much more influential role in making a prediction via the rule (5). This results in
a better balanced combination between the scene and the object models. The strong coupling is due
to solving an online loss-augmented SVM, as we explain below. Note that we are not claiming any
weakness of MLE in general. All our discussions are concentrated on learning upstream supervised
topic models, as generically represented by the model in Fig. 1.

4

Max-Margin Training

Now, we present the max-margin method for learning upstream scene understanding models.
4.1

Problem Definition

For the predictive rule (1), we use F (s, g, r, x; ?) , log p(s|g, r, x, ?) to denote the discriminant
function, which is more complicated than the commonly chosen linear form, in the sense we will
explain shortly. In the same spirit of max-margin classifiers (e.g., SVMs), we define the hinge loss
of the prediction rule (1) on D as
Rhinge (?) =

1 ?
max[??d (s) ? ?Fd (s; ?)],
s
D
d

where ??d (s) is a loss function (e.g., 0/1 loss), and ?Fd (s; ?) = F (sd , gd , rd , xd ; ?) ?
F (s, gd , rd , xd ; ?) is the margin favored by the true category sd over any other category s.
The problem with the above definition is that exactly computing the posterior distribution
p(s|g, r, x, ?) is intractable. As in MLE, we use a variational distribution qs to approximate it. By
using the Bayes?s rule and the variational bound in Eq. (4), we can lower bound the log-likelihood
log p(s|g, r, x, ?) = log p(s, r, x|g, ?) ? log p(r, x|g, ?) ? log p(s|g, ?) + L?? (qs , ?) ? c,

(6)

where c = log p(r, x|g, ?). Without causing ambiguity, we will use L?? (qs ) without ?. Since we
need to make some assumptions about qs , the equality in (6) usually does not hold. Therefore, the
tightest lower bound is an approximation of the intractable discriminant function
F (s, g, r, x; ?) ? log p(s|g, ?) + max L?? (qs ) ? c.
qs

(7)

Then, the margin is ?Fd (s; ?) = ?? ?fd (s) + maxqsd L?? (qsd ) ? maxqs L?? (qs ), of which the
linear term is the same as that in a linear SVM [7] and the difference between two variational bounds
causes the topic discovery to bias the learning of the scene classification model, as we shall see.
4

Using the variational discriminant function in Eq. (7) and applying the principle of regularized
empirical risk minimization, we define the max-margin learning of the joint scene and object model
as solving
?
min ?(?) + ?
?

d

(? max L?? (qsd )) + CRhinge (?),
qsd

(8)

where ?(?) is a regularizer of the parameters. Here, we define ?(?) , 21 ???22 . For the normal mean
?s or covariance matrix ?s , a similar ?2 -norm or Frobenius norm can be used without changing our
algorithm. The free parameters ? and C are positive and tradeoff the classification loss and the data
likelihood. When ? ? ?, the problem (8) reduces to the standard MLE of the joint scene model
with a fixed uniform prior on scene classes. Moreover, we can see the difference from the standard
MLE (2). Here, we minimize a hinge loss, which is defined on the joint prediction rule, while MLE
minimizes the log-likelihood loss log p(sd |gd , ?), which does not depend on the latent object model.
Therefore, our approach can be expected to achieve a closer dependence between the conditional
scene model and the latent object model. More insights will be provided in the next section.
4.2

Solving the Optimization Problem

The problem (8) is generally hard to solve because the model parameters and variational distributions are strongly coupled. Therefore, we develop a natural iterative procedure that estimates the
parameters ? and performs posterior inference alternatively. The intuition is that by fixing one part
(e.g., qs ) the other part (e.g., ?) can be efficiently done. Specifically, using the definitions, we
rewrite the problem (8) as a min-max optimization problem
min

max

?,{qsd } {s,qs }

(1
2

???22 ? (? + C)

?
d

L?? (qsd ) + C

)
?
[??? ?fd (s) + ??d (s) + L?? (qs )] ,

(9)

d

where the factor 1/D in Rhinge is absorbed in the constant C. This min-max problem can be
approximately solved with an iterative procedure. First, we infer the optimal variational posterior2
qs? = arg maxqs L?? (qs ) for each s and each training image. Then, we solve
min

?,{qsd }

(1
2

???22 ? (? + C)

?
d

L?? (qsd ) + C

?
d

)
max[??? ?fd (s) + ??d (s) + L?? (qs? )] ,
s

For this sub-step, again, we apply an alterative procedure to solve the minimization problem over ?
and qsd . We first infer the optimal variational posterior qs?d = arg maxqsd L?? (qsd ), and then we
estimate the parameters by solving the following problem
min
?

(1
2

???22 ? (? + C)

?
d

L?? (qs?d ) + C

?
d

)
max[??? ?fd (s) + ??d (s) + L?? (qs? )] ,
s

(10)

Since inferring qs?d is included in the step of inferring qs? (?s), the algorithm can be summarized
as a two-step EM-procedure that iteratively performs posterior inference of qs and max-margin
parameter estimation. Another way to understand this iterative procedure is from the definitions.
The first step of inferring qs? is to compute the discriminant function F under the current model.
Then, we update the model parameters ? by solving a large-margin learning problem. For brevity,
we present the parameter estimation only. The posterior inference is detailed in Appendix A.1.
Parameter Estimation: This step can be done with an alternating minimization procedure. For
the Gaussian parameters (?, ?) and multinomial parameters (?, ?), the estimation can be written
in a closed-form as in a standard MLE of CTMs [3] by using a loss-augmented prediction of s.
For brevity, we defer the details to the Appendix A.2. Now, we present the step of estimating ?,
which illustrates the essential difference between the large-margin approach and the standard MLE.
Specifically, the optimum solution of ? is obtained by solving the sub-problem3
min
?

?(
)
1
???22 + C
max[?? f (gd , s) + ??d (s) + L?? (qs? )] ? [?? f (gd , sd ) + L?? (qs?d )] ,
s
2
d

which is equivalent to a constrained problem by introducing a set of non-negative slack variables ?
D

min
?,?

?
1
???22 + C
?d s.t.: ?? ?fd (s) + [L?? (qs?d ) ? L?? (qs? )] ? ??d (s) ? ?d , ?d, s.
2

(11)

d=1

2

To retain an accurate large-margin criterion for estimating model parameters (especially ?), we do not
perform the maximization over s at this
? step.
3
The constant (w.r.t. ?) term ?C d L?? (qs?d ) is kept for easy explanation. It won?t change the estimation.

5

The constrained optimization problem is similar to that of a linear SVM [7]. However, the difference
is that we have the additional term
?L?d (s) , L?? (qs?d ) ? L?? (qs? ).

This term indicates that the estimation of the scene classification model is influenced by the topic
discovery procedure, which finds an optimum posterior distribution q ? . If ?L?d (s) < 0, s ?= sd ,
which means it is very likely that a wrong scene s explains the image content better than the true
scene sd , then the term ?L?d (s) acts in a role of augmenting the linear decision boundary ? to make
a correct prediction on this image by using the prediction rule (5). If ?L?d (s) > 0, which means
the true scene can explain the image content better than s, then the linear decision boundary can be
slightly relaxed. If we move the additional term to the right hand side, the problem (11) is to learn
a linear SVM, but with an online updated loss function ??d (s) ? ?L?d (s). We call this SVM an
online loss-augmented SVM. Solving the loss-augmented SVM will result in an amplified influence
of the scene classification model in the joint predictive rule (5) as shown in Fig. 1 (b).

5

Experiments

Now, we present empirical evaluation of our approach on the sports [13] and MIT indoor scene [20]
datasets. Our goal is to demonstrate the advantages of the max-margin method over the MLE for
learning upstream scene models with or without global features. Although the model in Fig. 1 can
also be used for object annotation, we report the performance on scene categorization only, which
is our main focus in this paper. For object annotation, which requires additional human annotated
examples of objects, some preliminary results are reported in the Appendix due to space limitation.
5.1

Datasets and Features

The sports data contain 1574 diverse scene images from 8 categories, as listed in Fig. 2 with example
images. The indoor scene dataset [20] contains 15620 scene images from 67 categories as listed in
Table 2. We use the method [1] to segment these images into small regions based on color, brightness and texture homogeneity. For each region, we extract color, texture and location features, and
quantize them into 30, 50 and 120 codewords, respectively. Similarly, the SIFT features extracted
from the small patches within each region are quantized into 300 SIFT codewords. We use the gist
features [19] as one example of global features. Extension to include other global features, such as
SIFT sparse codes [26], can be directly done without changing the model or the algorithm.
5.2

Models

For the upstream scene model as in Fig. 1, we compare the max-margin learning with the MLE
method, and we denote the scene models trained with max-margin training and MLE by MM-Scene
and MLE-Scene, respectively. For both methods, we evaluate the effectiveness of global features,
and we denote the scene models without global features by MM-Scene-NG and MLE-Scene-NG,
respectively. Since our main goal in this paper is to demonstrate the advantages of max-margin
learning in upstream supervised topic models, rather than dominance of such models over all others,
we just compare with one example of downstream models?the multi-class sLDA (Multi-sLDA)
[25]. Systematical comparison with other methods, including DiscLDA [12] and MedLDA [27], is
deferred to a full version. For the downstream Multi-sLDA, the image-wise scene category variable
S is generated from latent object variables O via a softmax function. For this downstream model,
the parameter estimation can be done with MLE as detailed in [25].
Finally, to show the usefulness of the object model in scene categorization, we also compare with the
margin-based multi-class SVM [7] and likelihood-based logistic regression for scene classification
based on the global features. For the SVM, we use the software SVMmulticlass 4 , which implements
a fast cutting-plane algorithm [11] to do parameter learning. We use the same software with slight
changes to learn the loss-augmented SVM in our max-margin method.
5.3

Scene Categorization on the 8-Class Sports Dataset

We partition the dataset equally into training and testing data. For all the models except SVM and
logistic regression, we run 5 times with random initialization of the topic parameters (e.g., ? and ?).
4

http://svmlight.joachims.org/svm multiclass.html

6

badminton

badminton

croquet

rockclimbing

rockclimbing snowboarding

bocce

bocce

croquet

rockclimbing

croquet

polo

polo

badminton

sailing

sailing

rowing

snowboarding

bocce

rowing

rowing

polo

sailing

snowboarding

Figure 2: Example images from each category in the sports dataset with predicted scene classes, where the

predictions in blue are correct while red ones are wrong predictions.
0.75

Scene Classification Accuracy

The average overall accuracy of scene categorization
0.7
on 8 categories and its standard deviation are shown in
0.65
0.6
Fig. 3. The result of logistic regression is shown in the
0.55
left green bar in Fig. 1 (c). We also show the confusion
0.5
matrix of the max-margin scene model with 100 latent
0.45
topics in Table 1, and example images from each cat0.4
egory are shown in Fig. 2 with predicted labels. Over0.35
all, the max-margin scene model with global features
0.3
achieves significant improvements as compared to all
0.25
10
20
30
40
50
60
70
80
90
100
other approaches we have tested. Interestingly, al# Topics
though we provide only scene categories as supervised
Figure 3: Classification accuracy of different
information during training, our best performance with models with respect to the number of topics.
global features is close to that reported in [13], where
additional supervision of objects is used. The outstanding performance of the max-margin method
for scene classification can be understood from the following aspects.
MM?Scene
MM?Scene?NG
MLE?Scene
MLE?Scene?NG
Multi?sLDA
Multi?SVM

Max-margin training: from the comparison of the max-margin approach with the standard MLE
in both cases of using global features and not using global features, we can see that the max-margin
learning can improve the performance dramatically, especially when the scene model uses global
features (about 3 percent). This is due to the well-balanced prediction rule achieved by the maxmargin method, as we have explained in Section 3.
Global features: from the comparison between the scene models with and without global features,
we can see that using the gist features can significantly (about 8 percent) improve the scene categorization accuracy in both MLE and max-margin training. We also did some preliminary experiments
on the SIFT sparse codes feature [26], which are a bit more expensive to extract. By using both gist
and sparse codes features, we can achieve dramatic improvements in both max-margin and MLE
methods. Specifically, the max-margin scene model achieves an accuracy of about 0.83 in scene
classification, and the likelihood-based model obtains an accuracy of about 0.80.
Object modeling: the superior performance of the max-margin learned MM-scene model comparing
to the SVM and logistic regression (See the left green bar of Fig. 1 (c)), which use global features
only, indicates that modeling objects can facilitate scene categorization. This is because the scene
classification model is influenced by the latent object modeling through the term ?L?d (s), which can
improve the decision boundary of a standard linear SVM for those images that have negative scores
of ?L?d (s), as we have discussed in the online loss-augmented SVM. However, object modeling
does not improve the classification accuracy and sometimes it can even be harmful when the scene
model is learned with the standard MLE. This is because the object model (using the state-of-the-art
representation) (e.g., MM-MLE-NG) alone performs much worse than global feature models (e.g.,
logistic regression), as shown in Fig. 1 and Fig. 3, and the standard MLE learns an imbalanced
prediction rule, as we have analyzed in Section 3. Given that the state-of-the-art object model is not
good, it is very encouraging to see that we can still obtain positive improvements by using the closely
coupled and well-balanced max-margin learning. These results indicate that further improvements
can be expected by improving the local object model, e.g., by incorporating rich features.
We also compare with the theme model [9], which is for scene categorization only. The theme model
uses a different image representation, where each image is a vector of image patch codewords. The
theme model achieves about 0.65 in classification accuracy, lower than that of MM-Scene.
7

Table 1: Confusion matrix for 100-topic MMScene on the sports dataset.

badminrocksnow0.717
bocce croquet polo
rowing sailing
ton
climbing
boarding
badminton 0.768 0.051 0.051 0.081 0.020 0.020 0.000 0.010
bocce
0.043 0.333 0.275 0.145 0.087 0.058 0.014 0.043
croquet
0.025 0.144 0.669 0.093 0.025 0.025 0.008 0.008
polo
0.220 0.055 0.099 0.516 0.022 0.022 0.011 0.055
rockclimbing 0.000 0.010 0.021 0.000 0.845 0.031 0.010 0.082
rowing
0.008 0.008 0.008 0.008 0.024 0.912 0.016 0.016
sailing
0.011 0.021 0.000 0.021 0.011 0.053 0.884 0.000
snowboarding 0.011 0.021 0.032 0.095 0.084 0.053 0.063 0.642

Table 2: The 67 indoor categories sorted by classification
accuracy by 70-topic MM-Scene.
buffet 0.85
green house 0.84
cloister 0.71
inside bus 0.61
movie theater 0.60
poolinside 0.59
church inside 0.56
classroom 0.55
concert hall 0.55
corridor 0.55
florist 0.55
trainstation 0.54
closet 0.51
elevator 0.49
nursery 0.44
bowling 0.41
gameroom 0.40

lobby 0.40
stairscase 0.25
hospitalroom 0.10
prison cell 0.39
studiomusic 0.24
kindergarden 0.10
casino 0.36
children room 0.21
laundromat 0.10
dining room 0.35
garage 0.20
office 0.10
kitchen 0.35
gym 0.20
restaurant kitchen 0.09
winecellar 0.34
hairsalon 0.20
shoeshop 0.09
library 0.31
livingroom 0.20
videostore 0.08
tv studio 0.30
operating room 0.20
airport inside 0.07
warehouse 0.29
pantry 0.20
bar 0.06
batchroom 0.26
subway 0.20
deli 0.06
bookstore 0.25
toystore 0.19
jewelleryshop 0.06
computerroom 0.25
artstudio 0.14
laboratorywet 0.05
dentaloffice 0.25 fastfood restaurant 0.13
locker room 0.05
grocerystore 0.25
auditorium 0.12
museum 0.05
inside subway 0.25
bakery 0.11
restaurant 0.05
mall 0.25
bedroom 0.11
waitingroom 0.04
meeting room 0.25
clothingstore 0.10

Scene Classification Accuracy

0.8
Finally, we examine the influence of the loss function
0/1
0/5
??d (s) on the performance of the max-margin scene
0.75
0/10
0/20
model. As we can see in problem (11), the loss function
0.7
0/30
??d (s) is another important factor that influences the
0/40
0/50
0.65
estimation of ? and its relative importance in the predic0.6
tion rule (5). Here, we use the 0/?-loss function, that is,
??d (s) = ? if s ?= sd ; otherwise 0. Fig. 4 shows the
0.55
performance of the 100-topic MM-Scene model when
0.5
using different loss functions. When ? is set between 10
Figure 4: Classification accuracy of MMand 20, the MM-Scene method stably achieves the best Scene with different loss functions ?? (s).
d
performance. The above results in Fig. 3 and Table 1
are achieved with ? selected from 5 to 40 with cross-validation during training.

5.4 Scene Categorization on the 67-Class MIT Indoor Scene Dataset

Scene Classification Accuracy

The MIT indoor dataset [20] contains complex scene
0.34
MLE?Scene?NG
images from 67 categories. We use the same training
MM?Scene?NG
0.32
SVM
and testing dataset as in [20], in which each category
0.3
LR
ROI+Gist(segmentation)
has about 80 images for training and about 20 images
0.28
ROI+Gist(annotation)
0.26
for testing. We compare the joint scene model with
MLE?Scene
MM?Scene
0.24
SVM, logistic regression (LR), and the prototype-based
0.22
methods [20]. Both the SVM and LR are based on the
0.2
global gist features only. For the joint scene model,
0.18
we set the number of latent topics at 70. The overall
0.16
0.14
performance of different methods are shown in Fig. 5
and the classification accuracy of each class is shown Figure 5: Classification accuracy on the 67in Table 2. For the prototype-based methods, we cite class MIT indoor dataset.
the results from [20]. We can see that the joint scene
model (both MLE-Scene and MM-Scene) significantly outperforms SVM and LR that use global
features only. The likelihood-based MLE-Scene slightly outperforms the ROI-Gist(segmentation),
which uses both the global gist features and local region-of-interest (ROI) features extracted from
automatically segmented regions [20]. By using max-margin training, the joint scene model (i.e.,
MM-Scene) achieves significant improvements compared to MLE-Scene. Moreover, the marginbased MM-Scene, which uses automatically segmented regions to extract features, outperforms the
ROI-Gist(annotation) method that uses human annotated interested regions.

6

Conclusions

In this paper, we address the weak coupling problem of the commonly used maximum likelihood
estimation in learning upstream scene understanding models by presenting a joint maximum margin and maximum likelihood learning method. The proposed approach achieves a close interplay
between the prediction model estimation and latent topic discovery, and thereby a well-balanced
prediction rule. The optimization problem is efficiently solved with a variational EM procedure,
which iteratively learns an online loss-augmented SVM. Finally, we demonstrate the advantages of
max-margin training and the effectiveness of using global features in scene understanding on both
an 8-category sports dataset and the 67-class MIT indoor scene data.
8

Acknowledgements
J.Z and E.P.X are supported by ONR N000140910758, NSF IIS-0713379, NSF Career DBI0546594, and an Alfred P. Sloan Research Fellowship to E.P.X. L.F-F is partially supported by
an NSF CAREER grant (IIS-0845230), a Google research award, and a Microsoft Research Fellowship. We also would like to thank Olga Russakovsky for helpful comments.

References
[1] P. Arbel?aez and L. Cohen. Constrained image segmentation from hierarchical boundaries. In CVPR,
2008.
[2] I. Biederman. On the semantics of a glance at a scene. Perceptual Organization, 213?253, 1981.
[3] D. Blei and J. Lafferty. Correlated topic models. In NIPS, 2006.
[4] D. Blei and J.D. McAuliffe. Supervised topic models. In NIPS, 2007.
[5] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. JMLR, (3):993?1022, 2003.
[6] L.-L. Cao and L. Fei-Fei. Spatially coherent latent topic model for concurrent segmentation and classification of objects and scenes. In ICCV, 2007.
[7] K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. JMLR, (2):265?292, 2001.
[8] L. Du, L. Ren, D. Dunson, and L. Carin. A bayesian model for simultaneous image cluster, annotation
and object segmentation. In NIPS, 2009.
[9] L. Fei-Fei and P. Perona. A bayesian hierarchical model for learning natural scene categories. In CVPR,
2005.
[10] A. Friedman. Framing pictures: The role of knowledge in automatized encoding and memory for gist.
Journal of Experimental Psychology: General, 108(3):316?355, 1979.
[11] T. Joachims, T. Finley, and C.-N. Yu. Cutting-plane training of structural SVMs. Machine Learning,
77(1):27?59, 2009.
[12] S. Lacoste-Jullien, F. Sha, and M. Jordan. DiscLDA: Discriminative learning for dimensionality reduction
and classification. In NIPS, 2008.
[13] L.-J. Li and L. Fei-Fei. What, where and who? classifying events by scene and object recognition. In
CVPR, 2007.
[14] L.-J. Li, R. Socher, and L. Fei-Fei. Towards total scene understanding: Classification, annotation and
segmentation in an automatic framework. In CVPR, 2009.
[15] D.C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical Programming, (45):503?528, 1989.
[16] D.G. Lowe. Object recognition from local scale-invariant features. In ICCV, 1999.
[17] K. Murphy, A. Torralba, and W. Freeman. Using the forest to see the trees: A graphical model relating
features, objects, and scenes. In NIPS, 2003.
[18] D. Navon. Forest before trees: The precedence of global features in visual perception. Perception and
Psychophysics, 5:197?200, 1969.
[19] A. Oliva and A. Torralba. Modeling the shape of the scene: a holistic representation of the spatial envelope. IJCV, 42(3):145?175, 2001.
[20] A. Quattoni and A. Torralba. Recognizing indoor scenes. In CVPR, 2009.
[21] B. Sch?
olkopf and A. Smola. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, 2001.
[22] J. Sivic, B.C. Russell, A. Efros, A. Zisserman, and W.T. Freeman. Discovering objects and their locatioins
in images. In ICCV, 2005.
[23] E. Sudderth, A. Torralba, W. Freeman, and A. Willsky. Learning hierarchical models of scenes, objects,
and parts. In CVPR, 2005.
[24] B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In NIPS, 2003.
[25] C. Wang, D. Blei, and L. Fei-Fei. Simultaneous image classification and annotation. In CVPR, 2009.
[26] J. Yang, K. Yu, Y. Gong, and T. Huang. Linear spatial pyramid matching using sparse coding forimage
classification. In CVPR, 2009.
[27] J. Zhu, A. Ahmed, and E.P. Xing. MedLDA: Maximum margin supervised topic models for regression
and classification. In ICML, 2009.

9

"
377,"Analytical study of the interplay between
architecture and predictability

Avner Priel, Ido Kanter, David A. Kessler
Minerva Center and Department of Physics, Bar Ilan University,
Ramat-Gan 52900, Israel.
e-mail: priel@mail.cc.biu.ac.il

(web-page: http://faculty.biu.ac.il/ """"'priel)

Abstract
We study model feed forward networks as time series predictors
in the stationary limit. The focus is on complex, yet non-chaotic,
behavior. The main question we address is whether the asymptotic
behavior is governed by the architecture, regardless the details of
the weights . We find hierarchies among classes of architectures
with respect to the attract or dimension of the long term sequence
they are capable of generating; larger number of hidden units can
generate higher dimensional attractors. In the case of a perceptron,
we develop the stationary solution for general weights, and show
that the flow is typically one dimensional. The relaxation time
from an arbitrary initial condition to the stationary solution is
found to scale linearly with the size of the network. In multilayer
networks, the number of hidden units gives bounds on the number
and dimension of the possible attractors. We conclude that long
term prediction (in the non-chaotic regime) with such models is
governed by attractor dynamics related to the architecture.
Neural networks provide an important tool as model free estimators for the solution
of problems when the real model is unknown, or weakly known. In the last decade
there has been a growing interest in the application of such tools in the area of time
series prediction (see Weigand and Gershenfeld, 1994). In this paper we analyse a
typical class of architectures used in this field, i.e. a feed forward network governed
by the following dynamic rule:
- ,
2 .. . , N
S 1t+l = S out ;
S ]t+l = st] - 1 J' (1)
where Sout is the network's output at time step t and Sj are the inputs at that time;
N is the size of the delayed input vector. The rational behind using time delayed
vectors as inputs is the theory of state space reconstruction of a dynamic system

A. Priel, 1. Kanter and D. A. Kessler

316

using delay coordinates (Takens 1981, Sauer Yorke and Casdagli 1991). This theory address the problem of reproducing a set of states associated with the dynamic
system using vectors obtained from the measured time series, and is widely used for
time series analysis. A similar architecture incorporating time delays is the TDNN
- time-delay neural network with a recurrent loop (Waibel et. al. 1989). This type
of networks is known to be appropriate for learning temporal sequences, e.g. speech
signal. In the context of time series, it is mostly used for short term predictions. Our
analysis focuses on the various long-time properties of the sequence generated by a
given architecture and the interplay between them. The aim of such an investigation is the understanding and characterization of the long term sequences generated
by such architectures, and the time scale to reach this asymptotic behavior. Such
knowledge is necessary to define adequate measures for the transition between a
locally dependent prediction and the long term behavior. Though some work has
been done on characterization of a dynamic system from its time series using neural networks, not much analytical results that connect architecture and long-time
prediction are available (see M. Mozer in Weigand and Gershenfeld, 1994). Nevertheless, practical considerations for choosing the architecture were investigated
extensively (Weigand and Gershenfeld, 1994 and references therein). It has been
shown that such networks are capable of generating chaotic like sequences. While
it is possible to reconstruct approximately the phase space of chaotic attractors (at
least in low dimension), it is clear that prediction of chaotic sequences is limited
by the very nature of such systems, namely the divergence of the distance between
nearby trajectories. Therefore one can only speak about short time predictions with
respect to such systems. Our focus is the ability to generate complex sequences,
and the relation between architecture and the dimension of such sequences.

1

Perceptron

We begin with a study of the simplest feed forward network, the perceptron. We
analyse a perceptron whose output Sout at time step t is given by:

Sou. = tanh [13 (t,(W; + WO)Sj) ]

(2)

where {3 is a gain parameter, N is the input size. The bias term ,Wo, plays the same
role as the common 'external field' used in the literature, while preserving the same
qualitative asymptotic solution. In a previous work (Eisenstein et. al. , 1995) it was
found that the stationary state (of a similar architecture but with a ""sign"" activation
function instead of the ""tanh"", equivalently (3 --t 00) is influenced primarily by one
of the larger Fourier components in the power spectrum of the weights vector W
of the perceptron. This observation motivates the following representation of the
vector W. Let us start with the case of a vector that consists of a singl? biased
Fourier component of the form:
Wj

= acos(27fKjjN)

j

= 1, ... ,N ;

Wo =b

(3)

where a, b are constants and K is a positive integer. This case is generalized later on,
however for clarity we treat first the simple case. Note that the vector W can always
be represented as a Fourier decomposition of its values. The stationary solution for
the sequence (SI) produced by the output of the percept ron , when inserting this
choice of the weights into equation (2), can be shown to be of the form:
SI

= tanh [A({3) cos(27fKljN) + B({3)]

There are two non-zero solutions possible for the variables (A, B):

(4)

317

The Interplay between Architecture and Predictability

A

t{3N a I:~l D(p)(A/2)2P-l (p!)-2

B

{3Nb I:~l D(p)S2 p-l ((2p)!)-1

B =0
(5)

where D(p) = 22p (2 2p - 1)B2p and B 2p are the Bernoulli numbers. Analysis
of equations (5) reveals the following behavior as a function of the parameter {3.
Each of the variables is the amplitude of an attractor. The attractor represented
by (A i- 0, B = 0) is a limit cycle while the attractor represented by (B i- 0, A = 0)
is !l fixed point of the dynamics. The onset of each of the attractors A(B) is at
{3cl = 2(aN)-1 ({3c2 = (bN)-l) respectively. One can identify three regimes: (1)
{3 < {3cl,c2 - the stable solution is Sl = O. (2) min({3cl, (3c2) < {3 < max({3cl, (3c2) the system flows for all initial conditions into the attractor whose {3c is smaller. (3)
{3 > {3cl,c2 - depending on the initial condition of the input vector, the system flows
into one of the attractors, namely, the stationary state is either a fixed point or a
periodic flow. {3cl is known as a Hopf bifurcation point. Naturally, the attractor
whose {3c is smaller has a larger basin of attraction, hence it is more probable to
attract the flow (in the third regime).
1.0

0.5
o
o
o
o
o
o
o
o
o
o

0.0

-0.5

00
00
000

00

Figure 1: Embedding of a sequence generated by a perceptron whose weights follow eq. 3
(6) . Periodic sequence (outer
curve) N = 128, k = 17, b = 0.3,
{3 = 1/40 and quasi periodic (inner) k = 17, ? = 0.123, (3
1/45 respectively.

000000000

-1.0
-1.0

-0.5

0.0

Sl

0.5

1.0

Next we discuss the more general case where the weights of eq. (3) includes an
arbitrary phase shift of the form:
Wj

= acos(27fKj/N -

7f?)

? E (-1,1)

The leading term of the stationary solution in the limit N
Sl

= tanh [A({3) cos(27f(K -

?)l/N)

?

(6)
1 is of the form:

+ B({3)]

(7)

where the higher harmonic corrections are of O( 1/ K). A note should be made here
that the phase shift in the weights is manifested as a frequency shift in the solution.
In addition, the attractor associated with A i- 0 is now a quasi-periodic flow in the
generic case when ? is irrational. The onset value ofthe fixed point ({3c2) is the same
as before, however the onset of the quasi-periodic orbit is (3cl = sin'(!4? 2(aN)-1.
The variables A, B follow similar equations to (5):

A

(3Na SinJ;4? I:~l D(p)(A/2)2P-l(p!)-2

B =0
(8)

A=O

The three regimes discussed above appear in this case as well. Figure 1 shows the
attractor associated with (A i- 0, B = 0) for the two cases where the series generated
by the output is embedded as a sequence of two dimensional vectors (Sl+l, Sl).

A. Priel, I Kanter and D. A. Kessler

318

The general weights can be written as a combination of their Fourier components
with different K's and ?'s:
m

Wj = Laicos(27fKd/N-7f?i)

?i E (-1,1)

(9)

i=l

When the different K's are not integer divisors of each other, the general solution
is similar to that described above:

[t,

Sl = tanh

A,({3) cos(27r(K, - <pi)l / N)

+ B({3)1

(10)

where m is the number of relevant Fourier components. As above, the variables
Ai ,B are coupled via self consistent equations. Nevertheless, the generic stationary
flow is one of the possible attractors, depending on /3 and the initial condition; i.e.
(Aq i- 0, Ai = 0 Vi i- q ,B = 0) or (B i- 0, Ai = 0). By now we can conclude that
the generic flow for the perceptron is one of three: a fixed point, periodic cycle or
quasi-periodic flow. The first two have a zero dimension while the last describes a
one dimensional flow. we stress that more complex flows are possible even in our
solution (eq. 10), however they require special relation between the frequencies and
a very high value of /3, typically more than an order of magnitude greater than
bifurcation value.

2

Relaxation time

At this stage the reader might wonder about the relation between the asymptotic
results presented above and the ability of such a model to predict. In fact, the
practical use of feed forward networks in time series prediction is divided into two
phases. In the first phase, the network is trained in an open loop using a given time
series. In the second phase, the network operates in a closed loop and the sequence it
generates is also used for the future predictions. Hence, it is clear from our analysis
that eventually the network will be driven to one of the attractors. The relevant
We
question is how long does it takes to arrive at such asymptotic behavior?
shall see that the characteristic time is governed by the gap between the largest and
the second largest eigenvalues of the linearized map. Let us start by reformulating
= (Sf, s~, ... ,Sj.,,)
eqs. (1,2) in a matrix form, i.e. we linearize the map. Denote
-t
-t
-t+1
and (S )' is the transposed vector. The map is then T(S)' = (S )' where

st

CN-l

CN

o
o

T=

o

0

0
0

(11)

o

1

The first row of T gives the next output value = si+ 1 while the rest of the matrix is
just the shift defined by eq. (1) . This matrix is known as the ""companion matrix""
(e.g. Ralston and Rabinowitz, 1978). The characteristic function of T can be
written as follows:
N

/3 '""
~

C

n

).n

=1

(12)

n=l

from which it is possible to extract the eigenvalues. At (3 = /3c the largest eigenvalue
of T is 1).11 = 1. Denote the second largest eigenvalue ).2 such that 1).21 = 1 - 6. .

The Interplay between Architecture and Predictability

0.002

<l

Figure 2: Scaling of ~ for a perceptron with two Fourier components, (eq. 9), with ai = 1,
Kl = 3, 1>1 = 0.121, K2 = 7,
1>2 = 0 ,Wo = 0.3 . The dashed
line is a linear fit of 0.1/N, N =
50, ... ,400.

0.001

o

319

o

0.01

0.02

1/N
Applying T T - times to an initial state vector results in a vector whose second
largest component is of order:
(13)

therefore we can define the characteristic relaxation time in the vicinity of an attractor to be T = ~ -1 . 1
We have analysed eq. (12) numerically for various cases of Ci, e.g. Wi composed of
one or two Fourier components. In all the cases (3 was chosen to be the minimal f3c to
ensure that the linearized form is valid. We found that ~,....., I/N. Figure 2 depicts
one example of two Fourier components. Next, we have simulated the network and
measured the average time ( T S ) it takes to flow into an attractor starting from
an arbitrary initial condition. The following simulations support the analytical
result ( T ,....., N ) for general (random) weights and high gain (13) value as well.
The threshold we apply for the decision whether the flow is already close enough
to the attractor is the ratio between the component with the largest power in the
-t
spectrum and the total power spectrum of the current state (S ), which should
exceed 0.95. The results presented in Figure 3 are an average over 100 samples
started from random initial condition. The weights are taken at random, however
we add a dominant Fourier component with no phase to control the bifurcation
point more easily. This component has an amplitude which is about twice the other
components to make sure that its bifurcation point is the smallest. We observe a
clear linear relation between this time and N (T S ,....., N ). The slope depends on the
actual values of the weights, however the power law scaling does not change.
On general principles, we expect the analytically derived scaling law for ~ to be
valid even beyond the linear regime. Indeed the numerical simulations (Figure 3)
support this conjecture.

3

Multilayer networks

For simplicity, we restrict the present analysis to a multilayer network (MLN) with
N inputs, H hidden units and a single linear output, however this restriction can

be removed, e.g. nonlinear output and more hidden layers. The units in the hidden
layer are the perceptrons discussed above and the output is given by:
(14)

INote that if one demand the L.R.S. of eq. (13) to be of O(~), then

T '""

~ -11og(~ -1).

A. Priel, 1. Kanter and D. A. Kessler

320

800

Figure 3: Scaling of r S for random weights with a dominant
component at K = 7, ? = 0, a =
1; All other amplitudes are randomly taken between (0,0.5) and
the phases are random as well.
{3 = 3.2/N. The dashed line is a
linear fit of eN, e = 2.73 ? 0.03.
N = 16, ... , 256.

600

I""'p

400
200
0

100

0

200

300

N
The dynamic rule is defined by eq. (1). First consider the case where the weights
of each hidden unit are of the form described by eq. (6), Le. each hidden unit has
only one (possibly biased) Fourier component:
m=l, ... ,H.

(15)

Following a similar treatment as for the perceptron, the stationary solution is a
combination of the perceptron-like solution:
H

Sl =

L tanh [Am({3) cos(27r(Km - ?m)l/N) + Bm({3)]

(16)

m=l

The variables Am, Bm are the solution of the self consistent coupled equations, however by contrast with the single perceptron, each hidden unit operates independently
and can potentially develop an attractor of the type described in section 1. The
number of attractors depends on {3 with a maximum of H attractors. The number
of non-zero Am's defines the attractor's dimension in the generic case of irrational
?'s associated with them. If different units do not share Fourier components with
a common divisor or harmonics of one another, it is easy to define the quantitative
result, otherwise, one has to analyse the coupled equations more carefully to find
the exact value of the variables. Nevertheless, each hidden unit exhibits only a
single highly dominant component (A 1= 0 or B 1= 0).
Generalization of this result to more than a single biased Fourier component is
straightforward. Each vector is of the form described in eq. (9) plus an index for the
hidden unit. The solution is a combination of the general perceptron solution, eq.
(10). This solution is much more involved and the coupled equations are complicated
but careful study of them reveals the same conclusion, namely each hidden unit
possess a single dominant Fourier component (possibly with several other much
smaller due to the other components in the vector). As the gain parameter {3
becomes larger, more components becomes available and the number of possible
attractors increases. For a very large value it is possible that higher harmonics
from different hidden units might interfere and complicate considerably the solution.
Still, one can trace the origin of this behavior by close inspection of the fields in
each hidden unit.
We have also measured the relaxation time associated with MLN's in simulations.
The preliminary results are similar to the perceptron, Le. r S ' "" N but the constant
prefactor is larger when the weights consist of more Fourier components.

The Interplay between Architecture and Predictability

4

321

Discussion

Neural networks were proved to be universal approximators (e.g. Hornik, 1991),
hence they are capable of approximating the prediction function of the delay coordinate vector. The conclusion should be that prediction is indeed possible. This
observation holds only for short times in general. As we have shown, long time
predictions are governed by the attractor dynamics described above. The results
point out the conclusion that the asymptotic behavior for this networks is dictated
by the architecture and not by the details of the weights. Moreover, the attractor dimension of the asymptotic sequence is typically bounded by the number of
hidden units in the first layer (assuming the network does not contain internal delays) . To prevent any misunderstanding we note again that this result refers to
the asymptotic behavior although the short term sequence can approximate a very
complicated attractor.
The main result can be interpreted as follows. Since the network is able to approximate the prediction function, the initial condition is followed by reasonable
predictions which are the mappings from the vicinity of the original manifold created by the network. As the trajectory evolves, it flows to one of the attractors
described above and the predictions are no longer valid. In other words, the initial
combination of solutions described in eq. (10) or its extension to MLN (with an
arbitrary number of non-zero variables, A's or B's) serves as the approximate mapping. Evolution of this approximation is manifested in the variables of the solution,
which eventually are attracted to a stable attractor (in the non-chaotic regime).
The time scale for the transition is given by the relaxation time developed above.
The formal study can be applied for practical purposes in two ways. First, taking
into account this behavior by probing the generated sequence and looking for its
indications. One such indication is stationarity of the power spectrum. Second, one
can incorporate ideas from local linear models in the reconstructed space to restrict
the inputs in such a way that they always remain in the vicinity of the original
manifold (Sauer, in Weigand and Gershenfeld, 1994).
Acknowledgments

This research has been supported by the Israel Science Foundation.
References

Weigand A. S. and Gershenfeld N. A. ; Time Series Prediction, Addison-Wesley,
Reading, MA, 1994.
E. Eisenstein, I. Kanter, D. A. Kessler and W. Kinzel; Generation and prediction
of time series by a neural network, Phys. Rev. Lett. 74,6 (1995).
Waibel A., Hanazawa T., Hinton G., Shikano K. and Lang K.; Phoneme recognition
using TDNN, IEEE Trans. Acoust., Speech & Signal Proc. 37(3), (1989).
Takens F., Detecting strange attractors in turbulence, in Lecture notes in mathematics vol. 898, Springer-Verlag, 1981.
T. Sauer, J. A. Yorke and M. Casdagli; Embedology, J. Stat. Phys. 65(3), (1991) .
Ralston A. and Rabinowitz P. ; A first course in numerical analysis, McGraw-Hill,
1978.
K. Hornik; Approximation capabilities of multilayer feed forward networks, Neural
Networks 4, (1991).

"
518,"Multiresolution Tangent Distance for
Affine-invariant Classification
Nuno Vasconcelos

Andrew Lippman

MIT Media Laboratory, 20 Ames St, E15-320M,
Cambridge, MA 02139, {nuno,lip }@media.mit.edu

Abstract
The ability to rely on similarity metrics invariant to image transformations is an important issue for image classification tasks such as face or
character recognition. We analyze an invariant metric that has performed
well for the latter - the tangent distance - and study its limitations when
applied to regular images, showing that the most significant among these
(convergence to local minima) can be drastically reduced by computing
the distance in a multiresolution setting. This leads to the multi resolution
tangent distance, which exhibits significantly higher invariance to image transformations, and can be easily combined with robust estimation
procedures.

1 Introduction
Image classification algorithms often rely on distance metrics which are too sensitive to
variations in the imaging environment or set up (e.g. the Euclidean and Hamming distances),
or on metrics which, even though less sensitive to these variations, are application specific
or too expensive from a computational point of view (e.g. deformable templates).
A solution to this problem, combining invariance to image transformations with computational simplicity and general purpose applicability was introduced by Simard et al in [7].
The key idea is that, when subject to spatial transformations, images describe manifolds in a
high dimensional space, and an invariant metric should measure the distance between those
manifolds instead of the distance between other properties of (or features extracted from)
the images themselves. Because these manifolds are complex, minimizing the distance between them is a difficult optimization problem which can, nevertheless, be made tractable
by considering the minimization of the distance between the tangents to the manifolds -the
tangent distance (TO) - instead of that between the manifolds themselves. While it has led
to impressive results for the problem of character recognition [8] , the linear approximation
inherent to the TO is too stringent for regular images, leading to invariance over only a very
narrow range of transformations.

844

N. Vasconcelos and A. Lippman

In this work we embed the distance computation in a multi resolution framework [3],
leading to the multiresolution tangent distance (MRTD). Multiresolution decompositions
are common in the vision literature and have been known to improve the performance of
image registration algorithms by extending the range over which linear approximations
hold [5, 1]. In particular, the MRTD has several appealing properties: 1) maintains
the general purpose nature of the TD; 2) can be easily combined with robust estimation
procedures, exhibiting invariance to moderate non-linear image variations (such as caused
by slight variations in shape or occlusions); 3) is amenable to computationally efficient
screening techniques where bad matches are discarded at low resolutions; and 4) can be
combined with several types of classifiers. Face recognition experiments show that the
MRTD exhibits a significantly extended invariance to image transformations, originating
improvements in recognition accuracy as high as 38%, for the hardest problems considered.

2 The tangent distance
Consider the manifold described by all the possible linear transformations that a pattern
lex) may be subject to
(1)
Tp [lex)] = 1('ljJ(x, p)),
where x are the spatial coordinates over which the pattern is defined, p is the set of
parameters which define the transformation, and 'ljJ is a function typically linear on p, but
not necessarily linear on x. Given two patterns M(x) and N(x), the distance between the
associated manifolds - manifold distance (MD) - is
T(M, N) = min IITq[M(x)] - Tp[N(x)]W.
p,q

(2)

For simplicity, we consider a version of the distance in which only one of the patterns is
subject to a transformation, i.e.
T(M, N)

= min
IIM(x) p

Tp[N(x)]lf,

(3)

but all results can be extended to the two-sided distance. Using the fact that
\7 p Tp[N(x)]

= \7pN('ljJ(x, p)) = \7 p '?(x, p)\7xN('?(x, p)),

(4)

where \7pTp is the gradient of Tp with respect to p, Tp[N(x)] can, for small p, be
approximated by a first order Taylor expansion around the identity transformation
Tp[N(x)] = N(x)

+ (p -

If\7p 'ljJ(x,p)\7 x N(x).

This is equivalent to approximating the manifold by a tangent hyper-plane, and leads to the
TD. Substituting this expression in equation 3, setting the gradient with respect to p to zero,
and solving for p leads to
p

~ [~'VP;6(X' P ) 'Vx N(x) 'V); N(X)'V~;6(x, P)]-' ~ D(x)'Vp;6(x, P l'VxN(x) + I,
(5)

where D(x) = M(x) - N(x). Given this optimal p, the TD between the two patterns
is computed using equations I and 3. The main limitation of this formulation is that it
relies on a first-order Taylor series approximation, which is valid only over a small range
of variation in the parameter vector p .

2.1

Manifold distance via Newton's method

The minimization of the MD of equation 3 can also be performed through Newton's method,
which consists of the iteration

pn+1

= pn _ 0: [\7~ T/p=pn] -I \7 p Tlp=pn

(6)

845

Multiresolution Tangent Distancefor Affine-invariant Classification

where \7 p / and \7~ / are, respectively, the gradient and Hessian of the cost function of
equation 3 with respect to the parameter p,
\7p/ = 2

L

[M(x) - Tp[N(x)]) V'pTp[N(x)]

x

V'~ /

= 2

L

[-V'pTp[N(x)]

\7~Tp[N(x)] + [M(x) -

N(x)]

V'~Tp[N(x)]]

.

x

Disregarding the term which contains second-order derivatives (V'~Tp[N(x)]), choosing
pO
I and Q:
1, using 4, and substituting in 6 leads to equation 5. I.e. the TO
corresponds to a single iteration of the minimization of the MD by a simplified version of
Newton's method, where sec!ond-orderderivatives are disregarded. This reduces the rate of
convergence of Newton's method, and a single iteration may not be enough to achieve the
local minimum, even for simple functions. It is, therefore, possible to achieve improvement
if the iteration described by equation 6 is repeated until convergence.

=

=

3 The multiresolution tangent distance
The iterative minimization of equation 6 suffers from two major drawbacks [2]: 1) it may
require a significant number of iterations for convergence and 2), it can easily get trapped
in local minima. Both these limitations can be, at least partially, avoided by embedding
the computation of the MD in a multiresolution framework, leading to the multiresolution
manifold distance (MRMD). For its computation, the patterns to classify are first subject to
a multiresolution decomposition, and the MD is then iteratively computed for each layer,
using the estimate obtained from the layer above as a starting point,

where, Dl(x) = M(x) - Tp~ [N(x)]. If only one iteration is allowed at each imageresolution, the MRMD becomes the multiresolution extension of the TO, i.e. the multi resolution
tangent distance (MRTO).
To illustrate the benefits of minimization over different scales consider the signal J(t) =
E{;=1 sin(wkt ), and the manifold generated by all its possible translations J'(t,d) =
J(t + d). Figure 1 depicts the multiresolution Gaussian decomposition of J(t), together
with the Euclidean distance to the points on the manifold as a function of the translation
associated with each of them (d). Notice that as the resolution increases, the distance
function has more local minima, and the range of translations over which an initial guess
is guaranteed to lead to convergence to the global minimum (at d = 0) is smaller. I.e.,
at higher resolutions, a better initial estimate is necessary to obtain the same performance
from the minimization algorithm.
Notice also that, since the function to minimize is very smooth at the lowest resolutions,
the minimization will require few iterations at these resolutions if a procedure such as
Newton's method is employed. Furthermore, since the minimum at one resolution is a good
guess for the minimum at the next resolution, the computational effort required to reach
that minimum will also be small. Finally, since a minimum at low resolutions is based on
coarse, or global, information about the function or patterns to be classified, it is likely to
be the global minimum of at least a significant region of the parameter space, if not the true
global minimum.

846

N. Vasconcelos and A. Lippman

?R B .~5ISa {\Z\Z\]
-UJj -F \lJ -t;: Ll
-I~
..::..

..

..

....

. . . . . . ... .. ~ .. . .... .

..:..

?.

??

??

????

..I~

??

??

--""' .

_ .:..

. . .. ...

. . ..

Figure 1: Top: Three scales of the multiresolution decomposition of J(t) . Bottom: Euclidean
distance VS. translation for each scale. Resolution decreases from left to right.

4 Affine-invariant classification
There are many linear transformations which can be used in equation 1. In this work, we
consider manifolds generated by affine transformations

1jJ(x,p)

=[

X

0

y

0

1000]
0 x yIP

= ~(x)p,

(8)

where P is the vector of parameters which characterize the transformation. Taking the
gradient of equation 8 with respect to p. V'p1jJ(x,p) = ~(x)T. using equation 4. and
substituting in equation 7.

p~+1

= pr

+""

[ ~ 4> (x) TV x N ' (x) viN' (x) 4> (xl ] -I

L D'(x)~(x)TV'xN'(x),

(9)

x

PI?'

where N'(x) = N(1jJ(x,
and D'(x) = M(x) - N'(x). For a given levell of the
multiresolution decomposition, the iterative process of equation 9 can be summarized as
follows.
1. Compute N'(x) by warping the pattern to classify N(x) according to the best
current estimate of p, and compute its spatial gradient V'xN'(x).
2. Update the estimate of PI according to equation 9.
3. Stop if convergence, otherwise go to 1.
Once the final PI is obtained, it is passed to the multiresolution level below (by doubling the
translation parameters), where it is used as initial estimate. Given the values of Pi which
minimize the MD between a pattern to classify and a set of prototypes in the database, a
K-nearest neighbor classifier is used to find the pattern's class.

5 Robust classifiers
One issue of importance for pattern recognition systems is that of robustness to outliers, i.e
errors which occur with low probability, but which can have large magnitude. Examples
are errors due to variation of facial features (e.g. faces shot with or without glasses) in
face recognition, errors due to undesired blobs of ink or uneven line thickness in character
recognition, or errors due to partial occlusions (such as a hand in front of a face) or partially

Multiresolution Tangent Distance/or Affine-invariant Classification

847

missing patterns (such as an undoted i). It is well known that a few (maybe even one)
outliers of high leverage are sufficient to throw mean squared error estimators completely
off-track [6] .
Several robust estimators have been proposed in the statistics literature to avoid this problem.
In this work we consider M-estimators [4] which can be very easily incorporated in the
MD classification framework. M-estimators are an extension of least squares estimators
where the square function is substituted by a functional p(x) which weighs large errors less
heavily. The robust-estimator version of the tangent distance then becomes to minimize the
cost function
T(M, N) = min
p(M(x) - Tp[N{x)]) ,
(10)
p

I:
x

and it is straightforward to show that the ""robust"" equivalent to equation 9 is

p~+' ~ pr +"" [~P""[D(X))oI>(X)TI7XN'(X)I7;;:N'(X)oI>(X)T]-' x
[~P'[D(X))oI>(X)Tl7xN' (X)] ,

(11)

where D(x) = M(x) - N'(x) and p'(x) and p""(x) are, respectively, the first and second
derivatives of the function p( x) with respect to its argument.

6 Experimental results
In this section, we report on experiments carried out to evaluate the performance of the MD
classifier. The first set of experiments was designed to test the invariance of the TD to affine
transformations of the input. The second set was designed to evaluate the improvement
obtained under the multiresolution framework.

6.1

Affine invariance of the tangent distance

Starting from a single view of a reference face, we created an artificial dataset composed
by 441 affine transformations of it. These transformations consisted of combinations of
all rotations in the range from - 30 to 30 degrees with increments of 3 degrees, with all
scaling transformations in the range from 70% to 130% with increments of 3%. The faces
associated with the extremes of the scaling/rotation space are represented on the left portion
of figure 2.
On the right of figure 2 are the distance surfaces obtained by measuring the distance
associated with several metrics at each of the points in the scaling/rotation space. Five
metrics were considered in this experiment: the Euclidean distance (ED), the TD, the MD
computed through Newton's method, the MRMD, and the MRTD.
While the TD exhibits some invariance to rotation and scaling, this invariance is restricted
to a small range of the parameter space and performance only slightly better than the
obtained with the ED. The performance of the MD computed through Newton's method
is dramatically superior, but still inferior to those achieved with the MRTD (which is very
close to zero over the entire parameter space considered in this experiment), and the MRMD.
The performance of the MRTD is in fact impressive given that it involves a computational
increase of less than 50% with respect to the TD, while each iteration of Newton's method
requires an increase of 100%, and several iterations are typically necessary to attain the
minimum MD.

N. Vasconcelos and A. Lippman

848

-30

!i

~

-0
a:

0

1.3

0.7

Scaling

Figure 2: Invariance of the tangent distance. In the right, the surfaces shown correspond to ED, TO,
MO through Newton's method, MRTO, and MRMO. This ordering corresponds to that of the nesting
of the surfaces, i.e. the ED is the cup-shaped surface in the center, while the MRMO is the flat surface
which is approximately zero everywhere.

6.2

Face recognition

To evaluate the performance of the multiresolution tangent distance on a real classification
task, we conducted a series of face recognition experiments, using the Olivetti Research
Laboratories (ORL) face database. This database is composed by 400 images of 40 subjects,
10 images per subject, and contains variations in pose, light conditions, expressions and
facial features, but small variability in terms of scaling, rotation, or translation. To correct
this limitation we created three artificial datasets by applying to each image three random
affine transformations drawn from three multivariate normal distributions centered on the
identity transformation with different covariances. A small sample of the faces in the
database is presented in figure 3, together with its transformed version under the set of
transformations of higher variability.

Figure 3: Left: sample of the ORL face database. Right: transformed version.
We next designed three experiments with increasing degree of difficulty. In the first, we
selected the first view of each subject as the test set, using the remaining nine views as
training data. In the second, the first five faces were used as test data while the remaining
five were used for training. Finally, in the third experiment, we reverted the roles of the
datasets used in the first. The recognition accuracy for each of these experiments and each
of the datasets is reported on figure 4 for the ED, the TO, the MRTD, and a robust version
of this distance (RMRTO) with p(x) = 1x2 if x::; aT and p(x) = ~2 otherwise, where T
is a threshold (set to 2.0 in our experiments), and a a robust version of the error standard
deviation defined as a = median lei - median (ei )1 /0.6745.
Several conclusions can be taken from this figure. First, it can be seen that the MRTD
provides a significantly higher invariance to linear transformations than the ED or the TO,

MultiresolUlion Tangent Distance for Affine-invariant Classification

849

increasing the recognition accuracy by as much as 37.8% in the hardest datasets. In fact,
for the easier tasks of experiments one and two, the performance of the multiresolution
classifier is almost constant and always above the level of 90% accuracy. It is only for the
harder experiment that the invariance of the MRTO classifier starts to break down. But even
in this case, the degradation is graceful- the recognition accuracy only drops below 75%
for considerable values of rotation and scaling (dataset D3).
On the other hand, the ED and the single resolution TO break down even for the easier
tasks, and fail dramatically when the hardest task is performed on the more difficult datasets.
Furthermore, their performance does not degrade gracefully, they seem to be more invariant
when the training set has five views than when it is composed by nine faces of each subject
in the database.
I

'iJ

....

'~--------~,~-=-=~~ ~

,

'

,

___ _ ...l _ _ _ _ _ __ _ l...

,
- - --1- -

,
,
,

,

- - -- - - --r- --- - - - - -t- -

. .. _

__ _____ _ L _ _ _ _ ___

,
,,
_ _ _ _ ______ L- __ _ __ __
,,
,,
am .

----

- -- - -- - -r-- ---- - -t - - - - --- - t,
,-

,,

...1 ___ __ _ __ l-

I

,

__

....

-------;----...---.........
:0
__ ~
L . _

_ _

....1_ _

_

I

.OII IIL.

r
?
iii""""
----- - - - j- ------ - -t - - - - - - - - r -TD""""""""

_

I .
_ _ ______ IL _____ __ ...1! _ _ _ ?_ _ _ _ .l-

11041

10>00_

OO~

_

JOQI _

..,CV _

,,
_ ____ __
,LI __ _ ____ ,,
,,
,,
_ __ ___ _ _
,,

,,
,L, _
,,

-10 _ _ ____ __ ... _

_ _ __ ____

j

l'W'> _ _ _ _ _ _ _

_

~

__ __ _ _

~ -- - ---- ~ -- --- -- - ~ -

_

~

_ _ _ _ _ _ _ ... _ _ _ _ _ _ _ _ ...

,

I

,
,

,
I

I
,

I

I

I

1Mb

IIRm

- ~?~?~:~j?~~~~~~~}~~~~~~~~~
:
:
'~

-- - - -- -- t-- -- -- -- -~ - -- ----t"" _ _ _ _ _ _ _ _ 1-

!

I
_

,

- -- - -~ -

,
,
,,
,,
...1_ ______ , _
,,
,
,,
,
"""".(1,1 ____ __ _ _ _ ~- - ---- --+ ---- --- -~,
,
,,
,
,
,

'!O.(U.

.,

.

---- - -I-- ------ - - ------ - r -m-

,GI;IIIIII_

110 m ?

? _

:1111,1,1 .

__ _ _ _

_

___ L ___ _ _ __ ...l ___ ___ ___ _

?>.011 _

_ _ __ ___

I
I
-

,
_

_

~

,
,

I____ _ ___
I

?

+____ _____ t _
,

,

_____ _ _ I _ _ _ _ _ ___ ~

I

__ __ ___ L _____ ____ _

,

,

,,
,

,;.

Figure 4: Recognition accuracy. From left to right: results from the first, second, and third
experiments. Oatasets are ordered by degree of variability: 00 is the ORL database 03 is subject to
the affine transfonnations of greater amplitude.

Acknowledgments
We would like to thank Federico Girosi for first bringing the tangent distance to our attention,
and for several stimulating discussions on the topic.

References
[1J P. Anandan, J. Bergen, K. Hanna, and R. Hingorani. Hierarchical Model-Based Motion Estimation. In M. Sezan and R. Lagendijk, editors, Motion Analysis and Image
Sequence Processing, chapter 1. Kluwer Academic Press, 1993.
[2J D. Bertsekas. Nonlinear Programming. Athena Scientific, 1995.
[3J P. Burt and E. Adelson. The Laplacian Pyramid as a Compact Image Code. IEEE
Trans. on Communications, Vol. 31:532-540,1983.
[4] P. Huber. Robust Statistics. John Wiley, 1981 .
[5] B. Lucas and T. Kanade. An Iterative Image Registration Technique with an Application
to Stereo Vision. In Proc. DARPA Image Understanding Workshop, 198 I.
[6J P. Rousseeuw and A. Leroy. Robust Regression and Outlier Detection. John Wiley,
1987.
[7] P. Simard, Y. Le Cun, and J. Denker. Efficient Pattern Recognition Using a New
Transformation Distance. In Proc. Neurallnfonnation Proc. Systems, Denver, USA,
1994.
[8] P. Simard, Y. Le Cun, and 1. Denker. Memory-based Character Recognition Using a
Transformation Invariant Metric. In Int. Conference on Pattern Recognition, Jerusalem,
Israel, 1994.

"
3661,"Active dendrites:
adaptation to spike-based communication
Bal?azs B Ujfalussy1,2
M?at?e Lengyel1
ubi@rmki.kfki.hu
m.lengyel@eng.cam.ac.uk
1
Computational & Biological Learning Lab, Dept. of Engineering, University of Cambridge, UK
2
Computational Neuroscience Group, Dept. of Biophysics, MTA KFKI RMKI, Budapest, Hungary

Abstract
Computational analyses of dendritic computations often assume stationary inputs to neurons, ignoring the pulsatile nature of spike-based communication between neurons and the moment-to-moment fluctuations caused by such spiking
inputs. Conversely, circuit computations with spiking neurons are usually formalized without regard to the rich nonlinear nature of dendritic processing. Here we
address the computational challenge faced by neurons that compute and represent
analogue quantities but communicate with digital spikes, and show that reliable
computation of even purely linear functions of inputs can require the interplay of
strongly nonlinear subunits within the postsynaptic dendritic tree. Our theory predicts a matching of dendritic nonlinearities and synaptic weight distributions to
the joint statistics of presynaptic inputs. This approach suggests normative roles
for some puzzling forms of nonlinear dendritic dynamics and plasticity.

1

Introduction

The operation of neural circuits fundamentally depends on the capacity of neurons to perform complex, nonlinear mappings from their inputs to their outputs. Since the vast majority of synaptic inputs
impinge the dendritic membrane, its morphology, and passive as well as active electrical properties
play important roles in determining the functional capabilities of a neuron. Indeed, both theoretical
and experimental studies suggest that active, nonlinear processing in dendritic trees can significantly
enhance the repertoire of singe neuron operations [1, 2].
However, previous functional approaches to dendritic processing were limited because they studied
dendritic computations in a firing rate-based framework [3, 4], essentially requiring both the inputs
and the output of a cell to have stationary firing rates for hundreds of milliseconds. Thus, they
ignored the effects and consequences of temporal variations in neural activities at the time scale
of inter-spike intervals characteristic of in vivo states [5]. Conversely, studies of spiking network
dynamics [6, 7] have ignored the complex and highly nonlinear effects of the dendritic tree.
Here we develop a computational theory that aims at explaining some of the morphological and
electrophysiological properties of dendritic trees as adaptations towards spike-based communication. In line with the vast majority of theories about neural network computations, the starting point
of our theory is that each neuron needs to compute some function of the membrane potential (or,
equivalently, the instantaneous firing rate) of its presynaptic partners. However, as the postsynaptic
neuron does not have direct access to the presynaptic membrane potentials, only to the spikes emitted by its presynaptic partners based on those potentials, computing the required function becomes a
non-trivial inference problem. That is, neurons need to perform computations on their inputs in the
face of significant uncertainty as to what those inputs exactly are, and so as to what their required
output might be.
In section 2 we formalize the problem of inferring some required output based on incomplete
spiking-based information about inputs and derive an optimal online estimator for some simple
1

but tractable cases. In section 3 we show that the optimal estimator exhibits highly nonlinear behavior closely matching aspects of active dendritic processing, even when the function of inputs to be
computed is purely linear. We also present predictions about how the statistics of presynaptic inputs
should be matched by the clustering patterns of synaptic inputs onto active subunits of the dendritic
tree. In section 4 we discuss our findings and ways to test our predictions experimentally.

2
2.1

Estimation from correlated spike trains
The need for nonlinear dendritic operations

Ideally, the (subthreshold) dynamics of the somatic membrane potential, v(t), should implement
some nonlinear function, f (u(t)), of the presynaptic membrane potentials, u(t).1
?

dv(t)
= f (u(t))
dt

v(t)

(1)

However, the presynaptic membrane potentials cannot be observed directly, only the presynaptic
spike trains s0:t that are stochastic functions of the presynaptic membrane potential trajectories.
Therefore, to minimise squared error, the postsynaptic membrane potential should represent the
mean of the posterior over possible output function values it should be computing based on the input
spike trains:
Z
dv(t)
?
' f (u(t)) P(u(t)|s0:t ) du(t) v(t)
(2)
dt
Biophysically, to a first approximation, the somatic membrane potential of the postsynaptic neuron
can be described as some function(al), f?, of the local dendritic membrane potentials, vd (t)
?

dv(t)
dt

=

f? vd (t)

v(t)

(3)

This is interesting because Pfister et al. [11, 12] have recently suggested that short-term synaptic
plasticity arranges for each local dendritic postsynaptic potential, vid , to (approximately) represent
the posterior mean of the corresponding presynaptic membrane potential:
Z
d
vi (t) ' ui (t) P(ui (t)|si,0:t ) dui
(4)

Thus, it would be tempting to say that in order to achieve the computational goal of Eq. 2, the way
the dendritic tree (together with the soma) should integrate these local potentials, as given by f?,
should be directly determined by the function that needs to be computed: f? = f . However, it is easy
to see that in general this is going to be incorrect:
! Z
Z
Y
f
u(t)
P(ui (t)|si,0:t ) du(t) 6= f (u(t)) P(u(t)|s0:t ) du(t)
(5)
i

where the l.h.s. is what the neuron implements (eqs. 3-4) and the r.h.s. is what it should compute
(eq. 2). The equality does not hold in general when f is non-linear or P(u(t)|s0:t ) does not factorise.

In the following, we are going to consider
Pthe case when the function, f (u), is a purely linear
combination of synaptic inputs, f (u) =
i ci ui . Such linear transformations seem to suggest
linear dendritic operations and, in combination with a single global ?somatic? nonlinearity, they are
often assumed in neural network models and descriptive models of neuronal signal processing [10].
However, as we will show below, estimation from the spike trains of multiple correlated presynaptic
neurons requires a non-linear integration of inputs even in this case.
1
Dynamics of this form are assumed by many neural network models, though the variables u amd v are
usually interpreted as instantaneous firing rates rather than membrane potentials [10]. However, just as in our
case (Eq. 8), the two are often taken to be related through a simple non-linear function which thus makes the
two frameworks essentially isomorphic.

2

2.2

The mOU-NP model

We assume that the hidden dynamics of presynaptic membrane potentials are described by a multivariate Ornstein?Uhlenbeck (mOU) process (discretised in time into t ! 0 time bins, thus formally
yielding an AR(1) process):
p
1
iid
ut = ut t + (u0 ut t ) t + qt t,
qt ? N (0, Q)
(6)
?
p
t
= ?ut t + qt t + u0
(7)
?
where we described all neurons with the same parameters: u0 , the resting potential and ? , the
membrane time constant (with ? = 1 ?t ). Importantly, Q is the covariance matrix parametrising
the correlations between the subthreshold membrane potential fluctuations of presynaptic neurons.
Spiking is described by a nonlinear-Poisson (NP) process where the instantaneous firing rate, r, is
an exponential function of u with exponent and ?baseline rate? g:
r(u) = g e

u

(8)

and the number of spikes emitted in a time bin, s, is Poisson with this rate:
(9)

P(s|u) = Poisson(s; t r(u))
The spiking process itself is independent i.e., the likelihood is factorised across cells:
Y
P(s|u) =
P(si |ui )

(10)

i

2.3

Assumed density filtering in the mOU-NP model

Our goal is to derive the time evolution of the posterior distribution of the membrane potential,
P(ut |s0:t ), given a particular spiking pattern observed. Ultimately, we will need to compute some
function of u underPthis distribution. For linear computations (see above), the final quantity of
interest depends on i ci ui which in the limit (of many presynaptic cells) is going to be Gaussiandistributed, and as such only dependent on the first two moments of the posterior. This motivates
us to perform assumed density filtering, by which we substitute the true posterior with a momentmatched multivariate Gaussian in each time step, P(ut |s0:t ) ' N (ut ; ?t , ?t ).
After some algebra (see Appendix for details) we obtain the following equations for the time evolution of the mean and covariance of the posterior under the generative process defined by Eqs. 7-10:
??

=

?
?

=

1
(u0 ?) + ? (s(t)
)
?
2
2
(?OU ?)
? ?
?

(11)
(12)

where si (t) is the spike train of presynaptic neuron i represented as a sum of Dirac-delta functions,
2?

ii

( ) is a vector (diagonal matrix) whose elements i = ii = g e ?i + 2 are the estimated firing
rates of the neurons, and ?OU = Q?
2 is the prior covariance matrix of the presynaptic membrane
potentials in the absence of any observation.
2.4

Modelling correlated up and down states

The mOU-NP process is a convenient and analytically tractable way to model correlations between
presynaptic neurons but it obviously falls short of the dynamical complexity of cortical ensembles
in many respects. Following and expanding on [12], here we considered one extension that allowed
us to model coordinated changes between more hyper- and depolarised states across presynaptic
neurons, such as those brought about by cortical up and down states.
In this extension, the ?resting? potential of each presynaptic neuron, u0 , could switch between two
different values, uup and udown , and followed first order Markovian dynamics. Up and down states in
cortical neurons are not independent but occur synchronously [13]. To reproduce these correlations
3

??

?
mean

?
0.6

?1
?2

??

??

0.5
0.1

?

1.2

????

????

0

?????

v (mV)
0
12

-1.2

??

?????

??

?????
???????????
???

???

?????

??

variance

?0.6

???
?

?

???

????

???

200
?????????

?

?

??
????

???

Figure 1: Simulation of the optimal estimator in the case of two presynaptic spikes with different
time delays ( t). A: The posterior means (Aa), variances, ?ii , and the covariance, ?12 (Ab). The
dynamics of the postsynaptic membrane potential, v (Ad) is described by Eq. 1, where f (u) =
u1 + u2 (Ac). B: The same as A on an extended time scale. C: The nonlinear summation of
two EPSPs, characterised by the ratio of the actual EPSP (cyan on Ad) and the linear sum of two
individual EPSPs (grey on Ad) is shown for different delays and correlations between the presynaptic
neurons. The summation is sublinear if the presynaptic neurons are positively correlated, whereas
negative correlations imply supralinear summation.
we introduced a global, binary state variable, x that influenced the Markovian dynamics of the
resting potential of individual neurons (see Appendix and Fig. 2A). Unfortunately, an analytical
solution to the optimal estimator was out of reach in this case, so we resorted to particle filtering
[14] to compute the output of the optimal estimator.

3
3.1

Nonlinear dendrites as near-optimal estimators
Correlated Ornstein-Uhlenbeck process

First, we analysed the estimation problem in case of mOU dynamics where we could derive an optimal estimator for the membrane potential. Postsynaptic dynamics needed to follow the linear sum
of presynaptic membrane potentials. Figure 1 shows the optimal postsynaptic response (Eqs. 11-12)
after observing a pair of spikes from two correlated presynaptic neurons with different time delays.
When one of the cells (black) emits a spike, this causes an instantaneous increase not only in the
membrane potential estimate of the neuron itself but also in those of all correlated neurons (red neuron in Fig. 1Aa and Ba). Consequently, the estimated firing rate, , of both cells increases. Albeit
indirectly, a spike also influences the uncertainty about the presynaptic membrane potentials ? quantified by the posterior covariance matrix. A spike itself does not change this covariance directly, but
since it increases estimated firing rates, the absence of even more spikes in the subsequent period
becomes more informative. This increased information rate following a spike decreases estimator
uncertainty about true membrane potential values for a short period (Fig. 1Ab and Bb). However, as
the estimated firing rate decreases back to its resting value nearly exponentially after the spike, the
estimated uncertainty also returns back to its steady state.
Importantly, the instantaneous increase of the posterior means in response to a spike is proportional
to the estimated uncertainty about the membrane potentials and to the estimator?s current belief
about the correlations between the neurons. As each spike influences not only the mean estimate
of the membrane potentials of other correlated neurons but also the uncertainty of these estimates,
the effect of a spike from one cell on the posterior mean depends on the spiking history of all other
correlated neurons (Fig. 1Ac-Ad).
4

In the example shown in Fig. 1, the postsynaptic dynamics is required to compute a purely linear
sum of two presynaptic membrane potentials, f (u) = u1 + u2 . However, depending on the prior
correlation between the two presynaptic neurons and the time delay between the two spikes, the
amplitude of the postsynaptic membrane potential change evoked by the pair of spikes can be either
larger or smaller than the linear sum of the individual excitatory postsynaptic potentials (EPSPs)
(Fig. 1Ad, C). EPSPs from independent neurons are additive, but if the presynaptic neurons are positively correlated then their spikes convey redundant information and they are integrated sublinearly.
Conversely, simultaneous spikes from negatively correlated presynaptic neurons are largely unexpected and induce supralinear summation. The deviation from the linear summation is proportional
to the magnitude of the correlation between the presynaptic neurons (Fig. 1C).
We compared the nonlinear integration of the inputs in the optimal estimator with experiments measuring synaptic integration in the dendritic tree of neurons. For a passive membrane, cable theory
[15] implies that inputs are integrated linearly only if they are on electronically separated dendritic
branches, but reduction of the driving force entails a sublinear interaction between co-localised inputs. Moreover, it has been found that active currents, the IA potassium current in particular, also
contribute to the sublinear integration within the dendritic tree [16, 17]. Our model predicts that
inputs that are integrated sublinearly are positively correlated (Fig. 1C).
In sum, we can already see that correlated inputs imply nonlinear integration in the postsynaptic
neuron, and that the form of nonlinearity needs to be matched to the degree and sign of correlations between inputs. However, the finding that supralinear interactions are only expected from
anticorrelated inputs defeats biological intuition. Another shortcoming of the mOU model is related
to the second-order effects of spikes on the posterior covariance. As the covariance matrix does not
change instantaneously after observing a presynaptic spikes (Fig. 1B), two spikes arriving simultaneously are summed linearly (not shown). At the other extreme, two spikes separated by long delays
again do not influence each other. Therefore the nonlinearity of the integration of two spikes has a
non-monotonic shape, which again is unlike the monotonic dependence of the degree of nonlinearity
on interspike intervals found in experiments [18, 19]. In order to overcome these limitations, we extended the model to incorporate correlated changes in the activity levels of presynaptic neurons [13].
3.2

Correlated up and down states

While the statistics of presynaptic membrane potentials exhibit more complex temporal dependencies in the extended model (Fig. 2A), importantly, the task is still assumed to be the same simple
linear computation as before: f (u) = u1 + u2 .
However, the more complex P(u) P
distribution means that we need to sum over the possible values
of the hidden variables: P(u) =
u0 P(u|u0 ) P(u0 ). The observation of a spike changes both
the conditional distributions, P(u|u0 ), and the probability of being in the up state, P(u0 = uup ),
by causing an upward shift in both. A second spike causes a further increase in the membrane
potential estimate, and, more importantly, in the probability of being in the up state for both neurons.
Since the probability of leaving the up state is low, the membrane potential estimate decays back
to its steady state more slowly if the probability of being in the up state is high (Fig. 2B). This
causes a supralinear increase in the membrane potential of the postsynaptic neuron which again
depends on the interspike interval, but this time supralinearity is predicted for positively correlated
presynaptic neurons (Fig. 2C,E). Note, that while in the mOU model, supralinear integration arises
due to dynamical changes in uncertainty (of membrane potential estimates), in the extended model
it is associated with a change in a hypothesis (about hidden up-down states).
This is qualitatively similar to what was found in pyramidal neurons in the neocortex [19] and in the
hippocampus [18, 20] that are able to switch from (sub)linear to supralinear integration of synaptic
inputs through the generation of dendritic spikes [21]. Specifically, in neocortical pyramidal neurons
Polsky et al. [19] found, that nearly synchronous inputs arriving to the same dendritic branch evoke
substantially larger postsynaptic responses than expected from the linear sum of the individual responses (Fig. 2D-E). While there is a good qualitative match between model and experiments, the
time scales of integration are off by a factor of 2. Neverthless, given that we did not perform exhaustive parameter fitting in our model, just simply set parameters to values that produced realistic
presynaptic membrane potential trajectories (cf. our Fig. 2A with [13]), we regard the match acceptable and are confident that with further fine tuning of parameters the match would also improve
quantitatively.
5

?

???

1 ms

????
?????????

32 ms

????

????

2 mV

?

100 ms
50 ms

100 ms

?

2 mV
15 ms

50 ms
30 ms

50 ms

2
?4

?2

0

?????????
??????????????
????????????

?20

20
60
time (ms)

100

?20

??????????

20
60
time (ms)

100

?????

????????
??????????????

????????
??????????????

2

0 ms

up state probability
0.0
0.4
0.8

??

?

?

?? ??

?

???????????????????
4
6
8
10

?

Membrane potential (mV)

?

0

10

20
30
40
??????????????????

50

0

20

40
60
80
??????????????????

100

Figure 2: A: Example voltage traces and spikes from the modeled presynaptic neurons (black and
red) with correlated up and down states. The green line indicates the value of the global up-down
state variable. B: InferenceP
in the model: The posterior probability of being in the up state (left)
and the posterior mean of i ui after observing two spikes (grey) from different neurons with
t = 8 ms latency. C: Supralinear summation in the switching mOU-NP model. D: Supralinear
summation by dendritic spikes in a cortical pyramidal neuron. E: Peak amplitude of the response
(red) and the linear sum (black squares) is shown for different delays in experiments (left) and the
model (right). (D and left panel in E are reproduced from [19]).
3.3

Nonlinear dendritic trees are necessary for purely linear computations

In the previous sections we demonstrated that optimal inference based on correlated spike trains
requires nonlinear interaction within the postsynaptic neuron, and we showed that the dynamics of
the optimal estimator is qualitatively similar to the dynamics of the somatic membrane potential of
a postsynaptic neuron with nonlinear dendritic processing. In this section we will build a simplified
model of dendritic signal processing and compare its performance directly to several alternative
models (see below) on a purely linear task, for which the neuron needs to compute the sum of
P10
presynaptic membrane potentials: f (u) = i=1 ui .

We model the dendritic estimator as a two-layer feed-forward network of simple units (Fig. 3A)
that has been proposed to closely mimic the repertoire of input-output transformations achievable
by active dendritic trees [22]. In this model, synaptic inputs impinge on units in the first layer,
corresponding to dendritic branches, where nonlinear integration of inputs arriving to a dendritic
branch is modeled by a sigmoidal input-output function, and the outputs of dendritic branch units
are in turn summed linearly in the single (somatic) unit of the second layer. We trained the model to
estimate f by changing the connection weights of the two layers corresponding to synaptic weights
(wji ) and branch coupling strengths (?
cj , see Appendix, Fig. 3A).
We compared the performance of the dendritic estimator to four alternative models (Figure 3B):
1. The linear estimator, which is similar to the dendritic estimator except that the dendrites are
linear.
2. The independent estimator, in which the individual synapses are independently optimal estimators of the corresponding presynaptic membrane potentials (Eq. 4) [11, 12], and the cell
combines these estimates linearly. Note that the only difference between the independent estimator and the optimal estimator is the assumption implicit to the former that presynaptic cells
are independent.
3. The scaled independent estimator still combines the synaptic potentials linearly, but the weights
of each synapse are rescaled to partially correct for the wrong assumption of independence.
4. Finally, the optimal estimator is represented by the differential equations 11-12.
The performance of the different
estimators were quantified by the estimation error normalized by
P
h( i ui v
?estimator )2 i
P
the variance of the signal,
. Figure 3C shows the estimation error of the five differvar[ i ui ]
ent models in the case of 10 uniformly correlated presynaptic neurons. If the presynaptic neurons
6

0 .8

?

?

???????????????????????????
0 .4
0 .6

?

??????
???????????
??????????????????

?????????

0 .2

???????

0

0 .5
???????????

0 .9

Figure 3: Performance of 5 different estimators are compared in the task of estimating f (u) =
PN
i=1 ui . A: Model of the dendritic estimator. B: Different estimators (see text for more details).
C: Estimation error, normalised with the variance of the signal. The number of presynaptic neurons
were N = 10. Error bars show standard deviations.
were independent, all three estimators that used dynamical synapses (?
vind , v?sind and v?opt ) were optimal, whereas the linear estimator had substantially larger error. Interestingly, the performance of
the dendritic estimator (yellow) was nearly optimal even if the individual synapses were not optimal estimators for the corresponding presynaptic membrane potentials. In fact, adding depressing
synapses to the dendritic model degraded its performance because the sublinear effect introduced
by the saturation of the sigmoidal dendritic nonlinearity interfered with that implied by synaptic
depression. When the correlation increased between the presynaptic neurons, the performance of
the estimators assuming independence (black and orange) became severely suboptimal, whereas the
dendritic estimator (yellow) remained closer to optimal.
Finally, in order to investigate the synaptic mechanisms underlying the remarkably high performance
of the dendritic estimator, we trained a dendritic estimator on a task where the presynaptic neurons
formed two groups. Neurons from different groups were independent or negatively correlated with
each other, cor(ui , uk ) = { 0.6, 0.3, 0}, while there were positive correlations between neurons from the same group, cor(ui , uj ) = {0.3, 0.6, 0.9} (Fig. 4A). The postsynaptic neuron had
two dendritic branches, each of them receiving input from each presynaptic neurons initially. After
tuning synaptic weights and branch coupling strengths to minimize estimation error, and pruning
synapses with weights below threshold, the model achieved near-optimal performance as before
(Fig. 4C). More importantly, we found that the structure of the presynaptic correlations was reflected in the synaptic connection patterns on the dendritic branches: most neurons developed stable
synaptic weights only on one of the two dendritic branches, and synapses originating from neurons
within the same group tended to cluster on the same branch (Fig. 4B).

4

Discussion

In the present paper we introduced a normative framework to describe single neuron computation
that sheds new light on nonlinear dendritic information processing. Following [12], we observe that
spike-based communication causes information loss in the nervous system, and neurons must infer
the variables relevant for the computation [23?25]. As a consequence of this spiking bottleneck,
signal processing in single neurons can be conceptually divided into two parts: the inference of
the relevant variables and the computation itself. When the presynaptic neurons are independent
then synapses with short term plasticity can optimally solve the inference problem [12] and nonlinear processing in the dendrites is only for computation. However, neurons in a population are
often tend to be correlated [5, 13] and so the postsynaptic neuron should combine spike trains from
such correlated neurons in order to find the optimal estimate of its output. We demonstrated that
the solution of this inference problem requires nonlinear interaction between synaptic inputs in the
7

???????

??????????????????
?????? ????

??????

?

? ??

?

?????????

???????

????????????????

? ??
? ??

???????????????

?
?
?

? ??

? ??

????????????????

? ??

?

Figure 4: Synaptic connectivity reflects the correlation structure of the input. A: The presynaptic
covariance matrix is block-diagonal, with two groups (neurons 1?4 and 5?8). Initially, each presynaptic neuron innervates both dendritic branches, and the weights, w, of the static synapses are then
tuned to minimize estimation error. B: Synaptic weights after training, and pruning the weakest
synapses. Columns corresponds to solutions of the error-minimization task with different presynaptic correlations and/or initial conditions, and rows are different synapses. The detailed connectivity
patterns differ across solutions, but neurons from the same group usually all innervate the same dendritic branch. Below: fraction of neurons in each solution innervating 0, 1 or 2 branches. The height
of the yellow (blue, green) bar indicates the proportion of presynaptic neurons innervating two (one,
zero, respectively) branches of the postsynaptic neuron. C: After training, the nonlinear dendritic
estimator performs close to optimal and much better than the linear neuron.
postsynaptic cell even if the computation itself is purely linear. Of course, actual neurons are usually
faced with both problems: they will need to compute nonlinear functions of correlated inputs and
thus their nonlinearities will serve both estimation and computation. In such cases our approach
allows dissecting the respective contributions of active dendritic processing towards estimation and
computation.
We demonstrated that the optimal estimator of the presynaptic membrane potentials can be closely
approximated by a nonlinear dendritic tree where the connectivity from the presynaptic cells to the
dendritic branches and the nonlinearities in the dendrites are tuned according to the dependency
structure of the input. Our theory predicts that independent neurons will innervate distant dendritic domains, whereas neurons that have correlated membrane potentials will impinge on nearby
dendritic locations, preferentially on the same dendritic branches, where synaptic integration in
nonlinear [19, 26]. More specifically, the theory predicts sublinear integration between positively
correlated neurons and supralinear integration through dendritic spiking between neurons with correlated changes in their activity levels. To directly test this prediction the membrane potentials of
several neurons need to be recorded under naturalistic in vivo conditions [5, 13] and then the subcellular topography of their connectivity with a common postsynaptic target needs to be determined.
Similar approaches have been used recently to characterize the connectivity between neurons with
different receptive field properties in vivo [27, 28].
Our model suggests that the postsynaptic neuron should store information about the dependency
structure of its presynaptic partners within its dendritic membrane. Online learning of this information based on the observed spiking patterns requires new, presumably non-associative forms of plasticity such as branch strength potentiation [29, 30] or activity-dependent structural plasticity [31].
Acknowledgments
We thank J-P Pfister for valuable insights and comments on earlier versions of the manuscript, and
P Dayan, B Gutkin, and Sz K?ali for useful discussions. This work has been supported by the Hungarian Scientific Research Fund (OTKA, grant number: 84471, BU) and the Welcome Trust (ML).
8

References
1. Koch, C. Biophysics of computation (Oxford University Press, 1999).
2. Stuart, G., Spruston, N. & Hausser, M. Dendrites (Oxford University Press, 2007).
3. Poirazi, P. & Mel, B.W. Impact of active dendrites and structural plasticity on the memory capacity of
neural tissue. Neuron 29, 779?96 (2001).
4. Poirazi, P., Brannon, T. & Mel, B.W. Arithmetic of subthreshold synaptic summation in a model CA1
pyramidal cell. Neuron 37, 977?87 (2003).
5. Crochet, S., Poulet, J.F., Kremer, Y. & Petersen, C.C. Synaptic mechanisms underlying sparse coding of
active touch. Neuron 69, 1160?75 (2011).
6. Maass, W. & Bishop, C. Pulsed Neural Networks (MIT Press, 1998).
7. Gerstner, W. & Kistler, W. Spiking Neuron Models (Cambridge University Press, 2002).
8. Rieke, F., Warland, D., de Ruyter van Steveninck, R. & Bialek, W. Spikes (MIT Press, 1996).
9. Deneve, S. Bayesian spiking neurons I: inference. Neural Comput. 20, 91?117 (2008).
10. Dayan, P. & Abbot, L.F. Theoretical neuroscience (The MIT press, 2001).
11. Pfister, J., Dayan, P. & Lengyel, M. Know thy neighbour: a normative theory of synaptic depression. Adv.
Neural Inf. Proc. Sys. 22, 1464?1472 (2009).
12. Pfister, J., Dayan, P. & Lengyel, M. Synapses with short-term plasticity are optimal estimators of presynaptic membrane potentials. Nat. Neurosci. 13, 1271?1275 (2010).
13. Poulet, J.F. & Petersen, C.C. Internal brain state regulates membrane potential synchrony in barrel cortex
of behaving mice. Nature 454, 881?5 (2008).
14. Doucet, A., De Freitas, N. & Gordon, N. Sequential Monte Carlo Methods in Practice (Springer, New
York, 2001).
15. Rall, W. Branching dendritic trees and motoneuron membrane resistivity. Exp. Neurol. 1, 491?527 (1959).
16. Hoffman, D.A., Magee, J.C., Colbert, C.M. & Johnston, D. K+ channel regulation of signal propagation
in dendrites of hippocampal pyramidal neurons. Nature 387, 869?75 (1997).
17. Cash, S. & Yuste, R. Linear summation of excitatory inputs by CA1 pyramidal neurons. Neuron 22,
383?94 (1999).
18. Gasparini, S., Migliore, M. & Magee, J.C. On the initiation and propagation of dendritic spikes in CA1
pyramidal neurons. J. Neurosci. 24, 11046?56 (2004).
19. Polsky, A., Mel, B.W. & Schiller, J. Computational subunits in thin dendrites of pyramidal cells. Nat.
Neurosci. 7, 621?7 (2004).
20. Margulis, M. & Tang, C.M. Temporal integration can readily switch between sublinear and supralinear
summation. J. Neurophysiol. 79, 2809?13 (1998).
21. Hausser, M., Spruston, N. & Stuart, G.J. Diversity and dynamics of dendritic signaling. Science 290,
739?44 (2000).
22. Poirazi, P., Brannon, T. & Mel, B.W. Pyramidal neuron as two-layer neural network. Neuron 37, 989?99
(2003).
23. Huys, Q.J., Zemel, R.S., Natarajan, R. & Dayan, P. Fast population coding. Neural Comput. 19, 404?41
(2007).
24. Natarajan, R., Huys, Q.J.M., Dayan, P. & Zemel, R.S. Encoding and decoding spikes for dynamics stimuli.
Neural Computation 20, 2325?2360 (2008).
25. Gerwinn, S., Macke, J. & Bethge, M. Bayesian population decoding with spiking neurons. Frontiers in
Computational Neuroscience 3 (2009).
26. Losonczy, A. & Magee, J.C. Integrative properties of radial oblique dendrites in hippocampal CA1 pyramidal neurons. Neuron 50, 291?307 (2006).
27. Bock, D.D. et al. Network anatomy and in vivo physiology of visual cortical neurons. Nature 471, 177?82
(2011).
28. Ko, H. et al. Functional specificity of local synaptic connections in neocortical networks. Nature (2011).
29. Losonczy, A., Makara, J.K. & Magee, J.C. Compartmentalized dendritic plasticity and input feature storage
in neurons. Nature 452, 436?41 (2008).
30. Makara, J.K., Losonczy, A., Wen, Q. & Magee, J.C. Experience-dependent compartmentalized dendritic
plasticity in rat hippocampal CA1 pyramidal neurons. Nat. Neurosci. 12, 1485?7 (2009).
31. Butz, M., Worgotter, F. & van Ooyen, A. Activity-dependent structural plasticity. Brain Res. Rev. 60,
287?305 (2009).

9

"
373,"Generalization in decision trees and DNF:
Does size matter?
Mostefa Golea\ Peter L. Bartlett h , Wee Sun Lee2 and Llew Mason 1
1 Department of Systems Engineering
Research School of Information
Sciences and Engineering
Australian National University
Canberra, ACT, 0200, Australia

2 School of Electrical Engineering
University College UNSW
Australian Defence Force Academy
Canberra, ACT, 2600, Australia

Abstract
Recent theoretical results for pattern classification with thresholded real-valued functions (such as support vector machines, sigmoid networks, and boosting) give bounds on misclassification
probability that do not depend on the size of the classifier, and
hence can be considerably smaller than the bounds that follow from
the VC theory. In this paper, we show that these techniques can
be more widely applied, by representing other boolean functions
as two-layer neural networks (thresholded convex combinations of
boolean functions). For example, we show that with high probability any decision tree of depth no more than d that is consistent with
m training examples has misclassification probability no more than
o ( (~ (Neff VCdim(U) log2 m log d)) 1/2), where U is the class of
node decision functions, and Neff ::; N can be thought of as the
effective number of leaves (it becomes small as the distribution on
the leaves induced by the training data gets far from uniform).
This bound is qualitatively different from the VC bound and can
be considerably smaller.
We use the same technique to give similar results for DNF formulae.

? Author to whom correspondence should be addressed

260

1

M. Golea, P Bartlett, W. S. Lee and L Mason

INTRODUCTION

Decision trees are widely used for pattern classification [2, 7]. For these problems,
results from the VC theory suggest that the amount of training data should grow
at least linearly with the size of the tree[4, 3]. However, empirical results suggest
that this is not necessary (see [6, 10]). For example, it has been observed that the
error rate is not always a monotonically increasing function of the tree size[6].
To see why the size of a tree is not always a good measure of its complexity, consider
two trees, A with N A leaves and B with N B leaves, where N B ? N A . Although A
is larger than B, if most of the classification in A is carried out by very few leaves
and the classification in B is equally distributed over the leaves, intuition suggests
that A is actually much simpler than B, since tree A can be approximated well by
a small tree with few leaves. In this paper, we formalize this intuition.
We give misclassification probability bounds for decision trees in terms of a new
complexity measure that depends on the distribution on the leaves that is induced
by the training data, and can be considerably smaller than the size of the tree.
These results build on recent theoretical results that give misclassification probability bounds for thresholded real-valued functions, including support vector machines,
sigmoid networks, and boosting (see [1, 8, 9]), that do not depend on the size of the
classifier. We extend these results to decision trees by considering a decision tree as
a thresholded convex combination of the leaf functions (the boolean functions that
specify, for a given leaf, which patterns reach that leaf). We can then apply the
misclassification probability bounds for such classifiers. In fact, we derive and use
a refinement of the previous bounds for convex combinations of base hypotheses,
in which the base hypotheses can come from several classes of different complexity,
and the VC-dimension of the base hypothesis class is replaced by the average (under the convex coefficients) of the VC-dimension of these classes. For decision trees,
the bounds we obtain depend on the effective number of leaves, a data dependent
quantity that reflects how uniformly the training data covers the tree's leaves. This
bound is qualitatively different from the VC bound, which depends on the total
number of leaves in the tree.
In the next section, we give some definitions and describe the techniques used. We

present bounds on the misclassification probability of a thresholded convex combination of boolean functions from base hypothesis classes, in terms of a misclassification
margin and the average VC-dimension of the base hypotheses. In Sections 3 and 4,
we use this result to give error bounds for decision trees and disjunctive normal
form (DNF) formulae.

2

GENERALIZATION ERROR IN TERMS OF MARGIN
AND AVERAGE COMPLEXITY

We begin with some definitions. For a class ti of { -1,1 }-valued functions defined on
the input space X, the convex hull co(ti) ofti is the set of [-1, l]-valued functions of
the form :Ei aihi, where ai ~ 0, :Ei ai = 1, and hi E ti. A function in co(ti) is used
for classification by composing it with the threshold function, sgn : IR ~ {-I, I},
which satisfies sgn(a) = 1 iff a ~ O. So f E co(ti) makes a mistake on the pair
(x,y) E X x {-1,1} iff sgn(f(x? =F y. We assume that labelled examples (x,y)
are generated according to some probability distribution V on X x {-I, I}, and we
let Pv [E] denote the probability under V of an event E. If S is a finite subset
of Z, we let Ps [E] denote the empirical probability of E (that is, the proportion
of points in S that lie in E). We use Ev [.] and Es [.] to denote expectation in a
similar way. For a function class H of {-I, l}-valued functions defined on the input

261

Generalization in Decision Trees and DNF: Does Size Matter?

space X, the growth function and VC dimension of H will be denoted by IIH (m)
and VCdim(H) respectively.
In [8], Schapire et al give the following bound on the misclassification probability
of a thresholded convex combination of functions , in terms of the proportion of
training data that is labelled to the correct side of the threshold by some margin.
(Notice that Pv [sgn(f(x? # y] ~ Pv [yf(x) ~ 0].)
Theorem 1 ([8]) Let V be a distribution on X x {-I, I}, 1? a hypothesis class
with VCdim(H) = d < 00 , and 8> O. With probability at least 1- 8 over a training
set S of m examples chosen according to V, every function f E co(1?) and every
8> 0 satisfy

P v [yf(x) ~ 0] ~ Ps [yf(x) ~ 8] +

1 (dl 2( /d)
(
0..;m
og 82m

+ log(1/8)

)

1/2) .

In Theorem 1, all of the base hypotheses in the convex combination f are elements
of a single class 1? with bounded VC-dimension. The following theorem generalizes
this result to the case in which these base hypotheses may be chosen from any of k
classes, 1?1, ... , 1?k, which can have different VC-dimensions. It also gives a related
result that shows the error decreases to twice the error estimate at a faster rate.

Theorem 2 Let V be a distribution on X x {-I, I}, 1?1, ... ,1?k hypothesis classes
with VCdim(Hi) = di , and 8 > O. With probability at least 1 - 8 over a training

set S of m examples chosen according to V, every function f E co (U~=1 1?i) and
every 8 > 0 satisfy both
Pv [yf(x) ~ 0] ~ Ps [yf(x) ~ 8]

( 1 (1

o ..;m

+

82 (dlogm + logk) log (m8 2 /d) + log(1/8)

)1/2) ,

Pv [yf(x) ~ 0] ~ 2Ps [yf(x) ~ 8] +

o

(! (812

=

where d E ?aidj; and the ai and
jiE{l, ... ,k}.

(dlogm + logk) log (m8 2 /d) +IOg(1/8?)),

ji

are defined by f

= Ei aihi

and hi E 1?j; for

Proof sketch: We shall sketch only the proof of the first inequality of the theorem. The proof closely follows the proof of Theorem 1 (see [8]). We consider
N
a number of approximating sets of the form eN,1 = { (l/N) Ei=1 hi : hi E 1?1; ,
A

A

}

(h, ... , IN) E {I, ... , k}N and N E N. Define eN = Ul eN,I'
For a given f = Ei aihi from co (U~=1 1?i ), we shall choose an approximation
where I

=

9 E eN by choosing hI, .. . , hN independently from {hI, h 2 , ... ,}, according to
the distribution defined by the coefficients ai. Let Q denote this distribution
on eN. As in [8], we can take the expectation under this random choice of
9 E eN to show that, for any 8 > 0, Pv [yf(x) ~ 0] ~ Eg_Q [PD [yg(x) ~ 8/2]] +
exp(-N82/8). Now, for a given I E {I, .. . ,k}N, the probability that there is
a 9 in eN,1 and a 8 > 0 for which Pv [yg(x) ~ 8/2] > Ps [yg(x) ~ 8/2] + fN,1

is at most 8(N + 1) rr~1

(2:/7) d exp( -mf~,zl32).
l

;

Applying the union bound

M. Golea, P. Bartlett, W S. Lee andL Mason

262

(over the values of 1), taking expectation over 9
(

I'V

Q, and setting EN,l

=

~ In (8(N + 1) n~1 (2;;; )"". kN / 6N ) ) 1'2 shows that, with probability at least

1 - 6N, every f and 8 > 0 satisfy Pv [yf(x) ~ 0] ~ Eg [P s [yg(x) ~ 8/2]] +
As above, we can bound the probability inside the first expectation
in terms of Ps [yf(x) ~ 81. Also, Jensen's inequality implies that Eg [ENtd ~
(~ (In(8(N + 1)/6N) + Nln k + N L..i aidj; In(2em))) 1/2. Setting 6N = 6/(N(N +

Eg [EN,d.

1)) and N =

r/-I In ( mf) 1gives the result.

I

Theorem 2 gives misclassification probability bounds only for thresholded convex
combinations of boolean functions. The key technique we use in the remainder of the
paper is to find representations in this form (that is, as two-layer neural networks)
of more arbitrary boolean functions. We have some freedom in choosing the convex
coefficients, and this choice affects both the error estimate Ps [yf(x) ~ 81 and the
average VC-dimension d. We attempt to choose the coefficients and the margin 8
so as to optimize the resulting bound on misclassification probability. In the next
two sections, we use this approach to find misclassification probability bounds for
decision trees and DNF formulae.

3

DECISION TREES

A two-class decision tree T is a tree whose internal decision nodes are labeled with
boolean functions from some class U and whose leaves are labeled with class labels U
from {-I, +1}. For a tree with N leaves, define the leaf functions, hi : X -+ {-I, I}
by hi(X) = 1 iff x reaches leaf i, for i = 1, ... ,N. Note that hi is the conjunction
of all tests on the path from the root to leaf i.
For a sample S and a tree T, let Pi = Ps [hi(X) = 1]. Clearly, P = (PI, .. "" PN) is
a probability vector. Let Ui E {-I, + I} denote the class assigned to leaf i. Define
the class of leaf functions for leaves up to depth j as
1lj = {h :

h =

UI /\ U2 /\ ?.? /\ U r

Ir

~

j,

Ui

E U}.

It is easy to show that VCdim(1lj) ~ 2jVCdim(U) In(2ej). Let d i denote the depth
of leaf i, so hi E 1ld;, and let d = maxi di .
The boolean function implemented by a decision tree T can be written as a
thresholded convex combination of the form T(x) = sgn(f(x?, where f(x) =
L..~I WWi ?hi(x) + 1)/2) = L..~I WWi hi(X)/2 + L..~l wwd2, with Wi > 0 and
L..~I Wi = 1. (To be precise, we need to enlarge the classes 1lj slightly to be closed
under negation. This does not affect the results by more than a constant.) We first
assume that the tree is consistent with the training sample. We will show later how
the results extend to the inconsistent case.
The second inequality of Theorem 2 shows that, for fixed 6 > 0 there is a constant c such that, for any distribution V, with probability at least 1 - 6 over
the sample S we have Pv [T(x) 'I y] ~ 2Ps [yf(x) ~ 8] + L~I widiB, where
B = ~ VCdim(U) log2 m log d. Different choices of the WiS and the 8 will yield different estimates of the error rate of T. We can assume (wlog) that PI ~ ... ~ PN.
A natural choice is Wi = Pi and Pj+I ::.; 8 < Pj for some j E {I, ... ,N} which gives

-b

Pv [T(x)

'I y] ~

2

LN
i=j+I

Pi

dB

+ (i2'

(1)

Generalization in Decision Trees and DNF: Does Size Matter?

263

where d = L:~1 Pidi . We can optimize this expression over the choices of j E
{I ... ,N} and () to give a bound on the misclassification probability of the tree.
Let pep, U) = L:~1 (Pi - IIN)2 be the quadratic distance between the prob-ability vector P = (PI, ... ,PN ) and the uniform probability vector U =
(liN, liN, ... , liN). Define Neff == N (1 - pep,
The parameter Neff is a measure of the effective number of leaves in the tree.

U?.

Theorem 3 For a fixed d > 0, there is a constant c that satisfies the following. Let
V be a distribution on X x { -1, I}. Consider the class of decision trees of depth 'Up
to d, with decision functions in U. With probability at least 1 - d over the training
set S (of size mY, every decision tree T that is consistent with S has
Pv [T(x)

1= y] ~ c ( Neff VCdlm(~ log m log d
?

2

) 1/2

,

where Neff is the effective number of leaves of T.

Proof: Supposing that () ~ (aIN)I/2 we optimize (1) by choice of (). If the chosen
() is actually smaller than ca/N)I/2 then we show that the optimized bound still
holds by a standard VC result. If () ~ (a/N)I/2 then L:~i+l Pi ~ (}2 Neff/d. So (1)
implies that P v [T (x) 1= y] ~ 2(}2 Neff /d + dB / (}2. The optimal choice of () is then
(~iB/Neff)I/4. So if (~iB/Neff)I/4 ~ (a/N)I/2, we have the result. Otherwise,
the upper bound we need to prove satisfies 2(2NeffB)I/2 > 2NB, and this result is
implied by standard VC results using a simple upper bound for the growth function
of the class of decision trees with N leaves. I

Thus the parameters that quantify the complexity of a tree are: a) the complexity
of the test function class U, and b) the effective number of leaves Neff. The effective
number of leaves can potentially be much smaller than the total number of leaves
in the tree [5]. Since this parameter is data-dependent, the same tree can be simple
for one set of PiS and complex for another set of PiS.
For trees that are not consistent with the training data, the procedure to estimate
the error rate is similar. By defining Qi = Ps [YO'i = -1 I hi(x) = 1] and PI =
Pi (l- Qi)/ (1 - Ps [T(x) 1= V]) we obtain the following result.
Theorem 4 For a fixed d > 0, there is a constant c that satisfies the following. Let
V be a distribution on X x { -1, 1}. Consider the class of decision trees of depth up
to d, with decision functions in U. With probability at least 1 - d over the training
set S (of size mY, every decision tree T has
Pv [T(x)

1= y] ~ Ps [T(x) 1= y] + c ( Neff VCdim ~ log mlogd

where c is a universal constant, and Neff = N(1of leaves ofT.

.

()

pep', U?

2

) 1/3

,

is the effective number

Notice that this definition of Neff generalizes the definition given before Theorem 3.

4

DNF AS THRESHOLDED CONVEX COMBINATIONS

A DNF formula defined on {-1, I}n is a disjunction of terms, where each term is a
conjunction of literals and a literal is either a variable or its negation. For a given
DNF formula g, we use N to denote the number of terms in g, ti to represent the ith

M. Golea, P. Bartlett, W S. Lee and L Mason

264

term in f, Li to represent the set of literals in ti, and Ni the size of L i . Each term ti
can be thought of as a member of the class HNi' the set of monomials with Ni literals. Clearly, IHi I =
The DNF 9 can be written as a thresholded convex combi-

et).

nation of the form g(x) = -sgn( - f(x)) = -sgn ( - L:f:,l Wi ?ti + 1)/2)) . (Recall
that sgn(a) = 1 iff a ~ 0.) Further, each term ti can be written as a thresholded convex combination of the form ti(X) = sgn(Ji(x)) = sgn (L:lkELi Vik ?lk(x) - 1)/2)) .
Assume for simplicity that the DNF is consistent (the results extend easily to the
inconsistent case). Let ')'+ (')'-) denote the fraction of positive (negative) examples under distribution V. Let P v + [.] (P v - [.]) denote probability with respect
to the distribution over the positive (negative) examples, and let Ps+ [.] (P s - [.])
be defined similarly, with respect to the sample S. Notice that P v [g(x) :f:. y] =
')'+Pv+ [g(x) = -l]+,),-P v - [(3i)ti(X) = 1], so the second inequality of Theorem 2
shows that, with probability at least 1- 8, for any 8 and any 8i s,

N ( 2P s - [- fi(x) ~ 8i ] + 8;
B)
Pv [g(x) :f:. y] :::; ')'+ ( 2P s + [I(x) :::; 8] + dB)
? + ')'- ~
where d = L:f:,l WiNi and B = c(lognlog2m+log(N/8)) /m. As in the case of
decision trees, different choices of 8, the 8is, and the weights yield different estimates
of the error. For an arbitrary order of the terms, let Pi be the fraction of positive
examples covered by term ti but not by terms ti-l, ... ,tl' We order the terms such
that for each i, with ti-l. ... ,tl fixed, Pi is maximized, so that PI 2:: ... ~ PN,
and we choose Wi = Pi. Likewise, for a given term ti with literals 11,'"" ,LN. in an
arbitrary order, let p~i) be the fraction of negative examples uncovered by literal
lk but not uncovered by lk-l, ... ,11' We order the literals of term ti in the same
greedy way as above so that pi i) ~ ... 2:: P~:, and we choose Vik = P~ i). For

PHI:::; 8 < Pi and pLiL ~ 8i < Pi~iL, where 1 :::; j :::; Nand 1 ~ ji :::; Ni, we get

N
dB)
N (N'
.
B)
P D [g(x) :f:. y] :::; ')'+ ( 2 i~l Pi + ?
+ ')'- ~ 2 kf+l p~,) + 8;
Now, let P

= (Pl,,,,,PN)

and for each term i let p(i)

= (pii), ... ,p~:).

Define

Neff = N(1 - pcP, U)) and N~~ = N i (1 - p(p(i) , U)), where U is the relevant
uniform distribution in each case. The parameter Neff is a measure of the effective
number of terms in the DNF formula. It can be much smaller than N; this would be
the case if few terms cover a large fraction of the positive examples. The parameter
N~~ is a measure of the effective number of literals in term ti. Again, it can be
much smaller than the actual number of literals in ti: this would be the case if few
literals of the term uncover a large fraction of the negative examples.
Optimizing over 8 and the 8i s as in the proof of Theorem 3 gives the following
result.
Theorem 5 For a fixed 8 > 0, there is a constant c that satisfies the following. Let
V be a distribution on X x {-I, I}. Consider the class of DNF formuLae with up to
N terms. With probabiLity at Least 1 - 8 over the training set S (of size mY, every
DNF formulae 9 that is consistent with S has
N

PD [g(x):f:. y]:::; ,),+(NeffdB)1/2

where d

= maxf:.l N i , ')'?

+ ')'-I~)N~~B)1/2

i=l
= Pv [y = ?1] and B = c(lognlog2 m + log(N/8))/m.

Generalization in Decision Trees and DNF: Does Size Matter?

5

265

CONCLUSIONS

The results in this paper show that structural complexity measures (such as size) of
decision trees and DNF formulae are not always the most appropriate in determining
their generalization behaviour, and that measures of complexity that depend on the
training data may give a more accurate descriptirm. Our analysis can be extended
to multi-class classification problems. A similar analysis implies similar bounds
on misclassification probability for decision lists, and it seems likely that these
techniques will also be applicable to other pattern classification methods.
The complexity parameter, Neff described here does not always give the best possible error bounds. For example, the effective number of leaves Neff in a decision tree
can be thought of as a single number that summarizes the probability distribution
over the leaves induced by the training data. It seems unlikely that such a number
will give optimal bounds for all distributions. In those cases, better bounds could be
obtained by using numerical techniques to optimize over the choice of (J and WiS. It
would be interesting to see how the bounds we obtain and those given by numerical
techniques reflect the generalization performance of classifiers used in practice.
Acknowledgements

Thanks to Yoav Freund and Rob Schapire for helpful comments.

References
[1] P. L. Bartlett. For valid generalization, the size of the weights is more important
than the size of the network. In Neural Information Processing Systems 9, pages
134-140. Morgan Kaufmann, San Mateo, CA, 1997.
[2] L. Breiman, J.H . Friedman, R.A. Olshen, and C.J. Stone. Classification and
Regression Trees. Wadsworth, Belmont, 1984.
[3] A. Ehrenfeucht and D. Haussler. Learning decision trees from random examples. Information and Computation, 82:231-246, 1989.
[4] U .M. Fayyad and K.B. Irani. What should be .'1inimized in a decision tree?
In AAAI-90, pages 249-754,1990.
[5] R. C. Holte. Very simple rules perform well on most commonly used databases.
Machine learning, 11:63-91, 1993.
[6] P.M. Murphy and M.J. pazzani. Exploring the decision forest: An empirical
investigation of Occam's razor in decision tree induction. Journal of Artificial
Intelligence Research, 1:257-275, 1994.
[7] J.R. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann,
1992.
[8] R. E. Schapire, Y. Freund, P. L. Bartlett, and W. S. Lee. Boosting the margin:
a new explanation for the effectiveness of voting methods. In Machine Learning:
Proceedings of the Fourteenth International Conference, pages 322-330, 1997.
[9] J. Shawe-Taylor, P. L. Bartlett, R. C. Williamson, and M. Anthony. A framework for structural risk minimisation. In Proc. 9th COLT, pages 68-76. ACM
Press, New York, NY, 1996.
[10] G.L. Webb. Further experimental evidence against the utility of Occam's razor.
Journal of Artificial Intelligence Research, 4:397-417, 1996.

"
5196,"Matrix Completion from Fewer Entries:
Spectral Detectability and Rank Estimation

Alaa Saade1 and Florent Krzakala1,2
Laboratoire de Physique Statistique, CNRS & ?cole Normale Sup?rieure, Paris, France.
2
Sorbonne Universit?s, Universit? Pierre et Marie Curie Paris 06, F-75005, Paris, France

1

Lenka Zdeborov?
Institut de Physique Th?orique, CEA Saclay and CNRS UMR 3681, 91191 Gif-sur-Yvette, France

Abstract
The completion of low rank matrices from few entries is a task with many practical
applications. We consider here two aspects of this problem: detectability, i.e. the
ability to estimate the rank r reliably from the fewest possible random entries,
and performance in achieving small reconstruction error. We propose a spectral
algorithm for these two tasks called MaCBetH (for Matrix Completion with the
Bethe Hessian). The rank is estimated as the number of negative eigenvalues of
the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial
condition for the minimization of the discrepancy between the estimated matrix
and the revealed entries. We analyze the performance in a random matrix setting
using results from the statistical mechanics of the Hopfield neural network, and
show in particular that?MaCBetH efficiently detects the rank r of a large n ?
m matrix from C(r)r nm entries, where C(r) is a constant close to 1. We
also evaluate the corresponding root-mean-square error empirically and show that
MaCBetH compares favorably to other existing approaches.
Matrix completion is the task of inferring the missing entries of a matrix given a subset of known
entries. Typically, this is possible because the matrix to be completed has (at least approximately)
low rank r. This problem has witnessed a burst of activity, see e.g. [1, 2, 3], motivated by many
applications such as collaborative filtering [1], quantum tomography [4] in physics, or the analysis
of a covariance matrix [1]. A commonly studied model for matrix completion assumes the matrix
to be exactly low rank, with the known entries chosen uniformly at random and observed without
noise. The most widely considered question in this setting is how many entries need to be revealed
such that the matrix can be completed exactly in a computationally efficient way [1, 3]. While our
present paper assumes the same model, the main questions we investigate are different.
The first question we address is detectability: how many random entries do we need to reveal in order
to be able to estimate the rank r reliably. This is motivated by the more generic problem of detecting
structure (in our case, low rank) hidden in partially observed data. It is reasonable to expect the
existence of a region where exact completion is hard or even impossible yet the rank estimation is
tractable. A second question we address is what is the minimum achievable root-mean-square error
(RMSE) in estimating the unknown elements of the matrix. In practice, even if exact reconstruction
is not possible, having a procedure that provides a very small RMSE might be quite sufficient.
In this paper we propose an algorithm called MaCBetH that gives the best known empirical performance for the two tasks above when the rank r is small. The rank in our algorithm is estimated as the
number of negative eigenvalues of an associated Bethe Hessian matrix [5, 6], and the corresponding
eigenvectors are used as an initial condition for the local optimization of a cost function commonly
considered in matrix completion (see e.g. [3]). In particular, in the random matrix setting, we show
1

?
that MaCBetH detects the rank of a large n ? m matrix from C(r)r nm entries, where C(r) is a
small constant, see Fig.?2, and C(r) ? 1 as r ? ?. The RMSE is evaluated empirically and, in the
regime close to C(r)r nm, compares very favorably to existing approache such as OptSpace [3].
This paper is organized as follows. We define the problem and present generally our approach in the
context of existing work in Sec. 1. In Sec. 2 we describe our algorithm and motivate its construction
via a spectral relaxation of the Hopfield model of neural network. Next, in Sec. 3 we show how the
performance of the proposed spectral method can be analyzed using, in parts, results from spin glass
theory and phase transitions, and rigorous results on the spectral density of large random matrices.
Finally, in Sec. 4 we present numerical simulations that demonstrate the efficiency of MaCBetH.
Implementations of our algorithms in the Julia and Matlab programming languages are available at
the SPHINX webpage http://www.lps.ens.fr/~krzakala/WASP.html.

1

Problem definition and relation to other work

Let Mtrue be a rank-r matrix such that
Mtrue = XY T ,

(1)

where X ? Rn?r and Y ? Rm?r are two (unknown) tall matrices. We observe only a small
fraction of the elements of Mtrue , chosen uniformly at random. We call E the subset of observed
entries, and M the (sparse) matrix supported on E whose nonzero elements are the revealed entries
T
of Mtrue . The aim is to reconstruct the rank r matrix Mtrue = XY
given M. An important
?
parameter which controls the difficulty of the problem is  = |E|/ nm. In the case of a square
matrix M, this is the average number of revealed entries per line or column.
In our numerical examples and theoretical justifications we shall generate the low rank matrix
Mtrue = XY T , using tall matrices X and Y with iid Gaussian elements, we call this the random matrix setting. The MaCBetH algorithm is, however, non-parametric and does not use any
prior knowledge about X and Y . The analysis we perform applies to the limit n ? ? while
m/n = ? = O(1) and r = O(1).
The matrix completion problem was popularized in [1] who proposed nuclear norm minimization
as a convex relaxation of the problem. The algorithmic complexity of the associated semidefinite
programming is, however, O(n2 m2 ). A low complexity procedure to solve the problem was later
proposed by [7] and is based on singular value decomposition (SVD). A considerable step towards
theoretical understanding of matrix completion from few entries was made in [3] who proved that
with the use of trimming
pthe performance of SVD-based matrix completion can be improved and a
RMSE proportional to nr/|E| can be achieved. The algorithm of [3] is referred to as OptSpace,
and empirically it achieves state-of-the-art RMSE in the regime of very few revealed entries.
OptSpace proceeds in three steps [3]. First, one trims the observed matrix M by setting to zero all
rows (resp. columns) with more revealed entries than twice the average number of revealed entries
per row (resp. per column). Second, a singular value decompositions is performed on the matrix
and only the first r components are kept. When the rank r is unknown it is estimated as the index for
which the ratio between two consecutive singular values has a minimum. Third, a local minimization
of the discrepancy between the observed entries and the estimate is performed. The initial condition
for this minimization is given by the first r left and right singular vectors from the second step.
In this work we improve upon OptSpace by replacing the first two steps by a different spectral
procedure that detects the rank and provides a better initial condition for the discrepancy minimization. Our method leverages on recent progress made in the task of detecting communities in the
stochastic block model [8, 5] with spectral methods. Both in community detection and matrix completion, traditional spectral methods fail in the very sparse regime due to the existence of spurious
large eigenvalues (or singular values) corresponding to localized eigenvectors [8, 3]. The authors
of [8, 5, 9] showed that using the non-backtracking matrix or the closely related Bethe Hessian as
a basis for the spectral method in community detection provides reliable rank estimation and better
inference performance. The present paper provides an analogous improvement for the matrix completion problem. In particular, we shall analyze the algorithm using tools from spin glass theory in
statistical mechanics, and show that there exists a phase transition between a phase where it is able
to detect the rank, and a phase where it is unable to do so.
2

2
2.1

Algorithm and motivation
The MaCBetH algorithm

A standard approach to the completion problem (see e.g. [3]) is to minimize the cost function
X
min
[Mij ? (XY T )ij ]2
X,Y

(2)

(ij)?E

over X ? Rn?r and Y ? Rm?r . This function is non-convex, and global optimization is hard.
One therefore resorts to a local optimization technique with a careful choice of the initial conditions
X0 , Y0 . In our method, given the matrix M, we consider a weighted bipartite undirected graph with
adjacency matrix A ? R(n+m)?(n+m)


0
M
A=
.
(3)
MT
0
We will refer to the graph thus defined as G. We now define the Bethe Hessian matrix H(?) ?
R(n+m)?(n+m) to be the matrix with elements
!
X
1
2
Hij (?) = 1 +
sinh ?Aik ?ij ? sinh(2?Aij ) ,
(4)
2
k??i

where ? is a parameter that we will fix to a well-defined value ?SG depending on the data, and ?i
stands for the neighbors of i in the graph G. Expression (4) corresponds to the matrix introduced in
[5], applied to the case of graphical model (6). The MaCBetH algorithm that is the main subject of
this paper is then, given the matrix A, which we assume to be centered:
Algorithm (MaCBetH)
1. Numerically solve for the value of ??SG such that F (??SG ) = 1, where
X
1
F (?) := ?
tanh2 (?Mij ) .
nm

(5)

(i,j)?E

2. Build the Bethe Hessian H(??SG ) following eq. (4).
3. Compute all its negative eigenvalues ?1 , ? ? ? , ?r? and corresponding eigenvectors
v1 , ? ? ? , vr?. r? is our estimate for the rank r. Set X0 (resp. Y0 ) to be the first n lines
(resp. the last m lines) of the matrix [v1 v2 ? ? ? vr?].
4. Perform local optimization of the cost function (2) with rank r? and initial condition X0 , Y0 .
In step 1, ??SG is an approximation of the optimal value of ?, for which H(?) has a maximum number
of negative eigenvalues (see section 3). Instead of this approximation, ? can be chosen in such a
way as to maximize the number of negative eigenvalues. We however observed numerically that
the algorithm is robust to some imprecision on the value of ??SG . In step 2 we could also use the
non-backtracking matrix weighted by tanh ?Mij , it was shown in [5] that the spectrum of the Bethe
Hessian and the non-backtracking matrix are closely related. In the next section, we will motivate
and analyze this algorithm (in the setting where Mtrue was generated from element-wise random
X and Y ) and show that in this case MaCBetH is able to infer the rank whenever  > c . Fig. 1
illustrates the spectral properties of the Bethe Hessian that justify this algorithm: the spectrum is
composed of a few informative negative eigenvalues, well separated from the bulk (which remains
positive). In particular, as observed in [8, 5], it avoids the spurious eigenvalues with localized
eigenvectors that make trimming necessary in the case of [3]. This algorithm is computationally
efficient as it is based on the eigenvalue decomposition of a sparse, symmetric matrix.
2.2

Motivation from a Hopfield model

We shall now motivate the construction of the MaCBetH algorithm from a graphical model perspective and a spectral relaxation. Given the observed matrix M from the previous section, we consider
3

the following graphical model
?
?
X
1
P ({s}, {t}) = exp ??
Mij si tj ? ,
Z

(6)

(i,j)?E

where the {si }1?i?n and {tj }1?j?m are binary variables, and ? is a parameter controlling the
strength of the interactions. This model is a (generalized) Hebbian Hopfield model on a bipartite
sparse graph, and is therefore known to have r modes (up to symmetries) correlated with the lines of
X and Y [10]. To study it, we can use the standard Bethe approximation which is widely believed
to be exact for such problems on large random graphs [11, 12]. In this approximation the means
E(si ), E(tj ) and moments E(si tj ) of each variable are approximated by the parameters bi , cj and
?ij that minimize the so-called Bethe free energy FBethe ({bi }, {cj }, {?ij }) that reads
X
X X  1 + bi si + cj tj + ?ij si tj 
FBethe ({bi }, {cj }, {?ij }) = ?
Mij ?ij +
?
4
s ,t
(i,j)?E

n
X

(i,j)?E

i

j

m
X

X  1 + bi si 
X  1 + cj t j 
+
+
,
(1 ? di ) ?
(1 ? dj ) ?
2
2
s
t
i=1
j=1
i

(7)

j

where ?(x) := x ln x, and di , dj are the degrees of nodes i and j in the graph G. Neural network
models such as eq. (6) have been extensively studied over the last decades (see e.g. [12, 13, 14, 15,
16] and references therein) and the phenomenology, that we shall review briefly here, is well known.
In particular, for ? small enough, the global minimum of the Bethe free energy corresponds to the
so-called paramagnetic state
?i, j,

bi = cj = 0, ?ij = tanh (?Mij ).

(8)

As we increase ?, above a certain value ?R , the model enters a retrieval phase, where the free energy
has local minima correlated with the factors X and Y . There are r local minima, called retrieval
l
states ({bli }, {clj }, {?ij
}) indexed by l = 1, ? ? ? , r such that, in the large n, m limit,
n

?l = 1 ? ? ? r,

1X
Xi,l bli > 0,
n i=1

m

1X
Yj,l clj > 0 .
m j=1

(9)

These retrieval states are therefore convenient initial conditions for the local optimization of eq. (2),
and we expect their number to tell us the correct rank. Increasing ? above a critical value ?SG the
system eventually enters a spin glass phase, marked by the appearance of many spurious minima.
It would be tempting to continue the Bethe approach leading to belief propagation, but we shall
instead consider a simpler spectral relaxation of the problem, following the same strategy as used
in [5, 6] for graph clustering. First, we use the fact that the paramagnetic state (8) is always a
stationary point of the Bethe free energy, for any value of ? [17, 18]. In order to detect the retrieval
states, we thus study its stability by looking for negative eigenvalues of the Hessian of the Bethe
free energy evaluated at the paramagnetic state (8). At this point, the elements of the Hessian
involving one derivative with respect to ?ij vanish, while the block involving two such derivatives
is a diagonal positive definite matrix [5, 17]. The remaining part is the matrix called Bethe Hessian
in [5] (which however considers a different graphical model than (6)). Eigenvectors corresponding
to its negative eigenvalues are thus expected to give an approximation of the retrieval states (9). The
picture exposed in this section is summarized in Figure 1 and motivates the MaCBetH algorithm.
Note that a similar approach was used in [16] to detect the retrieval states of a Hopfield model using
the weighted non-backtracking matrix [8], which linearizes the belief propagation equations rather
than the Bethe free energy, resulting in a larger, non-symmetric matrix. The Bethe Hessian, while
mathematically closely related, is also simpler to handle in practice.

3

Analysis of performance in detection

We now show how the performance of MaCBetH can be analyzed, and the spectral properties of the
matrix characterized using both tools from statistical mechanics and rigorous arguments.
4

7
6

4

1.2

0.9

3

0
0.7

0
0.2

0.8

0.2

0.9

0.4

0.6

0.5

?

1

0

1.2

? = 0.12824

0.09
0.2
0
-0.5

0

1

2

3

?

4

0

0.25
-1

0

5

6

0.5

7

0.5
-0.5

0

0.5

1

1.5

2

2.5

? = 0.25

0.12
0.03
0.08
0

0.04
0

8

?

Direct diag
BP
0.06

0.16

0.3

0.1

0

-1.5

0.2

?(?)

?(?)

0.8

Direct diag
0.18
BP

0.4

0

0.15

0.6
0.4

2
1

0.8

? = 0.05

Direct diag
BP

0.3

1

?(?)

?(?)

5

1.4

? = 0.01

Direct diag
BP

1.8

0

5

10

0

0.6

?

15

1.2
20

25

Figure 1: Spectral density of the Bethe Hessian for various values of the parameter ?. Red dots
are the result of the direct diagonalisation of the Bethe Hessian for a rank r = 5 and n = m = 104
matrix, with  = 15 revealed entries per row on average. The black curves are the solutions of (18)
computed with belief propagation on a graph of size 105 . We isolated the 5 smallest eigenvalues,
represented as small bars for convenience, and the inset is a zoom around these smallest eigenvalues.
For ? small enough (top plots), the Bethe Hessian is positive definite, signaling that the paramagnetic
state (8) is a local minimum of the Bethe free energy. As ? increases, the spectrum is shifted towards
the negative region and has 5 negative eigenvalues at the approximate value of ??SG = 0.12824 (to
be compared to ?R = 0.0832 for this case) evaluated by our algorithm (lower left plot). These
eigenvalues, corresponding to the retrieval states (9), become positive and eventually merge in the
bulk as ? is further increased (lower right plot), while the bulk of uninformative eigenvalues remains
at all values of ? in the positive region.
3.1

Analysis of the phase transition

We start by investigating the phase transition above which our spectral method will detect the correct
rank. Let xp = (xlp )1?l?r , yp = (ypl )1?l?r be random vectors with the same empirical distribution
as the lines of X and Y respectively. Using the statistical mechanics correspondence between the
negative eigenvalues of the Bethe Hessian and the appearance of phase transitions in model (6), we
can compute the values ?R and ?SG where instabilities towards, respectively, the retrieval states and
the spurious glassy states, arise. We have repeated the computations of [13, 14, 15, 16] in the case
of model (6), using the cavity method [12]. We refer the reader interested in the technical details of
the statistical mechanics approach to neural networks to [14, 15, 16].
Following a standard computation for locating phase transitions in the Bethe approximation (see e.g.
[12, 19]), the stability of the paramagnetic state (8) towards these two phases can be monitored in
terms of the two following parameters:
s
r
r
1
hY
 X

 X
i 2s
,
(10)
?(?) = lim E
tanh2 ? xlp ypl tanh2 ? xlp+1 ypl
s??

p=1

l=1

l=1

s
r
r
1
hY



i 2s
X
X
?(?) = lim E
tanh ?|x1p yp1 | + ? xlp ypl tanh ?|x1p+1 yp1 | + ? xlp+1 ypl
,
s??

p=1

l=2

(11)

l=2

where the expectation is over the distribution of the vectors xp , yp . The parameter ?(?) controls the
sensitivity of the paramagnetic solution to random noise, while ?(?) measures its sensitivity to a
perturbation in the direction of a retrieval state. ?SG and ?R are defined implicitly as ?(?SG ) = 1
and ?(?R ) = 1, i.e. the value beyond which the perturbation diverges. The existence of a retrieval
phase is equivalent to the condition ?SG > ?R , so that there exists a range of values of ? where the
retrieval states exist, but not the spurious ones. If this condition is met, by setting ? = ?SG in our
algorithm, we ensure the presence of meaningful negative eigenvalues of the Bethe Hessian.
5

We define the critical value of  = c such that ?SG > ?R if and only if  > c . In general, there is
no closed-form formula for this critical value, which is defined implicitly in terms of the functions ?
and ?. We thus computed c numerically using a population dynamics algorithm [12] and the results
?
for C(r) = c /r are presented on Figure 2. Quite remarkably, with the definition  = |E|/ nm,
the critical value c does not depend on the ratio m/n, only on the rank r.
1.5

C(r)
C(r ? ?)
1 + 0.812 r?3/4

1.4
1.3

C(r)

In the limit of large  and r it is possible to
obtain a simple closed-form formula. In this
case the observed entries of the matrix become
jointly Gaussian distributed, and uncorrelated,
and therefore independent. Expression (10)
then simplifies to
r
h
 X
i
?(?) =r?? E tanh2 ? xl y l . (12)

1.2
1.1
1

l=1
0.9

Note that the MaCBetH algorithm uses an empirical estimator F (?) ' ?(?) (5) of this
quantity to compute an approximation ??SG of
?SG purely from the revealed entries. In the
large r,  regime, both ?SG , ?R decay to 0, so
that we can further approximate

5

10

15

20

25

r

Figure 2: Location of the critical value as a function of the rank r. MaCBetH is able
? to estimate
the correct rank from |E| > C(r)r nm known
entries. We used a population dynamics algorithm
6
2
1 = ?(?SG ) ?r?? r?SG
E[x2 ]E[y 2 ] , (13) with a population of size 10 to compute the funcp
tions ? and ? from (10,11). The dotted line is a fit
1 = ?(?R ) ?r?? ?R E[x2 ]E[y 2 ] , (14) suggesting that C(r) ? 1 = O(r?3/4 ).
so that we reach the simple asymptotic expression, in the large , r limit, that c = r, or equivalently C(r) = 1. Interestingly, this result was
obtained as the detectability threshold in completion of rank r = O(n) matrices from O(n2 ) entries
in the Bayes optimal setting in [20].
? Notice, however, that exact completion in the setting of [20] is
only possible for  > r(m+n)/ nm: clearly detection and exact completion are different phenomena. The previous analysis can be extended beyond the random setting assumption, as long as the
empirical distribution of the entries is well defined, and the lines of X (resp. Y ) are approximately
orthogonal and centered. This condition is related to the standard incoherence property [1, 3].
3.2

Computation of the spectral density

In this section, we show how the spectral density of the Bethe Hessian can be computed analytically
on tree-like graphs such as those generated by picking uniformly at random the observed entries of
the matrix XY T . This further motivates our algorithm and in particular our choice of ? = ??SG ,
independently of section 3. The spectral density is defined as
?(?) =

n+m
1 X
?(? ? ?i ) ,
n,m?? n + m
i=1

lim

(15)

where the ?i ?s are the eigenvalues of the Bethe Hessian. Using again the cavity method, it can be
shown [21] that the spectral density (in which potential delta peaks have been removed) is given by
?(?) =

n+m
X
1
Im?i (?) ,
n,m?? ?(n + m)
i=1

lim

(16)

where the ?i are complex variables living on the vertices of the graph G, which are given by:

?1
X
X1
sinh2 (2?Ail )?l?i
,
(17)
?i = ? ? + 1 +
sinh2 ?Aik ?
4
k??i

l??i

where ?i is the set of neighbors of i. The ?i?j are the (linearly stable) solution of the following
belief propagation recursion:

?1
X 1
X
?i?j = ? ? + 1 +
sinh2 ?Aik ?
sinh2 (2?Ail )?l?i
.
(18)
4
k??i

l??i\j

6

Rank 10

Rank 3

10

3

Mean inferred rank

9
8

2.5

7
2

6
5

1.5

n = m = 500
n = m = 2000
n = m = 8000
n = m = 16000
Transition ?c

1
0.5
0

2

3

4

5

6

7

8

9

4
3
2
1
10

?

0

9

10

11

12

13

14

15

16

17

18

19

?

Figure 3: Mean inferred rank as a function of , for different sizes, averaged over 100 samples of
n ? m XY T matrices. The entries of X, Y are drawn from a Gaussian distribution of mean 0 and
variance 1. The theoretical transition is computed with a population dynamics algorithm (see section
3.1). The finite size effects are considerable but consistent with the asymptotic prediction.

This formula can be derived by turning the computation of the spectral density into a marginalization
problem for a graphical model on the graph G and then solving it using loopy belief propagation.
Quite remarkably, this approach leads to an asymptotically exact (and rigorous [22]) description of
the spectral density on Erd?os-R?nyi random graphs. Solving equation (18) numerically we obtain
the results shown on Fig. 1: the bulk of the spectrum, in particular, is always positive.
We now demonstrate that for any value of ? < ?SG , there exists an open set around ? = 0 where
the spectral density vanishes. This justifies independently or choice for the parameter ?. The proof
follows [5] and begins by noticing that ?i?j = cosh?2 (?Aij ) is a fixed point of the recursion (18)
for ? = 0. Since this fixed point is real, the corresponding spectral density is 0. Now consider
a small perturbation ?ij of this solution such that ?i?j = cosh?2 (?Aij )(1 + cosh?2 (?Aij )?ij ).
P
The linearized version of (18) writes ?i?j = l??i\j tanh2 (?Ail )?i?l . The linear operator thus
defined is a weighted version of the non-backtracking matrix of [8]. Its spectral radius is given by
? = ?(?), where ? is defined in 10. In particular, for ? < ?SG , ? < 1, so that a straightforward
application [5] of the implicit function theorem allows to show that there exists a neighborhood U of
0 such that for any ? ? U , there exists a real, linearly stable fixed point of (18), yielding a spectral
density equal to 0. At ? = ??SG , the informative eigenvalues (those outside of the bulk), are therefore
exactly the negative ones, which motivates independently our algorithm.

4

Numerical tests

Figure 3 illustrates the ability of the Bethe Hessian to infer the rank above the critical value c in
the limit of large size n, m (see section 3.1). In Figure 4, we demonstrate the suitability of the
eigenvectors of the Bethe Hessian as starting point for the minimization of the cost function (2). We
compare the final RMSE achieved on the reconstructed matrix XY T with 4 other initializations of
the optimization, including the largest singular vectors of the trimmed matrix M [3]. MaCBetH systematically outperforms all the other choices of initial conditions, providing a better initial condition
for the optimization of (2). Remarkably, the performance achieved by MaCBetH with the inferred
rank is essentially the same as the one achieved with an oracle rank. By contrast, estimating the correct rank from the (trimmed) SVD is more challenging. We note that for the choice of parameters
we consider, trimming had a negligible effect. Along the same lines, OptSpace [3] uses a different
minimization procedure, but from our tests we could not see any difference in performance due to
that. When using Alternating Least Squares [23, 24] as optimization method, we also obtained a
similar improvement in reconstruction by using the eigenvectors of the Bethe Hessian, instead of the
singular vectors of M, as initial condition.
7

P(RMSE < 10?1 )

Rank 3
1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

0.5

0.5

Macbeth OR
Tr-SVD OR
Random OR
Macbeth IR
Tr-SVD IR

0.4
0.3
0.2
0.1
0
1

P(RMSE < 10?8 )

Rank 10

1

10

20

30

40

0.4
0.3
0.2
0.1
0
10
1

50

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0

10

20

30

40

50

0
10

20

30

20

30

40

50

60

40

50

60

?

?

Figure 4: RMSE as a function of the number of revealed entries per row : comparison between
different initializations for the optimization of the cost function (2). The top row shows the probability that the achieved RMSE is smaller than 10?1 , while the bottom row shows the probability that
the final RMSE is smaller than 10?8 . The probabilities were estimated as the frequency of success
over 100 samples of matrices XY T of size 10000 ? 10000, with the entries of X, Y drawn from a
Gaussian distribution of mean 0 and variance 1. All methods optimize the cost function (2) using a
low storage BFGS algorithm [25] part of NLopt [26], starting from different initial conditions. The
maximum number of iterations was set to 1000. The initial conditions compared are MaCBetH with
oracle rank (MaCBetH OR) or inferred rank (MaCBetH IR), SVD of the observed matrix M after
trimming, with oracle rank (Tr-SVD OR), or inferred rank (Tr-SVD IR, note that this is equivalent
to OptSpace [3] in this regime), and random initial conditions with oracle rank (Random OR). For
the Tr-SVD IR method, we inferred the rank from the SVD by looking for an index for which the
ratio between two consecutive eigenvalues is minimized, as suggested in [27].

5

Conclusion

In this paper, we have presented MaCBetH, an algorithm for matrix completion that is efficient for
two distinct, complementary, tasks: (i) it has the ability to estimate a finite rank r reliably from
fewer random entries than other existing approaches, and (ii) it gives lower root-mean-square reconstruction errors than its competitors. The algorithm is built around the Bethe Hessian matrix and
leverages both on recent progresses in the construction of efficient spectral methods for clustering
of sparse networks [8, 5, 9], and on the OptSpace approach [3] for matrix completion.
The method presented here offers a number of possible future directions, including replacing the
minimization of the cost function by a message-passing type algorithm, the use of different neural
network models, or a more theoretical direction involving the computation of information theoretically optimal transitions for detectability.

Acknowledgment
Our research has received funding from the European Research Council under the European Union?s
7th Framework Programme (FP/2007-2013/ERC Grant Agreement 307087-SPARCS).

8

References
[1] E. J. Cand?s and B. Recht, ?Exact matrix completion via convex optimization,? Foundations of Computational mathematics, vol. 9, no. 6, pp. 717?772, 2009.
[2] E. J. Cand?s and T. Tao, ?The power of convex relaxation: Near-optimal matrix completion,? Information
Theory, IEEE Transactions on, vol. 56, no. 5, pp. 2053?2080, 2010.
[3] R. H. Keshavan, A. Montanari, and S. Oh, ?Matrix completion from a few entries,? Information Theory,
IEEE Transactions on, vol. 56, no. 6, pp. 2980?2998, 2010.
[4] D. Gross, Y.-K. Liu, S. T. Flammia, S. Becker, and J. Eisert, ?Quantum state tomography via compressed
sensing,? Physical review letters, vol. 105, no. 15, p. 150401, 2010.
[5] A. Saade, F. Krzakala, and L. Zdeborov?, ?Spectral clustering of graphs with the bethe hessian,? in Advances in Neural Information Processing Systems, 2014, pp. 406?414.
[6] A. Saade, F. Krzakala, M. Lelarge, and L. Zdeborov?, ?Spectral detection in the censored block model,?
IEEE International Symposium on Information Theory (ISIT2015), to appear, 2015.
[7] J.-F. Cai, E. J. Cand?s, and Z. Shen, ?A singular value thresholding algorithm for matrix completion,?
SIAM Journal on Optimization, vol. 20, no. 4, pp. 1956?1982, 2010.
[8] F. Krzakala, C. Moore, E. Mossel, J. Neeman, A. Sly, L. Zdeborov?, and P. Zhang, ?Spectral redemption
in clustering sparse networks,? Proc. Natl. Acad. Sci., vol. 110, no. 52, pp. 20 935?20 940, 2013.
[9] C. Bordenave, M. Lelarge, and L. Massouli?, ?Non-backtracking spectrum of random graphs: community
detection and non-regular ramanujan graphs,? 2015, arXiv:1501.06087.
[10] J. J. Hopfield, ?Neural networks and physical systems with emergent collective computational abilities,?
Proc. Nat. Acad. Sci., vol. 79, no. 8, pp. 2554?2558, 1982.
[11] J. S. Yedidia, W. T. Freeman, and Y. Weiss, ?Bethe free energy, kikuchi approximations, and belief propagation algorithms,? Advances in neural information processing systems, vol. 13, 2001.
[12] M. Mezard and A. Montanari, Information, Physics, and Computation. Oxford University Press, 2009.
[13] D. J. Amit, H. Gutfreund, and H. Sompolinsky, ?Spin-glass models of neural networks,? Physical Review
A, vol. 32, no. 2, p. 1007, 1985.
[14] B. Wemmenhove and A. Coolen, ?Finite connectivity attractor neural networks,? Journal of Physics A:
Mathematical and General, vol. 36, no. 37, p. 9617, 2003.
[15] I. P. Castillo and N. Skantzos, ?The little?hopfield model on a sparse random graph,? Journal of Physics
A: Mathematical and General, vol. 37, no. 39, p. 9087, 2004.
[16] P. Zhang, ?Nonbacktracking operator for the ising model and its applications in systems with multiple
states,? Physical Review E, vol. 91, no. 4, p. 042120, 2015.
[17] J. M. Mooij and H. J. Kappen, ?Validity estimates for loopy belief propagation on binary real-world
networks.? in Advances in Neural Information Processing Systems, 2004, pp. 945?952.
[18] F. Ricci-Tersenghi, ?The bethe approximation for solving the inverse ising problem: a comparison with
other inference methods,? J. Stat. Mech.: Th. and Exp., p. P08015, 2012.
[19] L. Zdeborov?, ?Statistical physics of hard optimization problems,? acta physica slovaca, vol. 59, no. 3,
pp. 169?303, 2009.
[20] Y. Kabashima, F. Krzakala, M. M?zard, A. Sakata, and L. Zdeborov?, ?Phase transitions and sample
complexity in bayes-optimal matrix factorization,? 2014, arXiv:1402.1298.
[21] T. Rogers, I. P. Castillo, R. K?hn, and K. Takeda, ?Cavity approach to the spectral density of sparse
symmetric random matrices,? Phys. Rev. E, vol. 78, no. 3, p. 031116, 2008.
[22] C. Bordenave and M. Lelarge, ?Resolvent of large random graphs,? Random Structures and Algorithms,
vol. 37, no. 3, pp. 332?352, 2010.
[23] P. Jain, P. Netrapalli, and S. Sanghavi, ?Low-rank matrix completion using alternating minimization,?
in Proceedings of the forty-fifth annual ACM symposium on Theory of computing. ACM, 2013, pp.
665?674.
[24] M. Hardt, ?Understanding alternating minimization for matrix completion,? in Foundations of Computer
Science (FOCS), 2014 IEEE 55th Annual Symposium on. IEEE, 2014, pp. 651?660.
[25] D. C. Liu and J. Nocedal, ?On the limited memory bfgs method for large scale optimization,? Mathematical programming, vol. 45, no. 1-3, pp. 503?528, 1989.
[26] S. G. Johnson, ?The nlopt nonlinear-optimization package,? 2014.
[27] R. H. Keshavan, A. Montanari, and S. Oh, ?Low-rank matrix completion with noisy observations: a quantitative comparison,? in 47th Annual Allerton Conference on Communication, Control, and Computing,
2009, pp. 1216?1222.

9

"
130,"Parallel Optimization of Motion
Controllers via Policy Iteration

J. A. Coelho Jr., R. Sitaraman, and R. A. Grupen
Department of Computer Science
University of Massachusetts, Amherst, 01003

Abstract
This paper describes a policy iteration algorithm for optimizing the
performance of a harmonic function-based controller with respect
to a user-defined index. Value functions are represented as potential distributions over the problem domain, being control policies
represented as gradient fields over the same domain. All intermediate policies are intrinsically safe, i.e. collisions are not promoted
during the adaptation process. The algorithm has efficient implementation in parallel SIMD architectures. One potential application - travel distance minimization - illustrates its usefulness.

1

INTRODUCTION

Harmonic functions have been proposed as a uniform framework for the solution of several versions of the motion planning problem. Connolly and Grupen [Connolly and Grupen, 1993] have demonstrated how harmonic functions
can be used to construct smooth, complete artificial potentials with no local minima.
In addition, these potentials meet the criteria established in
[Rimon and Koditschek, 1990] for navigation functions. This implies that the gradient of harmonic functions yields smooth (""realizable"") motion controllers.
By construction, harmonic function-based motion controllers will always command
the robot from any initial configuration to a goal configuration. The intermediate
configurations adopted by the robot are determined by the boundary constraints
and conductance properties set for the domain. Therefore, it is possible to tune
both factors so as to extremize user-specified performance indices (e.g. travel time
or energy) without affecting controller completeness.
Based on this idea, Singh et al. [Singh et al., 1994] devised a policy iteration method
for combining two harmonic function-based control policies into a controller that
minimized travel time on a given environment. The two initial control policies were

997

Parallel Optimization of Motion Controllers via Policy Iteration

derived from solutions to two distinct boundary constraints (Neumann and Dirichlet
constraints). The policy space spawned by the two control policies was parameterized by a mixing coefficient, that ultimately determined the obstacle avoidance behavior adopted by the robot. The resulting controller preserved obstacle avoidance,
ensuring safety at every iteration of the learning procedure.
This paper addresses the question of how to adjust the conductance properties associated with the problem domain 0, such as to extremize an user-specified performance index. Initially, conductance properties are homogeneous across 0, and the
resulting controller is optimal in the sense that it minimizes collision probabilities at
every step [Connolly, 1994]1. The method proposed is a policy iteration algorithm,
in which the policy space is parameterized by the set of node conductances.

2

PROBLEM CHARACTERIZATION

The problem consists in constructing a path controller ifo that maximizes an integral
performance index 'P defined over the set of all possible paths on a lattice for a closed
domain 0 C Rn, subjected to boundary constraints. The controller ifo is responsible
for generating the sequence of configurations from an initial configuration qo on the
lattice to the goal configuration qG, therefore determining the performance index
'P. In formal terms, the performance index 'P can be defined as follows:

Def. 1

Performance indez 'P :
qa

'P

for all q E L(O), where

'P qo

.* = L

f(q)?

q=qo

L(O) is a lattice over the domain 0, qo denotes an arbitrary configuration on L(O),
qG is the goal configuration, and f(q) is a function of the configuration q.

For example, one can define f(q) to be the available joint range associated with the
configuration q of a manipulator; in this case, 'P would be measuring the available
joint range associated with all paths generated within a given domain.

2.1

DERIVATION OF REFERENCE CONTROLLER

The derivation of ifo is very laborious, requiring the exploration of the set of all
possible paths. Out of this set, one is primarily interested in the subset of smooth
paths. We propose to solve a simpler problem, in which the derived controller
if is a numerical approximation to the optimal controller ifo, and (1) generates
smooth paths, (2) is admissible, and (3) locally maximizes P. To guarantee (1) and
(2), it is assumed that the control actions of if are proportional to the gradient of a
harmonic function f/J, represented as the voltage distribution across a resistive lattice
that tessellates the domain O. The condition (3) is achieved through incremental
changes in the set G of internodal conductancesj such changes maximize P locally.

.*

Necessary condition for optimality: Note that 'P qo defines a scalar field over
L(O). It is assumed that there exists a well-defined neighborhood .N(q) for node qj
in fact, it is assumed that every node q has two neighbors across each dimension.
Therefore, it is possible to compute the gradient over the scalar field Pqo.ff by locally
approximating its rate of change across all dimensions. The gradient VP qo defines
lThis is exactly the control policy derived by the TD(O) reinforcement learning method,
for the particular case of an agent travelling in a grid world with absorbing obstacle and
goal states, and being rewarded only for getting to the goal states (see [Connolly, 1994]).

998

J. A. COELHO Jr., R. SITARAMAN, R. A. GRUPEN

a reference controller; in the optimal situation, the actions of the controller if will
parallel the actions of the reference controller. One can now formulate a policy
iteration algorithm for the synthesis of the reference controller:

=

1. Compute if -V~, given conductances G;
2. Evaluate VP q:
- for each cell, compute 'P ;;..
1''''
- for each cell, compute V'P q.
3. Change G incrementally, minimizing the approz. error ? = f(if, VPq);
4. If ? is below a threshold ?o, stop. Otherwise, return to (1).
On convergence, the policy iteration algorithm will have derived a control policy
that maximizes l' globally, and is capable of generating smooth paths to the goal
configuration. The key step on the algorithm is step (3), or how to reduce the
current approximation error by changing the conductances G.

3

APPROXIMATION ALGORITHM

Given a set of internodal conductances, the approximation error
?

L

-

cos (if,

?

is defined as

VP)

(1)

qEL(n)
or the sum over L(n) of the cosine of the angle between vectors if and VP. The
approximation error ? is therefore a function ofthe set G of internodal conductances.
There exist 0(nd"") conductances in a n-dimensional grid, where d is the discretization adopted for each dimension. Discrete search methods for the set of conductance
values that minimizes ? are ruled out by the cardinality of the search space: 0(k nd""),
if k is the number of distinct values each conductance can assume. We will represent
conductances as real values and use gradient descent to minimize ?, according to
the approximation algorithm below:
1. Evaluate the apprommation error

?j

g;;

2. Compute the gradient V? =
j
3. Update conductances, making G = G - aVE;
4. Normalize conductances, such that minimum conductance gmin = 1;
Step (4) guarantees that every conductance g E G will be strictly positive. The
conductances in a resistive grid can be normalized without constraining the voltage distribution across it, due to the linear nature of the underlying circuit. The
complexity of the approximation algorithm is dominated by the computation of the
gradient V?(G). Each component of the vector V?(G) can be expressed as
8? = _ ' "" 8cos(ifq, VPq).
8g ?
L...J
8g?
'qEL(n)
,

(2)

By assumption, if is itself the gradient of a harmonic function ?> that describes
the voltage distribution across a resistive lattice. Therefore, the calculation of :;.
involves the evaluation of ~ over all domain L(n), or how the voltage ?>q is affected
by changes in a certain conductance gi.
For n-dimensional grids,

*? is a matrix with d"" rows and 0(nd"") columns. We posit

that the computation of every element of

it is unnecessary: the effects of changing

999

Parallel Optimization of Motion Controllers via Policy Iteration

g, will be more pronounced in a certain grid neighborhood of it, and essentially
negligible for nodes beyond that neighborhood. Furthermore, this simplification
allows for breaking up the original problem into smaller, independent sub-problems
suitable to simultaneous solution in parallel architectures.

3.1

THE LOCALITY ASSUMPTION

The first simplifying assumption considered in this work establishes bounds on
the neighborhood affected by changes on conductances at node ij specifically,
we will assume that changes in elements of g, affect only the voltage at nodes
in J/(i) , being J/(i) the set composed of node i and its direct neighbors. See
[Coelho Jr. et al., 1995] for a discussion on the validity of this assumption. In
particular, it is demonstrated that the effects of changing one conductance decay
exponentially with grid distance, for infinite 2D grids. Local changes in resistive
grids with higher dimensionality will be confined to even smaller neighborhoods.
The locality assumption simplifies the calculation of :;. to

But

~

[ if . V!,

8g,

lifllV'P1

1= lifllV'P1
1....

[8if.
8g,

l.

VP _ if? VP 8if. if
lifl2 (8g, )

Note that in the derivation above it is assumed that changes in G affects primarily
the control policy if, leaving VP relatively unaffected, at least in a first order
approximation.
Given that if = - V~, it follows that the component 7r; at node q can be approximated by the change of potential across the dimension j, as measured by the
potential on the corresponding neighboring nodes:

7r""1 = ?q- - ?q+, and 87r; = _1_ [8?q _ _ 8?q+]
, q

2b. 2

8g,

2b. 2

8g i

8gi'

where b. is the internodal distance on the lattice L(n).

3.2

DERIVATION OF

G;

The derivation of ~ involves computing the Thevenin equiValent circuit for the
resistive lattice, when every conductance 9 connected to node i is removed. For
clarity, a 2D resistive grid was chosen to illustrate the procedure. Figure 1 depicts
the equivalence warranted by Thevenin's theorem [Chua et al., 1987] and the relevant variables for the derivation of ~. As shown, the equivalent circuit for the
resistive grid consists of a four-port resistor, driven by four independent voltage
?4Y and the current
sources. The relation between the voltage vector = [?t
vector r = [it ... i 4 ]T is expressed as

i

Rf+w,

(3)

w

where R is the impedance matrix for the grid equivalent circuit and is the vector
of open-circuit voltage sources. The grid equivalent circuit behaves exactly like the
whole resistive gridj there is no approximation error.

1000

J. A. COELHO Jr., R. SITARAMAN, R. A. GRUPEN

...+...............
!

cJl2

Grid Equivalent
Circuit

i cJl 3
1i3
cJl4.
... +i ..............
cJl o

Figure 1: Equivalence established by Thevenin's theorem.
The derivation of the 20 parameters (the elements of Rand w) of the equivalent
circuit is detailed in [Coelho Jr. et al., 1995]j it involves a series ofrelaxation operations that can be efficiently implemented in SIMD architectures. The total number
of relaxations for a grid with n l nodes is exactly 6n - 12, or an average of 1/2n
relaxations per link. In the context of this paper, it is assumed that Rand w are
known. Our primary interest is to compute how changes in conductances g1c affect
the voltage vector
or the matrix

i,

84>
8g

= I84>j I,
8g1c

for

{Jk?

1, . .. ,4
1, ... ,4.

The elements of ~ can be computed by derivating each of the four equality relations
in Equation 3 with respect to g1c, resulting in a system of 16 linear equations, and 16
variables - the elements of ~. Notice that each element of i can be expressed as a
linear function of the potentials

4

i, by applying Kirchhoff's laws [Chua et al., 1987]:

APPLICATION EXAMPLE

A robot moves repeatedly toward a goal configuration. Its initial configuration is
not known in advance, and every configuration is equally likely of being the initial
configuration. The problem is to construct a motion controller that minimizes the
overall travel distance for the whole configuration space. If the configuration space
o is discretized into a number of cells, define the combined travel distance D(?T) as

D(?T)

L

dq,if,

(4)

qEL(O)

where dq,if is the travel distance from cell q to the goal configuration qG, and robot
displacements are determined by the controller?T. Figure 2 depicts an instance
of the travel distance minimization problem, and the paths corresponding to its
optimal solution, given the obstacle distribution and the goal configuration shown.
A resistive grid with 17 x 17 nodes was chosen to represent the control policies
generated by our algorithm. Initially, the resistive grid is homogeneous, with all
internodal resistances set to 10. Figure 3 indicates the paths the robot takes when
commanded by ifO, the initial control policy derived from an homogeneous resistive
grid.

Parallel Optimization of Motion Controllers via Policy Iteration
16r----,....---,....----,r-----,

16,....----,r-----,-----,----,

12

12

12

1001

16

Figure 2: Paths for optimal solution of
the travel distance minimization problem.

Figure 3: Paths for the initial solution
of the same problem.

The conductances in the resistive grid were then adjusted over 400 steps of the policy
iteration algorithm, and Figure 4 is a plot of the overall travel distance as a function
of the number of steps. It also shows the optimal travel distance (horizontal line),
corresponding to the optimal solution depicted in Figure 2. The plot shows that
convergence is initially fast; in fact, the first 140 iterations are responsible for 90%
of the overall improvement. After 400 iterations, the travel distance is within 2.8%
of its optimal value. This residual error may be explained by the approximation
incurred in using a discrete resistive grid to represent the potential distribution.
Figure 5 shows the paths taken by the robot after convergence. The final paths are
straightened versions of the paths in Figure 3. Notice also that some of the final
paths originating on the left of the I-shaped obstacle take the robot south of the
obstacle, resembling the optimal paths depicted in Figure 2.

5

CONCLUSION

This paper presented a policy iteration algorithm for the synthesis of provably correct navigation functions that also extremize user-specified performance indices.
The algorithm proposed solves the optimal feedback control problem, in which the
final control policy optimizes the performance index over the whole domain, assuming that every state in the domain is as likely of being the initial state as any other
state.
The algorithm modifies an existing harmonic function-based path controller by incrementally changing the conductances in a resistive grid. Departing from an homogeneous grid, the algorithm transforms an optimal controller (i.e. a controller that
minimizes collision probabilities) into another optimal controller, that extremizes
locally the performance index of interest. The tradeoff may require reducing the
safety margin between the robot and obstacles, but collision avoidance is preserved
at each step of the algorithm.
Other Applications: The algorithm presented can be used (1) in the synthesis
of time-optimal velocity controllers, and (2) in the optimization of non-holonomic
path controllers. The algorithm can also be a component technology for Intelligent
Vehicle Highway Systems (IVHS), by combining (1) and (2).

J. A. COELHO Jr .? R. SITARAMAN. R. A. GRUPEN

1002
1170....----,----,----r----,

16r-----.r-----.....---.,---.,

17.~

12

--------------~

1710

1680

16500L---IOO~-~200'"":--~300~-~400

Figure 4: Overall travel distance, as a
function of iteration steps.

12

16

Figure 5: Final paths, after 800 policy
iteration steps.

Performance on Parallel Architectures: The proposed algorithm is computationally demandingj however, it is suitable for implementation on parallel architectures. Its sequential implementation on a SPARC 10 workstation requires ~ 30 sec.
per iteration, for the example presented. We estimate that a parallel implementation of the proposed example would require ~ 4.3 ms per iteration, or 1. 7 seconds
for 400 iterations, given conservative speedups available on parallel architectures
[Coelho Jr. et al., 1995].
Acknowledgements
This work was supported in part by grants NSF CCR-9410077, IRI-9116297, IRI9208920, and CNPq 202107/90.6.

References
[Chua et aI., 1987] Chua, L., Desoer, C., and Kuh, E. (1987). Linear and Nonlinear
Circuits. McGraw-Hill, Inc., New York, NY.
[Coelho Jr. et al., 1995] Coelho Jr., J., Sitaraman, R., and Grupen, R. (1995).
Control-oriented tuning of harmonic functions. Technical Report CMPSCI Technical Report 95-112, Dept. Computer Science, University of Massachusetts.
[Connolly, 1994] Connolly, C. I. (1994). Harmonic functions and collision probabilities. In Proc. 1994 IEEE Int. Conf. Robotics Automat., pages 3015-3019.
IEEE.
[Connolly and Grupen, 1993] Connolly, C. I. and Grupen, R. (1993). The applications of harmonic functions to robotics. Journal of Robotic Systems, 10(7):931946.
[Rimon and Koditschek, 1990] Rimon, E. and Koditschek, D. (1990). Exact robot
navigation in geometrically complicated but topologically simple spaces. In Proc .
1990 IEEE Int. Conf. Robotics Automat., volume 3, pages 1937-1942, Cincinnati,
OH.
[Singh et aI., 1994] Singh, S., Barto, A., Grupen, R., and Connolly, C. (1994). Robust reinforcement learning in motion planning. In Advances in Neural Information Processing Systems 6, pages 655-662, San Francisco, CA. Morgan Kaufmann
Publishers.

"
1350,"Morton-Style Factorial Coding of Color in
Primary Visual Cortex

Javier R. Movellan
Institute for Neural Computation
University of California San Diego
La Jolla, CA 92093-0515
movellan@inc.ucsd.edu

Thomas Wachtler
Sloan Center for Theoretical Neurobiology
The Salk Institute
La Jolla, CA 92037, USA
thomas@salk.edu

Thomas D. Albright
Howard Hughes Medical Institute
The Salk Institute
La Jolla, CA 92037, USA
tom@salk.edu

Terrence Sejnowski
Computational Neurobiology Laboratory
The Salk Institute
La Jolla, CA 92037, USA
terry@salk.edu

Abstract
We introduce the notion of Morton-style factorial coding and illustrate
how it may help understand information integration and perceptual coding in the brain. We show that by focusing on average responses one
may miss the existence of factorial coding mechanisms that become only
apparent when analyzing spike count histograms. We show evidence
suggesting that the classical/non-classical receptive field organization in
the cortex effectively enforces the development of Morton-style factorial
codes. This may provide some cues to help understand perceptual coding in the brain and to develop new unsupervised learning algorithms.
While methods like ICA (Bell & Sejnowski, 1997) develop independent
codes, in Morton-style coding the goal is to make two or more external
aspects of the world become independent when conditioning on internal
representations.
In this paper we introduce the notion of Morton-style factorial coding and illustrate how it
may help analyze information integration and perceptual organization in the brain. In the
neurosciences factorial codes are often studied in the context of mean tuning curves. A
tuning curve is called separable if it can be expressed as the product of terms selectively
influenced by different stimulus dimensions. Separable tuning curves are taken as evidence of factorial coding mechanisms. In this paper we show that by focusing on average
responses one may miss the existence of factorial coding mechanisms that become only
apparent when analyzing spike count histograms.
Morton (1969) analyzed a wide variety of psychophysical experiments on word perception
and showed that they could be explained using a model in which stimulus and context
have separable effects on perception. More precisely, in Mortons? model the joint effect of
stimulus and context on a perceptual representation can be obtained by multiplying terms

selectively controlled by stimulus and by context, i.e.,

	
 	

	
 
 
  
  
	
  
 	
(1)

	
 is the empirical probability of perceiving the perceptual alternative  in
where
 in context  ,   
	
 represents the support of stimulus  for percept
response

 the support
 and ! to	stimulus

of the context for percept . Massaro (1987b, 1987a, 1989a) has

shown that this form of factorization describes accurately a wide variety of psychophysical
studies in domains such as word recognition, phoneme recognition, audiovisual speech
recognition, and recognition of facial expressions.
Morton-style factorial codes used to be taken as evidence for a feedforward coding mechanism (Massaro, 1989b) but Movellan & McClelland (2001) showed that neural networks
with feedback connections can develop factorial codes when they follow an architectural
constraint named ?channel separability?. Channel separability is defined as follows: First
we identify the neurons which have a direct influence on the observed responses (e.g., the
set of neurons that affect an electrode). For a given set of response units, the stimulus
chanel is defined as the set of units modulated by the stimulus provided the response specification units are excised from the rest of the network. The context channel is the set of
units modulated by the context provided the response units are excised from the rest of
the networks. Two channels are called separable if they have no units in common. Channel separability implies that the influences of an information source upon the channel of
another information source should be mediated via the response specification units (see
Figure 1). While the models used in Movellan and McClelland (2001) are a simplification of actual neural circuits, the analysis suggests that the form of separability expressed
in the the Morton-Massaro model may be a useful paradigm for the study of information
integration in the brain. Indeed it is quite remarkable that the functional organization of
cortex into classical/non-classical receptive fields provides a separable architecture (See
Figure 1). Such organization may be nature?s way of enforcing Morton-style perceptual
coding. In this paper we present evidence in favor of this view by investigating how color
is encoded in primary visual cortex.
It is well known that stimuli of equal chromaticity can evoke different color percepts, depending on the visual context (Wesner & Shevell, 1992; Brown & MacLeod, 1997). Context dependent responses to color stimuli have been found in V4 (Zeki, 1983). More recently the last three authors of this article investigated the chromatic tuning properties of
V1 cells in response to stimuli presented in different chromatic contexts (Wachtler, Sejnowski, & Albright, 2003). The experiment showed that the background color, outside
the cell?s classical receptive field, had a significant effect on the response to colors inside
the receptive field. No attempt was made to model the form of such influence. In this
paper we analyze quantitatively the results of that experiment and show that a large proportion of these neurons, adhered to the Morton-Massaro law, i.e., stimulus and context had a
separable influence on the spike count histograms of these cells.

1 Methods
The animal preparation and methods of this experiment are described in Wachtler et al.
(in press) in great detail. Here we briefly describe the portion of the experiment relevant
to us. Two adult female rhesus monkeys were used in the study. Extracellular potentials
from single isolated neurons were recorded from two macaque monkeys. The monkeys
were awake and were required to fixate a small fixation target for the duration of each trial
(2500 ms.). Amplified electrical activity from the cortex was passed to a data acquisition
system for spike detection and sorting. Once a neuron was isolated, its receptive field was
determined using flashed and moving bars of different size, orientation, and color. All the

     
      

 


Background Channel 
 
 

 
 
Response Specification 

Response

Stimulus Channel

Response
Specification
Units

Stimulus
Relays

Context
Relays

Stimulus
Sensors

Context
Sensors

Stimulus

Context

Input





 
 



 




















Electrode

 
 

 
 




	

	

	

	










	
	
	  
 
 
 
 
 
 
 
 
  
	

	

	

	

	
	
	
	
	










	
	
	









	

	

	


	
	 

	 

	 
	
	
	  
 
 
 
 
 
 
 
 
  

	

Background

Stimulus





 
 



 








































 
 
 
 
Background

Figure 1: Left: A network with separable context and stimulus processing channels. Right:
The arrows connecting the stimulus to the unit in the center represent the classical receptive
field of that unit. External inputs affecting the classical receptive field are called ?stimuli?
and all the other inputs are called ?background?. In this preparation the stimulus and background channels are separable.
neurons recorded had receptive fields at eccentricities between  and  .
Once the receptive fields were located, the color tuning of the neurons was mapped by
flashing 8 stimuli of different chromaticity. The stimuli were homogenous color squares,
centered on and at least twice as large as the receptive field of the neuron under study. They
were flashed for 500 ms. Chromaticity was defined in a color space similar to the one used
in Derrington, Krauskopf, and Lennie (1984). Cone excitations were calculated on the basis
of the human cone fundamentals proposed by Stockman (Stockman, MaCleod, & Johnson,
1993). The origin of the color space corresponded to a homogeneous gray background to
which the animal had been adapted (luminance 48 cd/m  ). The three coordinate axis of the
color space corresponded to L versus M-cone contrast, S-cone contrast, and achromatic luminance. The 8 color stimuli were isoluminant with the gray background, had a fixed color
contrast (distance from origin of color space) and had chromatic directions corresponding
to polar angles  .
After several presentations of the stimuli, the chromatic directions for which the neurons
showed a clear response were determined, and one of them was selected as the second
background condition. In the second condition, the color of the background changed during
stimulus presentation (i.e., for 500 ms) to a different color. This color was isoluminant with
the gray background, was in the direction of a stimulus color to which the cell showed clear
response, but was of lower chromatic contrast than the stimulus colors. In subsequent trials
combinations of the 8 stimulus and 2 background conditions were presented in random
order.
For each trial we recored the number of spikes in a 100 ms window starting 50 ms after
stimulus onset. This time window was chosen because color tuning was usually more
pronounced in the first response phase as compared to later periods of the response and
because it maximized the effects of context. Data were recorded for a total of 94 units. Of
these, 20 neurons were selected for having the strongest background effect and a minimum
of 16 trials per condition. No other criteria were used for the selection of these neurons.

2 Results
Figure 2 shows example tuning curves of 4 different neurons. The thick lines represent
the average response for a particular color stimulus in the plane defined by the first two
chromatic axis. The dark curve represents responses for the gray background condition.
The light curve represents responses for the color background condition. The boxes around
the tuning curves represent average response rates as a function of stimulus onset for the
two background conditions.
Testing whether a code is factorial is like testing for the absence of interaction terms in
Analysis of Variance (ANOVA). The complexity (i.e., degrees of freedom) of an ANOVA
model without interaction terms is identical to the complexity of the Morton-Massaro
model. When testing for interaction effects we analyze whether the addition of interaction terms provides significant improvement on data fit over a simple additive model. In
our case we investigate whether the addition of non-factorial terms provides a significant
improvement on data fit over the factorial Morton-Massaro model. For each neuron there
were 8 stimulus conditions, 2 background conditions, and 10 response alternatives, one per
bin in the spike count histogram. The probabilities of the spike count histogram add up to

 independent probability estimates per neuron.
one thus, there is a total of
In this case the Morton-Massaro model requires
parameters

(Movellan & McClelland, 2001), thus there is a total of 63 nonfactorial terms.

  


	
 
   


 
 



For each neuron we fitted Morton-Massaro?s model and performed a standard likelihood
test to see whether the additional nonfactorial terms improved data fit significantly (i.e.,
whether the deviations from the Morton-Massaro factorial model where significant). We
found that of the 20 neurons only 5 showed significant deviations from the Morton-Massaro
 ). While the Morton-Massaro
model (chi-square test, 63 degrees of freedom,
model had 81 parameters many of them were highly redundant. We also evaluated a 30 parameter version of the model by performing PCA independently on the stimulus and on the
context parameters of the full model and deleting coefficients with small eigenvalues. The
30 parameter model provided fits almost indistinguishable from the 81 parameter model. In
this case only 4 neurons showed significant deviations from the model (chi-square, 124 df,
 ). On a pool of 20 neurons compliant with the Morton-Massaro model one would
expect the test to mistakenly reject 1 neuron by chance. Rejection of 4 or more neurons out
of 20 is not inconsistent with the idea that all the neurons were in fact compliant with the
Morton-Massaro model (
 , binomial test).

 	 	

 	 	

 	 	

Figure 2 shows the obtained and predicted spike count histograms for a typical neuron. The
top row represents the 8 stimulus conditions with gray background. The bottom row shows
the 8 conditions with color background. Lines represent spike count histograms predicted
by the Morton-Massaro model, dots represent obtained spike count histograms.
In order to test the statistical power of the likelihood-ratio test, we generated 20 neurons
with random histograms. The histograms were unimodal, with peak response randomly
selected between 0 and 9, with fall-offs similar to those found in the actual neurons and
with the same number of observations per condition as in the actual neurons. We then fitted
the 81-parameter Morton-Massaro model to each of these neurons and tested it using a
likelihood ratio test. All the simulated neurons exhibited statistically significant deviations
 ) suggesting that the test was quite sensitive.
from the model (chi-square, 63 df,

 	 	

Finally, for comparison purposes we tested a model of information integration that uses the
same number of parameters as the Morton-Massaro model but in which the stimulus and
context terms are are combined additively instead of multiplicatively, i.e.,

 
  		
  
   		
 
  
	
  

 

(2)

Figure 2: Effect of the stimulus and background on the chromatic mean tuning curves of
4 neurons. The thick dark and light lines show mean responses in the isoluminant plane
(x axis: L-M cone variation; y axis: S cone variation) for the two background conditions.
Black: gray background; Light: colored background. The 8 boxes around each tuning
curve shows the average response rate as a function of the time from stimulus onset for the
two background conditions.

Figure 3: Predicted (lines) and obtained (dots) spike count histograms for a typical neuron.
The horizontal axis represents spike counts in a 100 ms. window. The vertical axis represents probabilities. Each row represents a different background condition. Each column
represents a different stimulus condition.

  	 	

After fitting the new model, we performed a likelihood-ratio test. 80 % of the neurons
 ).
showed significant deviations from this model (chi-square, 63 df,

3 Relation to Tuning Curve Separability
In neuroscience separability is commonly studied in the context of mean tuning curves. For
example, a tuning curve is called (multiplicatively) separable if the conditional expected
value of a neuron?s response can be decomposed as the product of two different factors each
selectively influenced by a single stimulus dimension. An important aspect of the MortonMassaro model is that it applies to entire response histograms, not to expected values. If the
Morton-Massaro model holds, then separability appears in the following sense: If we are
allowed to see the response histograms for all the stimuli in background condition A and
the response histogram for a reference stimulus in background condition B, then it should
be possible to predict the response histograms for any stimulus in background condition B.
For example, by looking at the top row of Figure 1 and one of the cells of the bottom row
of Figure 1, it should be possible to reproduce all the other cells in the bottom row.

Obviously if we can predict response histograms then we can also predict tuning curves,
since they are based on averages of response histograms. Most importantly, there are forms
of separability of the tuning curve that become only apparent when studying the entire
response histogram. Figure 4 illustrates this fact with an example. The curve shows the
tuning curves of a particular neuron from an experiment fitted using the Morton-Massaro
model. These curves were obtained by fitting the entire spike count histograms for each
stimulus and background condition, and then obtaining the mean response for the predicted
histograms. The large open circles represent the obtained average responses. The dots
represent 95 % confidence intervals around those responses. Note that the two tuning
curves do not appear separable in a discernable way (it is not possible to predict curve B by
looking at curve A and a single point of curve B). Separability becomes only apparent when
the entire histogram is analyzed, not just the tuning curves based on response averages.

Figure 4: Tuning curves for a typical neuron as predicted by the Morton-Massaro model.
The two curves represent the average response of the neuron to isoluminant stimulus, for
two different background conditions. The elongated curve corresponds to the homogenous
gray background and the circular curve to the colored background. The open dots are the
obtained mean responses. The dots represent 95 % confidence interval of those responses.
Note that the predicted curves do not appear separable in a classic sense. However since
they are generated by Morton?s model the underlying code is factorial. This becomes apparent only when one looks at spike count histograms, not just mean tuning curves.

4 Discussion
We introduced the notion of Morton-style factorial coding and illustrated how it may help
analyze information integration and perceptual organization in the brain. We showed that
by focusing on average responses one may miss the existence of factorial coding mechanisms that become only apparent when analyzing spike count histograms. The results of
our study suggest that V1 represents color using a Morton-style factorial code. This may
provide some cues to help understand perceptual coding in the brain and to develop new
unsupervised learning algorithms. While methods like ICA (Bell & Sejnowski, 1997) develop independent codes, in Morton-style coding the goal is to make two or more external
aspects of the world become independent when conditioning on internal representations.
Morton-style coding is optimal when the statistics of stimulus and background exhibit a
particular property: when conditioning on each possible response category (i.e., spike
counts) the empirical likelihood ratios of stimulus and background factorize. Our study
suggests that Morton coding of color in natural scenes should be optimal or approximately
optimal, a prediction that can be tested via statistical analysis of color in natural scenes.

Acknowledgments
This project was supported by NSF?s grant ITR IIS-0223052.

5 References
Bell, A., & Sejnowski, T. (1997). The ?independent components? of natural scenes are edge filters.
Vision Research, 37(23), 3327?3338.
Brown, R. O., & MacLeod, D. I. A. (1997). Color appearance depends on the variance of surround
colors. Current Biology, (7), 844?849.
Derrington, A. M., Krauskopf, J., & Lennie, P. (1984). Chromatic mechanisms in lateral geniculate
nucleus of macaque. Journal of Physiology, 357, 241?265.
Domingos, P., & Pazzani, M. (1997). On the optimality of the simple Bayesian classifier under
zero-one loss. Journal of Machine Learning, 29, 103?130.
Massaro, D. W. (1987a). Categorical perception: A fuzzy logical model of categorization behavior.
In S. Harnad (Ed.), Categorical perception. Cambridge,England: Cambridge University Press.
Massaro, D. W. (1987b). Speech perception by ear and eye: A paradigm for psychological research.
Hillsdale, NJ: Erlbaum.
Massaro, D. W. (1989a). Perceiving talking faces. Cambridge, Massachusetts: MIT Press.
Massaro, D. W. (1989b). Testing between the TRACE model and the fuzzy logical model of speech
perception. Cognitive Psychology, 21, 398?421.
Morton, J. (1969). The interaction of information in word recognition. Psychological Review, 76,
165?178.
Movellan, J. R., & McClelland, J. L. (2001). The Morton-Massaro law of information integration:
Implications for models of perception. Psychological Review, (1), 113?148.
Stockman, A., MaCleod, D. I. A., & Johnson, N. E. (1993). Spectral sensitivities of the human cones.
Journal of the Optical Society of America A, (10), 2491?2521.
Wachtler, T., Sejnowski, T. J., & Albright, T. D. (2003). Representation of color stimuli in awake
macaque primary visual cortex. Neuron, 37, 1?20.
Wesner, M. F., & Shevell, S. K. (1992). Color perception within a chromatic context: Changes in
red/green equilibria caused by noncontiguous light. Vision Research, (32), 1623?1634.
Zeki, S. (1983). Colour coding in cerebral cortex: the responses of wavelength selective and colourcoded cells in monkey visual cortex to changes in wavelenght composition. Neuroscience, 9,
767?781.

"
467,"?
Experiences with Bayesian Learning In
a
Real World Application

Peter Sykacek, Georg Dorffner
Austrian Research Institute for Artificial Intelligence
Schottengasse 3, A-10ID Vienna Austria
peter, georg@ai.univie.ac.at
Peter Rappelsberger
Institute for Neurophysiology at the University Vienna
Wahringer StraBe 17, A-lOgO Wien
Peter.Rappelsberger@univie.ac.at
Josef Zeitlhofer
Department of Neurology at the AKH Vienna
Wahringer Giirtel 18-20, A-lOgO Wien
Josef.Zeitlhofer@univie.ac.at

Abstract

This paper reports about an application of Bayes' inferred neural network classifiers in the field of automatic sleep staging. The
reason for using Bayesian learning for this task is two-fold. First,
Bayesian inference is known to embody regularization automatically. Second, a side effect of Bayesian learning leads to larger
variance of network outputs in regions without training data. This
results in well known moderation effects, which can be used to
detect outliers. In a 5 fold cross-validation experiment the full
Bayesian solution found with R. Neals hybrid Monte Carlo algorithm, was not better than a single maximum a-posteriori (MAP)
solution found with D.J. MacKay's evidence approximation. In a
second experiment we studied the properties of both solutions in
rejecting classification of movement artefacts.

Experiences with Bayesian Learning in a Real World Application

1

965

Introduction

Sleep staging is usually based on rules defined by Rechtschaffen and Kales (see [8]).
Rechtschaffen and Kales rules define 4 sleep stages, stage one to four, as well as rapid
eye movement (REM) and wakefulness. In [1] J. Bentrup and S. Ray report that
every year nearly one million US citizens consulted their physicians concerning their
sleep. Since sleep staging is a tedious task (one all night recording on average takes
abou t 3 hours to score manually), much effort was spent in designing automatic
sleep stagers.
Sleep staging is a classification problem which was solved using classical statistical
t.echniques or techniques emerged from the field of artificial intelligence (AI) . Among
classical techniques especially the k nearest neighbor technique was used. In [1]
J. Bentrup and S. Ray report that the classical technique outperformed their AI
approaches. Among techniques from the field of AI, researchers used inductive
learning to build tree based classifiers (e.g. ID3, C4.5) as reported by M. Kubat et.
a1. in [4]. Neural networks have also been used to build a classifier from training
examples. Among those who used multi layer perceptron networks to build the
classifier, the work of R. Schaltenbrand et. a1. seems most interesting. In [to] they
use a separate network to refuse classification of too distant input vectors. The
performance usually reported is in the range of 75 to 85 percent.
\Vhich enhancements to these approaches can be made to get a. reliable system
wit.h hopefully better performance? According to S. Roberts et . al. in [9], outlier
detection is important to get reliable results in a critical (e.g. medical) environment.
To get reliable results one must refuse classification of dubious inputs. Those inputs
are marked separately for further inspection by a human expert. To be able to
detect such dubious inputs, we use Bayesian inference to calculate a distribution
over the neural network weights. This approach automatically incorporates the
calculation of confidence for each network estimate. Bayesian inference has the
further advantage that regularization is part of the learning algorithm. Additional
methods like weight decay penalty a.nd cross validation for decay parameter tuning
are no longer needed . Bayesian inference for neural networks was among others
investigated by D.J. MacKay (see [5]), Thodberg (see [11]) and Buntine and Weigend
(~ee [3]).
The a.im of this paper is to study how Bayesian inference leads to probabilities for
classes, which together with doubt levels allow to refuse classification of outliers.
As we are interested in evaluating the resulting performance, we use a comparative
method on the same data set and use a significance test, such that the effect of the
method can easily be evaluated.

2

Methods

In this section we give a short description of the inference techniques used to perform
the experiments. We have used two approaches using neural networks as classifiers
and an instance based approach in order to make the performance estimates comparable to other methods .

2.1

Architecture for polychotomous classification

For polychotomous classification problems usually a l-of-c target coding scheme is
used. Usually it is sufficient to use a network architecture with one hidden layer. In
[2] pp. 237-240, C. Bishop gives a general motivation for the softmax data model,

P. Sykacek, G. Dorffner; P. Rappelsberger and 1. Zeitlhofer

966

which should be used if one wants the network outputs to be probabilities for classes.
If we assume that the class conditional densities, p(? I Ck), of the hidden unit activation vector, ?, are from the general family of exponential distributions, then using
t.he transformation in (1), allows to interpret the network outputs as probabilities
for classes. This transformation is known as normalized exponential or softmax
activation function.
p(Ck 1?) =
exp(ak)
(1)
Lkl exp(ak l )

In ! 1) t.he value ak is the value at output node k before applying softmax activation. Softmax transformation of the activations in the output layer is used for both
network approaches used in this paper.
2.2

Bayesian Inference

In [6] D.J. MacKay uses Bayesian inference and marginalization to get moderated
probabilities for classes in regions where the network is uncertain about the class
label. In conjunction with doubt levels this allows to suppress a classification of
such patterns. A closer investigation of this approach showed that marginalization
leads to moderated probabilities, but the degree of moderation heavily depends on
the direction in which we move away from the region with sufficient training data.
Therefore one has to be careful about whether the moderation effect should be used
for outliers detection.
A Bayesian solution for neural networks is a posterior distribution over weight space
calculated via Bayes' theorem using a prior over weights.

(2)
In (2), w is the weight vector of the network and V represents the training data.
Two different possibilities are known to calculate the posterior in (2). In [5] D.J .
MacKay derives an analytical expression assuming a Gaussian distribution. In [7]
R. Neal uses a hybrid Monte Carlo method to sample from the posterior. For one
input pattern, the posterior over weight space will lead to a distribution of network
outputs.
For a classification problem, following MacKay [6], the network estimate is calculated by marginalization over the output distribution.
P(C 1

I~, V)

=.J

P(C 1

=

I~, w)p(w I V)dw

Jy(~,

w)p(w I V)dw

(3)

In general, the distribution over output activations will have small variance in regions well represented in the training data and large variance everywhere else. The
reason for that is the influence of the likelihood term p(V I w), which forces the
network mapping to lie close to the desired one in regions with training data, but
which has no influence on the network mapping in regions without training data.
At least for for generalized linear models applied to regression, this property is
quantifiable. In [12] C. Williams et.al. showed that the error bar is proportional to
the inverse input data density p(~)-l. A similar relation is also plausible for the
output activation in classification problems.

Experiences with Bayesian Learning in a Real World Application

967

Due to the nonlinearity of the softmax transformation, marginalization will moderate probabilities for classes. Moderation will be larger in regions with large variance
of the output activation. Compared to a decision made with the most probable
weight, the network guess for the class label will be less certain. This moderation
effect allows to reject classification of outlying patterns.
Since upper integral can not be solved analytically for classification problems, there
are t.wo possibilities to solve it. In [6] D.J. MacKay uses an approximation. Using
hybrid Monte Carlo sampling as an implementation of Bayesian inference (see R.
Neal in [7]), there is no need to perform upper integration analytically. The hybrid
Monte Carlo algorithm samples from the posterior and upper integral is calculated
as a finite sum.
1 L
P(C 1 I~, 1)) ~ L LY(~' Wi)

(4)

i=l

Assuming, that the posterior over weights is represented exactly by the sampled
weights, there is no need to limit the number of hidden units, if a correct (scaled)
prior is used. Consequently in the experiments the network size was chosen to be
large. We used 25 hidden units. Implementation details of the hybrid Monte Carlo
algorithm may be found in [7].
2.3

The Competitor

The classifier, used to give performance estimates to compare to, is built as a two
layer perceptron network with softmax transformation applied to the outputs. As
an error function we use the cross entropy error including a consistent weight decay
penalty, as it is e.g. proposed by C. Bishop in [2], pp. 338. The decay parameters
are estimated with D.J. MacKay's evidence approximation ( see [5] for details).
Note that the restriction of D.J. MacKay's implementation of Bayesian learning,
which has no solution to arrive at moderated probabilities in l-of-c classification
problems, do not apply here since we use only one MAP value. The key problem
with this approach is the Gaussian approximation of the posterior over weights,
which is used to derive the most probable decay parameters. This approximation is
certainly only valid if the number of network parameters is small compared to the
number of training samples. One consequence is, that the size of the network has
to be restricted . Our model uses 6 hidden units.
To make the performance of the Bayes inferred classifier also comparable to other
methods, we decided to include performance estimates of a k nearest neighbor
algorithm. This algorithm is easy to implement and from [1] we have some evidence
that its performance is good.

3

Experiments and Results

In this sect.ion we discuss the results of a sleep staging experiment based on the
t.echniques described in the ""Methods"" section.

3.1

Data

All experiments are performed with spectral features calculated from a database of 5
different healthy subjects. All recordings were scored according to the Rechtschaffen
& Kales rules. The data pool consisted from data calculated for all electrodes

968

P. Sykacek, G. Doif.fner, P. Rappelsberger and J. Zeitlhofer

available, which were horizontal eye movement, vertical eye movement and 18 EEG
f'lectrodes placed with respect to the international 10-20 system.
The data were transformed into the frequency domain. We used power density
values as well as coherency between different electrodes, which is a correlation
coefficient expressed as a function of frequency as input features. All data were
transformed to zero mean and unit variance. From the resulting feature space we
selected 10 features, which were used as inputs for classification. Feature selection
was done with a suboptimal search algorithm which used the performance of a k
nearest neighbor classifier for evaluation. We used more than 2300 samples during
t.raining and about 580 for testing.

3.2

Analysis of Both Classifiers

The analysis of both classifiers described in the ""Methods"" section should reveal
whether besides good classification performance the Bayes' inferred classifier is also
,apable of refusing outlying test patterns. Increasing the doubt level should lead to
better results of the classifier trained by Bayesian Inference if the test data contains
out.lying patterns. We performed two experiments. During the first experiment
Wf' calculated results from a 5 fold cross validation, where training is done with 4
subjects and tests are performed with one independent test person. In a second
j,f'St. we examine the differences of both algorithms on patterns which are definitely
outliers. We used the same classifiers as in the first experiment. Test patterns for
t his experiment were classified movement artefacts, which should not be classified
as one of the sleep stages.
The classifier used in conjunction with Bayesian inference was a 2-layer neural
net.work with 10 inputs, 25 hidden units with sigmoid activation and five output
units with softmax activation. The large number of hidden units is motivated by
the results reported from R. Neal in [7]. R. Neal studied the properties of neural
networks in a Bayesian framework when using Gaussian priors over weights. He
concluded that there is no need for limiting the complexity of the network when
using a correct Bayesian approach. The standard deviation of the Gaussian prior
1S scaled by the number of hidden units.
For the comparative approach we used a neural network with 10 inputs, 6 hidden
units and 5 outputs with softmax activation. Optimization was done via the BFGS
algorit.hm (see C. Bishop in [2]) with automatic weight decay parameter tuning
(D.J. MacKay's evidence approximation). As described in the methods section,
the smaller network used here is motivated by the Gaussian approximation of the
posterior over weights, which is used in the expression for the most probable decay
parameters.
The third result is a result achieved with a k nearest neighbor classifier with k set
to three.
All results are summaried in table 1. Each column summarizes the results achieved
with one of the algorithms and a certain doubt level during the cross validation run.
As the k nearest neighbor classifier gives only coarse probability estimates, we give
only the performance estimate when all test patterns are classified.
An examination of table 1 shows that the differences between the MAP-solution
and the Bayesian solution are extremely small. Consequently, using a t-test, the
O-hypothesis could not be rejected at any reasonable significance level. On the other
hand compared to the Bayesian solution, the performance of the k nearest neighbor
classifier is significantly lower (the significance level is 0.001).

Experiences with Bayesian Learning in a Real World Application

969

Table 1: Classification Performance
Doubt Cases
Mean Perf.
Std. Dev.
Doubt Cases
Mean Perf.
Std. Dev.
Doubt Cases
Mean Perf.
Std. Dev.

MAP
0
5%
100/0
78.6% 80.4% 81.6%
9.1%
9.4%
9.4%
Bayes
0
5%
10%
78.4% 80.2% 82.2%
9.0%
8.6%
9.4%
k nearest neighbor
0
5%
10%
74.6%
8.4%
-

15%
83.2'70
9.1%
15%
83.6%
9.1%
15%
-

Table 2: Rejection of Movement Periods
Method
recognized outliers

No.
0
1
2
0
1

MAP
%
0%
7.7%
15.4%
0%
7.7%

No.
1

6
5
5
3

Bayes
%
7.1%
46.f%
38.5%
38.5%
23.1%

The last experiment revealed that both training algorithms lead to comparable performance estimates, when clean data is used. When using the classifier in practice
there is no guarantee that the data are clean. One common problem of all night
recordings are the so called movement periods, which are periods with muscle activity due to movements of the sleeping subject. During a second experiment we tried
t.o assess the robustness of both neural classifiers against such inputs. During this
experiment we used a fixed doubt level, for which approximately 5% of the clean
t.est. data from the last experiment were rejected. With this doubt level we classified
13 movement periods, which should not be assigned to any of the other stages. The
number of correctly refused outlying patterns are shown in table 2. Analysis of the
results with a t-test showed a significant higher rate of removed outliers for the full
Bayesian approach. Nevertheless as the number of misclassified outliers is large,
one has to be careful in using this side-effect of Bayesian inference.

4

Conclusion

Using Bayesian Inference for neural network training is an approach which leads to
better classification results compared with simpler training procedures. Comparing
wit.h the ""one MAP"" solution, we observed significantly larger reliability in detecting
dubious patterns. The large amount of remaining misclassified patterns, which were
obviously outlying, shows that we should not rely blindly on the moderating effect
of marginalization. Despite the large amount of time which is required to calculate
t.he solution, Bayesian inference has relevance for practical applications. On one
hand the Bayesian solution shows good performance. But the main reason is the
a.bility to encode a validity region of the model into the solution. Compared to all
methods which do not aim at a predictive distribution, this is a clear advantage for
Bayesian inference.

P. Sykacek, G. Doif.fner, P. Rappelsberger and 1. Zeitlhofer

970

Acknowledgements

We want to acknowledge the work of R. Neal from the Departments of Statistics
and Computer Science at the University of Toronto, who made his implementation
of hybrid Monte-Carlo sampling for Bayesian inference available electronically. His
software was used to calculate the full Bayes' inferred classification results. We also
want to express gratitude to S. Roberts from Imperial College London, one of the
partners in the ANNDEE project. His work and his consequence in insisting on
confidence measures for network decisions had a large positive impact on our work.
This work was sponsored by the Austrian Federal Ministry of Science and Transport.
It was done in the framework of the BIOMED 1 concerted action ANNDEE, financed

by the European Commission, DG. XII.

References
[1] J.A. Bentrup and S.R. Ray. An examination of inductive learning algorithms for
the classification of sleep signals. Technical Report UIUCDCS-R-93-1792, Dept of
Computer Science, University of Illinois, Urbana-Champaign, 1993.
[3] C. M. Bishop. Neural Networks for Pattern Recognition. Clarendon Press, Oxford,
1995.
[3] W. L. Buntine and A. S. Weigend. Bayesian back-propagation. Complex Systems,
5:603-643, 1991.
[4] M. Kubat, G. Pfurtscheller, and D. Flotzinger. Discrimination and classification using
bot.h binary and continuous variables. Biological Cybernetics, 70:443-448, 1994.
[5] D. J. C. MacKay. Bayesian interpolation. Neural Computation, 4:415-447, 1992.
[6] D. J. C. MacKay. The evidence framework applied to classification networks. Neural
Computation, 4:720-736, 1992.
[7] R. M. Neal. Bayesian Learning for Neural Networks. Springer, New York, 1996.
[8] A. Rechtschaffen and A. Kales. A manual of standardized terminology, techniques
and scoring system for sleep stages of human subjects. NIH Publication No. 204, US
Government Printing Office, Washington, DC., 1968.
[9] S. Roberts, L. Tarassenko, J. Pardey, and D. Siegwart. A confidence measure for
artificial neural networks. In International Conference Neural Networks and Expert
Systems in Medicine and Healthcare, pages 23-30, Plymouth, UK, 1994.
[10] N. Schaltenbrand, R. Lengelle, and J.P. Macher. Neural network model: application
to automatic analysis of human sleep. Computers and Biomedical Research, 26:157171, 1993.
~ll]

H. H. Thodberg. A review of bayesian neural networks with an application to near
infrared spectroscopy. IEEE Transactions on Neural Networks, 7(1):56-72, January
1996.

[12] C. K. I. Williams, C. Quazaz, C. M. Bishop, and H. Zhu. On the relationship between
bayesian error bars and the input data density. In Fourth International Conference
on Artificial Neural Networks, Churchill Col/ege, University of Cambridge, UK. lEE
Conference Publication No. 409, pages 160-165, 1995.

"
4507,"Statistical analysis of coupled time series with Kernel
Cross-Spectral Density operators.

Michel Besserve
MPI for Intelligent Systems and MPI for Biological Cybernetics, T?ubingen, Germany
michel.besserve@tuebingen.mpg.de
Nikos K. Logothetis
MPI for Biological Cybernetics, T?ubingen
nikos.logothetis@tuebingen.mpg.de

Bernhard Sch?olkopf
MPI for Intelligent Systems, T?ubingen
bs@tuebingen.mpg.de

Abstract
Many applications require the analysis of complex interactions between time series. These interactions can be non-linear and involve vector valued as well as
complex data structures such as graphs or strings. Here we provide a general
framework for the statistical analysis of these dependencies when random variables are sampled from stationary time-series of arbitrary objects. To achieve this
goal, we study the properties of the Kernel Cross-Spectral Density (KCSD) operator induced by positive definite kernels on arbitrary input domains. This framework enables us to develop an independence test between time series, as well as a
similarity measure to compare different types of coupling. The performance of our
test is compared to the HSIC test using i.i.d. assumptions, showing improvements
in terms of detection errors, as well as the suitability of this approach for testing
dependency in complex dynamical systems. This similarity measure enables us to
identify different types of interactions in electrophysiological neural time series.

1

Introduction

Complex dynamical systems can often be observed by monitoring time series of one or more variables. Finding and characterizing dependencies between several of these time series is key to understand the underlying mechanisms of these systems. This problem can be addressed easily in
linear systems [4], however non-linear systems are much more challenging. Whereas higher order
statistics can provide helpful tools in specific contexts [15], and have been extensively used in system identification, causal inference and blind source separation (see for example [10, 13, 5]); it is
difficult to derive a general approach with solid theoretical results accounting for a broad range of
interactions. Especially, studying the relationships between time series of arbitrary objects such as
texts or graphs within a general framework is largely unaddressed.
On the other hand, the dependency between independent identically distributed (i.i.d.) samples
of arbitrary objects can be studied elegantly in the framework of positive definite kernels [19]. It
relies on defining cross-covariance operators between variables mapped implicitly to Reproducing
Kernel Hilbert Spaces (RKHS) [7]. It has been shown that when using a characteristic kernel for
the mapping [9], the properties of RKHS operators are related to statistical independence between
input variables and allow testing for it in a principled way with the Hilbert-Schmidt Independence
Criterion (HSIC) test [11]. However, the suitability of this test relies heavily on the assumption
that i.i.d. samples of random variables are used. This assumption is obviously violated in any nontrivial setting involving time series, and as a consequence trying to use HSIC in this context can
lead to incorrect conclusions. Zhang et al. established a framework in the context of Markov chains
1

[22], showing that a structured HSIC test still provides good asymptotic properties for absolutely
regular processes. However, this methodology has not been assessed extensively in empirical time
series. Moreover, beyond the detection of interactions, it is important to be able to characterize the
nature of the coupling between time series. It was recently suggested that generalizing the concept
of cross-spectral density to Reproducible Kernel Hilbert Spaces (RKHS) could help formulate nonlinear dependency measures for time series [2]. However, no statistical assessment of this measure
has been established. In this paper, after recalling the concept of kernel spectral density operator,
we characterize its statistical properties. In particular, we define independence tests based on this
concept as well as a similarity measure to compare different types of couplings. We use these tests
in section 4 to compute the statistical dependencies between simulated time series of various types
of objects, as well as recordings of neural activity in the visual cortex of non-human primates. We
show that our technique reliably detects complex interactions and provides a characterization of
these interactions in the frequency domain.

2

Background and notations

Random variables in Reproducing Kernel Hilbert Spaces
Let X1 and X2 be two (possibly non vectorial) input domains. Let k1 (., .) : X1 ? X1 ? C and
k2 (., .) : X2 ? X2 ? C be two positive definite kernels, associated to two separable Hilbert spaces
of functions, H1 and H2 respectively. For i ? {1, 2}, they
a canonical mapping from x ? Xi

 define

to x = ki (., x) ? Hi , such that ?f ? Hi , f (x) = f, x H (see [19] for more details). In the
i
same way, this mapping can be extended to random variables, so that the random variable Xi ? Xi
is mapped to the random element Xi ? Hi . Statistical objects extending the classical mean and
covariance to random variables in the RKHS are defined as follows:
? the Mean Element (see [1, 3]): ?i = E [Xi ] ,
? the Cross-covariance operator (see [6]): Cij = Cov [Xi , Xj ] = E[Xi ? X?j ] ? ?i ? ??j ,
where we
usethe tensor product notation f ? g ? to represent the rank one operator defined by
f ? g ? = g, . f (following [3]). As a consequence, the cross-covariance can be seen as an operator
in L(Hj , Hi ), the Hilbert space of linear Hilbert-Schmidt operators from Hj to Hi (isomorphic to
Hi ? Hj? ). Interestingly, the link between Cij and covariance in the input domains is given by the
Hilbert-Schmidt scalar product



Cij , fi ? fj? HS = Cov [fi (Xi ), fj (Xj )] , ?(fi , fj ) ? Hi ? Hj
Moreover, the Hilbert-Schmidt norm of the operator in this space has been proved to be a measure of
independence between two random variables, whenever kernels are characteristic [11]. Extension of
this result has been provided in [22] for Markov chains. If the time series are assumed to be k-order
Markovian, then results of the classical HSIC can be generalized for a structured HSIC using universal kernels based on the state vectors (x1 (t), . . . , x1 (t + k), x2 (t), . . . , x2 (t + k)). The statistical
performance of this methodology has not been studied extensively, in particular its sensitivity to the
dimension of the state vector. The following sections propose an alternative methodology.
Kernel Cross-Spectral Density operator
Consider a bivariate discrete time random process on X1 ? X2 : {(X1 (t), X2 (t))}t?Z . We assume
stationarity of the process and thus use the following translation invariant notations for the mean
elements and cross-covariance operators:
EXi (t) = ?i ,

Cov [Xi (t + ? ), Xj (t)] = Cij (? )

The cross-spectral density operator was introduced for stationary signals in [2] based on second
order cumulants. Under mild assumptions, it is a Hilbert-Schmidt operator defined for all normalized
frequencies ? ? [0 ; 1] as:
X
X
S12 (?) =
C12 (k) exp(?k2??) =
C12 (k)z ?k , for z = e2?i? .
k?Z

k?Z

2

This object summarizes all the cross-spectral properties between the families of processes
{f (X1 )}f ?H1 and {g(X2 )}g?H2 in the sense that the cross-spectrum between f (X1 ) and g(X2 )



f,g
(?) = f, S12 g . We therefore refer to this object as the Kernel Cross-Spectral
is given by S12
Density operator (KCSD).

3

Statistical properties of KCSD

Measuring independence with the KCSD
One interesting characteristic of the KCSD is given by the following theorem [2]:
Theorem 1. Assume the kernels k1 and k2 are characteristic [9]. The processes X1 and X2 are
pairwise
independent
(i.e. for all integers t and t?, X1 (t) and X2 (t0 ) are independent), if and only


if 
S12 (?)
HS = 0, ?? ? [0 , 1].
While this theorem states that KCSD can be used to test pairwise independence between time series,
it does not imply independence between arbitrary sets of random variables taken from each time
series in general. However, if the joint probability distribution of the time series is encoded by a
Directed Acyclic Graph (DAG), the following Theorem shows that independence in this broader
sense is achieved under mild assumptions.
Proposition 2. If the joint probability distribution of time series is encoded by a DAG with no
confounder under the Markov property and faithfulness assumption, pairwise independence between
time series implies the mutual independence relationship {X1 (t)}t?Z ?
? {X2 (t)}t?Z .
Proof. The proof uses the fact that the faithfulness and Markov property assumptions provide an
equivalence between the independence of two sets of random variables and the d-separation of the
corresponding sets of nodes in the DAG (see [17]). We start by assuming pairwise independence
between the time series.
For arbitrary times t and t0 , assume the DAG contains an arrow linking the nodes X1 (t) and X2 (t0 ).
This is an unblocked path linking this two nodes; thus they are not d-separated. As a consequence of
faithfulness, X1 (t) and X2 (t0 ) are not independent. Since this contradicts our initial assumptions,
there cannot exist any arrow between X1 (t) and X2 (t0 ).
Since this holds for all t and t0 , there is no path linking the nodes of each time series and we have
{X1 (t)}t?Z ?
? {X2 (t)}t?Z according to the Markov property (any joint probability distribution on
the nodes will factorize in two terms, one for each time series).
As a consequence, the use of KCSD to test for independence is justified under the widely used
faithfulness and Markov assumptions of graphical models. As a comparison, the structured HSIC
proposed in [22] is theoretically able to capture all dependencies within the range of k samples by
assuming k-order Markovian time series.
Fourth order kernel cumulant operator
Statistical properties of KCSD require assumptions regarding the higher order statistics of the time
series. Analogously to covariance, higher order statistics can be generalized as operators in (tensor
products of) RKHSs. An important example in our setting is the joint quadricumulant (4th order
cumulant) (see [4]). We skip the general expression of this cumulant to focus on its simplified form
for four centered scalar random variables:
?(X1 , X2 , X3 , X4 ) = E[X1 X2 X3 X4 ] ? E[X1 X2 ]E[X3 X4 ] ? E[X1 X3 ]E[X2 X4 ]
? E[X1 X4 ]E[X2 X3 ] (1)
This object can be generalized to the case random variables mapped in two RKHSs. The quadricu?
?
mulant operator K1234 is a linear operator
in the Hilbert space


 L(H1 ? H2 , H1 ? H2 ), such that
?
?
?(f1 (X1 ),f2 (X2 ),f3 (X3 ),f4 (X4 )) = f1 ?f2 ,K1234 f3 ?f4 , for arbitrary elements fi . The properties of this operator will be useful in the next sections due to the following lemma.
Lemma 3. [Property of the tensor quadricumulant] Let Xc1 , Xc3 be centered random elements in
the Hilbert space H1 and Xc2 , Xc4 centered random elements in H2 (the centered random element is
defined by Xci = Xj ? ?j ), then


 

 






E Xc1 , Xc3 H1 Xc2 , Xc4 H2 = Tr K1234 + C 1,2 , C 3,4 + Tr C 1,3 Tr C 2,4 + C 1,4 , C 3,2
3

In the case of two jointly stationary time series, we define the translation invariant quadricumulant
between the two stationary time series as:
K12 (?1 , ?2 , ?3 ) = K1234 (X1 (t + ?1 ), X2 (t + ?2 ), X1 (t + ?3 ), X2 (t))
Estimation with the Kernel Periodogram
In the following, we address the problem of estimating the properties of cross-spectral density operators from finite samples. The idea for doing this analytically is to select samples from a time-series
with a tapering window function w : R 7? R with a support included in [0, 1]. By scaling this
window according to wT (k) = w(k/T ), and multiplying it with the time series, T samples of the
sequence can be selected. The windowed periodogram estimate of the KCSD operator for T successive samples of the time series is
1
c
c
?
PT12 (?) =
2 FT [X1 ](?)?FT [X2 ](?) ,
T kwk
? 1
2
with Xci (k) = Xi (k) ? ?i and kwk =
w2 (t)dt
0

PT

where FT [Xc1 ] = k=1wT (k)(Xc1 (k))z ?k , for z = e2?i? , is the windowed Fourier transform of
the delayed time series in the RKHS. Properties of the windowed Fourier transform are related to
the regularity of the tapering window. In particular, we will chose a tapering window of bounded
variation. In such a case, the following lemma holds (see supplementary material for the proof).
Lemma 4. [A property of
variation functions]
Pbounded
P+?Let w be a bounded function of bounded
+?
variation then for all k,  t=?? wT (t + k)w(t) ? t=?? wT (t)2  ? C|k|
Using this assumption, the above periodogram estimate is asymptotically unbiased as shown in the
following theorem
P
Theorem 5. Let w be a bounded function of bounded variation,
 if k?Z |k|kC12 (k)kHS < +?,
P
P


k?Z |k| Tr(Cii (k)) < +? and
(k,i,j)?Z3 Tr [K12 (k, i, j)] < +?,
then

lim E PT12 (?) = S12 (?), ? 6? 0

(mod 1/2)
Proof. By definition,
 P

P
1
c
?k
c
?n ?
?
PT12 (z) = T kwk
2
k?Z wT (k)X1 (k)z
n?Z wT (n)X2 (n)z
P
P
1
n?k
wT (k)wT (n)Xc1 (k)?Xc2 (n)?
= T kwk
2
k?Z
n?Z z
P
P
1
??
c
c
?
= T kwk
2
??Z z
n?Z wT (n + ?)wT (n)X1 (n + ?)?X2 (n) , using ? = k ? n.
T ?+?

Thus using Lemma 4,
P
P
1
??
( n?Z wT (n)2 + O(|?|))C12 (?)
E PT12 (z) = T kwk
2
??Z z
P
1
= kwk
2(
n?Z

wT (n)2 P
) ??Z
T



P
z ?? C12 (?)+ T1 O( ??Z |?|
C12 (?)
HS )

?

T ?+?

S12 .

However, the squared Hilbert-Schmidt norm of PT12 (?) is an asymptotically biased estimator of the
population KCSD squared norm according to the following theorem.
Theorem 6. Under the assumptions of Theorem 5, for ? 6? 0 (mod 1/2)


2


2
lim E
PT12 (?)
= 
S12 (?)
+ Tr(S11 (?)) Tr(S22 (?))
T ?+?

HS

HS

The proof of Theorem 5 is based on the decomposition in Lemma 3 and is provided in supplementary
information.
This estimate requires specific bias estimation techniques to develop an independence test, we will
call it the biased estimate of the KCSD squared norm. Having the KCSD defined in an Hilbert space
also enables to define similarity between two KCSD operators, so that it is possible to compare
quantitatively whether different dynamical systems have similar couplings. The following theorem
shows how periodograms enable to estimate the scalar product between two KCSD operators, which
reflects their similarity.
4

Theorem 7. Assume assumptions of Theorem 5 hold for two independent samples of bivariate
time series{(X1 (t), X2 (t))}t=...,?1,0,1,... and {(X3 (t), X4 (t))}t=...,?1,0,1,... , mapped with the same
couple of reproducing kernels.






Then lim E PT12 (?), PT34 (?) HS = S12 (?), S34 (?) HS , ? 6? 0 (mod 1/2)
T ?+?

The proof of Theorem 7 is similar to the one of Theorem 6 provided as supplemental information.
Interestingly, this estimate of the scalar product between KCSD operators is unbiased. This comes
from the assumption that the two bivariate series are independent. This provides a new opportunity
to estimate the Hilbert-Schmidt norm as well, in case two independent samples of the same bivariate
series are available.
Corollary 8. Assume assumptions of Theorem 5 hold for the bivariate time series
? 1 (t), X
? 2 (t))}t?Z an independent copy of the same time series,
{(X1 (t), X2 (t))}t?Z and assume {(X
T
? T (?), respectively.
providing the periodogram estimates P12 (?) and P
12
Then


2




? T (?)
= 
S12 (?)
HS , ? 6? 0
lim E PT12 (?), P
12
HS

T ?+?

(mod 1/2)

In many experimental settings, such as in neuroscience, it is possible to measure the same time series
in several independent trials. In such a case, corollary 8 states that estimating the Hilbert-Schmidt
norm of the KCSD without bias is possible using two intependent trials. We will call this estimate
the unbiased estimate of the KCSD squared norm.
These estimate can be computed efficiently for T equispaced frequency samples using the fast
Fourier transform of the centered kernel matrices of the two time series. In general, the choice
of the kernel is a trade-off between the capacity to capture complex dependencies (a characteristic kernel being better in this respect), and the convergence rate of the estimate (simpler kernels related to lower order statistics usually require less samples). Related theoretical analysis can be found in [8, 12]. Unless otherwise stated, the Gaussian RBF kernel with bandwidth
2
parameter ?, k(x, y) = exp(kx ? yk /2? 2 ), will be used as a characteristic kernel for vector spaces. Let Kij denote the kernel matrix between the i-th and j-th time series (such that
(Kij )k,l = k(xi (k), xj (l))), W the windowing matrix (such that (W)k,l = wT (k)wT (l)) and
M be the centering matrix M = I ? 1T 1TT /T , then we can define the windowed centered kernel
? ij = (MKij M) ? W. Defining the Discrete Fourier Transform matrix F, such that
matrices K
?
(F)k,l = exp(?i2?kl/T )/ T , the estimated scalar product is



PT12 , PT34


?=(0,1,...,(T ?1))/T

= kwk

?4



? 13 F?1 ? diag F?1 K
? 24 F ,
diag FK

which can be efficiently computed using the Fast Fourier Transform (? is the Hadamard product).
The biased and unbiased squared norm estimates can be trivially retrieved from the above expression.
Shuffling independence tests
According to Theorem 1, pairwise independence between time series requires the cross-spectral
density operator to be zero for all frequencies. We can thus test independence by testing whether
the Hilbert-Schmidt norm of the operator vanishes for each frequency. We rely on Theorem 6 and
Corollary 8 to compute biased and unbiased estimates of this norm. To achieve this, we generate
a distribution of the Hilbert-Schmidt norm statistics under the null hypothesis by cutting the time
interval in non-overlapping blocks and matching the blocks of each time series in pairs at random.
Due to the central limit theorem, for a sufficiently large number of time windows, the empirical
average of the statistics approaches a Gaussian distribution. We thus test whether the empirical
mean differs from the one under the null distribution using a t-statistic. To prevent false positive
resulting from multiple hypothesis testing, we control the Family-wise Error Rate (FWER) of the
tests performed for each frequency. Following [16], we estimate a global maximum distribution on
the family of t-statistics across frequencies under the null hypothesis, and use the percentile of this
distribution to assess the significance of the original t-statistics.
5

Squared norm estimate: linear kernel

0.3
4
0.2

0

0.1

0.05

0

0

?2
?4

0

0.5

1

time (s)

?0.1

1.5

0

5

number of dependencies detected (%)

number of dependencies detected (%)

Detection probability for biased kcsd
1

linear
rbf ?=.1

0.8

.2
.63

0.6

1.5
3.9

0.4

10
0.2

0

2

10

number of samples

3

10

10

15

20

frequency (Hz)

25

30

Detection probability for unbiased kcsd
1

?0.05

0

5

10

15

20

frequency (Hz)

0.6

60

0.4

40

0.2

2

10

20

number of samples

30

type I (C=0)

80

80

25

100

100

0.8

0

biased
unbiased

0.1

error rate (%)

2

Squared norm estimate: RBF kernel

0.15

biased
unbiased

type II (C=.4)
type II (C=2)

60
40
20

3

10

0

block hsic

hsic

linear kcsd

kcsd

Figure 1: Results for the phase-amplitude coupling system. Top-left: example time course. Topmiddle: estimate of the KCSD squared norm with a linear kernel. Top-right: estimate of the KCSD
squared norm with an RBF kernel. Bottom-left: performance of the biased kcsd test as a function of
number of samples. Bottom-middle: performance of the unbiased kcsd test as a function of number
of samples. Bottom-right: Rate of type I and type II errors for several independence tests.

4

Experiments

In the following, we validate the performance of our test, called kcsd, on several datasets in the
biased and unbiased case. There is no general time series analysis tool in the literature to compare
with our approach on all these datasets. So our main source of comparison will be the HSIC test of
independence (assuming data is i.i.d.). This enables us, to compare both approaches using the same
kernels. For vector data, one can compare the performance of our approach with a linear dependency
measure: we do this by implementing our test using a linear kernel (instead of an RBF kernel), and
we call it linear kscd. Finally, we use the alternative approach of structured HSIC [22] by cutting
the time series in time windows (using the same approach as our independence test) and considering
each of them as a single multivariate sample. This will be called block hsic. The bandwidth of the
HSIC methods is chosen proportional to the median norm of the sample points in the vector space.
The p-value for all independence tests will be set to 5%.
Phase amplitude coupling
We first simulate a non-linear dependency between two time series by generating two oscillations at
frequencies f1 and f2 , and introducing a modulation of the amplitude of the second oscillation by
the phase of the first one. This is achieved using the following discrete time equations:


?1 (k + 1) = ?1 (k) + .11 (k) + 2?f1 Ts
x1 (k) =
cos(?1 (k))
?2 (k + 1) = ?2 (k) + .12 (k) + 2?f2 Ts
x2 (k) = (2 + C sin ?1 (k)) cos(?2 (k))
Where the i are i.i.d normal. A simulation with f1 = 4Hz and f2 = 20Hz for a sampling frequency 1/Ts =100Hz is plotted on Figure 1 (top-left panel). For the parameters of the periodogram,
we used a window length of 50 samples (.5 s). We used a Gaussian RBF kernel to compute nonlinear dependencies between the two time series after standardizing each of them (divide them by
their standard deviation). The top-middle and top-right panels of Figure 1 plot the mean and standard errors of the estimate of the squared Hilbert-Schmidt norm for this system (for C = .1) for a
linear and a Gaussian RBF kernel (with ? = 1) respectively. The bias of the first estimate appears
clearly in both cases at the two power picks of the signals for the biased estimate. In the second
(unbiased) estimate, the spectrum exhibits a zero mean for all but one peak (at 4Hz for the RBF
kernel), which corresponds to the expected frequency of non-linear interaction between the time
series. The observed negative values are also a direct consequence of the unbiased property of our
estimate (Corollary 8). The influence of the bandwidth parameter of the kernel was studied in the
case of weakly coupled time series (C = .4 ). The bottom left and middle panels of Figure 1 show
6

.1

.2

.1

1
.7

.2

.7

2

.2

.4

3

.1

1
.01

.1

.89

.7

3

.3

.1

biased
unbiased

15
10
5
0
0.1

0.2

0.5

1

2

frequency (Hz)

5

10

20

1

2

time (s)

3

4

100

error rate (%)

20

0

.1

Transition probabilities

KCSD norm estimate

0.5
0
?0.5

.6

2

.5

state 3
state 2
state 1

type I error
type II error

80
60
40
20
0

block hsic

hsic

kcsd

Figure 2: Markov chain dynamical system. Upper left: Markov transition probabilities, fluctuating
between the values indicated in both graphs. Upper right: example of simulated time series. Bottom
left: the biased and unbiased KCSD norm estimates in the frequency domain. Bottom right: type I
and type II errors for hsic and kcsd tests
the influence of this parameter on the number of samples required to actually reject the null hypothesis and detect the dependency for biased and unbiased estimates respectively. It was observed that
choosing an hyper-parameter close to the standard deviation of the signal (here 1.5) was an optimal
strategy, and that the test relying on the unbiased estimate outperformed the biased estimate. We
thus used the unbiased estimate in our subsequent analysis. The coupling parameter C was further
varied to test the performance of independence tests both in case the null hypothesis of independence is true (C=0), and when it should be rejected (C = .4 for weak coupling, C = 2 for strong
coupling). These two settings enable to quantify the type I and type II error of the tests, respectively.
The bottom-right panel of Figure 1 reports these errors for several independence tests. Showing the
superiority of our method especially for type II errors. In particular, methods based on HSIC fail to
detect weak dependencies in the time series.
Time varying Markov chain
We now illustrate the use of our test in an hybrid setting. We generate a symbolic time series x2
using the alphabet S = [1, 2, 3], controlled by a scalar time series x1 . The coupling is achieved by
modulating across time the transition probabilities of the Markov transition matrix generating the
symbolic time series x2 using the current value of the scalar time series x1 . This model is described
by the following equations with f1 = 1Hz.
(
?1 (k + 1)
= ?1 (k) + .11 (k) + 2?f1 Ts
x1 (k + 1)
=
sin(?1 (k + 1))
p(x2 (k + 1) = Si |x2 (k) = Sj ) =
Mij + ?Mij x1 (k)
Since x1 is bounded between -1 and 1, the Markov transition matrix fluctuates across time between
two models represented Figure 2 (top-left panel). A model without these fluctuations (?M = 0)
was simulated as well to measure type I error. The time course of such an hybrid system is illustrated
on the top-right panel of the same figure. In order to measure the dependency between these two
time series, we use a k-spectrum kernel [14] for x2 and a RBF kernel for x1 . For the k-spectrum
kernel, we use k=2 (using k=1, i.e. counting occurrences of single symbols was less efficient) and
we computed the kernel between words of 3 successive symbols of the time series. We used an RBF
kernel with ? = 1, decimated the signals by a factor 2 and signals were cut in time windows of 100
samples. The biased and unbiased estimates of the KCSD norm are represented at the bottom-left
of Figure 2 and show a clear peak at the modulating frequency (1Hz). The independence test results
shown at the bottom-right of Figure 2 illustrate again the superiority of KCSD for type II error,
whereas type I error stays in an acceptable range.
7

Figure 3: Left: Experimental setup of LFP recordings in anesthetized monkey during visual stimulation with a movie. Right: Proportion of detected dependencies for the unbiased kcsd test of
interactions between Gamma band and wide band LFP for different kernels.
Neural data: local field potentials from monkey visual cortex
We analyzed dependencies between local field potential (LFP) time series recorded in the primary
visual cortex of one anesthetized monkey during visual stimulation by a commercial movie (see
Figure 3 for a scheme of the experiment). LFP activity reflects the non-linear interplay between a
large variety of underlying mechanisms. Here we investigate this interplay by extracting LFP activity
in two frequency bands within the same electrode and quantify the non-linear interactions between
them with our approach. LFPs were filtered into two frequency bands: 1/ a wide band ranging from
1 to 100Hz which contains a rich variety of rhythms and 2/ a high gamma band ranging from 60 to
100Hz which as been shown to play a role in the processing of visual information.
Both of these time series were sampled at 1000Hz. Using non-overlapping time windows of 1s
points, we computed the Hilbert-Schmidt norm of the KCSD operator between gamma and large
band time series originating from the same electrode. We performed statistical testing for all frequencies between 1 and 500Hz (using a Fourier transform on 2048 points). The results of the test
averaged over all recording sites is plotted on Figure 3. We observe a highly reliable detection of
interactions in the gamma band, using either a linear or non-linear kernel. This is due to the fact
that the Gamma band LFP is a filtered version of the wide band LFP, making these signals highly
correlated in the Gamma band. However, in addition to this obvious linear dependency, we observe
significant interactions in the lowest frequencies (0.5-2Hz) which can not be explained by linear interaction (and is thus not detected by the linear kernel). This characteristic illustrates the non-linear
interaction between the high frequency gamma rhythm and other lower frequencies of the brain electrical activity, which has been reported in other studies [21]. This also shows the interpretability of
our approach as a test of non-linear dependency in the frequency domain.

5

Conclusion

An independence test for time series based on the concept of Kernel Cross Spectral Density estimation was introduced in this paper. It generalizes the linear approach based on the Fourier transform
in several respects. First, it allows quantification of non-linear interactions for time series living
in vector spaces. Moreover, it can measure dependencies between more complex objects, including sequences in an arbitrary alphabet, or graphs, as long as an appropriate positive definite kernel
can be defined in the space of each time series. This paper provides asymptotic properties of the
KCSD estimates, as well as an efficient approach to compute them on real data. The space of KCSD
operators constitutes a very general framework to analyze dependencies in multivariate and highly
structured dynamical systems. Following [13, 18], our independence test can further be combined
to recent developments in kernel time series prediction techniques [20] to define general and reliable
multivariate causal inference techniques.
Acknowledgments. MB is grateful to Dominik Janzing for fruitful discussions and advice.

8

References
[1] A. Berlinet and C. Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics.
Kluwer Academic Boston, 2004.
[2] M. Besserve, D. Janzing, N. Logothetis, and B. Sch?olkopf. Finding dependencies between frequencies
with the kernel cross-spectral density. In IEEE International Conference on Acoustics, Speech and Signal
Processing, pages 2080?2083, 2011.
[3] G. Blanchard, O. Bousquet, and L. Zwald. Statistical properties of kernel principal component analysis.
Machine Learning, 66(2-3):259?294, 2007.
[4] D. Brillinger. Time series: data analysis and theory. Holt, Rinehart, and Winston, New York, 1974.
[5] J.-F. Cardoso. High-order contrasts for independent component analysis. Neural computation, 11(1):157?
192, 1999.
[6] K. Fukumizu, F. Bach, and A. Gretton. Statistical convergence of kernel CCA. In Advances in Neural
Information Processing Systems 18, pages 387?394, 2006.
[7] K. Fukumizu, F. Bach, and M. Jordan. Dimensionality reduction for supervised learning with reproducing
kernel Hilbert spaces. J. Mach. Learn. Res., 5:73?99, 2004.
[8] K. Fukumizu, A. Gretton, G. R. Lanckriet, B. Sch?olkopf, and B. K. Sriperumbudur. Kernel choice and
classifiability for RKHS embeddings of probability distributions. In Advances in Neural Information
Processing Systems 21, pages 1750?1758, 2009.
[9] K. Fukumizu, A. Gretton, X. Sun, and B. Sch?olkopf. Kernel Measures of Conditional Dependence. In
Advances in Neural Information Processing Systems 20, pages 489?496, 2008.
[10] G. B. Giannakis and J. M. Mendel. Identification of nonminimum phase systems using higher order
statistics. Acoustics, Speech and Signal Processing, IEEE Transactions on, 37(3):360?377, 1989.
[11] A. Gretton, K. Fukumizu, C. Teo, L. Song, B. Sch?olkopf, and A. Smola. A kernel statistical test of
independence. In Advances in Neural Information Processing Systems 20, pages 585?592. 2008.
[12] A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, K. Fukumizu, and B. K. Sriperumbudur. Optimal kernel choice for large-scale two-sample tests. In Advances in Neural Information
Processing Systems 25, pages 1214?1222, 2012.
[13] A. Hyv?arinen, S. Shimizu, and P. O. Hoyer. Causal modelling combining instantaneous and lagged effects:
an identifiable model based on non-gaussianity. In Proceedings of the 25th International Conference on
Machine Learning, pages 424?431. ACM, 2008.
[14] C. Leslie, E. Eskin, and W. Noble. The spectrum kernel: a string kernel for SVM protein classification.
In Pac Symp Biocomput., 2002.
[15] C. Nikias and A. Petropulu. Higher-Order Spectra Analysis - A Non-linear Signal Processing Framework.
Prentice-Hall PTR, Englewood Cliffs, NJ, 1993.
[16] D. Pantazis, T. Nichols, S. Baillet, and R. Leahy. A comparison of random field theory and permutation
methods for the statistical analysis of MEG data. NeuroImage, 25:383 ? 394, 2005.
[17] J. Pearl. Causality - Models, Reasoning, and Inference. Cambridge University Press, Cambridge, UK,
2000.
[18] J. Peters, D. Janzing, and B. Sch?olkopf. Causal inference on time series using structural equation models.
In Advances in Neural Information Processing Systems 26, 2013.
[19] B. Sch?olkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.
[20] V. Sindhwani, H. Q. Minh, and A. C. Lozano. Scalable matrix-valued kernel learning for high-dimensional
nonlinear multivariate regression and granger causality. In Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence, 2013.
[21] K. Whittingstall and N. K. Logothetis. Frequency-band coupling in surface EEG reflects spiking activity
in monkey visual cortex. Neuron, 64:281?9, 2009.
[22] X. Zhang, L. Song, A. Gretton, and A. Smola. Kernel Measures of Independence for Non-IID Data. In
Advances in Neural Information Processing Systems 21, pages 1937?1944, 2009.

9

"
3405,"Batch Bayesian Optimization
via Simulation Matching

Javad Azimi, Alan Fern, Xiaoli Z. Fern
School of EECS, Oregon State University
{azimi, afern, xfern}@eecs.oregonstate.edu

Abstract
Bayesian optimization methods are often used to optimize unknown functions that
are costly to evaluate. Typically, these methods sequentially select inputs to be
evaluated one at a time based on a posterior over the unknown function that is
updated after each evaluation. In many applications, however, it is desirable to
perform multiple evaluations in parallel, which requires selecting batches of multiple inputs to evaluate at once. In this paper, we propose a novel approach to
batch Bayesian optimization, providing a policy for selecting batches of inputs
with the goal of optimizing the function as efficiently as possible. The key idea is
to exploit the availability of high-quality and efficient sequential policies, by using
Monte-Carlo simulation to select input batches that closely match their expected
behavior. Our experimental results on six benchmarks show that the proposed approach significantly outperforms two baselines and can lead to large advantages
over a top sequential approach in terms of performance per unit time.

1

Introduction

We consider the problem of maximizing an unknown function f (x) when each evaluation of the
function has a high cost. In such cases, standard optimization techniques such as empirical gradient
methods are not practical due to the high number of function evaluations that they demand. Rather,
Bayesian optimization (BO) methods [12, 4] have demonstrated significant promise in their ability
to effectively optimize a function given only a small number of evaluations. BO gains this efficiency
by leveraging Bayesian models that take into account all previously observed evaluations in order
to better inform future evaluation choices. In particular, typical BO methods continually maintain a
posterior over f (x) that is used to select the next input to evaluate. The result of the evaluation is
then used to update the posterior and the process repeats. There are a number of well established
policies for selecting the next input to evaluate given the current posterior. We will refer to such
policies as sequential policies to stress the fact that they select one input at a time.
In many applications it is possible and desirable to run multiple function evaluations in parallel.
This is the case, for example, when the underlying function corresponds to a controlled laboratory
experiment where multiple experimental setups are examined simultaneously, or when the underlying function is the result of a costly computer simulation and multiple simulations can be run across
different processors in parallel. In such cases, existing sequential policies are not sufficient. Rather,
batch mode BO is more appropriate, where policies select a batch of multiple inputs to be evaluated
at once. To the best of our knowledge and as noted in [4], there is no established work on BO that
considers the batch selection problem, except for a brief treatment in [21]. The main contribution of
this work is to propose an approach to batch BO and to demonstrate its effectiveness.
The key motivation behind our approach comes from the fact that the sequential mode of BO has a
fundamental advantage over BO in batch mode. This is because in sequential mode, each function
evaluation is immediately used to obtain a more accurate posterior of f (x), which in turn will allow
1

a selection policy to make more informed choices about the next input. Given an effective sequential
selection policy, our goal is then to design a batch policy that approximates its behavior.
In particular, our batch policy attempts to select a batch that ?matches? the expected behavior of a
sequential policy as closely as possible. The approach generates Monte-Carlo simulations of a sequential policy given the current posterior, and then derives an optimization problem over possible
batches aimed at minimizing the loss between the sequential policy and the batch. We consider two
variants of this optimization problem that yield a continuous weighted k-means problem and a combinatorial weighted k-medoid problem. We solve the k-means variant via k-means clustering and
show that the k-medoid variant corresponds to minimizing a non-increasing supermodular function,
for which there is an efficient approximation algorithm [9].
We evaluate our approach on a collection of six functions and compare it to random and another
baseline batch policy based on submodular maximization. The results show that our approach significantly outperforms these baselines and can lead to large advantages over a top sequential approach
in terms of performance per unit time.

2

Problem Setup

Let X ? Rn be an n-dimensional input space, where we will often refer to elements of X as an
experiment and assume that each dimension i is bounded in [Ai , Bi ]. We assume an unknown realvalued function f : X ? R, which represents the expected value of the dependent variable after
running an experiment. For example, f (x) might correspond to the result of a wet-lab experiment
or a computer simulation with input parameters x. Conducting an experiment x produces a noisy
outcome y = f (x) + , where  is a noise term that might be 0 in some applications.
Our objective is to find an experiment x ? X that approximately maximizes f by requesting a
limited number of experiments and observing their outcomes. Furthermore we are interested in
applications where (1) running experiments is costly (e.g. in terms of laboratory or simulation time);
and (2) it is desirable to run k > 1 experiments in parallel. This motivates the problem of selecting
a sequence of batches, each containing k experiments, where the choice of a batch can depend on
the results observed from all previous experiments. We will refer to the rule for selecting a batch
based on previous experiments as the batch policy. The main goal of this paper is to develop a batch
policy that optimizes the unknown function as efficiently as possible.
Due to the high cost of experiments, traditional optimization techniques such as empirical gradient
ascent are not practical for our setting, due to their high demands on the number of experiments.
Rather, we build on Bayesian optimization (BO) [10, 12, 4], which leverages Bayesian modeling
in an attempt to achieve more efficient optimization. In particular, BO maintains a posterior over
the unknown function based on previously observed experiments, e.g. represented via a Gaussian
Process (GP) [19]. This posterior is used to select the next experiment to be run in a way that attempts
to trade-off exploring new parts of the experimental space and exploiting parts that look promising.
While the BO literature has provided a number of effective policies, they are all sequential policies,
where only a single experiment is selected and run at a time. Thus, the main novelty of our work is
in defining a batch policy in the context of BO, which is described in the next section.

3

Simulation Matching for Batch Selection

Given a data set D of previously observed experiments, which induces a posterior distribution over
the unknown function, we now consider how to select the next batch of k experiments. A key issue in
making this choice is to manage the trade-off between exploration and exploitation. The policy must
attempt to explore by requesting experiments from unexplored parts of the input space, at the same
time also attempt to optimize the unknown function via experiments that look promising given the
current data. While, under most measures, optimizing this trade-off is computationally intractable,
there are a number of heuristic sequential policies from the BO literature that are computationally
efficient and perform very well in practice. For example, one such policy selects the next experiment
to be the one that has the ?maximum expected improvement? according to the current posterior
[14, 10]. The main idea behind our approach is to leverage such sequential policies by selecting a
batch of k > 1 experiments that ?closely matches? the sequential policy?s expected behavior.
More formally, let ? be a sequential policy. Given a data set D of prior experimental results, ? returns
the next experiment x ? X to be selected. As is standard in BO, we assume we have a posterior
2

density P (f | D) over the unknown function f , such as a Gaussian Process. Given this density we
can define a density over the outcomes of executing policy ? for k steps, each outcome consisting
of a set of k selected experiments. Let S?k be the random variable denoting the set of k experiments
resulting from such k-step executions, which has a well defined density over all possible sets given
the posterior of f . Importantly, it is generally straightforward to use Monte Carlo simulation to
sample values of S?k .1 Our batch policy is based on generating a number of samples of S?k , which
are used to define an objective for optimizing a batch of k experiments. Below we describe this
objective and a variant, followed by a description of how we optimize the proposed objectives.
3.1 Batch Objective Function
Our goal is to select a batch B of k experiments that best ?matches the expected behavior? of a base
sequential policy ? conditioned on the observed data D. More precisely, we consider a batch B to
be a good match for a policy execution if B contains an experiment that is close to the best of the k
experiments selected by the policy. To specify this objective we first introduce some notation. Given
a function f and a set of experiments S, we define x? (f, S) = arg maxx?S f (x) to be the maximizer
of f in S. Also, for any experiment x and set B we define nn(x, B) = arg minx0 ?B k x ? x0 k to
be the nearest neighbor of x in set B. Our objective can now be written as selecting a batch B that
minimizes




OBJ(B) = ES?k Ef |S?k k x? (f, S?k ) ? nn(x? (f, S?k ), B) k2 | D | D .
Note that this nested expectation is the result of decomposing the joint posterior over S?k and f as
P (f, S?k | D) = P (f | S?k , D) ? P (S?k | D). If we assume that the unknown function f (x) is
Lipschitz continuous then minimizing this objective can be viewed as minimizing an upper bound
on the expected performance difference between the sequential policy and the selected batch. Here
the performance of a policy or a batch is equal to the output value of the best selected experiment.
We will approximate this objective by replacing the outer expectation over S?k with a sample average
over n samples {S1 , . . . , Sn } of S?k as follows, recalling that each Si is a set of k experiments:


1X
OBJ(B) ?
Ef |Si k x? (f, Si ) ? nn(x? (f, Si ), B) k2 | D
n i
1XX
=
Pr(x = x? (f, Si ) | D, Si )? k x ? nn(x, B) k2
n i
x?Si
1XX
=
?i,x ? k x ? nn(x, B) k2
(1)
n i
x?Si

The second step follows by noting that x? (f, Si ) must be one of the k experiments in Si .
We now define our objective as minimizing (1) over batch B. The objective corresponds to a
weighted k-means clustering problem, where we must select B to minimize the weighted distortion between the simulated points and their closest points in B. The weight on each simulated
experiment ?i,x corresponds to the probability that the experiment x ? Si achieves the maximum
value of the unknown f among the experiments in Si , conditioned on D and the fact that S?k = Si .
We refer to this objective as the k-means objective.
We also consider a variant of this objective where the goal is to find a B that minimizes
(1) under
S
the constraint that B is restricted to experiments in the simulations, i.e. B ? i Si s.t. |B| = k.
This objective corresponds to the weighted k-medoid clustering problem, which is often considered
to improve robustness to outliers in clustering. Accordingly we will refer to this objective as the
k-medoid objective and note that given a fixed set of simulations this corresponds to a discrete
optimization problem.
3.2 Optimization Approach
The above k-means and k-medoid objectives involve the weights ?i,x = P (x = x?i (f ) | D, S?k =
Si ), for each x ? Si . In general these weights will be difficult to compute exactly, particularly
1
For example, this can be done by starting with D and selecting the first experiment x1 using ? and then
using P (f | D) to simulate the result y1 of experiment x1 . This simulated experiment is added to D and the
process repeats for k ? 1 additional experiments.

3

Algorithm 1 Greedy Weighted k-Medoid Algorithm
Input:S = {(x1 , w1 ), . . . , (xm , wm )}, k
Output:B
B ? {x1 , . . . , xm } // initialize batch to all data points
while |B| > k do P
m
x ? arg minx?B j=1 wj ? k xj ? nn(xj , B \ x) k // point that influences objective the least
B ?B\x
end while
return B

due to the conditioning on the set Si . In this work, we approximate those weights by dropping the
conditioning on Si , for which it is then possible to derive a closed form when the posterior over f is
represented as a Gaussian Process (GP). We have found that this approach leads to good empirical
performance. In particular, instead of using the weights ?i,x we use the weights ?
? i,x = P (x =
x?i (f ) | D). When the posterior over f is represented as a GP, as in our experiments, the joint
distribution over experimental outcomes in Si = {xi,1 , . . . , xi,k } is normally distributed. That is,
the random vector hf (xi,1 ), . . . , f (xi,k )i ? N (?, ?), where the mean ? and covariance ? have
standard closed forms given by the GP conditioned on D. From this, it is clear that for a GP the
computation of ?
? i,x is equivalent to computing the probability that the ith component of a normally
distributed vector is larger than the other components. A closed form solution for this probability is
given by the following proposition.

Proposition 1. If (y1 , y2 , . . . , yk ) ? N ?y , ?y then for any i ? {1, . . . , k},
P (yi ? y1 , yi ? y2 , . . . , yi ? yk ) =

k?1
Y

(1 ? ?(??j ))

(2)

j=1

? 1
where ?(.) is standard normal cdf, ? = (?1 , ?2 , ? ? ?, ?k?1 ) = A?y A0 2 A?y , such that A ?
R(k?1)?k is a sparse matrix that for any j = 1, 2, ? ? ?, k ? 1 we have Aj,i = 1, and for any 1 ? p < i
we have Ap,p = ?1 , and for any i < p ? k we have Ap?1,p = ?1.
Using this approach to compute the weights we can now consider optimizing the k-means and kmedoid objectives from (1), both of which are known to be NP-hard problems. For the k-means
objective we solveSforSthe set B by simply applying the k-means clustering algorithm [13] to the
weighted data set i x?Si {(x, ?
? i,x )}. The k cluster centers are returned as our batch B.
The k-medoid objective is well known [22] and the weighted k-medoid clustering algorithm [11]
has been shown to perform well and be robust to outliers in the data. While we have experimented
with this algorithm and obtained good results, we have achieved results that are as good or better
using an alternative greedy algorithm that provides certain approximation guarantees. Pseudo-code
for this algorithm is shown in Figure 1. The input to the algorithm is the set of weighted experiments
and the batch size k. The algorithm initializes the batch B to include all of the input experiments,
which achieves the minimum objective value of zero. The algorithm then iteratively removes one
experiment from B at a time until |B| = k, each time removing the element whose removal results
in the smallest increase in the k-medoid objective.
This greedy algorithm is motivated by theoretical results on the minimization of non-increasing,
supermodular set functions.
Definition 1. Suppose S is a finite set, f : 2S ? R+ is a supermodular set function if for all
B1 ? B2 ? S and {x} ? S \ B2 , it holds that f (B1 ) ? f (B1 ? {x}) ? f (B2 ) ? f (B2 ? {x}).
Thus, a set function is supermodular if adding an element to a smaller set provides no less improvement than adding the element to a larger set. Also, a set function is non-increasing if for any set
S and element x if f (S) ? f (S ? {x}). It can be shown that our k-medoid objective function of
(1) is bothSa non-increasing and supermodular function of B and achieves a minimum value of zero
for B = i Si . It follows that we can obtain an approximation guarantee for the described greedy
algorithm in [9].
4

Theorem 1. [9] Let f be a monotonic non-increasing supermodular function over subsets of the
finite set S, |S| = m and f (S) = 0. Let B be the set of the elements returned by the greedy
algorithm 1 s.t |B| = k, q = m ? k and B ? = argminB 0 ?S,|B 0 |=k f (B 0 ), then


q
1
q+t
et ? 1
f (B) ?
? 1 f (B ? ) ?
f (B ? )
(3)
t
q
t
where t is the steepness parameter [9] of function f .
Notice that the approximation bound involves the steepness parameter t of f , which characterizes
the rate of decrease of f . This is unavoidable since it is known that achieving a constant factor
approximation guarantee is not possible unless P=NP [17]. Further this bound has been shown to be
tight for any t [9]. Note that this is in contrast to guarantees for greedy maximization of submodular
functions [7] for which there are constant factor guarantees. Also note that the greedy algorithm
we use is qualitatively different from the one used for submodular maximization, since it greedily
removes elements from B rather than greedily adding elements to B.

4

Implementation Details and Baselines

GP Posterior. Our batch selection approach described above requires that we maintain a posterior
over the unknown function f . For this purpose we use a zero-mean GP prior with a zero-mean
Gaussian noise model with variance equal to 0.01. The GP covariance is specified by a Gaussian
1
2
kernel K(x, x0 ) = ? exp ? 2w
where ymax is the
k x ? x0 k2 , with signal variance ? = ymax
maximum value of the unknown function. In all of our experiments we used a simple rule of thumb
Pd
to set the kernel width w to 0.01 i=1 li where li is the input space length in dimension i. We have
found this rule to work well for a variety of problems. An alternative would be to use a validationbased approach for selecting the kernel parameters. In the BO setting, however, we have found this
to be unreliable since the number of data points is relatively small.
Base Sequential Policy. Our batch selection approach also requires a base sequential policy ? to be
used for simulation matching. This policy must be able to select the next experiment given any set
of prior experimental observations D. In our experiments, we use a policy based on the Maximum
Expected Improvement (MEI) heuristic [14, 10] which is a very successful sequential policy for BO
and has been shown to converge in the limit to the global optimum. Given data D the MEI policy
simply selects the next experiment to be the one that maximizes the expected improvement over the
current set of experiments with respect to maximizing the unknown function. More formally, let y ?
be the value of the best/largest experimental outcome observed so far in D. The MEI value of an
experiment x is given by MEI(x) = Ef [max{f (x) ? y ? , 0} | D]. For our GP posterior over f we
?
??(x)
can derive a closed form for this given by: u = y ?(x)
where y ? is our best currently observed
value. For any given example x, the MEI can be computed as follows:
MEI(x)

=

?(x) [?u?(?u) + ?(u)] ,

u=

y ? ? ?(x)
?(x)

where ? and ? are the standard normal cumulative distribution and density functions and ?(x) and
?(x) are the mean and variance of f (x) according to the GP given D, which have simple closed
forms. Note that we have also evaluated our simulation-matching approach with an alternative
sequential policy known as Maximum Probability of Improvement [16, 10]. The results (not shown
in this paper) are similar to those obtained from MEI, showing that our general approach works well
for different base policies.
The computation of the MEI policy requires maximizing MEI(x) over the input space X . In general, this function does not have a unique local maximum and various strategies have been tried for
maximizing it. In our experiments, we (approximately) maximize the MEI function using the DIRECT black-box optimization procedure, which has shown good optimization performance as well
as computational efficiency in practice.
Baseline Batch Policies. To the best of our knowledge there is no well-known batch policy for
Bayesian optimization. However, in our experiments we will compare against two baselines. The
first baseline is random selection, where a batch of k random experiments is returned at each step. Interestingly, in the case of batch active learning for classification, the random batch selection strategy
5

Function
Cosines
Rosenbrock
Michalewicz

Table 1: Benchmark Functions.
Mathematical representation
1 ? (u2 + v 2 ? 0.3cos(3?u) ? 0.3cos(3?v))
u = 1.6x ? 0.5, v = 1.6y ? 0.5
10 ? 100(y ? x2 )2 ? (1 ? x)2
  2 20
P5
i.x
? i=1 sin(xi ). sin ? i

has been surprisingly effective and is often difficult to outperform with more sophisticated strategies
[8]. However, as our experiments will show, our approach will dominate random.
Our second, more sophisticate, baseline is based on selecting a batch of experiments whose expected
maximum output is the largest. More formally, we consider selecting a size k batch B that maximizes the objective Ef [maxx?B f (x) | D], which we will refer to as the EMAX objective. For
our GP prior, each set B = {x1 , . . . , xk } can be viewed as defining a normally distributed vector hf (x1 ), . . . , f (xk )i ? N (?, ?). Even in this case, finding the optimal set B is known to be
NP-hard. However, for the case where f is assumed to be non-negative, the EMAX objective is
a non-negative, submodular, non-decreasing function of B. Together these properties imply that a
simple greedy algorithm can achieve an approximation ratio of 1 ? e?1 [7]. The algorithm starts
with an empty B and greedily adds experiments to B, each time selecting the one that improves the
EMAX objective the most. Unfortunately, in general there is no closed form solution for evaluating
the EMAX objective, even in our case of normally distributed vectors [20]. Therefore, to implement the greedy algorithm, which requires many evaluations of the EMAX objective, we use Monte
Carlo sampling, where for a given set B we sample the corresponding normally distributed vector
and average the maximum values across the samples.

5

Experimental Results

In this section we evaluate our proposed batch BO approach and the baseline approaches on six
different benchmarks.
5.1 Benchmark Functions
We consider three well-known synthetic benchmark functions: Cosines and Rosenbrock [1, 5],
which are over [0, 1]2 , and Michalewicz [15], which is over [0, ?]5 . Table 1 gives the formulas
for each of these functions. Two additional benchmark functions Hydrogen and FuelCell, which
range over [0, 1]2 , are derived from real-world experimental data sets. In both cases, the benchmark function was created by fitting regression models to data sets resulting from real experiments.
The Hydrogen data set is the result of data collected as part of a study on biosolar hydrogen production [6], where the goal was to maximize the hydrogen production of a particular bacteria by
optimizing the PH and Nitrogen levels of the growth medium. The FuelCell data set was collected
as part of a study investigating the influence of anodes? nano-structure on the power output of microbial fuel cells [3]. The experimental inputs include the average area and average circularity of
the nano-particles [18]. Contour plots of the four 2-d functions are shown in Figure 1.
The last benchmark function is derived from the Cart-Pole [2] problem, which is a commonly used
reinforcement learning problem. The goal is to optimize the parameters of a controller for a wheeled
cart with the objective of balancing a pole. The controller is parameterized by four parameters
giving a 4-d space of experiments in [1, ?1]4 . Given a setting for these parameters, the benchmark
function is implemented by using the standard Cart-Pole simulator to return the reward received for
the controller.
5.2 Results
Figures 2 and 3 show the performance of our methods on all six benchmark functions for batch sizes
5 and 10 respectively. Each graph contains 5 curves, each corresponding to a different BO approach
(see below). Each curve is the result of taking an average of 100 independent runs. The x-axis of
each graph represents the total number of experiments and the y-axis represents the regret values,
where the regret of a policy at a particular point is the difference between the best possible output
value (or an upper bound if the value is not known) and the best value found by the policy. Hence the
regret is always positive and smaller values are preferred. Each run of a policy initializes the data set
to contain 5 randomly selected experiments for the 2-d functions and 20 random initial experiments
for the higher dimensional functions.
6

1

1

1

0.9

0.9

0.9

0.8

0.8

0.8

0.7

0.7

0.7

0.6

0.6

0.6

0.5

0.5

0.5

0.4

0.4

0.4

0.3

0.3

0.3

0.2

0.2

0.2

0.1

0.1

0

0

0.1

0.2

0.3

Fuel Cell

0.4

0.5

0.6

0.7

0.8

0.9

0

1

0.1

0

0.1

Hydrogen

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

0

1

0

0.1

Cosines

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Rosenbrock

Figure 1: The contour plots for the four 2?dimension proposed test functions.

0.65

0.3

Sequential
k?medoid
k?means
EMAX
Random

0.6
0.55
0.5

Sequential
k?medoid
k?means
EMAX
Random

0.6

0.5

0.45
0.4
0.35

Regret

0.2

Regret

Regret

Sequential
k?medoid
k?means
EMAX
Random

0.25

0.15

0.4

0.3

0.1

0.3

0.2

0.05

0.25
0.2
10

15

20
25
# of Experimets

30

0
10

35

15

20
25
# of Experimets

Fuel Cell

20
25
# of Experimets

0.35

Cosines

700

Regret

0.15

300

0.05
200
25

35

2.6

500

0.1

30

2.7

600

400

20
25
# of Experimets

Sequential
k?medoid
k?means
EMAX
Random

2.8

0.2

35

3

800

0.3
0.25

30

2.9

Regret

0.4

15

15

Hydrogen
Sequential
k?medoid
k?means
EMAX
Random

0.45

Regret

0.1
10

35

900

0.5

0
10

30

2.5
2.4
2.3

Sequential
k?medoid
k?means
EMAX
Random

50

2.2
2.1

75

Rosenbrock

100
125
# of Experimets

150

175

2
25

200

30

35

Cart-Pole

40

45
50
55
60
# of Experimets

65

70

75

80

Michalewicz

Figure 2: Performance evaluation with batch size 5.
0.25

0.65
Sequential
k?medoid
k?means
EMAX
Random

0.6
0.55

0.6

Sequential
k?medoid
k?means
EMAX
Random

0.2

0.5
0.45

0.4

0.15

Regret

Regret

Regret

0.5
0.45

0.25

0.3

0.05

0.2

0.25

0.15
20

25
# of Experimets

30

0
15

35

20

Fuel Cell

30

0.1
15

35

20

Hydrogen
Sequential
k?medoid
k?means
EMAX
Random

0.3
0.25

25
# of Experimets

30

35

Cosines
3
Sequential
k?medoid
k?means
EMAX
Random

2.9

800

2.8
700

2.7
600

0.15

500

0.1

400

0.05

300

20

25
# of Experimets

Rosenbrock

30

35

200
30

Regret

0.2

Regret

Regret

25
# of Experimets

900

0.35

0
15

0.4
0.35
0.3

0.1

0.35

0.2
15

Sequential
k?medoid
k?means
EMAX
Random

0.55

2.6
2.5
2.4

Sequential
k?medoid
k?means
EMAX
Random

60

2.3
2.2
90
120
# of Experimets

150

180

200

2.1
30

40

Cart-Pole
Figure 3: Performance evaluation with batch size 10.
7

50
60
# of Experimets

Michalewicz

70

80

Each graph gives curves for four batch approaches including our baselines Random and EMAX,
along with our proposed approaches based on the k-means and k-medoid objectives, which are
optimized by weighted k-means clustering and the greedy Algorithm 1 respectively. In addition, for
reference we plot the performance of the base Sequential MEI BO policy (k = 1) on each graph.
Note that since the batch approaches request either 5 or 10 experiments at a time, their curves only
contain data points at those intervals. For example, for the batch size 5 results the first point on a
batch curve corresponds to 10 experiments, including the initial 5 experiments and the first requested
batch. The next point on the batch curve is for 15 experiments which includes the next requested
batch and so on. Rather the Sequential policy has a point at every step since it requests experiments
one at a time. It is important to realize that we generally expect a good sequential policy to do better,
or no worse, than a batch policy with respect to performance per number of experiments. Thus, the
Sequential curve can be typically viewed as an upper performance bound and provides an indication
of how much loss is incurred when moving to a batch setting in terms of efficiency per experiment.
Comparison to Baselines. The major observation from our results is that for all benchmarks and
for both batch sizes the proposed k-means and k-medoid approaches significantly outperform the
baselines. This provides strong validation for our proposed simulation-matching approach to batch
selection.
k-means vs. k-medoid. In most cases, the k-means and k-medoid approaches perform similarly.
However, for both batch sizes k-medoid often does shows a small improvement over k-means and
appears to have a significant advantage in FuelCell. The only exception is in Hydrogen where kmeans shows a small advantage over k-medoid for small numbers of experiments. Overall, both
approaches appear to be effective and in these domains k-medoid has a slight edge.
Batch vs. Sequential. The advantage of Sequential over our batch approaches varies with the benchmark. However, in most cases, our proposed batch approaches catch up to Sequential in a relatively
small number of experiments and in some cases, the batch policies are similar to Sequential from
the start. The main exception is Cart-Pole for batch size 10, where the batch policies appear to be
significantly less efficient in terms of performance versus number of experiments. Generally, we see
that the difference between our batch policies and Sequential is larger for batch size 10 than batch
size 5, which is expected, since larger batch sizes imply that less information per experiment is used
in making decisions.
It is clear, however, that if we evaluate the performance of our batch policies in terms of experimental time, then there is a very significant advantage over Sequential. In particular, the amount of
experimental time for a policy is approximately equal to the number of requested batches, assuming
that the batch size is selected to allow for all selected experiments to be run in parallel. This means,
for example, that for the batch size 5 results, 5 time steps for the batch approaches correspond to
30 total experiments (5 initial + 5 batches). We can compare this point to the first point on the
Sequential curve, which also corresponds to 5 time steps (5 experiments beyond the initial 5). In all
cases, the batch policies yield a very large improvement in regret reduction per unit time, which is
the primary motivation for batch selection.

6

Summary and Future Work

In this paper we introduced a novel approach to batch BO based on the idea of simulation matching.
The key idea of our approach is to design batches of experiments that approximately match the
expected performance of high-quality sequential policies for BO. We considered two variants of
the matching problem and showed that both approaches significantly outperformed two baselines
including random batch selection on six benchmark functions. For future work we plan to consider
the general idea of simulation matching for other problems, such as active learning, where there are
also good sequential policies and batch selection is often warranted. In addition, we plan to consider
less myopic approaches for selecting each batch and the problem of batch size selection, where there
is a choice about batch size that must take into account the current data and experimental budget.

Acknowledgments
The authors acknowledge the support of the NSF under grants IIS-0905678.
8

References
[1] B. S. Anderson, A. W. More, and D. Cohn. A nonparametric approach to noisy and costly optimization.
In ICML, 2000.
[2] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that can solve difficult
learning control problems. 13:835?846, 1983.
[3] D. Bond and D. Lovley. Electricity production by geobacter sulfurreducens attached to electrodes. Applications of Environmental Microbiology, 69:1548?1555, 2003.
[4] E. Brochu, M. Cora, and N. de Freitas. A tutorial on Bayesian optimization of expensive cost functions,
with application to active user modeling and hierarchical reinforcement learning. Technical Report TR2009-23, Department of Computer Science, University of British Columbia, 2009.
[5] M. Brunato, R. Battiti, and S. Pasupuleti. A memory-based rash optimizer. In AAAI-06 Workshop on
Heuristic Search, Memory Based Heuristics and Their applications, 2006.
[6] E. H. Burrows, W.-K. Wong, X. Fern, F. W. Chaplen, and R. L. Ely. Optimization of ph and nitrogen
for enhanced hydrogen production by synechocystis sp. pcc 6803 via statistical and machine learning
methods. Biotechnology Progress, 25:1009?1017, 2009.
[7] M. F. G Nemhauser, L Wolsey. An analysis of the approximations for maximizing submodular set functions. Mathematical Programmingn, 14:265?294, 1978.
[8] Y. Guo and D. Schuurmans. Discriminative batch mode active learning. Proceedings of Advances in
Neural Information Processing Systems (NIPS2007), 6, 2007.
[9] V. P. Il?ev. An approximation guarantee of the greedy descent algorithm for minimizing a supermodular
set function. Discrete Applied Mathematics, 114(1-3):131?146, 2001.
[10] D. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of Global
Optimization, 21:345?383, 2001.
[11] L. Kaufman and P. J. Rousseeuw. Clustering by means of medoids. Statistical data analysis based on L1
norm, pages 405?416, 1987.
[12] D. Lizotte. Practical Bayesian optimization. PhD thesis, University of Alberta, 2008.
[13] S. Lloyd. Least squares quantization in PCM. IEEE Transactions on Information Theory, 28(2):129?137,
1982.
[14] M. Locatelli. Bayesian algorithms for one-dimensional globaloptimization. J. of Global Optimization,
10(1):57?76, 1997.
[15] Z. Michalewicz. Genetic algorithms + data structures = evolution programs (2nd, extended ed.).
Springer-Verlag New York, Inc., New York, NY, USA, 1994.
[16] A. Moore and J. Schneider. Memory-based stochastic optimization. In NIPS, 1995.
[17] G. Nemhauser and L. Wolsey. Integer and combinatorial optimization. Wiley New York, 1999.
[18] D. Park and J. Zeikus. Improved fuel cell and electrode designs for producing electricity from microbial
degradation. Biotechnol.Bioeng., 81(3):348?355, 2003.
[19] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT, 2006.
[20] A. M. Ross. Computing Bounds on the Expected Maximum of Correlated Normal Variables . Methodology and Computing in Applied Probability, 2008.
[21] M. Schonlau. Computer Experiments and Global Optimization. PhD thesis, University of Waterloo, 1997.
[22] H. D. Vinod. Integer programming and the theory of grouping. Journal of the American Statistical
Association, 64(326):506?519, 1969.

9

"
658,"Boxlets: a Fast Convolution Algorithm for
Signal Processing and Neural Networks

Patrice Y. Simard?, Leon Botton, Patrick Haffner and Yann LeCnn
AT&T Labs-Research
100 Schultz Drive, Red Bank, NJ 07701-7033
patrice@microsoft.com
{leon b ,haffner ,yann }@research.att.com

Abstract
Signal processing and pattern recognition algorithms make extensive use of convolution. In many cases, computational accuracy is
not as important as computational speed. In feature extraction,
for instance, the features of interest in a signal are usually quite
distorted. This form of noise justifies some level of quantization in
order to achieve faster feature extraction . Our approach consists
of approximating regions of the signal with low degree polynomials, and then differentiating the resulting signals in order to obtain
impulse functions (or derivatives of impulse functions). With this
representation, convolution becomes extremely simple and can be
implemented quite effectively. The true convolution can be recovered by integrating the result of the convolution. This method
yields substantial speed up in feature extraction and is applicable
to convolutional neural networks.

1

Introduction

In pattern recognition, convolution is an important tool because of its translation
invariance properties. Feature extraction is a typical example: The distance between
a small pattern (i.e. feature) is computed at all positions (i.e. translations) inside a
larger one. The resulting ""distance image"" is typically obtained by convolving the
feature template with the larger pattern. In the remainder of this paper we will use
the terms image and pattern interchangeably (because of the topology implied by
translation invariance).
There are many ways to convolve images efficiently. For instance, a multiplication
of images of the same size in the Fourier domain corresponds to a convolution of
the two images in the original space. Of course this requires J{ N log N operations
(where N is the number of pixels of the image and J{ is a constant) just to go in and
out of the Fourier domain. These methods are usually not appropriate for feature
extraction because the feature to be extracted is small with respect to the image.
For instance, if the image and the feature have respectively 32 x 32 and 5 x 5 pixels,
? Now with Microsoft, One Microsoft Way, Redmond, WA 98052

P Y. Simard, L. BOllou, P Haffner and Y. Le Cun

572

the full convolution can be done in 25 x 1024 multiply-adds. In contrast, it would
require 2 x J{ x 1024 x 10 to go in and out of the Fourier domain.
Fortunately, in most pattern recognition applications, the interesting features are
already quite distorted when they appear in real images. Because of this inherent
noise, the feature extraction process can usually be approximated (to a certain degree) without affecting the performance. For example, the result of the convolution
is often quantized or thresholded to yield the presence and location of distinctive
features ll]. Because precision is typically not critical at this stage (features are
rarely optimal, thresholding is a crude operation), it is often possible to quantize
the signals before the convolution with negligible degradation of performance.
The subtlety lies in choosing a quantization scheme which can speed up the convolution while maintaining the same level of performance. We now introduce the
convolution algorithm, from which we will deduce the constraints it imposes on
quantization.
The main algorithm introduced in this paper is based on a fundamental property of
convolutions. Assuming that 1 and 9 have finite support and that
denotes the
n-th integral of 1 (or the n-th derivative if n is negative), we can write the following
convolution identity:

r

(J * g)n

= r * 9 = 1 * gn

(1)

where * denotes the convolution operator. Note that 1 or 9 are not necessarily
differentiable. For instance, the impulse function (also called Dirac delta function),
denoted J, verifies the identity:

(2)
where J~ denotes the n-th integral of the delta function, translated by a (Ja(x) =
J(x - a)). Equations 1 and 2 are not new to signal processing. Heckbert has developed an effective filtering algorithm [2] where the filter 9 is a simple combination
of polynomial of degree n - 1. Convolution between a signal 1 and the filter 9 can
be written as
I*g =
*g-n
(3)
where
is the n-th integral of the signal, and the n-th derivative of the filter
9 can be written exclusively with delta functions (resulting from differentiating
n - 1 degree polynomials n times). Since convolving with an impulse function is
a trivial operation, the computation of Equation 3 can be carried out effectively.
Unfortunately, Heckbert's algorithm is limited to simple polynomial filters and is
only interesting when the filter is wide and when the Fourier transform is unavailable
(such as in variable length filters).
In contrast, in feature extraction, we are interested in small and arbitrary filters
(the features). Under these conditions, the key to fast convolution is to quantize
the images to combinations of low degree polynomials, which are differentiated,
convolved and then integrated. The algorithm is summarized by equation:
1 * 9 ~ F * C = (F- n * C-m)m+n
(4)

r

r

where F and C are polynomial approximation of 1 and g, such that F- n and
C- m can be written as sums of impulse functions and their derivatives. Since the
convolution F- n *C- m only involves applying Equation 2, it can be computed quite
effectively. The computation of the convolution is illustrated in Figure 1. Let 1
and 9 be two arbitrary I-dimensional signals (top of the figure). Let's assume that
1 and 9 can both be approximated by partitions of polynomials, F and C. On
the figure , the polynomials are of degree 0 (they are constant), and are depicted in
the second line. The details on how to compute F and C will be explained in the
next section. In the next step, F and C are differentiated once, yielding successions
of impulse functions (third line in the figure). The impulse representation has the
advantage of having a finite support, and of being easy to convolve. Indeed two
impulse functions can be convolved using Equation 2 (4 x 3 = 12 multiply-adds on
the figure). Finally the result of the convolution must be integrated twice to yield
F

*C =

(F- 1

* C- 1 )2

(5)

573

Boxlets: A Fast Convolution Algorithm

Original

G

=

Quantization

F
Differentiation

V

------,11-__

'r-I

G'

-1...'- - - - - L . . - - - r _ - - 1 -

----..I

t

t

FIG'
Convolution

FIG
Double
Integration

Figure 1: Example of convolution between I-dimensional function
the approximations of f and 9 are piecewise constant .

2

f and

g , where

Quantization: from Images to Boxlets

The goal of this section is to suggest efficient ways to approximate an image f by
cover of polynomials of degree d suited for convolution. Let S be the space on
Cj = 0 for i =f. j ,
which f is defined , and let C = {cd be a partition of S (Ci
and
Ci = S). For each Ci, let Pi be a polynomial of degree d which minimizes
equatIOn:

Ui

n

(6)
The uniqueness of Pi is guaranteed if Ci is convex. The problem is to find a cover
C which minimizes both the number of Ci and I.:i ei. Many different compromises
are possible, but since the computational cost of the convolution is proportional
to the number of regions, it seemed reasonable to chose the largest regions with a
maximum error bounded by a threshold K . Since each region will be differentiated
and integrated along the directions of the axes, the boundaries of the CiS are restricted to be parallel to the axes , hence the appellation boxlet. There are still many
ways to compute valid partitions of boxlets and polynomials. We have investigated
two very different approaches which both yield a polynomial cover of the image in
reasonable time. The first algorithm is greedy. It uses a procedure which, starting
from a top left corner , finds the biggest boxlet Ci which satisfies ei < K without
overlapping another boxlet . The algorithm starts with the top left corner of the
image, and keeps a list of all possible starting points (uncovered top left corners)
sorted by X and Y positions. When the list is exhausted, the algorithm terminates.
Surprisingly, this algorithm can run in O(d(N + PlogN)), where N is the number
of pixels, P is the number of boxlets and d is the order of the polynomials PiS.
Another much simpler algorithm consists of recursively splitting boxlets, starting
from a boxlet which encompass the whole image, until ei < K for all the leaves
of the tree. This algorithm runs in O(dN) , is much easier to implement, and is
faster (better time constant). Furthermore , even though the first algorithm yields
a polynomial coverage with less boxlets, the second algorithm yields less impulse
functions after differentiation because more impulse functions can be combined (see
next section). Both algorithms rely on the fact that Equation 6 can be computed

P. Y. Simard, L. Bottou, P. Haffner and Y. Le Cun

574

Figure 2: Effects of boxletization: original (top left), greedy (bottom left) with a
threshold of tO,OOO, and recursive (top and bottom right) with a threshold of 10,000.
in constant time. This computation requires the following quantities

L

f(x, y),

L

f(x, y)2 ,

L

f(x, y)x,

L

f(x, y)y,

L

f(x, y)xy,...

(7)

~~------~v~------~""~--------------v-------------~

degree

a

degree 1

to be pre-computed over the whole image, for the greedy algorithm, or over recursively embedded regions, for the recursive algorithm. In the case of the recursive
algorithm these quantities are computed bottom up and very efficiently. To prevent
the sums to become too large a limit can be imposed on the maximum size of Ci.
The coefficients of the polynomials are quickly evaluated by solving a small linear
system using the first two sums for polynomials of degree a (constants), the first 5
sums for polynomials of degree 1, and so on.
Figure 2 illustrates the results of the quantization algorithms. The top left corner
is a fraction of the original image. The bottom left image illustrates the boxletization of the greedy algorithm, with polynomials of degree 1, and ei <= 10, 000
( 13000 boxlets, 62000 impulse (and its derivative) functions . The top right image
illustrates the boxletization of the recursive algorithm, with polynomials of degree
o and ei <= 10, 000 ( 47000 boxlets, 58000 impulse functions). The bottom right
is the same as top right without displaying the boxlet boundaries. In this case the
pixel to impulse function ratio 5.8.

3

Differentiation: from Boxlets to Impulse Functions

If Pi is a polynomial of degree d, its (d + 1)-th derivative can be written as a sum of
impulse function's derivatives, which are zero everywhere but at the corners of Ci.
These impulse functions summarize the boundary conditions and completely characterize Pi. They can be represented by four (d + 1)-dimensional vectors associated
with the 4 corners of Ci. Figure 3 (top) illustrates the impulse functions at the 4

575

Boxlets: A Fast Convolution Algorithm

Polynomial
(constant)

X derivative

l-~C~)
?1.

Yd envatlve
(of X derivative)
~~

D

D

D

~

D

D

~

D
Polynomial covering
(constants)

Derivatives

Combined

D

Sorted list
representation

Figure 3: Differentiation of a constant polynomial in 2D (top).
derivative of adjacent polynomials (bottom)

Combining the

corners when the polynomial is a constant (degree zero). Note that the polynomial
must be differentiated d + 1 times (in this example the polynomial is a constant,
so d = 0), with respect to each dimension of the input space. This is illustrated at
the top of Figure 3. The cover C being a partition, boundary conditions between
adjacent squares do simplify, that is, the same derivatives of a impulse functions
at the same location can be combined by adding their coefficients. It is very advantageous to do so because it will reduce the computation of the convolution in
the next step. This is illustrated in Figure 3 (bottom). This combining of impulse
functions is one of the reason why the recurslve algorithm for the quantization is
preferred to the greedy algorithm. In the recursive algorithm, the boundaries of
boxlets are often aligned , so that the impulse functions of adjacent boxlets can be
combined . Typically, after simplification, there are only 20% more impulse functions than there are boxlets. In contrast, the greedy algorithm generates up to 60%
more impulse functions than boxlets, due to the fact that there are no alignment
constraints. For the same threshold the recursive algorithm generates 20% to 30%
less impulse functions than the greedy algorithm.
Finding which impulse functions can be combined is a difficult task because the
recursive representation returned by the recursive algorithm does not provide any
means for matching the bottom of squares on one line, with the top of squares
from below that line. Sorting takes O(P log P) computational steps (where P is the
number of impulse functions) and is therefore too expensive. A better algorithm is
to visit the recursive tree and accumulate all the top corners into sorted (horizontal)
lists. A similar procedure sorts all the bottom corners (also into horizontal lists).
The horizontal lists corresponding to the same vertical positions can then be merged
in O(P) operations. The complete algorithm which quantizes an image of N pixels
and returns sorted lists of impulse functions runs in O(dN) (where d is the degree
of the polynomials).

4

Results

The convolution speed of the algorithm was tested with feature extraction on the
image shown on the top left of Figure 2. The image is quantized, but the feature
is not. The feature is tabulated in kernels of sizes 5 x 5, 10 x 10, 15 x 15 and
20 x 20 . If the kernel is decomposable, the algorithm can be modified to do two 1D
convolutions instead of the present 2D convolution.
The quantization of the image is done with constant polynomials, and with thresholds varying from 1,000 to 40,000. This corresponds to varying the pixel to impulse
function ratio from 2.3 to 13.7. Since the feature is not quantized , these ratios
correspond exactly to the ratios of number of multiply-adds for the standard convolution versus the boxlet convolution (excluding quantization and integration). The

P Y. Simard, L. Bottou, P Haffner and Y. Le Cun

576

8.4

12.5

13.4

13.8

Table 1: Convolution speed-up factors

Horizontal convolution

A

*

(a)
Convolution of runs

(b)

---

A

*

~'\-

I
ld..bA

?-)-------------- - - ---r ------T-------

(C)

~
'w w'

\.

----

(d)~;%)+r: ~ ~
Figure 4: Run length X convolution

actual speed up factors are summarized in Table 1. The four last columns indicate
the measured time ratios between the standard convolution and the boxlet convolution. For each threshold value, the top line indicates the time ratio of standard
convolution versus quantization, convolution and integration time for the boxlet
convolution. The bottom line does not take into account the quantization time.
The feature size was varied from 5 x 5 to 20 x 20. Thus with a threshold of 10,000
and a 5 x 5 kernel, the quantization ratio is 5.8, and the speed up factor is 2.8.
The loss in image quality can be seen by comparing the top left and the bottom
right images. If several features are extracted, the quantization time of the image
is shared amongst the features and the speed up factor is closer to 4.7.
It should be noted that these speed up factors depend on the quantization level
which depends on the data and affects the accuracy of the result. The good news is
that for each application the optimal threshold (the maximum level of quantization
which has negligible effect on the result) can be evaluated quickly. Once the optimal
threshold has been determined, one can enjoy the speed up factor. It is remarkable
that with a quantization factor as low as 2.3, the speed up ratio can range from
1.5 to 2.3, depending on the number of features. We believe that this method is
directly applicable to forward propagation in convolutional neural nets (although
no results are available at this time) .
The next application shows a case where quantization has no adverse effect on the
accuracy of the convolution, and yet large speed ups are obtained.

Boxlets: A Fast Convolution Algorithm

5

577

Binary images and run-length encoding

The quantization steps described in Sections 2 and 3 become particularly simple
when the image is binary. If the threshold is set to zero, and if only the X derivative is considered, the impulse representation is equivalent to run-length encoding.
Indeed the position of each positive impulse function codes the beginning of a run,
while the position of each negative impulses code the end of a run. The horizontal
convolution can be computed effectively using the boxlet convolution algorithm.
This is illustrated in Figure 4. In (a), the distance between two binary images must
be evaluated for every horizontal position (horizontal translation invariant distance).
The result is obtained by convolving each horizontal line and by computing the sum
of each of the convolution functions. The convolution of two runs, is depicted in
(b), while the summation of all the convolutions of two runs is depicted in (c). If
an impulse representation is used for the runs (a first derivative) , each summation
of a convolution between two runs requires only 4 additions of impulse functions,
as depicted in (d). The result must be integrated twice, according to Equation 5.
The speed up factors can be considerable depending on the width of the images (an
order of magnitude if the width is 40 pixels), and there is no accuracy penalty.

Figure 5: Binary image (left) and compact impulse function encoding (right).
This speed up also generalizes to 2-dimensional encoding of binary images. The gain
comes from the frequent cancellations of impulse functions of adjacent boxlets. The
number of impulse functions is proportional to the contour length of the binary
shapes. In this case, the boxlet computation is mostly an efficient algorithm for
2-dimensional run-length encoding. This is illustrated in Figure 5. As with runlength encoding, a considerable speed up is obtained for convolution, at no accuracy
penalty cost.

6

Conclusion

When convolutions are used for feature extraction, preCISIon can often be sacrificed for speed with negligible degradation of performance. The boxlet convolution
method combines quantization and convolution to offer a continuous adjustable
trade-off between accuracy and speed. In some cases (such as in relatively simple
binary images) large speed ups can come with no adverse effects. The algorithm is
directly applicable to the forward propagation in convolutional neural networks and
in pattern matching when translation invariance results from the use of convolution.

References
[1] Yann LeCun and Yoshua Bengio, ""Convolutional networks for images, speech,
and time-series,"" in The Han'dbook of Brain Theory and Neural Networks, M. A.
Arbib, Ed. 1995, MIT Press.
[2] Paul S. Heckbert, ""Filtering by repeated integration,"" in ACM SIGGRAPH
conference on Computer graphics, Dallas, TX , August 1986, vol. 20, pp. 315321.

"
5525,"A Neural Network that Learns to Interpret
Myocardial Planar Thallium Scintigrams

Charles Rosenberg, Ph.D:

Jacob Erel, M.D.

Department of Computer Science
Hebrew University
Jerusalem, Israel

Department of Cardiology
Sapir Medical Center
Meir General Hospital
Kfar Saba, Israel

Henri Atlan, M.D., PhD.
Department of Biophysics and Nuclear Medicine
Hadassah Medical Center
Jerusalem, Israel

Abstract
The planar thallium-201 myocardial perfusion scintigram is a widely used
diagnostic technique for detecting and estimating the risk of coronary
artery disease. Neural networks learned to interpret 100 thallium scintigrams as determined by individual expert ratings. Standard error backpropagation was compared to standard LMS, and LMS combined with
one layer of RBF units. Using the ""leave-one-out"" method, generalization was tested on all 100 cases. Training time was determined automatically from cross-validation perfonnance. Best perfonnance was attained
by the RBF/LMS network with three hidden units per view and compares
favorably with human experts.

1 Introduction
Coronary artery disease (CAD) is one of the leading causes of death in the Western World.
The planar thallium-201 is considered to be a reliable diagnostic tool in the detection of
? Current address: Geriatrics, Research, Educational and Clinical Center, VA Medical Center, Salt
Lake City, Utah.

755

756

Rosenberg, Erel, and Atlan

CAD. Thallium is a radioactive isotope that distributes in mammalian tissues after intervenous administration and is imaged by a gamma camera. The resulting scintigram is visually
interpreted by the physician for the presence or absence of defects - areas with relatively
lower perfusion levels. In myocardial applications, thallium is used to measure myocardial
ischemia and to differentiate between viable and non-viable (infarcted) heart muscle (pohost and Henzlova, 1990).
Diagnosis of CAD is based on the comparison of two sets of images, one set acquired
immediately after a standard effort test (BRUCE protocol), and the second following a
delay period of four hours. During this delay, the thallium redistributes in the heart muscle
and spontaneously decays. Defects caused by scar tissue are relatively unchanged over
the delay period (fixed defect), while those caused by ischemia are partially or completely
filled-in (reversible defect) (Beller, 1991; Datz et al., 1992).
Image interpretation is difficult for a number of reasons: the inherent variability in biological systems which makes each case essentially unique, the vast amount of irrelevant and
noisy information in an image, and the ""context-dependency"" of the interpretation on data
from many other tests and clinical history. Interpretation can also be significantly affected
by attentional shifts, perceptual abilities, and mental state (Franken Jr. and Berbaum, 1991;
Cuar6n et al., 1980).
While networks have found considerable application in ECG processing (e.g. (Artis et al.,
1991)) and clinical decision-making (Baxt, 1991b; Baxt, 1991a), they have thus far found
limited application in the field of nuclear medicine. Non-cardiac imaging applications include the grading of breast carcinomas (Dawson et al., 1991) and the discrimination of normal vs. Alzheimer's PET scans (Kippenhan et al., 1990). Of the studies dealing specifically
with cardiac imaging, neural networks have been applied to several problems in cardiology
including the identification of stenosis (Porenta et al., 1990; Cios et al., 1989; Cios et al.,
1991; Cianflone et al., 1990; Fujita et al., 1992). These studies encouraged us to explore
the use of neural networks in the interpretation of cardiac scintigraphy.

2

Methods

We trained one network consisting of a layer of gaussian RBF units in an unsupervised fashion to discover features in circumferential profiles in planar thallium scintigraphy. Then a
second network was trained in a supervised way to map these features to physician's visual
interpretations of those images using the delta rule (Widrow and Hoff, 1960). This architecture was previously found to compare favorably to other network learning algorithms
(2-layer backpropagation and single-layer networks) on this task (Rosenberg et al., 1993;
Erel et al., 1993).
In our experiments, all of the input vectors representing single views f were first normalized
to unit length V = IIfll . The activation value of a gaussian unit, OJ, is then given by:
(1)
netj

O1? = exp(--)
w

(2)

where j is an index to a gaussian unit and i is an input unit index. The width of the gaussian,

A Neural Network that Learns to Interpret Myocardial Planar Thallium Scintigrams
R~gion.1

Output

IL.

0

Ii:1/1

Ii:1/1

I\-

~

Scores

0

~

)(

IL.

<

IL.

Ii.
;!;

0

Ii.
~

IL.

.:.

t.

<;I
t-

t.

III I

Ill!

?
?

Severe
Moderate

o""

Mild
Normal

RBF
Input

ANT

LAO 45

LAT

VIEWS

Figure 1: The network architecture. The first layer (Input) encoded the three circumferential profiles representing the three views, anterior (ANT), left lateral oblique (LAO). and
left lateral (LAT). The second layer consisted of radial basis function (RBF) units, the third
layer, semi-linear units trained in a supervised fashion. The outputs of the network corresponded to the visual scores as given by the expert observer. An additional unit per view
encoded the scaling factor of the input patterns lost as a result of input normalization.

given by w, was fixed at 0.25 for all units 1?
The gaussian units were trained using a competitive learning rule which moves the center
of the unit closest to the current input pattern (Omax, i.e. the ""winner"") closer to the input
pattern2 :
~tui,winner

2.1

= 1](v; -

Wi,winner)

(3)

Data Acquisition and Selection

Scintigraphic images were acquired for each of three views: anterior (ANT), left lateral
oblique (LAO 45), and left lateral (LAT) for each patient case. Acquisition was performed
twice, once immediately following a standard effort test and once following a delay period
of four hours. Each image was pre-processed to produce a circumferential profile (Garcia
et aI., 1981; Francisco et aI., 1982) , in which maximum pixel counts within each of 60,
6? contiguous segmental regions are plotted as a function of angle (Garcia, 1991). Preprocessing involved positioning of the region of interest (ROI), interpolative background
subtraction, smoothing and rotational alignment to the heart's apex (Garcia, 1991).
1We have considered applying the learning rule to the unit widths (w) as well as the RBF weights,
however we have not as yet pursued this possibility.
2Following Rumelhart and Zipser (Rumelhart and Zipser, 1986), the other units were also pulled
towards the input vector, although to a much smaller extent than the winner. We used a ratio of 1 to
100.
3The profiles were generated using the Elscint CTL software package for planar quantitative
thallium-20l based on the Cedars-Sinai technique (Garcia et aI., 1981; Maddahi et aI., 1981; Areeda
et aI., 1982).

757

758

Rosenberg, Ere!, and Atlan

Lesion

mild

moderate

severe

Total

single

12

5

0

17

multiple

16

16

11

43

Total

28

21

11

60

Table 1: Distribution of Abnormal Cases as Scored by the Expert Observer. Defects occurring in any combination of two or more regions (even the proximal and distal subregions
of a single area) were treated as one multiple defect. The severity level of multiple lesions
was based on the most severe lesion present.
Cases were pre-selected based on the following criteria (Beller, 1991):
? Insufficient exercise. Cases in which the heart rate was less than 130 b.p.m. were
eliminated, as this level of stress is generally deemed insufficient to accurately
distinguish normal from abnormal conditions.
? Positional abnormalities. In a few cases, the ""region of interest"" was not positioned or aligned correctly by the technician.
? Increased lung uptake. Typically in cases of multi-vessel disease, a significant
proportion of the perfusion occurs in the lungs as well as in the heart, making it
more difficult to determine the condition of the heart due to the partially overlapping positions of the heart and lungs.
? Breast artifacts.
Cases were selected at random between August, 1989 and March, 1992. Approximately a
third of the cases were eliminated due to insufficient heart rate, 4-5% due to breast artifacts,
4% due to lung uptake, and 1-2% due to positional abnormalities. A set of one hundred
usable cases remained.
2.2

Visual Interpretation

Each case was visually scored by a single expert observer for each of nine anatomical regions generally accepted as those that best relate to the coronary circulation: Septal: proximal and distal, Anterior: proximal and distal, Apex, Inferior: proximal and distal, and
Posterior-Lateral: proximal and distal. Scoring for each region was from normal (I) to
severe (4), indicating the level of the observed perfusion deficit.
Intra-observer variability was examined by having the observer re-interpret 17 of the cases
a second time. The observer was unable to remember the cases from the first reading and
could not refer to the previous scores.
Exact matches were obtained on 91.5% of the regions; only 8 of the 153 total regions (5%)
were labeled as a defect (mild, moderate or severe) on one occasion and not on the other.
All differences, when they occurred, were of a single rating level4 ?
4In contrast, measured inter-observer variability was much higher. A set of 13 cases was individ-

A Neural Network that Learns to Interpret Myocardial Planar Thallium Scintigrams

2.3

The Network Model

The input units of the network were divided into 3 groups of 60 units each, each group
representing the circumferential profile for a single view. A set of 3 RBF units were assigned
to each input group. Then a second layer of weights was trained using the delta rule to
reproduce the target visual scores assigned by the expert observer. The categorical visual
scores were translated to numerical values to make the data suitable for network learning:
normal =0.0, mild defect =0.3, moderate defect =0.7, and severe defect = 1.0.
In order to make efficient use of the available data, we actually trained 100 identical networks; each network was trained on a subset of 99 of the 100 cases and tested on the remaining one. This procedure, sometimes referred to as the ""leave-one-out"" or ""jack-knife""
method, enabled us to determine the generalization performance for each case. This procedure was followed for both the RBF and the delta rule training 5. Training of a single
network took only a few minutes of Sun 4 computer time.

3 Results
Because of the larger numbers of confusions between normal and mild regions in both the
inter- and intra-observer scores, disease was defined as moderate or severe defects. The
threshold value dividing the output values of the network into these two sets was varied
from 0 to 1 in 0.01 step increments. The number of agreements between the expert observer
and the network were computed for each threshold value. The resulting scores, accumulated
over all threshold values, were plotted as a Receiver Operating Characteristic (ROC) curve.
Best performance (percent correct) was achieved with a threshold value of 0.28, which
yielded an overall accuracy of 88.7% (798/900 regions) on the stress data. However, this
value of the threshold heavily favored specificity over sensitivity due to the preponderance
of normal regions in the data. Using the decision threshold which maximized the sum
of sensitivity and specificity, 0.10, accuracy dropped to 84.9% (764/900) but sensitivity
improved to 0.771 (121/157), and specificity was 0.865 (643/743).

3.1

Distinguishing Fixed vs. Reversible Defects

In order to take into account the delayed distribution as well as the stress set of images, the
network was essentially duplicated: one network processed the stress data, and the other,
ually interpreted by 3 expert observers in a previous experiment (Rosenberg et aI., 1993). Percent
agreement (exact matches) between the observers was 82% (288/351). Of the 63 mis-matches, 5 or
about 8% of the regions were of 2 levels of severity. There were no differences of 3 levels of severity.
Approximately two-thirds of the disagreements were between normal and mild regions. These results
indicate that the single observer data employed in the present study are more reliable than the mixed
consensus and individual scores used previously.
5Details of network learning were as follows: Each of the 100 networks was initialized and trained
in the same way. RBF-to-output unit weights were initialized to small random values between 0.5 and
-0.5. Input-to-RBF unit weights were first randomized and then normalized so that the weight vectors
to each RBF unit were of unit length. Unsupervised, competitive training of the RBF units continued
for 100 ""epochs"" or complete sweeps through the set of 99 cases: 20 epochs with a learning rate (11)
of 0.1 followed by 80 epochs at 0.01 without momentum (0'). Supervised training using a learning
rate of 0.05 and momentum 0.9, was terminated based on cross-validation testing after 200 epochs.
Further training led to over-training and poorer generalization.

759

760

Rosenberg, Erel, and Atlan

the redistribution data. (For details, see (Erel et al., 1993).)
The combined network exhibited only a limited ability to distinguish between scar and
ischemia. Performance on scar detection was good (sens. 0.728 (75/103), spec. 0.878
(700{797?, but the sensitivity of the network on ischemia detection was only 0.185 (10/54).
This result may be explained, at least in part, by the much smaller number of ischemic regions included in the data set as compared with scars (54 versus 103).

4 Conclusions and Future Directions
We suspect that our major limitation is in defect sampling. In order that a statistical system
(networks or otherwise) generalize well to new cases, the data used in training must be
representative of the full population of data likely to be sampled. This is unlikely to happen
when the number of positive cases is on the order of 50, as was the case with ischemia,
since each possible defect location, plus all the possible combinations of locations must be
included.
A variant ofbackpropagation, called competitive backpropagation, has recently been developed which is claimed to generalize appropriately in the presence of multiple defects (Cho
and Reggia, 1993). Weights in this network are constrained to take on positive values,
so that diagnoses made by the system add constructively. In a standard backpropagation
network, multiple diseases can cancel each other out, due to complex interactions of both
positive and negative connection strengths. We are currently planning to investigate the
application of this learning algorithm to the problem of ischemia detection.
Other improvements and extensions include:
? Elicit confidence ratings. Expert visual interpretations could be augmented by
degree of confidence ratings. Highly ambiguous cases could be reduced in importance or eliminated. The ratings could also be used as additional targets for
the network6: cases indicated by the network with low levels of confidence would
require closer inspection by a physician. Initial results are promising in this regard.
? Provide additional information. We have not yet incorporated clinical history,
gender, and examination EKG. Clinical history has been found to have a profound
impact on interpretation of radiographs (Doubilet and Herman, 1981). The inclusion of these variables should allow the network to approximate more closely a
complete diagnosis, and boost the utility of the network in the clinical setting.
? Add constraints. Currently we do not utilize the angles that relate the three views.
It may be possible to build these angles in as constraints and thereby cut down on
the number of free network parameters.
? Expand application. Besides planar thallium, our approach may also be applied
to non-planar 3-D imaging technologies such as SPECT and other nuclear agents or
stress-inducing modalities such as dipyridamole. Preliminary results are promising in this regard.
6See (fesauro and Sejnowski, 1988) for a related idea.

A Neural Network that Learns to Interpret Myocardial Planar Thallium Scintigrams

Acknowledgements
The authors wish to thank Mr. Haim Karger for technical assistance, and the Departments
of Computer Science and Psychology at the Hebrew University for computational support.
We would also like to thank Drs. David Shechter, Moshe Bocher, Roland Chisin and the
staff of the Department of Medical Biophysics and Nuclear Medicine for their help, both
large and small, and two anonymous reviewers. Terry Sejnowski suggested our use of RBF
units.

References
Areeda, J., Train, K. v., Garcia, E. Y., Maddahi, J., Rosanki, A., Waxman, A., and Berman,
D. (1982). Improved analysis of segmental thallium-201 myocardial scintigrams:
Quantitation of distribution, washout, and redistribution. In Esser, P. D., editor, Digital
Imaging. Society of Nuclear Medicine, New York.
Artis, S., Mark, R, and Moody, G. (1991). Detection of atrial fibrillation using artificial
neural networks. In Computers in Cardiology, pages 173-176, Venice, Italy. IEEE,
IEEE Computer Society Press.
Baxt, W. (1991a). Use of an artificial neural network for data analysis in clinical decisionmaking: The diagnosis of acute coronary occlusion. Neural Computation, 2:480-489.
Baxt, W. (1991b). Use of an artificial neural network for the diagnosis of myocardial infarction. Annals of Internal Medicine, 115:843-848.
Beller, G. A. (1991). Myocardial perfusion imaging with thallium-201. In Marcus, M. L.,
Schelbert, H. R., Skorton, D. J., and Wolf, G. L., editors, Cardiac Imaging. W. B.
Sanders.
Cho, S. and Reggia, J. (1993). Multiple disorder diagnosis with adaptive competitive neural
networks. Artificial Intelligence in Medicine. To appear.
Cianfione, D., Carandente, 0., Fragasso, G., Margononato, A., Meloni, C., Rossetti, E.,
Gerundini, P., and Chiechia, S. L. (1990). A neural network based model of predicting
the probability of coronary lesion from myocardial perfusion SPECT data. In Proceedings of the 37th Annual Meeting of the Society of Nuclear Medicine, page 797.
Cios, K. J., Goodenday, L. S., Merhi, M., and Langenderfer, R. (1989). Neural networks in
detection of coronary artery disease. In Computers in Cardiology Conference, pages
33-37, Jerusalem, Israel. IEEE, IEEE Computer Society Press.
Cios, K. J., Shin, 1., and Goodenday, L. S. (1991). Using fuzzy sets to diagnose coronary
artery stenosis. Computer, pages 57-63.
Cuar6n, A., Acero, A., Cardena, M., Huerta, D., Rodriguez, A., and de Garay, R. (1980). Interobserver variability in the interpretation of myocardial images with Tc-99m-Iabeled
diphosponate and pyrophosphate. Journal of Nuclear Medicine, 21(1):1-9.
Datz; E, Gabor, E, Christian, P., Gullber, G., Menzel, C., and Morton, K. (1992). The use of
computer-assisted diagnosis in cardiac-perfusion nuclear medicine studies: A review.
Journal of Digital Imaging, 5(4):1-14.
Dawson, A., Austin, R, and Weinberg, D. (1991). Nuclear grading of breast carcinoma by
image analysis. American Journal of Clinical Pathology, 95(4):S29-S37.

761

762

Rosenberg, Erel, and Atlan

Doubilet, P. and Herman, P. (1981). Interpretation of radiographs: Effect of clinical history.
American Journal of Roentgenology, 137: 1055-1058.
Erel, J., Rosenberg, c., and Atlan, H. (1993). Neural network for automatic interpretation
of thallium scintigrams. In preparation.
Francisco, D. A., Collins, S. M., and et al., R. T. G. (1982). Tomographic thallium-201
myocardial perfusion scintigrams after maximal coronary artery vasodiliation with intravenous dipyridamole: Comparison of qualitative and quantitative approaches. Circulation, 66(2).
Franken Jr., E. A. and Berbaum, K. S. (1991). Perceptual aspects of cardiac imaging. In
Marcus, M. L., Schelbert, H. R., Skorton, D. J., and Wolf, G. L., editors, Cardiac
Imaging. W. B. Sanders.
Fujita, H., Katafuchi, T., Uehara, T., and Nishimura, T. (1992). Application of artificial
neural network to computer-aided diagnosis of coronary artery disease in myocardial
SPECT bull's-eye images. The Journal of Nuclear Medicine, 33(2):272-276.
Garcia, E. V. (1991). Physics and instrumentation of radionuclide imaging. In Marcus,
M. L., Schelbert, H. R., Skorton, D. J., and Wolf, G. L., editors, Cardiac Imaging. W.
B. Sanders.
Garcia, E. V., Maddahi, J., Berman, D. S., and Waxman, A. (1981). Space-time quantitation
of thallium-201 myocardial scintigraphy. Journal of Nuclear Medicine, 22:309-317.
Kippenhan, J., Barker, W., Pascal, S., and Duara, R. (1990). A neural-network classifier
applied to PET scans of normal and Alzheimer's disease (AD) patients. In The Proceedings of the 37th Annual Meeting of the Society of Nuclear Medicine, volume 31,
Washington, D.C.
Maddahi, J., Garcia, E. V., Berman, D. S., Waxman, A., Swan, H. J. C., and Forrester,
J. (1981). Improved noninvasive assessment of coronary artery disease by quantitative analysis of regional stress myocardial distribution and washout of thallium-20l.
Circulation, 64 :924-935.
Pohost, G. M. and Henzlova, M. J. (1990). The value of thallium-201 imaging. New
Eng land Journal of Medicine, 323(3): 190-192.
Porenta, G., Kundrat, S., Dorffner, G., Petta, P., Duit, J., and r, H. S. (19~0). Computer based
image interpretations of thallium- 201 scintigrams: Assessment of coronary artery
disease using the parallel distributed processing approach. In Proceedings of the 37th
Annual Meeting of the Society of Nuclear Medicine, page 825.
Rosenberg, C., Erel, J., and Atlan, H. (1993). A neural network that learns to interpret
myocardial planar thallium scintigrams. Neural Computation. To appear.
Rumelhart, D. and Zipser, D. (1986). Feature discovery by competitive learning. In Rumelhart, D. and McClelland, J., editors, Parallel Distributed Processing, volume 1, chapter 5, pages 151-193. MIT Press, Cambridge, Mass.
Tesauro, G. and Sejnowski, T. J. (1988). A parallel network that learns to play backgammon.
Technical Report CCSR-88-2, University of Illinois at Urbana-Champaign Center for
Complex Systems Research.
Widrow, B. and Hoff, M. (1960). Adaptive switching circuits. In 1960 IRE WESCON
Convention Record, volume 4, pages 96-104. IRE, New York.

PART X

IMPLEMENTATIONS

"
694,"Spike-Based Compared to Rate-Based
Hebbian Learning

Richard Kempter*
Institut fur Theoretische Physik
Technische Universitat Munchen
D-85747 Garching, Germany

Wulfram Gerstner
Swiss Federal Institute of Technology
Center of Neuromimetic Systems, EPFL-DI
CH-1015 Lausanne, Switzerland

J. Leo van Hemmen
Institut fur Theoretische Physik
Technische Universitat Munchen
D-85747 Garching, Germany

Abstract
A correlation-based learning rule at the spike level is formulated,
mathematically analyzed, and compared to learning in a firing-rate
description. A differential equation for the learning dynamics is
derived under the assumption that the time scales of learning and
spiking can be separated. For a linear Poissonian neuron model
which receives time-dependent stochastic input we show that spike
correlations on a millisecond time scale play indeed a role. Correlations between input and output spikes tend to stabilize structure
formation, provided that the form of the learning window is in
accordance with Hebb's principle. Conditions for an intrinsic normalization of the average synaptic weight are discussed.

1

Introduction

Most learning rules are formulated in terms of mean firing rates, viz., a continuous
variable reflecting the mean activity of a neuron. For example, a 'Hebbian' (Hebb
1949) learning rule which is driven by the correlations between presynaptic and
postsynaptic rates may be used to generate neuronal receptive fields (e.g., Linsker
1986, MacKay and Miller 1990, Wimbauer et al. 1997) with properties similar to
those of real neurons. A rate-based description, however, neglects effects which are
due to the pulse structure of neuronal signals. During recent years experimental and
* email: kempter@physik.tu-muenchen.de (corresponding author)

R. Kempter. W Gerstner and J L. van Hemmen

126

theoretical evidence has accumulated which suggests that temporal coincidences
between spikes on a millisecond or even sub-millisecond scale play an important
role in neuronal information processing (e.g., Bialek et al. 1991, Carr 1993, Abeles
1994, Gerstner et al. 1996). Moreover, changes of synaptic efficacy depend on
the precise timing of postsynaptic action potentials and presynaptic input spikes
(Markram et al. 1997, Zhang et al. 1998). A synaptic weight is found to increase, if
presynaptic firing precedes a postsynaptic spike and decreased otherwise. In contrast
to the standard rate models of Hebbian learning, the spike-based learning rule
discussed in this paper takes these effects into account. For mathematical details
and numerical simulations the reader is referred to Kempter et al. (1999) .

2

Derivation of the Learning Equation

2.1

Specification of the Hebb Rule

We consider a neuron that receives input from N ? 1 synapses with efficacies J i ,
1 :::; i :::; N. We assume that changes of Ji are induced by pre- and postsynaptic
spikes. The learning rule consists of three parts. (i) Let tf be the time of the m th
input spike arriving at synapse i. The arrival of the spike induces the weight Ji to
change by an amount win which can be positive or negative. (ii) Let t n be the nth
output spike of the neuron under consideration. This event triggers the change of all
N efficacies by an amount w out which can also be positive or negative. (iii) Finally,
time differences between input spikes influence the change of the efficacies. Given
a time difference s = tf - t n between input and output spikes, Ji is changed by an
amount W(s) where the learning window W is a real valued function (Fig. 1). The
learning window can be motivated by local chemical processes at the level of the
synapse (Gerstner et al. 1998, Senn et al. 1999). Here we simply assume that such
a learning window exist and take some (arbitrary) functional dependence W(s) .
Figure 1: An example of a learning window W as a function of the delay s =
tf - t n between a postsynaptic firing time
t n and presynaptic spike arrival tf at
synapse i. Note that for s < 0 the presynaptic spike precedes postsynaptic firing.
Starting at time t with an efficacy Ji(t), the total change 6.Ji(t) = Ji(t + T) - Ji(t)
in a time interval T is calculated by summing the contributions of all input and
output spikes in the time interval [t, t + 7]. Describing the input spike train at
synapse i by a series of 8 functions, s:n(t) = Lm 8(t - tf), and, similarly, output
spikes by sout(t) = Ln 8(t - t n), we can formulate the rules (i)--:(iii):

!

t+T

b.J,(t) =

2.2

dt'

[

!

t+T

Wi""

S;""(t')

+ wont 8""nt(t') +

dt"" W(t"" - t') S;""(t"") 8""nt(t')

]

(1)

Separation of Time Scales

The total change 6..Ji (t) is subject to noise due to stochastic spike arrival and,
possibly, stochastic generation of output spikes. We therefore study the expected
development of the weights J i , denoted by angular brackets. We make the substitution s = til - t' on the right-hand side of (1), divide both sides by T, and take

127

Spike-Based Compared to Rate-Based Hebbian Learning

the expectation value:

(tlJt?)(t)

_1

T

T

I

t +Tdt '

[win (s!n)(t') + W out (sout) (t')]

t

+-1 It+T dt' It+T-t'
T

t

ds W(s) (s!n(t'

+ s) sout(t'))

(2)

t-t'

We may interpret (s~n)(t) for 1 ::; i ::; Nand (sout)(t) as instantaneous firing
rates. I They may vary on very short time scales - shorter, e.g., than average
interspike intervals. Such a model is consistent with the idea of temporal coding,
since it does not rely on temporally averaged mean firing rates.
We note, however, that due to the integral over time on the right-hand side of (2)
temporal averaging is indeed important. If T is much larger than typical interspike
intervals , we may define mean firing rates v!n(t) = (s~n)(t) and vout(t) = (sout)(t)
where we have used the notation f(t) = T- l Itt+T dt' f(t'). The mean firing rates
must be distinguished from the previously defined instantaneous rates (s~n) and
(sout) which are defined as an expectation value and have a high temporal resolution. In contrast , the mean firing rates vi n and v out vary slowly (time scale of the
order of T) as a function of time.
If the learning time T is much larger than the width of the learning window, the

integration over s in (2) can be extended to run from -00 to 00 without introducing
a noticeable error. With the definition of a temporally averaged correlation,

(3)
the last term on the right of (2) reduces to I~oo ds W(s) Ci(s; t). Thus , correlations
between pre- and postsynaptic spikes enter spike-based Hebbian learning through
Ci convolved with the learning window W. We remark that the correlation Ci(s; t)
may change as a function of s on a fast time scale. Note that, by definition, s < 0
implies that a presynaptic spike precedes the output spike - and this is when we
expect (for excitatory synapses) a positive correlation between input and output.
As usual in the theory of Hebbian learning, we require learning to be a slow process.
The correlation Ci can then be evaluated for a constant J i and the left-hand side
of (2) can be rewritten as a differential on the slow time scale of learning

:t

2.3

Ji(t) == ji = win v!n(t) + W out vout(t) +

i:

ds W(S) Ci(S; t)

(4)

Relation to Rate-Based Hebbian Learning

In neural network theory, the hypothesis of Hebb (Hebb 1949) is usually formulated
as a learning rule where the change of a synaptic efficacy Ji depends on the correlation between the mean firing rate vl n of the i th presynaptic and the mean firing
rate vout of a postsynaptic neuron, viz. ,
ji = ao + al v!n + a2 vout + a3 v!n vout + a4 (v~n)2 + a5 (v out )2 ,
(5)
where ao, aI, a2, a3 , a4, and a5 are proportionality constants. Apart from the decay
term ao and the 'Hebbian' term vi n vout proportional to the product of input and
1 An example of rapidly changing instantaneous rates can be found in the auditory
system . The auditory nerve carries noisy spike trains with a stochastic intensity modulated
at the frequency of the applied acoustic tone. In the barn owl, a significant modulation of
the rates is seen up to a frequency of 8 kHz (e.g., Carr 1993).

R. Kempler, W Gerstner and J L. van Hemmen

128

output rates, there are also synaptic changes which are driven separately by the preand postsynaptic rates. The parameters ao, ... , as may depend on J i . Equation (5)
is a general formulation up to second order in the rates; see, e.g., (Linsker 1986).
To get (5) from (4) two approximations are necessary. First, if there are no correlations between input and output spikes apart from the correlations contained in the
rates, we can approximate (SJn(t + s) sout(t)) ~ (s~n)(t + s) (SOUtHt). Second, if
these rates change slowly as compared to T, then we have Ci(s; t) ~ v;n(t+s) vout(t).
Since we have assumed that the learning time T is long compared to the width of
the learning window, we may simplify further and set vJn(t + s) ~ v!n(t), hence
J~oo ds W(s) Ci(s; t) ~ W(O) vjn(t) vout(t), where W(O) = J~oo ds W(s). We may
now identify W(O) with a3. By further comparison of (5) with (4) we identify
win with al and wo ut with a2, and we are able to reduce (4) to (5) by setting
ao = a4 = as = O.
The above set of of assumption which is necessary to derive (5) from (4) does,
however, not hold in general. According to the results of Markram et aI. (1997) the
width of the learning window in cortical pyramidal cells is in the range of ~ 100 ms.
A mean rate formulation thus requires that all changes of the activity are slow on
a time scale of lOOms. This is not necessarily the case. The existence of oscillatory
activity in the cortex in the range of 50 Hz implies activity changes every 20 ms.
Much faster activity changes on a time scale of 1 ms and below are found in the
auditory system (e.g., Carr 1993). Furthermore, beyond the correlations between
mean activities additional correlations between spikes may exist; see below. Because
of all these reasons, the learning rule (5) in the simple rate formulation is insufficient .
In the following we will study the full spike-based learning equation (4).

3
3.1

Stochastically Spiking Neurons
Poisson Input and Stochastic Neuron Model

To proceed with the analysis of (4) we need to determine the correlations C i between
input spikes at synapse i and output spikes. The correlations depend strongly on
the neuron model under consideration. To highlight the main points of learning
we study a linear inhomogeneous Poisson neuron as a toy model. Input spike
trains arriving at the N synapses are statistically independent and generated by an
inhomogeneous Poisson process with time-dependent intensities (Sin) (t) = ,\~n (t),
with 1 ~ i ~ N. A spike arriving at tf at synapse i , evokes a postsynaptic potential
(PSP) with time course E(t - tf) which we assume to be excitatory (EPSP). The
amplitude is given by the synaptic efficacy Ji(t) > O. The membrane potential u of
the neuron is the linear superposition of all contributions
N

u(t)

= Uo + L
i=l

L Ji(t) E(t - t~)

(6)

m

where Uo is the resting potential. Output spikes are assumed to be generated
stochastically with a time dependent rate ,\out(t) which depends linearly upon the
membrane potential
N

,\out(t)

= f3 [u(t)l+

= Vo

+L
i=l

L Ji(t) E(t - tf)?

(7)

m

with a linear function f3[ul+ = f30 + f31 u for u > 0 and zero otherwise. After the
second equality sign, we have formally set Vo = Uo + f30 and f31 = 1. vo > can

129

Spike-Based Compared to Rate-Based Hebbian Learning

be interpreted as the spontaneous firing rate. For excitatory synapses a negative
u is impossible and that's what we have used after the second equality sign. The
sums run over all spike arrival times at all synapses. Note that the spike generation
process is independent of previous output spikes. In particular, the Poisson model
does not include refractoriness.
In the context of (4), we are interested in the expectation values for input and
output. The expected input is (s~n)(t) = A~n(t). The expected output is

(sout)(t) = va +

L Ji(t) 10

00

d8?(s)

A~n(t -

8) ,

(8)

t

The expected output rate in (8) depends on the convolution of ? with the input rates.
In the following we will denote the convolved rates by A~n(t) = 1000 d8 ?(8)A~n(t - 8).
Next we consider the expected correlations between input and output, (s!n(t +
8) sout(t)), which we need in (3):
(s~n (t

+ 8) sout(t))

= A~n (t

+ 8) [Va + J i (t) ?( -8) + L Jj (t) A~n(t)]

(9)

j

The first term inside the square brackets is the spontaneous output rate. The second
term is the specific contribution of an input spike at time t + 8 to the output rate
at t. It vanishes for 8 > 0 (Fig. 2). The sum in (9) contains the mean contributions
of all synapses to an output spike at time t. Inserting (9) in (3) and assuming the
weights J j to be constant in the time interval [t, t + T] we obtain

C i (8; t) =

L

Jj(t) A~n(t + 8) A~n(t)

+ A~n(t + 8) [Va + Ji(t) ?( -8)].

(10)

j

For excitatory synapses, the second term gives for 8 < 0 a positive contribution
to the correlation function - as it should be. (Recall that 8 < 0 means that a
presynaptic spike precedes postsynaptic firing.)

[ ... ](t')

---?r --------o

-'-----r----,,---'---------___.

t+s

3.2

t

t'

Figure 2: Interpretation of the term in square
brackets in (9). The dotted line is the contribution of an input spike at time t + 8 to the
output rate as a function of t', viz., J i (t) ?(t' t - 8). Adding this to the mean rate contribution, Va + Lj Jj(t') A~n(t') (dashed line), we
obtain the rate inside the square brackets of
(9) (full line). At time t' = t the contribution
of an input spike at time t + 8 is Ji(t) ?( -s).

Learning Equation

The assumption of identical and constant mean input rates, A~n(t) = v:n(t) = v in
for all i, reduces the number of free parameters in (4) and eliminates all effects of
rate coding. We introduce r~n(t) := [W(O)]-l J.~oo d8 W(8)A~n(t + 8) and define
(11)
Using (8), (10), (11) in (4) we find for the evolution on the slow time scale oflearning

ji(t) = kl

+ L Jj(t) [Qij(t) + k2 + k3 bij] ,
j

where

(12)

R. Kempter. W Gerstner and 1. L. van Hemmen

130

[w out + W(O)
[w out + W(O)
v in /

4

vin]

Vo

+ win v in

vin] v in

ds?(-s) W(s) .

(13)
(14)
(15)

Discussion

Equation (12), which is the central result of our analysis, describes the expected
dynamics of synaptic weights for a spike-based Hebbian learning rule (1) under the
assumption of a linear inhomogeneous Poisson neuron. Linsker (1986) has derived a
mathematically equivalent equation starting from (5) and a linear graded response
neuron, a rate-based model. An equation of this type has been analyzed by MacKay
and Miller (1990). The difference between Linsker's equation and (12) is, apart from
a slightly different notation, the term k3 6ij and the interpretation of Qij.
4.1

Interpretation of Qij

In (12) correlations between spikes on time scales down to milliseconds or below
can enter the driving term Qij for structure formation; cf. (11). In contrast to that,
Linsker 's ansatz is based on a firing rate description, where the term Qij contains
correlations between mean firing rates only. In his Qij term, mean firing rates take
the place of r~n and A~n. If we use a standard interpretation of rate coding, a mean
firing rate corresponds to a temporally averaged quantity with an averaging window
or a hundred milliseconds or more.
Formally, we could define mean rates by temporal averaging with either ?( s) or
W(s) as the averaging window. In this sense, Linsker's 'rates ' have been made
more precise by (11). Note, however, that (11) is asymmetric: one of the rates
should be convolved with ? , the other one with W.
4.2

Relevance of the k3 term

The most important difference between Linsker's rate-based learning rule and our
Eq. (12) is the existence of a term k3 I: O. We now argue that for a causal chain of
events k3 ex: I dx ?(x) W( -x) must be positive. [We have set x = -s in (15).] First,
without loss of generality, the integral can be restricted to x > 0 since ?(x) is a
response kernel and vanishes for x < O. For excitatory synapses, ?(x) is positive for
x > O. Second, experiments on excitatory synapses show that W(s) is positive for
s < 0 (Markram et al. 1997, Zhang et al. 1998). Thus the integral I dx ?(x) W( -x)
is positive - and so is k 3 .
There is also a more general argument for k3 > 0 based on a literal interpretation of
Hebb's statement (Hebb 1949). Let us recall that s < 0 in (15) means that a presynaptic spike precedes postsynaptic spiking. For excitatory synapses, a presynaptic
spike which precedes postsynaptic firing may be the cause of the postsynaptic activity. [As Hebb puts it, it has 'contributed in firing the postsynaptic cell'.] Thus,
the Hebb rul~ 'predicts ' that for excitatory synapses W(s) is positive for s < O.
Hence, k3 = vln I ds ?( - s) W (s) > 0 as claimed above.
A positive k3 term in (12) gives rise to an exponential growth of weights. Thus any
existing structure in the distribution of weights is enhanced. This contributes to the
stability of weight distributions, especially when there are few and strong synapses
(Gerstner et al. 1996).

Spike-Based Compared to Rate-Based Hebbian Learning

4.3

131

Intrinsic Normalization

Let us suppose that no input synapse is special and impose the (weak) condition
that N - 1 Li Qij = Qo > 0 independent of the synapse index j . We find then from
(12) that the average weight Jo := N-l Li J i has a fixed point Jo = -kd[Qo +
k2 + N- 1 k 3 ]. The fixed point is stable if Qo + k2 + N- 1 k 3 < O. We have shown
above that k3 > O. Furthermore, Qo > 0 according to our assumption. The only
way to enforce stability is therefore a term k2 which is sufficiently negative. Let
us now turn to the definition of k2 in (14). To achieve k2 < 0, either W(O) (the
integral over W) must be sufficiently negative; this corresponds to a learning rule
which is , on the average, anti-Hebbian. Or , for W(O) > 0, the linear term wo ut in
(1) must be sufficiently negative. In addition, for excitatory synapses a reasonable
fixed point Jo has to be positive. For a stable fixed point this is only possible for
kl > 0, which, in turn, implies win to be sufficiently positive; cf. (13).
Intrinsic normalization of synaptic weights is an interesting property, since it allows
neurons to stay at an optimal operating point even while synapses are changing.
Auditory neurons may use such a mechanism to stay during learning in the regime
where coincidence detection is possible (Gerstner et al. 1996, Kempter et al. 1998).
Cortical neurons might use the same principles to operate in the regime of high
variability (Abbott, invited NIPS talk, this volume).
4.4

Conclusions

Spike-based learning is different from simple rate-based learning rules. A spikebased learning rule can pick up correlations in the input on a millisecond time
scale. Mathematically, the main difference to rate-based Hebbian learning is the
existence of a k3 term which accounts for the causal relation between input and
output spikes. Correlations between input and output spikes on a millisecond time
scale playa role and tend to stabilize existing strong synapses.

References
Abeles M. , 1994, In Domany E. et al., editors, Models of Neural Networks II, pp.
121- 140, New York. Springer.
Bialek W . et al. , 1991, Science, 252:1855- 1857.
Carr C. E., 1993 , Annu. Rev. Neurosci. , 16:223-243.
Gerstner W. et al., 1996, Nature, 383:76-78.
Gerstner W. et al., 1998, In W. Maass and C. M. Bishop., editors, Pulsed Neural
Networks, pp. 353-377, Cambridge. MIT-Press.
Hebb D.O. , 1949, The Organization of Behavior. Wiley, New York.
Kempter R. et al., 1998, Neural Comput. , 10:1987- 2017.
Kempter R. et al., 1999, Phys. Rev. E, In Press.
Linsker R., 1986, Proc. Natl. Acad. Sci. USA, 83:7508- 7512.
MacKay D. J. C., Miller K. D. , 1990, Network, 1:257- 297.
Markram H. et al., 1997, Science, 275:213-215.
Senn W. et al., 1999, preprint, Univ. Bern.
Wimbauer S. et al., 1997, BioI. Cybern., 77:453-461.
Zhang L.I. et al. , 1998, Nature, 395 :37- 44

"
1581,"Applying Metric-Trees to Belief-Point POMDPs

Joelle Pineau, Geoffrey Gordon
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
{jpineau,ggordon}@cs.cmu.edu

Sebastian Thrun
Computer Science Department
Stanford University
Stanford, CA 94305
thrun@stanford.edu

Abstract
Recent developments in grid-based and point-based approximation algorithms for POMDPs have greatly improved the tractability of POMDP
planning. These approaches operate on sets of belief points by individually learning a value function for each point. In reality, belief points
exist in a highly-structured metric simplex, but current POMDP algorithms do not exploit this property. This paper presents a new metric-tree
algorithm which can be used in the context of POMDP planning to sort
belief points spatially, and then perform fast value function updates over
groups of points. We present results showing that this approach can reduce computation in point-based POMDP algorithms for a wide range of
problems.

1

Introduction

Planning under uncertainty is a central problem in the field of robotics as well as many
other AI applications. In terms of representational effectiveness, the Partially Observable
Markov Decision Process (POMDP) is among the most promising frameworks for this
problem. However the practical use of POMDPs has been severely limited by the computational requirement of planning in such a rich representation. POMDP planning is difficult
because it involves learning action selection strategies contingent on all possible types of
state uncertainty. This means that whenever the robot?s world state cannot be observed,
the planner must maintain a belief (namely a probability distribution over possible states)
to summarize the robot?s recent history of actions taken and observations received. The
POMDP planner then learns an optimal future action selection for each possible belief. As
the planning horizon grows (linearly), so does the number of possible beliefs (exponentially), which causes the computational intractability of exact POMDP planning.
In recent years, a number of approximate algorithms have been proposed which overcome
this issue by simply refusing to consider all possible beliefs, and instead selecting (and
planning for) a small set of representative belief points. During execution, should the robot
encounter a belief for which it has no plan, it finds the nearest known belief point and
follows its plan. Such approaches, often known as grid-based [1, 4, 13], or point-based [8,
9] algorithms, have had significant success with increasingly large planning domains. They
formulate the plan optimization problem as a value iteration procedure, and estimate the
cost/reward of applying a sequence of actions from a given belief point. The value of

each action sequence can be expressed as an ?-vector, and a key step in many algorithms
consists of evaluating many candidate ?-vectors (set ?) at each belief point (set B).
These B ? ? (point-to-vector) comparisons?which are typically the main bottleneck in
scaling point-based algorithms?are reminiscent of many M ? N comparison problems
that arise in statistical learning tasks, such as kNN, mixture models, kernel regression, etc.
Recent work has shown that for these problems, one can significantly reduce the number of
necessary comparisons by using appropriate metric data structures, such as KD-trees and
ball-trees [3, 6, 12]. Given this insight, we extend the metric-tree approach to POMDP
planning, with the specific goal of reducing the number of B ? ? comparisons. This paper
describes our algorithm for building and searching a metric-tree over belief points.
In addition to improving the scalability of POMDP planning, this approach features a number of interesting ideas for generalizing metric-tree algorithms. For example, when using
trees for POMDPs, we move away from point-to-point search procedures for which the
trees are typically used, and leverage metric constraints to prune point-to-vector comparisons. We show how it is often possible to evaluate the usefulness of an ?-vector over an
entire sub-region of the belief simplex without explicitly evaluating it at each belief point
in that sub-region. While our new metric-tree approach offers significant potential for all
point-based approaches, in this paper we apply it in the context of the PBVI algorithm [8],
and show that it can effectively reduce computation without compromising plan quality.

2

Partially Observable Markov Decision Processes

We adopt the standard POMDP formulation [5], defining a problem by the n-tuple:
{S, A, Z, T, O, R, ?, b0 }, where S is a set of (discrete) world states describing the problem domain, A is a set of possible actions, and Z is a set of possible observations providing (possibly noisy and/or partial) state information. The distribution T (s, a, s 0 ) describes state-to-state transition probabilities; distribution O(s, a, z) describes observation
emission probabilities; function R(s, a) represents the reward received for applying action
a in state s; ? represents the discount factor; and b0 specifies the initial belief distribution. An |S|-dimensional vector, bt , represents the agent?s belief about the state of the
world at time t, and is expressed as a probability distribution over states. This belief is
updated after each time step?to
reflect the latest pair (at?1 , zt )?using a Bayesian filter:
P
bt (s0 ) := c O(s0 , at?1 , zt ) s?S T (s, at?1 , s0 )bt?1 (s), where c is a normalizing constant.

The goal of POMDP
P planning is to find a sequence of actions maximizing the expected
sum of rewards E[ t ? t R(st , at )], for all belief. The corresponding value function can be
P
formulated as a Bellman equation: V (b) = maxa?A R(b, a) + ? b0 ?B T (b, a, b0 )V (b0 )

By definition there exist an infinite number of belief points. However when optimized exactly, the value function is always piecewise linear and convex in the belief (Fig. 1a). After
n value iterations, the solution consists of a finite set of ?-vectors: Vn = {?0 , ?1 , ..., ?m }.
Each ?-vector represents an |S|-dimensional hyper-plane,
P and defines the value function
over a bounded region of the belief: Vn (b) = max??Vn s?S ?(s)b(s). When performing
exact value updates, the set of ?-vectors can (and often does) grow exponentially with the
planning horizon. Therefore exact algorithms tend to be impractical for all but the smallest
problems. We leave out a full discussion of exact POMDP planning (see [5] for more) and
focus instead on the much more tractable point-based approximate algorithm.

3

Point-based value iteration for POMDPs

The main motivation behind the point-based algorithm is to exploit the fact that most beliefs are never, or very rarely, encountered, and thus resources are better spent planning

for those beliefs that are most likely to be reached. Many classical POMDP algorithms
do not exploit this insight. Point-based value iteration algorithms on the other hand apply value backups only to a finite set of pre-selected (and likely to be encountered) belief
points B = {b0 , b1 , ..., bq }. They initialize a separate ?-vector for each selected point, and
repeatedly update the value of that ?-vector. As shown in Figure 1b, by maintaining a full
?-vector for each belief point, we can preserve the piecewise linearity and convexity of
the value function, and define a value function over the entire belief simplex. This is an
approximation, as some vectors may be missed, but by appropriately selecting points, we
can bound the approximation error (see [8] for details).
V={ ? 0 ,? 1 ,? 2 ,? 3 }

V={ ? 0 ,? 1 ,? 3 }

b2

b1

(a)

b0

b3

(b)

Figure 1: (a) Value iteration with exact updates. (b) Value iteration with point-based updates.
There are generally two phases to point-based algorithms. First, a set of belief points is selected, and second, a series of backup operations are applied over ?-vectors for that set of
points. In practice, steps of value iteration and steps of belief set expansion can be repeatedly interleaved to produce an anytime algorithm that can gradually trade-off computation
time and solution quality. The question of how to best select belief points is somewhat
orthogonal to the ideas in this paper and is discussed in detail in [8]. We therefore focus
on describing how to do point-based value backups, before showing how this step can be
significantly accelerated by the use of appropriate metric data structures.
The traditional value iteration POMDP backup operation is formulated as a dynamic program, where we build the n-th horizon value function V from the previous solution V 0 :
""
#
X
X
XX
0
0
0 0
V (b)

=

=

max

R(s, a)b(s)+ ?

a?A

max
a?A

""

s?S

X
z?Z

max

z?Z

max

?0 ?V 0

""
X R(s, a)
|Z|

s?S

?0 ?V 0

T (s, a, s )O(z, s , a)? (s )b(s)

s?S s0 ?S

b(s)+ ?

XX

0

0

0

0

T (s, a, s )O(z, s , a)? (s )b(s)

s?S s0 ?S

(1)

##

To plan for a finite set of belief points B, we can modify this operation such that only
one ?-vector per belief point is maintained and therefore we only consider V (b) at points
b ? B. This is implemented using three steps. First, we take each vector in V 0 and project
it backward (according to the model) for a given action, observation pair. In doing so, we
generate intermediate sets ?a,z , ?a ? A, ?z ? Z:
X
R(s, a)
0
0
0 0
0
0
a,z
a,z
?

?

?i (s) =

|Z|

+?

s0 ?S

T (s, a, s )O(z, s , a)?i (s ), ??i ? V (Step 1) (2)

Second for each b ? B, we construct ?a (?a ? A). This sum over observations1 includes
the maximum ?a,z (at a given b) from each ?a,z :
X
a
?b

=

z?Z

1

argmax(? ? b) (Step 2)

(3)

???a,z

In exact updates, this step requires taking a cross-sum over observations, which is
O(|S| |A| |V 0 ||Z| ). By operating over a finite set of points, the cross-sum reduces to a simple sum,
which is the main reason behind the computational speed-up obtained in point-based algorithms.

Finally, we find the best action for each belief point:
?

V

argmax(?ab ? b),
?a ,?a?A
b

?b ? B (Step 3)

(4)

The main bottleneck in applying point-based algorithms to larger POMDPs is in step 2
where we perform a B ? ? comparison2 : for every b ? B, we must find the best vector
from a given set ?a,z . This is usually implemented as a sequential search, exhaustively
comparing ? ? b for every b ? B and every ? ? ?a,z , in order to find the best ? at
each b (with overall time-complexity O(|A| |Z| |S| |B| |V 0 |)). While this is not entirely
unreasonable, it is by far the slowest step. It also completely ignores the highly structured
nature of the belief space.
Belief points exist in a metric space and there is much to be gained from exploiting this
property. For example, given the piecewise linearity and convexity of the value function, it
is more likely that two nearby points will share similar values (and policies) than points that
are far away. Consequently it could be much more efficient to evaluate an ?-vector over
sets of nearby points, rather than by exhaustively looking at all the points separately. In the
next section, we describe a new type of metric-tree which structures data points based on a
distance metric over the belief simplex. We then show how this kind of tree can be used to
efficiently evaluate ?-vectors over sets of belief points (or belief regions).

4

Metric-trees for belief spaces

Metric data structures offer a way to organize large sets of data points according to distances
between the points. By organizing the data appropriately, it is possible to satisfy many
different statistical queries over the elements of the set, without explicitly considering all
points. Instances of metric data structures such as KD-trees, ball-trees and metric-trees have
been shown to be useful for a wide range of learning tasks (e.g. nearest-neighbor, kernel
regression, mixture modeling), including some with high-dimensional and non-Euclidean
spaces. The metric-tree [12] in particular offers a very general approach to the problem of
structural data partitioning. It consists of a hierarchical tree built by recursively splitting the
set of points into spatially tighter subsets, assuming only that the distance between points
is a metric.
4.1

Building a metric-tree from belief points

Each node ? in a metric-tree is represented by its center ?c , its radius ?r , and a set of points
?B that fall within its radius. To recursively construct the tree?starting with node ? and
building children nodes ? 1 and ? 2 ?we first pick two candidate centers (one per child) at
the extremes of the ??s region: ?c1 = maxb??D D(?c , b), and ?c2 = maxb??D D(?c1 , b). In
a single-step approximation to k-nearest-neighbor (k=2), we then re-allocate each point in
?B to the child with the closest center (ties are broken randomly):
1
?B
?b

2
?B
?b

if D(?c1 , b) < D(?c2 , b)

(5)

if D(?c1 , b) > D(?c2 , b)

Finally we update the centers and calculate the radius for each child:
1
?c1 = Center{?B
}

?r1

= max
1
b??B

D(?c1 , b)

2
?c2 = Center{?B
}

?r2

= max
2
b??B

D(?c2 , b)

(6)
(7)

Step 1 projects all vectors ? ? V 0 for any (a, z) pair. In the worse-case, this has time-complexity
O(|A| |Z| |S|2 |V 0 |), however most problems have very sparse transition matrices and this is typically
much closer to O(|A| |Z| |S| |V 0 |). Step 3 is also relatively efficient at O(|A| |Z| |S| |B|).
2

The general metric-tree algorithm allows a variety of ways to calculate centers and distances. For the centers, the most common choice is the centroid of the points and this is
what we use when building a tree over belief points. We have tried other options, but with
negligible impact. For the distance metric, we select the max-norm: D(?c , b) = ||?c ?b||? ,
which allows for fast searching as described in the next section. While the radius determines the size of the region enclosed by each node, the choice of distance metric determines its shape (e.g. with Euclidean distance, we would get hyper-balls of radius ? r ). In
the case of the max-norm, each node defines an |S|-dimensional hyper-cube of length 2?? r .
Figure 2 shows how the first two-levels of a tree are built, assuming a 3-state problem.
P(s2)

n0

n1

n0

n2
nr
nc

n1

n2

bi bj

P(s1)

(a)

(b)

(c)

...
(d)

Figure 2: (a) Belief points. (b) Top node. (c) Level-1 left and right nodes. (d) Corresponding tree
While we need to compute the center and radius for each node to build the tree, there are
additional statistics which we also store about each node. These are specific to using trees
in the context of belief-state planning, and are necessary to evaluate ? vectors over regions
of the belief simplex. For a given node ? containing data points ?B , we compute ?min and
?max , the vectors containing respectively the min and max belief in each dimension:
?min (s) = min b(s), ?s ? S
b??B

4.2

?max (s) = max b(s), ?s ? S
b??B

(8)

Searching over sub-regions of the simplex

Once the tree is built, it can be used for fast statistical queries. In our case, the goal is to
compute argmax???a,z (? ? b) for all belief points. To do this, we consider the ? vectors
one at a time, and decide whether a new candidate ?i is better than any of the previous
vectors {?0 . . . ?i?1 }. With the belief points organized in a tree, we can often assess this
over sets of points by consulting a high-level node ?, rather than by assessing this for each
belief point separately.
We start at the root node of the tree. There are four different situations we can encounter
as we traverse the tree: first, there might be no single previous ?-vector that is best for all
belief points below the current node (Fig. 3a). In this case we proceed to the children of the
current node without performing any tests. In the other three cases there is a single dominant alpha-vector at the current node; the cases are that the newest vector ?i dominates it
(Fig. 3b), is dominated by it (Fig. 3c), or neither (Fig. 3d). If we can prove that ? i dominates or is dominated by the previous one, we can prune the search and avoid checking the
current node?s children; otherwise we must check the children recursively.
We seek an efficient test to determine whether one vector, ?i , dominates another, ?j , over
the belief points contained within a node. The test must be conservative: it must never
erroneously say that one vector dominates another. It is acceptable for the test to miss
some pruning opportunities?the consequence is an increase in run-time as we check more
nodes than necessary?but this is best avoided if possible. The most thorough test would
check whether ? ? b is positive or negative at every belief sample b under the current node

?i

?

?i

i

?i
?

?

?

?

c

r

(a)

?

?

c

r

(b)

?

?

c

r

c

r

(c)

(d)

Figure 3: Possible scenarios when evaluation a new vector ? at a node ?, assuming a 2-state domain.
(a) ? is a split node. (b) ?i is dominant. (c) ?i is dominated. (d) ?i is partially dominant.

(where ? = (?i ? ?j )). All positive would mean that ?i dominates ?j , all negative the
reverse, and mixed positive and negative would mean that neither dominates the other. Of
course, this test renders the tree useless, since all points are checked individually. Instead,
we test whether ??b is positive or negative over a convex region R which includes all of the
belief samples that belong to the current node. The smaller the region, the more accurate
our test will be; on the other hand, if the region is too complicated we won?t be able to
carry out the test efficiently. (Note that we can always test some region R by solving one
linear program to find l = minb?R b ? ?, another to find h = maxb?R b ? ?, and testing
whether l < 0 < h. But this is expensive and we prefer a more efficient test.)
P(s2)

)
(s 3

ax

?m

?max(s1)

?max(s2)

(b)

)
(s 3

(a)

?min(s1)

in

P(s1)

?m

?min(s2)

(c)

(d)

Figure 4: Several possible convex regions over subsets of belief points, assuming a 3-state domain.
We tested several types of region. The simplest type is an axis-parallel bounding box
(Fig. 4a), ?min ? b ? ?max for vectors ?min P
and ?max (as defined in Eq. 8). We also
tested the simplex defined by b ?P?min and s?S b(s) = 1 (Fig. 4b), as well as the
simplex defined by b ? ?max and s?S b(s) = 1 (Fig. 4c). The most effective test we
discoveredPassumes R is the intersection of the bounding box ?min ? b ? ?max with
the plane s?S b(s) = 1 (Fig. 4d). For each of these shapes, minimizing or maximizing
b ? ? takes time O(d) (where d=#states): for the box (Fig. 4a) we check each dimension
independently, and for the simplices (Figs 4b, 4c) we check each corner exhaustively. For
the last shape (Fig. 4d), maximizing with respect to b is the same as computing ? s.t. b(s) =
?min (s) if ?(s) < ? and b(s) = ?max (s) if ?(s) > ?. We can find ? in expected time O(d)
using a modification of the quick-median algorithm. In practice, not all O(d) algorithms
are equivalent. Empirical results show that checking the corners of regions (b) and (c) and
taking the tightest bounds provides the fastest algorithm. This is what we used for the
results presented below.

5

Results and Discussion

We have conducted a set of experiments to test the effectiveness of the tree structure in
reducing computations. While still preliminary, these results illustrate a few interesting

properties of metric-trees when used in conjunction with point-based POMDP planning.
Figure 5 presents results for six well-known POMDP problems, ranging in size from 4 to
870 states (for problem descriptions see [2], except for Coffee [10] and Tag [8]). While all
these problems have been successfully solved by previous approaches, it is interesting to
observe the level of speed-up that can be obtained by leveraging metric-tree data structures.
In Fig. 5(a)-(f) we show the number of B ? ? (point-to-vector) comparisons required, with
and without a tree, for different numbers of belief points. In Fig. 5(g)-(h) we show the
computation time (as a function of the number of belief points) required for two of the
problems. The No-Tree results were generated by applying the original PBVI algorithm
(Section 2, [8]). The Tree results (which count comparisons on both internal and leaf
nodes) were generated by embedding the tree searching procedure described in Section 4.2
within the same point-based POMDP algorithm. For some of the problems, we also show
performance using an -tree, where the test for vector dominance can reject (i.e. declare ? i
is dominated, Fig. 3c) a new vector that is within  of the current best vector.
6

4

x 10

7

4

x 10

7

x 10

2

x 10

6
# comparisons

1.5

6

4

1

4
3
2

0.5

2

1.5

5

# comparisons

8
# comparisons

2

No Tree
Tree
Epsilon?Tree

# comparisons

10

1

0.5

1

2000
# belief points

3000

0
0

4000

1000

(a) Hanks, |S|=4

0
0

5000

10

# comparisons

4

6

4

0
0

0
0

0
0

(e) Hallway, |S|=60

1200

100

200
300
# belief points

400

(f) Tag, |S|=870

500

100

200
300
# belief points

400

500

(d) Tiger-grid, |S|=36
4

5

x 10

4

10

5

1000

0
0

500

15

2

400
600
800
# belief points

400

20

2

200

200
300
# belief points

25

x 10

8

6

100

(c) Coffee, |S|=32

6

x 10

8
# comparisons

4000

(b) SACI, |S|=12

7

10

2000
3000
# belief points

TIME (secs)

1000

TIME (secs)

0
0

3

2

1

0.5

1
1.5
# belief points

2

2.5
4
x 10

(g) SACI, |S|=12

0
0

200

400
600
# belief points

800

1000

(h) Tag, |S|=870

Figure 5: Results of PBVI algorithm with and without metric-tree.
These early results show that, in various proportions, the tree can cut down on the number
of comparisons. This illustrates how the use of metric-trees can effectively reduce POMDP
computational load. The -tree is particularly effective at reducing the number of comparisons in some domains (e.g. SACI, Tag). The much smaller effect shown in the other
problems may be attributed to a poorly tuned  (we used  = 0.01 in all experiments). The
question of how to set  such that we most reduce computation, while maintaining good
control performance, tends to be highly problem-dependent.
In keeping with other metric-tree applications, our results show that computational savings
increase with the number of belief points. What is more surprising is to see the trees paying
off with so few data points (most applications of KD-trees start seeing benefits with 1000+
data points.) This may be partially attributed to the compactness of our convex test region
(Fig. 4d), and to the fact that we do not search on split nodes (Fig. 3a); however, it is
most likely due to the nature of our search problem: many ? vectors are accepted/rejected
before visiting any leaf nodes, which is different from typical metric-tree applications. We
are particularly encouraged to see trees having a noticeable effect with very few data points
because, in some domains, good control policies can also be extracted with few data points.
We notice that the effect of using trees is negligible in some larger problems (e.g. Tigergrid), while still pronounced in others of equal or larger size (e.g. Coffee, Tag). This is

likely due to the intrinsic dimensionality of each problem.3 Metric-trees often perform
well in high-dimensional datasets with low intrinsic dimensionality; this also appears to be
true of metric-trees applied to vector sorting. While this suggests that our current algorithm
is not as effective in problems with intrinsic high-dimensionality, a slightly different tree
structure or search procedure may well help in those cases. Recent work has proposed new
kinds of metric-trees that can better handle point-based searches in high-dimensions [7],
and some of this may be applicable to the POMDP ?-vector sorting problem.

6

Conclusion

We have described a new type of metric-tree which can be used for sorting belief points
and accelerating value updates in POMDPs. Early experiments indicate that the tree structure, by appropriately pruning unnecessary ?-vectors over large regions of the belief, can
accelerate planning for a range problems. The promising performance of the approach on
the Tag domain opens the door to larger experiments.
Acknowledgments
This research was supported by DARPA (MARS program) and NSF (ITR initiative).

References
[1] R. I. Brafman. A heuristic variable grid solution method for POMDPs. In Proceedings of the
Fourteenth National Conference on Artificial Intelligence (AAAI), pages 727?733, 1997.
[2] A. Cassandra. http://www.cs.brown.edu/research/ai/pomdp/examples/index.html.
[3] J. H. Friendman, J. L. Bengley, and R. A. Finkel. An algorithm for finding best matches in
logarithmic expected time. ACM Transactions on Mathematical Software, 3(3):209?226, 1977.
[4] M. Hauskrecht. Value-function approximations for partially observable Markov decision processes. Journal of Artificial Intelligence Research, 13:33?94, 2000.
[5] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable
stochastic domains. Artificial Intelligence, 101:99?134, 1998.
[6] A. W. Moore. Very fast EM-based mixture model clustering using multiresolution KD-trees. In
Advances in Neural Information Processing Systems (NIPS), volume 11, 1999.
[7] A. W. Moore. The anchors hierarchy: Using the triangle inequality to survive high dimensional
data. Technical Report CMU-RI-TR-00-05, Carnegie Mellon, 2000.
[8] J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration: An anytime algorithm for
POMDPs. In International Joint Conference on Artificial Intelligence (IJCAI), 2003.
[9] K.-M. Poon. A fast heuristic algorithm for decision-theoretic planning. Master?s thesis, The
Hong-Kong University of Science and Technology, 2001.
[10] P. Poupart and C. Boutilier. Value-directed compression of POMDPs. In Advances in Neural
Information Processing Systems (NIPS), volume 15, 2003.
[11] N. Roy and G. Gordon. Exponential family PCA for belief compression in POMDPs. In
Advances in Neural Information Processing Systems (NIPS), volume 15, 2003.
[12] J. K. Uhlmann. Satisfying general proximity/similarity queries with metric trees. Information
Processing Letters, 40:175?179, 1991.
[13] R. Zhou and E. A. Hansen. An improved grid-based approximation algorithm for POMDPs. In
Proceedings of the 17th International Joint Conference on Artificial Intelligence (IJCAI), 2001.
3

The coffee domain is known to have an intrinsic dimensionality of 7 [10]. We do not know the
intrinsic dimensionality of the Tag domain, but many robot applications produce belief points that
exist in sub-dimensional manifolds [11].

"
2188,"Optimal Single-Class Classification Strategies

Ran El-Yaniv
Department of Computer Science
Technion- Israel Institute of Technology
Technion, Israel 32000
rani@cs.technion.ac.il

Mordechai Nisenson
Department of Computer Science
Technion - Israel Institute of Technology
Technion, Israel 32000
motin@cs.technion.ac.il

Abstract
We consider single-class classification (SCC) as a two-person game between the
learner and an adversary. In this game the target distribution is completely known
to the learner and the learner?s goal is to construct a classifier capable of guaranteeing a given tolerance for the false-positive error while minimizing the false
negative error. We identify both ?hard? and ?soft? optimal classification strategies
for different types of games and demonstrate that soft classification can provide
a significant advantage. Our optimal strategies and bounds provide worst-case
lower bounds for standard, finite-sample SCC and also motivate new approaches
to solving SCC.

1 Introduction
In Single-Class Classification (SCC) the learner observes a training set of examples sampled from
one target class. The goal is to create a classifier that can distinguish the target class from other
classes, unknown to the learner during training. This problem is the essence of a great many applications such as intrusion, fault and novelty detection. SCC has been receiving much research attention in the machine learning and pattern recognition communities (for example, the survey papers
[7, 8, 4] cite, altogether, over 100 papers). The extensive body of work on SCC, which encompasses
mainly empirical studies of heuristic approaches, suffers from a lack of theoretical contributions and
few principled (empirical) comparative studies of the proposed solutions. Thus, despite the extent
of the existing literature, some of the very basic questions have remained unresolved.
Let P (x) be the underlying distribution of the target class, defined over some space ?. We call P the
target distribution. Let 0 < ? < 1 be a given tolerance parameter. The learner observes a training
set sampled from P and should then construct a classifier capable of distinguishing the target class.
We view the SCC problem as a game between the learner and an adversary. The adversary selects
another distribution Q over ? and then a new element of ? is drawn from ?P + (1 ? ?)Q, where ?
is a switching parameter (unknown to the learner). The goal of the learner is to minimize the false
negative error, while guaranteeing that the false positive error will be at most ?.
The main consideration in previous SCC studies has been statistical: how can we guarantee a prescribed false positive rate (?) given a finite sample from P ? This question led to many solutions,
almost all revolving around the idea of low-density rejection. The proposed approaches are typically
generative or discriminative. Generative solutions range from full density estimation [2], to partial
density estimation such as quantile estimation [5], level set estimation [1, 9] or local density estimation [3]. In discriminative methods one attempts to generate a decision boundary appropriately
enclosing the high density regions of the training set [11].
In this paper we abstract away the statistical estimation component of the problem and model a
setting where the learner has a very large sample from the target class. In fact, we assume that the
learner knows the target distribution P precisely. While this assumption would render almost the

entire body of SCC literature superfluous, it turns out that a significant, decision-theoretic component of the SCC problem remains ? one that has so far been overlooked. In any case, the results we
obtain here immediately apply to other SCC instances as lower bounds.
The fundamental question arising in our setting is: What are optimal strategies for the learner? In
particular, is the popular low-density rejection strategy optimal? While most or all SCC papers
adopted this strategy, nowhere in the literature could we find a formal justification.
The partially good news is that low-density rejection is worst-case optimal, but only if the learner is
confined to ?hard? decision strategies. In general, the worst-case optimal learner strategy should be
?soft?; that is, the learner should play a randomized strategy, which could result in a very significant
gain. We first identify a monotonicity property of optimal SCC strategies and use it to establish
the optimality of low-density rejection in the ?hard? case. We then show an equivalence between
low-density rejection and a constrained two-class classification problem where the other class is the
uniform distribution over ?. This equivalence motivates a new approach to solving SCC problems.
We next turn our attention to the power of the adversary, an issue that has been overlooked in the
literature but has crucial impact on the relevancy of SCC solutions in applications. For example,
when considering an intrusion detection application (see, e.g., [6]), it is necessary to assume that the
?attacking distribution? has some worst-case characteristics and it is important to quantify precisely
what the adversary knows or can do. The simple observation in this setting is that an omniscient and
unlimited adversary, who knows all parameters of the game including the learner?s strategy, would
completely demolish the learner who uses hard strategies. By using a soft strategy, however, the
learner can achieve on average the biased coin false negative rate of 1 ? ?.
We then analyze the case of an omniscient but limited adversary, who must select a sufficiently
distant Q satisfying DKL (Q||P ) ? ?, for some known parameter ?. One of our main contributions
is a complete analysis of this game, including identification of the optimal strategy for the learner
and the adversary, as well as the best achievable false negative rate. The optimal learner strategy and
best achievable rate are obtained via a solution of a linear program specified in terms of the problem
parameters. These results are immediately applicable as lower bounds for standard (finite-sample)
SCC problems, but may also be used to inspire new types of algorithms for standard SCC. While we
do not have a closed form expression for the best achievable false-negative rate, we provide a few
numerical examples demonstrating and comparing the optimal ?hard? and ?soft? performance.

2 Problem Formulation
The single-class classification (SCC) problem is defined as a game between the learner and an
adversary. The learner receives a training sample of examples from a target distribution P defined
over some space ?. On the basis of this training sample, the learner should select a rejection function
r : ? ? [0, 1], where for each ? ? ?, r? = r(?) is the probability with which the learner will
reject ?. On the basis of any knowledge of P and/or r(?), the adversary selects selects an attacking
distribution Q, defined over ?. Then, a new example is drawn from ?P +(1??)Q, where 0 < ? < 1,
is a switching probability unknown to the learner. The rejection rate of the learner, using a rejection
?
function r, with respect to any distribution D (over ?), is ?(D) = ?(r, D) = ED {r(?)}. For
notational convenience whenever we decorate r (e.g., r? ,r? ), the corresponding ? will be decorated
accordingly (e.g., ?? ,?? ). The two main quantities of interest here are the false positive rate (type I
error) ?(P ), and the false negative rate (type II error) 1 ? ?(Q).
Before the start of the game, the learner receives a tolerance parameter 0 < ? < 1, giving the
maximally allowed false positive rate. A rejection function r(?) is valid if its false positive rate
?(P ) ? ?. A valid rejection function (strategy) is optimal if it guarantees the smallest false negative
rate amongst all valid strategies.
We consider a model where the learner knows the target distribution P exactly, thus focusing on the
decision-theoretic component in SCC. Clearly, our model approximates a setting where the learner
has a very large training set, but the results we obtain immediately apply, in any case, as lower
bounds to other SCC instances.
This SCC game is a two-person zero sum game where the payoff to the learner is ?(Q). The set
?
R? (P ) = {r : ?(P ) ? ?} of valid rejection functions is the learner?s strategy space. Let Q be the

strategy space of the adversary, consisting of all allowable distributions Q that can be selected by
the adversary. We are concerned with optimal learner strategies for game variants distinguished by
the adversary?s knowledge of the learner?s strategy, P and/or of ? and by other limitations on Q.
We distinguish a special type of this game, which we call the hard setting, where the learner must
deterministically reject or accept new events; that is, r : ? ? {0, 1}, and such rejection functions
are termed ?hard.? The more general game defined above (with ?soft? functions) is called the soft
setting. As far as we know, only the hard setting has been considered in the SCC literature thus far.
In the soft setting, given any rejection function, the learner can reduce the type II error by rejecting
more (i.e., by increasing r(?)). Therefore, for an optimal r(?) we have ?(P ) = ? (rather than
?(P ) ? ?). It follows that the switching parameter ? is immaterial to the selection of an optimal
strategy. Specifically, the combined error of an optimal strategy is ??(P ) + (1 ? ?)(1 ? ?(Q)) =
?? + (1 ? ?)(1 ? ?(Q)), which is minimized by minimizing the type II error, 1 ? ?(Q).
?

We assume throughout this paper a finite support of size N ; that is, ? = {1, . . . , N } and P =
?
{p1 , . . . , pN } and Q = {q1 , . . . , qN } are probability mass functions. Additionally, a ?probability
distribution? refers to a distribution over the fixed support set ?. Note that this assumption still
leaves us with an infinite game because the learner?s pure strategy space, R? (P ), is infinite.1

3 Characterizing Monotone Rejection Functions
In this section we characterize the structure of optimal learner strategies. Intuitively, it seems plausible that the learner should not assign higher rejection values to higher probability events under P .
That is, one may expect that a reasonable rejection function r(?) would be monotonically decreasing
with probability values (i.e., if pj ? pk then rj ? rk ). Such monotonicity is a key justification for
a very large body of SCC work, which is based on low density rejection strategies. Surprisingly,
optimal monotone strategies are not always guaranteed as shown in the following example.
Example 3.1 (Non-Monotone Optimality) In the hard setting, take N = 3, P =
(0.06, 0.09, 0.85) and ? = 0.1. The two ?-valid hard rejection functions are r? = (1, 0, 0) and
r?? = (0, 1, 0). Let Q = {Q = (0.01, 0.02, 0.97)}. Clearly ?? (Q) = 0.01 and ??? (Q) = 0.02
and therefore, r?? (?) is optimal despite breaking monotonicity. More generally, this example holds if
Q = {Q : q2 ? q1 ? ?} for any 0 < ? ? 1.
In the soft setting, let N = 2, P = (0.2, 0.8), and ? = 0.1. We note that R? (P ) = {r? =
(0.1 + 4?, 0.1 ? ?)}, for ? ? [?0.025, 0.1]. We take Q = {Q = (0.1, 0.9)}. Then ?? (Q) = 0.1 +
0.4? ? 0.9? = 0.1 ? 0.5?. This is clearly maximized when we minimize ? by taking ? = ?0.025, and
then the optimal rejection function is (0, 0.125), which clearly breaks monotonicity. This example
also holds for Q = {Q : q2 ? cq1 } for any c > 4.
Fix P and ?. For any adversary strategy space, Q, let R?? (P ) be the set of optimal valid rejection
?
functions, R?? = {r ? R? (P ) : minQ?Q ?(Q) = maxr? ?R? (P ) minQ?Q ?? (Q)}.2 We note that
?
R? is never empty in the cases we consider. A simple observation is that for any r ? R?? there
exists r? ? R?? such that r? (i) = r(i) for all i such that pi > 0 and for zero probabilities, pj = 0,
r? (j) = 1.
The following property ensures that R?? will include a monotone (optimal) hard strategy, which
means that the search space for the learner can be conveniently confined to monotone strategies.
While the set of all distributions satisfies this property, later on we will consider limited strategic
adversary spaces where this property still holds.3
1

The game is conveniently described in extensive form (i.e., game tree) where in the first move the learner
selects a rejection function, followed by a chance move to determine the source (either P or Q) of the test
example (with probability ?). In the case where Q is selected, the adversary chooses (randomly using Q) the
test example. In this game the choice of Q depends on knowledge of P and r(?).
2
For certain strategy spaces, Q, it may be necessary to consider the infimum rather than the minimum. In
such cases it may be necessary to replace ?Q ? Q? (in definitions, theorems, etc.) with ?Q ? cl(Q)?, where
cl(Q) is the closure of Q.
3
All properties defined in this paper could be made weaker for the purposes of the proofs, but this would
needlessly complicate them. Indeed, the way they are currently defined is sufficient for most ?reasonable? Q.

Definition 3.2 (Property A) Let P be a distribution. A set of distributions Q has Property A w.r.t.
P if for all j, k and Q ? Q such that pj < pk and qj < qk , there exists Q? ? Q such that qk? ? qj ,
qj? ? qk and for all i 6= j, k, we have qi? = qi .
Theorem 3.3 (Monotone Hard Decisions) When the learner is restricted to hard-decisions and Q
satisfies Property A w.r.t. P , then ?r ? R?? such that pj < pk ? r(j) ? r(k).4
Proof: Let us assume by contradiction that no such rejection function exists in R?? . Let r ? R?? .
Let j be such that pj = min?:r(?)=0 p? . Then, there must exist k, such that pj < pk and r(k) = 1
(otherwise r is monotone). Define r? to be r with the values of j and k swapped; that is, r? (j) =
1, r? (k) = 0 and for all other i, r? (i) = r(i). We note that ?? (P ) = ?(P ) + pj ? pk < ?(P ) ?
?. Let Q? ? Q be such that minQ ?? (Q) = ?? (Q? ) = ?(Q? ) + qj? ? qk? . Thus, if qj? ? qk? ,
?? (Q? ) ? ?(Q? ). Otherwise, there exists Q? ? as in Property A and in particular, q ? ?k ? qj? . As a
result, ?? (Q? ) = ?(Q? ? ) + qj? ? q ? ?k ? ?(Q? ? ). Therefore, there always exists Q ? Q such that
?? (Q? ) ? ?(Q) (either Q = Q? or Q = Q? ? ). Consequently, minQ ?? (Q) ? minQ ?(Q), and thus,
r? ? R?? . As long as there are more j, k pairs which need to have their rejection levels fixed, we
label r = r? and repeat the above procedure. Since the only changes are made to r? (j) and r? (k),
and since j is the non-rejected event with minimal probability, the procedure will be repeated at
most N times. The final r? is in R?? and satisfies pj < pk ? r(j) ? r(k). Contradiction.

Theorem 3.3 provides a formal justification for the low-density rejection strategy (LDRS), popular
in the SCC literature. Specifically, assume w.l.o.g. p1 ? p2 ? ? ? ? ? pN . The corresponding ?-valid
P
low density rejection function places rj = 1 iff ji=1 pi ? ?.
Our discussion on soft decisions is facilitated by Property B and Theorem 3.5 that follow.

Definition 3.4 (Property B) Let P be a distribution. A set of distributions Q has Property B w.r.t.
q
P if for all j, k and Q ? Q such that 0 < pj ? pk and pjj < pqkk , there exists Q? ? Q such that
qj?
pj

?

?
qk
pk

and for all i 6= j, k, qi? = qi .

The rather technical proof of the following theorem is omitted for lack of space (and appears in the
adjoining, supplementary appendix).
Theorem 3.5 (Monotone Soft Decisions) If Q satisfies Property B w.r.t. P , then ?r ? R?? such
that: (i)pi = 0 ? r(i) = 1; (ii) pj < pk ? r(j) ? r(k); and (iii) pj = pk ? r(j) = r(k).

4 Low-Density Rejection and Two-Class Classification
In this section we focus on the hard setting. We show that the low-density rejection strategy (LDRS
- defined in Section 3) is optimal. Moreover we show that the optimal hard performance can be obtained by solving a constrained two-class classification problem where the other class is the uniform
distribution over ?. The results here consider families Q that satisfy the following property.
Definition 4.1 (Property C) Let P be a distribution. A set of distributions Q has Property C w.r.t.
P if for all j, k and Q ? Q such that pj = pk there exists Q? ? Q such that qk? = qj , qj? = qk and
for all i 6= j, k, qi? = qi .
We state without proof the following lemma (the proof can be found in the appendix).
Lemma 4.2 Let r? be a ?-valid low-density rejection function (LDRS). Let r be any monotone ?valid rejection function. Then minQ?Q ?? (Q) ? minQ?Q ?(Q) for any Q satisfying Property C.
Example 4.3 (Violation of Property C) We illustrate here that violating Property C may result in
a violation of Lemma 4.2. Let N = 5, P = (0.02, 0.03, 0.05, 0.05, 0.85), and ? = 0.1. Then the
two ?-valid LDRS rejection functions are r = (1, 1, 1, 0, 0) and r? = (1, 1, 0, 1, 0). Let Q = {Q :
q3 ? q4 > ?} for some 0 < ? < 1. Then, for any Q ? Q, ?(Q) ? ?? (Q) = q3 ? q4 > ?, and
therefore, for the LDRS, r? , there exists a monotone r such that minQ?Q ?? (Q) < minQ?Q ?(Q).
4

Here we must consider a weaker notion of monotonicity for hard strategies to be both valid and optimal.

When Q satisfies Property A, then by Theorem 3.3 there exists a monotone optimal rejection function. Therefore, the following corollary of Lemma 4.2 establishes the optimality of any LDRS.
Corollary 4.4 Any ?-valid LDRS is optimal if Q satisfies both Property A and Property C.
Thus, any LDRS strategy is indeed worst-case optimal when the learner is willing to be confined
to hard rejection functions and when the adversary?s space satisfies Property A and Property C. We
now show that an (optimal) LDRS solution is equivalent to an optimal solution of the following
constrained Bayesian two-class decision problem. Let the first class c1 have distribution P (x) and
the second class, c2 , have the uniform distribution U (x) = 1/N . Let 0 < c < 1 and 0 < ? <
(N ?c + 1 ? c)/N ?c. The classes have priors Pr{c1 } = c and Pr{c2 } = 1 ? c. The loss function ?ij ,
giving the cost of deciding ci instead of cj (i, j = 1, 2), is ?11 = ?22 = 0, ?12 = (N c+1?c)/(1?c)
and ?21 = ?. The goal is to construct a classifier
PC(x) ? {c1 , c2 ) that minimizes the total Bayesian
risk under the constraint that, for a given ?, x:C(x)=c2 P (x) ? ?. We term this problem ?the
Bayesian binary problem.?
Theorem 4.5 An optimal binary classifier for the Bayesian binary problem induces an optimal
(hard) solution to the SCC problem (an LDRS) when Q satisfies properties A and C.
Proof Sketch: Let C ? (?) be an optimal classifier for the Bayesian binary problem. Any classifier
C(?) induces a hard rejection function r(?) by taking r(x) = 1 ? C(x) = c2 . Therefore, the set of
?
feasible classifiers (satisfying the constraint)
P clearly induces R? (P ). Let Mi (C) = {x : C(x) = i}.
Note that the constraint is equivalent to x?M2 (C) P (x) ? ?. The Bayes risk for classifying x
?

as i is Ri (x) = ?ii Pr{ci |x} + ?i(3?i) Pr{c3?i |x} = ?i(3?i) Pr{c3?i |x}. The total Bayes risk is
P
? P
R(C) = x?M1 (C) R1 (x) + x?M2 (C) R2 (x), which is minimized at C ? (?). It is not difficult to
show that R1 (?) and R2 (?) are monotonically decreasing and increasing, respectively. It therefore
follows that x ? M1 (C ? ), y ? M2 (C ? ) ? P (x) ? P (y) (otherwise, by swapping C ? (x) and
C ? (y), the constraint can be maintained and R(C ? ) decreased).
It is also not difficult to show that
P
R1 (x) ? 1 > R2 (x) for any x. Thus, it follows that y?M2 (C ? ) P (y) + minx?M1 (C ? ) P (x) > ?
(otherwise, some x could be transferred from M1 (C ? ) to M2 (C ? ), reducing R(C ? )). Together,
these two properties immediately imply that C ? (?) induces a ?-valid LDRS.

Theorem 4.5 motivates a different approach to SCC in which we sample from the uniform distribution over ? and then attempt to approximate the optimal Bayes solution to the constrained binary
problem. It also justifies certain heuristics found in the literature [10, 11].

5 The Omniscient Adversary: Games, Strategies and Bounds
5.1 Unrestricted Adversary
In the first game we analyze an adversary who is completely unrestricted. This means that Q is
the set of all distributions. Unsurprisingly, this game leaves little opportunity for the learner. For
?
?
any rejection function r(?), define rmin = mini r(i) and Imin (r) = {i : r(i) = rmin }. For any
PN
PN
distribution D, ?(D) = i=1 di r(i) ? i=1 di rmin = rmin , in particular, ? = ?(P ) ? rmin
and minQ ?(Q) ? rmin . By choosing Q such that qi = 1 for some i ? Imin (r), the adversary
can achieve ?(Q) = rmin (the same rejection rate is achieved by taking any Q with qi = 0 for all
?
i 6? Imin (r)). In the soft setting, minQ ?(Q) is maximized by the rejection function r? (i) = ? for
?
all pi > 0 (r? (i) = 1 for all pi = 0) This is equivalent to flipping a ?-biased coin for non-null
events (under P ). The best achievable Type II Error is 1 ? ?. In the hard setting, clearly rmin = 0
(otherwise 1 > ? ? 1), and the best achievable Type II Error is precisely 1. That is, absolutely
nothing can be achieved.
This simple analysis shows the futility of the SCC game when the adversary is too powerful. In
order to consider SCC problems at all one must consider reasonable restrictions on the adversary
that lead to more useful games. One type of restriction would be to limit the adversary?s knowledge
of r(?), P and/or of ?. Another type would be to directly limit the strategic choices available to the
adversary. In the next section we focus on the latter type.

5.2 A Constrained Adversary
In seeking a quantifiable constraint on Q it is helpful to recall that the essence of the SCC problem is
to try to distinguish between two probability distributions (albeit one of them unknown). A natural
constraint is a lower bound on the ?distance? between these distributions. Following similar results
in hypothesis testing, we would like to consider games in which the adversary must select Q such that
D(P ||Q) ? ?, for some constant ? > 0, where D(?||?) is the KL-divergence. Unfortunately, this
constraint is vacuous since D(P ||Q) explodes when qi ? pi (for any i). In this case the adversary
can optimally play the same strategy as in the unrestricted game while meeting the KL-divergence
constraint. Fortunately, by taking D(Q||P ) ? ?, we can effectively constrain the adversary.
We note, as usual, that the learner can (and should) reject with probability 1 any null events under
P . Thus, an adversary would be foolish to choose a distribution Q that has any probability for
?
these events. Therefore, we henceforth assume w.l.o.g. that ? = ?(P ) = {? : p? > 0}. Taking
P
?
?
D(Q||P ) = N
i=1 qi log(qi /pi ), we then define Q = Q? = {Q : D(Q||P ) ? ?}. We note that Q?
possesses properties A, B and C w.r.t. P ,5 and by Theorems 3.3 and 3.5 there exists a monotone
r ? R?? (in both the hard and soft settings) and by Corollary 4.4, any ?-valid LDRS is hard-optimal.
If maxi pi ? 2?? , then any Q which is concentrated on a single event meets the constraint
D(Q||P ) ? ?. Then, the adversary can play the same strategy as in the unrestricted game,
and the learner should select r? as before. For the game to be non-trivial it is thus required that
? > log(1/ maxi pi ). Similarly, if the optimal r is such that there exists j ? Imin (r) (that is
r(j) = rmin ) and pj ? 2?? , then a distribution Q that is completely concentrated on j has
D(Q||P ) ? ? and achieves ?(Q) = rmin as in the unrestricted game. Therefore, r = r? , and
so maximizes rmin . We thus assume that the optimal r has no such j.
We begin our analysis of the game by identifying some useful characteristics of optimal adversary
strategies in Lemma 5.1. Then Theorem 5.2 shows that the effective support of an optimal Q has
a size of two at most. Based on these properties, we provide in Theorem 5.3 a linear program that
computes the optimal rejection function. The following lemma is stated without its (technical) proof.
Lemma 5.1 If Q minimizes ?(Q) and meets the constraint D(Q||P ) ? ? then: (i) D(Q||P ) = ?;
q
(ii) pj < pk and qk > 0 ? r(j) > r(k); (iii) pj < pk and qj > 0 ? qj log pjj + qk log pqkk >
(qj + qk ) log

qj +qk
pk ;

(iv) pj < pk and qj > 0 ?

qj
pj

>

qk
pk ;

and (v) qj , qk > 0 ? pj 6= pk .

Theorem 5.2 Any optimal adversarial strategy Q has an effective support of size at most two.
Proof Sketch: Assume by contradiction that an optimal Q? has an effective support of size J ? 3.
W.l.o.g. we rename events such that the first J events are the effective support of Q? (i.e., qi? > 0,
i = 1, . . . , J). From part (i) of Lemma 5.1, Q? is a global minimizer of ?(Q) subject to the
PJ
PJ
constraints i=1 qi log pqii = ?, qi > 0 (i = 1, . . . , J) and i=1 qi = 1. The Lagrangian of this
problem is
!
!
J
J
J
X
X
X
qi
L(Q, ?) =
r(i)qi + ?1
qi log ? ? + ?2
qi ? 1 .
(1)
pi
i=1
i=1
i=1
It is not hard to show, using parts (iv) and (v) of Lemma 5.1, that 
Q? is an extremum
point of (1).

qi?
?L(Q? ,?)
= r(i) + ?1 log pi + 1 + ?2 = 0. Solving
Taking the partial derivatives of (1) we have:
?qi
?L(Q? ,?)
?q1

q?

?

q?

,?)
= ?L(Q
for ?1 , we get ?1 = (r(2) ? r(1))/(log p11 ? log p22 ). If we assume (w.l.o.g.)
?q2
that p1 < p2 , then, from parts (ii) and (iv) of Lemma 5.1, r(2) < r(1) and q1? /p1 > q2? /p2 . Thus
2
= ?qi1 < 0, and (1) is strictly concave. Therefore, since Q? is
?1 < 0. Therefore, for all i, ? L(Q,?)
?qi2
an extremum of the (strictly concave) Lagrangian function, it is the unique global maximum.
?

By part (iv) of Lemma 5.1, the smooth function fP,? (q1 , q2 , . . . , qJ?1 ) = D(Q||P ) ? ? has a root
at Q? where no partial derivative is zero. Therefore, it has an infinite number of roots in any convex
5

For any pair j, k such that pj ? pk , D(Q||P ) does not decrease by transferring all the probability from k
q
q +q
to j in Q: qj log pjj + qk log pqk ? (qj + qk ) log jpj k .
k

? 6= Q? , where q?i > 0
domain where Q? is an internal point. Thus, there exists another distribution, Q
for i = 1, . . . , J, which meets the equality criteria of the Lagrangian. Since Q? is the unique global
? = L(Q,
? ?) < L(Q? , ?) = ?(Q? ). Contradiction.
maximum of L(Q, ?): ?(Q)

We now turn our attention to the learner?s selection of r(?). As already noted, it is sufficient for the
learner to consider only monotone rejection functions. Since for these functions pj = pk ? r(j) =
r(k), the learner can partition ? into K = K(P ) event subsets, which correspond, by probability,
to ?level sets?, S1 , S2 , . . . , SK (all events in a level set S have probability PS ). We re-index these
subsets such that 0 < PS1 < PS2 < ? ? ? < PSK . Define K variables r1 , r2 , . . . , rK , representing
the rejection rate assigned to each of the K level sets (?? ? Si , r(?) = ri ). We group our level sets
by probability: L = {S : PS < 2?? }, M = {S : PS = 2?? }, and H = {S : PS > 2?? }.
By Theorem 5.2, the optimal Q which the adversary selects will have an effective support of size
2 at most. If it has an effective support of size 1, then the event ? for which q? = 1 cannot be
from a level set in L or H (otherwise, part (i) of Lemma 5.1 would be violated). Therefore it must
belong to the single level set in M . Thus, if M = {Sm } (for some index m), then there are feasible
solutions Q such that q? = 1 (for ? ? Sm ), all of which have ?(Q) = rm . If, on the other hand,
Q has an effective support of size 2, then it is not hard to show that one of the two events must
be from a level set Sl ? L, and the other, from a level set Sh ? H (since all other combinations
result in a violation of either part (i) or part (iii) of Lemma 5.1). Then, there is a single solution to
l
ql log PqSl + (1 ? ql ) log 1?q
PSh = ?, where ql and 1 ? ql are the probabilities that Q assigns to the
l
events from Sl and Sh , respectively. For such a distribution, ?(Q) = ql rl + (1 ? ql )rh .
Therefore, the adversary?s choice of an optimal distribution, Q, must have one of |L||H| + |M | ?
2
? K4 ? (possibly different) rejection rates. Each of these rates, ?1 , ?2 , . . . , ?|L||H|+|M| , is a linear
combination of at most two variables, ri and rj . We introduce an additional variable, z, to represent
the max-min rejection rate. We thus have:
Theorem 5.3 An optimal soft rejection function and the lower-bound on the Type II Error, 1 ? z, is
obtained by solving the following linear program:6 maximizer1 ,r2 ,...,rK ,z z, subject to:
K
X

ri |Si |PSi = ?, 1 ? r1 ? r2 ? ? ? ? ? rK ? 0, ?i ? z, i ? {1, 2, . . . , |L||H| + |M |}.

i=1

5.2.1 Numerical Examples
We now compare the performance of hard and soft rejection strategies for this constrained game
(D(Q||P ) ? ?) for various values of ?, and two different families of target distributions, P over
support N = 50. The families are arbitrary probability mass functions over N events and discretized Gaussians (over N bins). For each ? we generated 50 random distributions P for each of
the families.7 For each such P we solved the optimal hard and soft strategies and computed the
corresponding worst-case optimal type II error, 1 ? ?(Q).
The results for ? = 0.05 are shown in Figure 5.2.1. Other results (not presented) for a wide variety
of the problem parameters (e.g., N , ?) are qualitatively the same. It is evident that both the soft and
hard strategies are ineffective for small ?. Clearly, the soft approach has significantly lower error
than the hard approach (until ? becomes ?sufficiently large?).
6
Let r ? be the solution to the linear program. Our derivation of the linear program is dependent on the
assumption that there is no event j ? Imin (r ? ) such that pj ? 2?? (see discussion preceding Lemma 5.1). If
r ? contradicts this assumption then, as discussed, the optimal strategy is r ? , which is optimal. It is not hard to
prove that in this case r ? = r ? anyway, and thus the solution to the linear program is always optimal.
7
Since maxQ D(Q||P ) = log(1/ mini pi ), it is necessary that mini pi ? 2?? when generating P (to
ensure that a ?-distant Q exists). Distributions in the first family of arbitrarily random distributions (a) are
generated by sampling a point (p1 ) uniformly in (0, 2?? ]. The other N ? 1 points are drawn i.i.d. ? U (0, 1],
and then normalized so that their sum is 1 ? p1 . The second family (b) are Gaussians centered at 0 and
discretized over N evenly spaced bins in the range [?10, 10]. A (discretized) random Gaussian N (0, ?) is
selected by choosing ? uniformly in some range [?min , ?max ]. ?min is set to the minimum ? ensuring that the
first/last bin will not have ?zero? probability (due to limited precision). ?max was set so that the cumulative
probability in the first/last bin will be 2?? , if possible (otherwise ?max is arbitrarily set to 10 ? ?min ).

Hard
0.8

0.6

0.4

0.2

0
0

1

Soft

2

4

6

8

Lambda (?)

(a) Arbitrary

10

12

Worst Case Type II Error

Worst Case Type II Error

1

Soft
Hard

0.9
0.8
0.7
0.6
0.5
0.4
0

2

4

6

8

10

12

Lambda (?)

(b) Gaussians

Figure 1: Type II Error vs. ?, for N = 50 and ? = 0.05. 50 distributions were generated for each
value of ? (? = 0.5, 0.1, ? ? ? , 12.5). Error bars depict standard error of the mean (SEM).

6 Concluding Remarks
We have introduced a game-theoretic approach to the SCC problem. This approach lends itself well
to analysis, allowing us to prove under what conditions low-density rejection is hard-optimal and if
an optimal monotone rejection function is guaranteed to exist. Our analysis introduces soft decision
strategies, which allow for significantly better performance. Observing the learner?s futility when
facing an omniscient and unlimited adversary, we considered restricted adversaries and provided
full analysis of an interesting family of constrained games. This work opens up many new avenues
for future research. We believe that our results could be useful for inspiring new algorithms for
finite-sample SCC problems. For example, the equivalence of low-density rejection to the Bayesian
binary problem as shown in Section 3.3 obviously motivates a new approach. Clearly, the utilization
of randomized strategies should be carried over to the finite sample case as well. Our approach can
be extended and developed in several ways. A very interesting setting to consider is one in which the
adversary has partial knowledge of the problem parameters and the learner?s strategy. For example,
the adversary may only know that P is in some subspace. Additionally, it is desirable to extend
our analysis to infinite and continuous event spaces. Finally, it would be very nice to determine an
explicit expression for the lower bound obtained by the linear program of Theorem 5.3.

References
[1] S. Ben-David and M. Lindenbaum. Learning distributions by their density-levels - a paradigm for learning
without a teacher. In EuroCOLT, pages 53?68, 1995.
[2] C.M. Bishop. Novelty detection and neural network validation. IEE Proceedings - Vision, Image, and
Signal Processing, 141(4):217?222, 1994.
[3] M.M. Breunig, H.P. Kriegel, R.T. Ng, and J. Sander. Lof: Identifying density-based local outliers. In
SIGMOD Conference, pages 93?104, 2000.
[4] V. Hodge and J. Austin. A survey of outlier detection methodologies. Artificial Intelligence Review,
22(2):85?126, 2004.
[5] G.R.G. Lanckriet, L. El Ghaoui, and M.I. Jordan. Robust novelty detection with single-class mpm. In
NIPS, pages 905?912, 2002.
[6] A. Lazarevic, L. Ert?oz, V. Kumar, A. Ozgur, and J. Srivastava. A comparative study of anomaly detection
schemes in network intrusion detection. In SDM, 2003.
[7] M. Markou and S. Singh. Novelty detection: a review ? part 1: statistical approaches. Signal Processing,
83(12):2481?2497, 2003.
[8] M. Markou and S. Singh. Novelty detection: a review ? part 2: neural network based approaches. Signal
Processing, 83(12):2499?2521, 2003.
[9] I. Steinwart, D. Hush, and C. Scovel. A classification framework for anomaly detection. Journal of
Machine Learning Research, 6, 2005.
[10] David M. J. Tax and Robert P. W. Duin. Uniform object generation for optimizing one-class classifiers.
Journal of Machine Learning Research, 2:155?173, 2002.
[11] H. Yu. Single-class classification with mapping convergence. Machine Learning, 61(1-3):49?69, 2005.

"
6155,"A Powerful Generative Model Using Random Weights
for the Deep Image Representation

Kun He?, Yan Wang ?
Department of Computer Science and Technology
Huazhong University of Science and Technology, Wuhan 430074, China
brooklet60@hust.edu.cn, yanwang@hust.edu.cn
John Hopcroft
Department of Computer Science
Cornell University, Ithaca 14850, NY, USA
jeh@cs.cornell.edu

Abstract
To what extent is the success of deep visualization due to the training? Could
we do deep visualization using untrained, random weight networks? To address
this issue, we explore new and powerful generative models for three popular deep
visualization tasks using untrained, random weight convolutional neural networks.
First we invert representations in feature spaces and reconstruct images from white
noise inputs. The reconstruction quality is statistically higher than that of the same
method applied on well trained networks with the same architecture. Next we
synthesize textures using scaled correlations of representations in multiple layers
and our results are almost indistinguishable with the original natural texture and
the synthesized textures based on the trained network. Third, by recasting the
content of an image in the style of various artworks, we create artistic images with
high perceptual quality, highly competitive to the prior work of Gatys et al. on
pretrained networks. To our knowledge this is the first demonstration of image
representations using untrained deep neural networks. Our work provides a new
and fascinating tool to study the representation of deep network architecture and
sheds light on new understandings on deep visualization. It may possibly lead to a
way to compare network architectures without training.

1

Introduction

In recent years, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs),
have demonstrated highly competitive results on object recognition and image classification [1, 2, 3, 4].
With advances in training, there is a growing trend towards understanding the inner working of these
deep networks. By training on a very large image data set, DNNs develop a representation of images
that makes object information increasingly explicit at various levels of the hierarchical architecture.
Significant visualization techniques have been developed to understand the deep image representations
on trained networks [5, 6, 7, 8, 9, 10, 11].
Inversion techniques have been developed to create synthetic images with feature representations
similar to the representations of an original image in one or several layers of the network. Feature
representations are a function ? of the source image x0 . An approximate inverse ??1 is used to
?
?

The three authors contributing equally.
Corresponding author.

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

construct a new image x from the code ?(x0 ) by reducing some statistical discrepancy between
?(x) and ?(x0 ). Mahendran et al. [7] use the pretrained CNN AlexNet [2] and define a squared
Euclidean loss on the activations to capture the representation differences and reconstruct the image.
Gatys et al. [8, 12] define a squared loss on the correlations between feature maps of some layers
and synthesize natural textures of high perceptual quality using the pretrained CNN called VGG [3].
Gatys et al. [13] then combine the loss on the correlations as a proxy to the style of a painting and the
loss on the activations to represent the content of an image, and successfully create artistic images
by converting the artistic style to the content image, inspiring several followups [14, 15]. Another
stream of visualization aims to understand what each neuron has learned in a pretrained network
and synthesize an image that maximally activates individual features [5, 9] or the class prediction
scores [6]. Nguyen et al. further try multifaceted visualization to separate and visualize different
features that a neuron learns [16].
Feature inversion and neural activation maximization both start from a white noise image and calculate
the gradient via backpropagation to morph the white noise image and output a natural image. In
addition, some regularizers are incorporated as a natural image prior to improve the visualization
quality, including ??norm [6], total variation [7], jitter [7], Gaussian blur [9], data-driven patch
priors [17], etc. The method of visualizing the feature representation on the intermediate layers sheds
light on the information represented at each layer of the pretrained CNN.
A third set of researchers trains a separate feed-forward CNN with deconvolutional layers using
representations or correlations of the feature maps produced in the original network as the input and
the source image as the target to learn the inversion of the original network. The philosophy is to
train another neural network to inverse the representation and speedup the visualization on image
reconstruction [10, 18], texture synthesis [19] or even style transfer [15]. Instead of designing a
natural prior, some researchers incorporate adversarial training [20] to improve the realism of the
generated images [18]. Their trained deconvolutional network could give similar qualitative results as
the inversion technique does and is two or three orders of magnitude faster, as the previous inversion
technique needs a forward and backward pass through the pretrained network. This technique is
slightly different from the previous two in that it does not focus on understanding representations
encoded in the original CNN but on the visualization of original images by training another network.
It is well recognized that deep visualization techniques conduct a direct analysis of the visual information contained in image representations, and help us understand the representation encoded
at the intermediate layers of the well trained DNNs. In this paper, we raise a fundamental issue
that other researchers rarely address: Could we do deep visualization using untrained, random
weight DNNs? What kind of deep visualization could be applied on random weight DNNs?
This would allow us to separate the contribution of training from the contribution of the network structure. It might even give us a method to evaluate deep network architectures without
spending days and significant computing resources in training networks so that we could compare them. Also, it will be useful not to have to store the weights, which can have significant
impact for mobile applications. Though Gray et al. demonstrated that the VGG architecture with
random weights failed in generating textures and resulted in white noise images in an experiment
indicating the trained filters might be crucial for texture generation [8], we conjecture the success
of deep visualization mainly originates from the intrinsic nonlinearity and complexity of the deep
network hierarchical structure rather than from the training, and that the architecture itself may
cause the inversion invariant to the original image. Gatys et al.?s unsuccessful attempt on the texture
synthesis using the VGG architecture with random weights may be due to their inappropriate scale of
the weighting factors.
To verify our hypothesis, we try three popular inversion tasks for visualization using the CNN
architecture with random weights. Our results strongly suggest that this is true. Applying inversion
techniques on the untrained VGG with random weights, we reconstruct high perceptual quality
images. The results are qualitatively better than the reconstructed images produced on the pretrained
VGG with the same architecture. Then, we try to synthesize natural textures using the random weight
VGG. With automatic normalization to scale the squared correlation loss for different activation
layers, we succeed in generating similar textures as the prior work of Gatys et al. [8] on well-trained
VGG. Furthermore, we continue the experiments on style transfer, combining the content of an image
and the style of an artwork, and create artistic imagery using random weight CNN.

2

To our knowledge this is the first demonstration of image representations using untrained deep neural
networks. Our work provides a new and fascinating tool to study the perception and representation of
deep network architecture, and shed light on new understandings on deep visualization. Our work
will inspire more possibilities of using the generative power of CNNs with random weights, which
do not need long training time on multi-GPUs. Furthermore, it is very hard to prove why trained
deep neural networks work so well. Based on the networks with random weights, we might be able
to prove some properties of the deep networks. Our work using random weights shows a possible
way to start developing a theory of deep learning since with well-trained weights, theorems might be
impossible.

2

Methods

In order to better understand the deep representation in the CNN architecture, we focus on three
tasks: inverting the image representation, synthesizing texture, and creating artistic style images.
Our methods are similar in spirit to existing methods [7, 8, 13]. The main difference is that we
use untrained weights instead of trained weights, and we apply weighting factors determined by a
pre-process to normalize the different impact scales of different activation layers on the input layer.
Compared with purely random weight CNN, we select a random weight CNN among a set of random
weight CNNs to get slightly better results.
For the reference network, we choose VGG-19 [3], a convolutional neural network trained on the
1.3 million-image ILSVRC 2012 ImageNet dataset [1] using the Caffe-framework [22]. The VGG
architecture has 16 convolutional and 5 pooling layers, followed by 3 fully connected layers. Gatys
et al. re-train the VGG-19 network using average pooling instead of maximum pooling, which they
suggest could improve the gradient flow and obtain slightly better results [8]. They only consider
the convolutional and pooling layers for texture synthesis, and they rescale the weights such that the
mean activation of each filter over the images and positions is 1. Their trained network is denoted as
VGG in the following discussion. We adopt the same architecture, replacing the weights with purely
random values from a Gaussian distribution N (0, ?). The standard deviation, ?, is set to a small
number like 0.015 in the experiments. The VGG-based random weight network created as described
in the following subsection is used as our reference network, denoted as ranVGG in the following
discussion.
Inverting deep representations. Given a representation function F l : RH?W ?C ? RNl ?Ml for
the lth layer of a deep network and F l (x0 ) for an input image x0 , we want to reconstruct an image x
that minimizes the L2 loss among the representations of x0 and x.
?l
x? = argmin Lcontent (x, x0 , l) = argmin
kF l (x) ? F l (x0 )k22
(1)
x?RH?W ?C
x?RH?W ?C 2Nl Ml
Here H and W denote the size of the image, C = 3 the color channels, and ?l the weighting factor.
We regard the feature map matrix F l as the representation function of the lth layer which has Nl ? Ml
l
dimensions where Nl is the number of distinct feature maps, each of size Ml when vectorised. Fik
th
denotes the activation of the i filter at position k.
The representations are a chain of non-linear filter banks even if untrained random weights are applied
to the network. We initialize the pre_image with white noise, and apply the L_BFGS gradient descent
using standard error backpropagation to morph the input pre_image to the target.


?L(x, x0 , l) ?F l 
xt+1 = xt ?
(2)
?F l
?x xt

?L(x, x0 , l) 
?l
= l l (F l (xt ) ? F l (x0 ))i,k
(3)

l
N M
?Fi,k
xt
The weighting factor ?l is applied to normalize the gradient impact on the morphing image x. We use
a pre-processing procedure to determine the value of ?l . For the current layer l, we approximately
calculate the maximum possible gradient by Equation (4), and back propagate the gradient to the
input layer. Then we regard the reciprocal of the absolute mean gradient over all pixels and RGB
channels as the value of ?l such that the gradient impact of different layers is approximately of the
same scale. This normalization doesn?t affect the reconstruction from the activations of a single layer,
3

but is added for the combination of content and style for the style transfer task.
W H C

1
1 XXX ?L(x0 , x0 , l) 
=
?l
W HC 
?xi,j,k  l 0
i=1 j=1 k=1

(4)

F (x )=0

To stabilize the reconstruction quality, we apply a greedy approach to build a ?stacked"" random
weight network ranVGG based on the VGG-19 architecture. Select one single image as the reference
image and starting from the first convolutional layer, we build the stacked random weight VGG by
sampling, selecting and fixing the weights of each layer in forward order. For the current layer l,
fix the weights of the previous l ? 1 layers and sample several sets of random weights connecting
the lth layer. Then reconstruct the target image using the rectified representation of layer l, and
choose weights yielding the smallest loss. Experiments in the next section show our success on the
reconstruction by using the untrained, random weight CNN, ranVGG.
Texture synthesis. Can we synthesize natural textures based on the feature space of an untrained
deep network? To address this issue, we refer to the method proposed by Gatys et al.[8] and use the
correlations between feature responses on each layer as the textureP
representation. The inner product
l
l
between pairwise feature maps i and j within each layer l, Glij = k Fik
Fjk
, defines a gram matrix
l
l
l T
G = F (F ) . We seek a texture image x that minimizes the L2 loss among the correlations of the
representations of several candidate layers for x and a groundtruth image x0 .
X
x? = argmin Ltexture = argmin
?l E(x, x0 , l),
(5)
x?RH?W ?C

x?RH?W ?C l?L

where the contribution of layer l to the total loss is defined as
1
E(x, x0 , l) =
kGl (F l (x)) ? Gl (F l (x0 ))k22 .
4Nl2 Ml2
The derivative of E(x, x0 , l) with respect to the activations F l in layer l is [8]:
1
?E(x, x0 , l)
= 2 2 {(F l (x))T [Gl (F l (x)) ? Gl (F l (x0 ))]}i,k
l
Nl M l
?Fi,k

(6)

(7)

The weighting factor ?l is defined similarly to ?l , but here we use the loss contribution E(x, x0 , l) of
layer l to get its gradient impact on the input layer.
W H C

1
1 XXX ?E(x0 , x0 , l) 
(8)
=
?l
W HC 
?xi,j,k  l 0
i

j

k

F (x )=0

We then perform the L_BFGS gradient descent using standard error backpropagation to morph the
input image to a synthesized texture image using the untrained ranVGG.
Style transfer. Can we use the untrained deep network to create artistic images? Referring to the
prior work of Gatys et al.[13] from the feature responses of VGG trained on ImageNet, we use an
untrained VGG and succeed in separating and recombining content and style of arbitrary images.
The objective requires terms for content and style respectively with suitable combination factors. For
content we use the method of reconstruction on medium layer representations, and for style we use
the method of synthesising texture on some lower through higher layer representation correlations.
Let xc be the content image and xs the style image. We combine the content of the former and the
style of the latter by optimizing the following objective:
x? = argmin ?Lcontent (x, xc ) + ?Ltexture (x, xs ) + ?R(x)
(9)
x?RH?W ?C

Here ? and ? are the contributing factors for content and style respectively. We apply a regularizer
R(x), total variation(TV) [7] defined as the squared sum on the adjacent pixel?s difference of x, to
encourage the spatial smoothness in the output image.

3

Experiments

This section evaluates the results obtained by our model using the untrained network ranVGG 3 .
3

https://github.com/mileyan/random_weights

4

The input image is required to be of size 256 ? 256 if we want to invert the representation of the fully
connected layers. Else, the input could be of arbitrary size.
Inverting deep representations. We select several source images from the ILSVRC 2012 challenge [1] validation data as examples for the inversion task, and choose a monkey image as the
reference image to build the stacked ranVGG (Note that using other image as the reference image
also returns similar results). As compared with the inverting technique of Mahendran et al. [7], we
only consider the Euclidean loss over the activations and ignore the regularizer they used to capture
the natural image prior. ranVGG contains 19 layers of random weights (16 convolutional layers and 3
fully connected layers), plus 5 pooling layers. Mahendran et al. use a reference network AlexNet [2]
which contains 8 layers of trained weights (5 convolutional layers and 3 fully connected layers), plus
3 pooling layers.
Figure 1 shows that we reach higher perceptive reconstructions. The reason may lie in the fact
that the VGG architecture uses filters with a small receptive field of 3 ? 3 and we adopt average
pooling. Though shallower than VGG, their reference network, AlexNet, adopts larger filters and
uses maximum pooling, which makes it harder to get images well inverted and easily leads to spikes.
That?s why they used regularizers to polish the reconstructed image. Figure 2 shows more examples
(house, flamingo, girl).
Figure 3 shows the variations on an example image, the girl. As compared with the VGG with purely
random weights, ranVGG (the VGG with stacked random weights) exhibits lower variations and
lower reconstruction distances. As compared with the trained VGG, both stacked ranVGG and VGG
with purely random weights exhibit lower reconstruction distance with lower variations. ranVGG
demonstrates a more stable and high performance for the inversion task and is slightly better than an
purely random VGG. So we will use ranVGG for the following experiments.
To compare the convergence of ranVGG and VGG, Figure 4 shows the loss (average Euclidean
distance) along the gradient descent iterations on an example image, the house. The reconstruction
converges much quicker on ranVGG and yields higher perceptual quality results. Note that the
reconstruction on VGG remains the same even if we double the iteration limits to 4000 iterations.
Texture synthesis. Figure 5 shows the textures synthesized by our model on ranVGG for several
natural texture images (fifth row) selected from a texture website4 and an artwork named Starry
Night by Vincent van Gohn 1989. Each row of images was generated using an increasing number
of convolutional layers to constrain the gradient descent. conv1_1 for the first row, conv1_1 and
conv2_1 for the second row, etc (the labels at each row indicate the top-most layer included). The
joint matching of conv1_1, conv2_1, and con3_1 (third row) already exhibits high quality texture
representations. Adding one more layer of conv4_1 (fourth row) could slightly improve the natural
textures. By comparison, results of Gatys et al.[8] on the trained VGG using four convolutional layers
up to conv4_1 are as shown at the bottom row.
Our experiments show that with suitable weighted factors, calculated automatically by our method,
ranVGG could synthesize complex natural textures that are almost indistinguishable with the original
texture and the synthesized texture on the trained VGG. Trained VGG generates slightly better
textures on neatly arranged original textures (cargo at the second column of Figure 5).
Style transfer. We select conv2_2 as the content layer, and use the combination of conv1_1,
conv2_1, ..., conv5_1 as the style. We set the ratio of ? : ? : ? = 100 : 1 : 1000 in the experiments.
We first compare our style transfer results with the prior work of Gatys et al.[13] on several wellknown artworks for the style: Starry Night by Vincent van Gohn 1989, Der Schrei by Edward Munch
1893, Picasso by Pablo Picasso 1907, Woman with a Hat by Henri Matisse 1905, Meadow with
Poplars by Claude Monet 1875. As shown in Figure 6, the second row, by recasting the content of a
university image in the style of the five artworks, we obtain different artistic images based on the
untrained ranVGG (second row). Our results are comparable to their work [13] on the pretrained
VGG (third row), and are in the same order of magnitude. They have slightly smoother lines and
textures which may attributed to the training. We further try the content and style combination on
some Chinese paintings and scenery photographs, as shown in Figure 7, and create high perceptual
artistic Chinese paintings that well combine the style of the painting and the content of the sceneries.
4

http://www.textures.com/

5

pool2

pool3/conv3

pool4/conv4

pool5

[7] on AlexNet

Ours on VGG

Ours on ranVGG

pool1

Figure 1: Reconstructions from layers of ranVGG (top) and the pretrained VGG (middle) and
[7] (bottom). As AlexNet only contains 3 pooling layers, we compare their results on conv3 and
conv4 with ours on pool3 and pool4. Our method on ranVGG demonstrates a higher perceptive
quality, especially on the higher layers. Note that VGG is much deeper than AlexNet even when we
compare on the same pooling layer.

VGG

VGG

ranVGG

ranVGG

VGG

pool5

pool3

pool1

ranVGG

Figure 2: Reconstructions from different pooling layers of the untrained ranVGG and the
pretrained VGG. ranVGG demonstrates a higher perceptive quality, especially on the higher layers.
The pretrained VGG could rarely reconstruct even the contours from representations of the fifth
pooling layer.

Figure 3: Variations in samples on the girl image, with maximum, minimum, mean and quartiles.

6

Figure 4: Reconstruction qualities of conv5_1 during the gradient descent iterations.

Cargo

Floors

Flowers

Leaves

Nigh Starry

trained conv4_1

original

conv4_1

conv3_1

conv2_1

conv1_1

Camouflage

Figure 5: Generated textures using random weights. Each row corresponds to a different processing stage in ranVGG. Considering only the lowest layer, conv1_1, the synthesised textures are
of lowest granularity, showing very local structure. Increasing the number of layers on which we
match the texture representation (conv1_1 plus conv2_1 for the second row, etc), we have higher
organizations of the previous local structure. The third row and the fourth row show high-quality
synthesized textures of the original images. The lowest row corresponds to the result of using the
trained VGG to match the texture representation from conv1_1, conv2_1 conv3_1 and conv4_1.
Der Schrei

Photograph

Picasso Woman with a Hat

Meadow with Poplars

[13] on VGG

Ours on ranVGG

Original

Starry Night

Figure 6: Artistic style images of ours on the untrained ranVGG (medium row) and of Gatys
et al.[8] on the pretrained VGG (bottom row). We select a university image (first row, center) and
several well-known artworks for the style (first row, others images). The third column under the
photograph are for the Picasso. We obtain similar quality results as compared with Gatys et al.[13].
7

Chinese painting

Photograph

Created image

Figure 7: Style transfer of Chinese paintings on the untrained ranVGG. We select several
Chinese paintings for the style (first column), including The Great Wall by Songyan Qian 1975,
a painting of anonymous author and Beautiful landscape by Ping Yang. We select the mountain
photographs (second column) as the content images. The created images performed on the untrained
ranVGG are shown in the third column, which seem to have learned how to paint the rocks and clouds
from paintings of the first column and transfer the style to the content to ?draw? Chinese landscape
paintings.

4

Discussion

Our work offers a testable hypothesis about the representation of image appearance based only on
the network structure. The success on the untrained, random weight networks on deep visualization
raises several fundamental questions in the area of deep learning. Researchers have developed many
visualization techniques to understand the representation of well trained deep networks. However, if
we could do the same or similar visualization using an untrained network, then the understanding
is not for the training but for the network architecture. What is the difference of a trained network
and a random weight network with the same architecture, and how could we explore the difference?
What else could one do using the generative power of untrained, random weight networks? Explore
other visualization tasks in computer vision developed on the well-trained network, such as image
morphing [23], would be a promising aspect.
Training deep neural networks not only requires a long time but also significant high performance
computing resources. The VGG network, which contains 11-19 weight layers depending on the
typical architecture [3], takes 2 to 3 weeks on a system equipped with 4 NVIDIA Titan Black GPUs
for training a single net. The residual network ResNet, which achieved state-of-the-art results in
image classification and detection in 2015 [4], takes 3.5 days for the 18-layer model and 14 days for
the 101-layer model using 4 NVIDIA Kepler GPU.5 Could we evaluate a network structure without
taking a long time to train it? There are some prior works to deal with this issue but they deal with
much shallow networks [21]. In future work, we will address this issue by utilizing the untrained
network to attempt to compare networks quickly without having to train them.

Acknowledgments
This research work was supported by US Army Research Office(W911NF-14-1-0477) and
National Science Foundation of China(61472147) and National Science Foundation of Hubei
Province(2015CFB566).
5

http://torch.ch/blog/2016/02/04/resnets.html

8

References
[1] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer Vision, 115(3):211?252, 2015.
[2] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional
neural networks. In NIPS, pages 1097?1105, 2012.
[3] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In
ICLR, 2015.
[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In CVPR, 2016.
[5] Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer features of
a deep network. University de Montreal Technical Report 4323, 2009.
[6] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising
image classification models and saliency maps. In ICLR, 2014.
[7] Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them.
In CVPR, pages 5188?5196, 2015.
[8] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Texture synthesis using convolutional neural
networks. In NIPS, pages 262?270, May 2015.
[9] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural networks
through deep visualization. In Deep Learning Workshop at ICML, 2015.
[10] Alexey Dosovitskiy and Thomas Brox. Inverting visual representations with convolutional networks. In
CVPR, pages 4829?4837, 2016.
[11] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence
predictions for unrecognizable images. In CVPR, 2015.
[12] L. A. Gatys, A. S. Ecker, and M. Bethge. Texture synthesis and the controlled generation of natural stimuli
using convolutional neural networks. arXiv:1505.07376, 2015.
[13] Leon A Gatys, Alexander S Ecker, and Matthias Bethge.
arXiv:1508.06576, 2015.

A neural algorithm of artistic style.

[14] Yaroslav Nikulin and Roman Novak. Exploring the neural algorithm of artistic style. arXiv:1602.07188,
2016.
[15] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and
super-resolution. In ECCV, 2016.
[16] Anh Mai Nguyen, Jason Yosinski, and Jeff Clune. Multifaceted feature visualization: Uncovering the
different types of features learned by each neuron in deep neural networks. arXiv:1602.03616, 2016.
[17] Donglai Wei, Bolei Zhou, Antonio Torralba, and William T. Freeman. Understanding intra-class knowledge
inside CNN. arXiv:1507.02379, 2015.
[18] Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on
deep networks. In NIPS, 2016.
[19] Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Victor Lempitsky. Texture networks: Feed-forward
synthesis of textures and stylized images. In ICML, 2016.
[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pages 2672?2680, 2014.
[21] Andrew Saxe, Pang W Koh, Zhenghao Chen, Maneesh Bhand, Bipin Suresh, and Andrew Y Ng. On
random weights and unsupervised feature learning. In ICML, pages 1089?1096, 2011.
[22] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio
Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In
Proceedings of the ACM International Conference on Multimedia, ACM, pages 675?678, 2014.
[23] Jacob R. Gardner, Paul Upchurch, Matt J. Kusner, Yixuan Li, Kilian Q. Weinberger, and John E. Hopcroft.
Deep manifold traversal: Changing labels with convolutional features. arXiv:1511.06421, 2015.

9

"
3710,"REMARKS ON INTERPOLATION AND
RECOGNITION USING NEURAL NETS

Eduardo D. Sontag?
SYCON - Center for Systems and Control
Rutgers University
New Brunswick, NJ 08903

Abstract
We consider different types of single-hidden-Iayer feedforward nets: with
or without direct input to output connections, and using either threshold or sigmoidal activation functions. The main results show that direct
connections in threshold nets double the recognition but not the interpolation power, while using sigmoids rather than thresholds allows (at least)
doubling both. Various results are also given on VC dimension and other
measures of recognition capabilities.

1

INTRODUCTION

In this work we continue to develop the theme of comparing threshold and sigmoidal
feedforward nets. In (Sontag and Sussmann, 1989) we showed that the ""generalized delta rule"" (backpropagation) can give rise to pathological behavior -namely,
the existence of spurious local minima even when no hidden neurons are used,in contrast to the situation that holds for threshold nets. On the other hand, in
(Sontag and Sussmann, 1989) we remarked that provided that the right variant be
'Used, separable sets do give rise to globally convergent back propagation, in complete analogy to the classical perceptron learning theorem. These results and those
obtained by other authors probably settle most general questions about the case of
no hidden units, so the next step is to look at the case of single hidden layers. In
(Sontag, 1989) we announced the fact that sigmoidal activations (at least) double
recognition power. Here we provide details, and we make several further remarks
on this as well as on the topic of interpolation.
Nets with one hidden layer are known to be in principle sufficient for arbitrary
recognition tasks. This follows from the approximation theorems proved by various
.""E-mail: sontag@hilbert.rutgers.edu

940

Sontag
authors: (Funahashi,1988), (Cybenko,1989), and (Hornik et. al., 1989). However,
what is far less clear is how many neurons are needed for achieving a given recognition, interpolation, or approximation objective. This is of importance both in its
practical aspects (having rough estimates of how many neurons will be needed is essential when applying back propagation) and in evaluating generalization properties
(larger nets tend to lead to poorer generalization). It is known and easy to prove
(see for instance (Arai, 1989), (Chester, 1990)) that one can basically interpolate
values at any n + 1 points using an n-neuron net, and in particular that any n + 1point set can be dichotomized by such nets. Among other facts, we point out here
that allowing direct input to output connections permits doubling the recognition
power to 2n, and the same result is achieved if sigmoidal neurons are used but such
direct connections are not allowed. Further, we remark that approximate interpolation of 2n - 1 points is also possible, provided that sigmoidal units be employed
(but direct connections in threshold nets do not suffice).
The dimension of the input space (that is, the number of ""input units"") can influence the number of neurons needed, are least for dichotomy problems for suitably
chosen sets. In particular, Baum had shown some time back (Baum, 1988) that
the VC dimension of threshold nets with a fixed number of hidden units is at least
proportional to this dimension. We give lower bounds, in dimension two, at least
doubling the VC dimension if sigmoids or direct connections are allowed.
Lack of space precludes the inclusion of proofs; references to technical reports are
given as appropriate. A full-length version of this paper is also available from the
author.

2

DICHOTOMIES

The first few definitions are standard. Let N be a positive integer. A dichotomy
or two-coloring (S_, S+) on a set S ~ JRN is a partition S = S_ U S+ of S into two
disjoint subsets. A function I : JRN -. JR will be said to implement this dichotomy
if it holds that

I(u) > 0 for u E S+ and I(u) < 0 for u E S_ .
Let F be a class of functions from JRN to JR, assumed to be nontrivial, in the sense
that for each point u E lRN there is some 11 E F so that II (u) > 0 and some h E F
so that h (u) < O. This class shatters the set S ~ RN if each dichotomy on S can
be implemented by some I E F.
Here we consider, for any class of functions F as above, the following measures of
classification power. First we introduce J1. and J1., dealing with ""best"" and ""worst""
cases respectively: J1.(F) denotes the largest integer I 2: 1 (possibly 00) so that there
is at least some set S of cardinality I in JRN which can be shattered by F, while
J1.(F) is the largest integer I 2: 1 (possibly 00) so that every set of cardinality I can
be shattered by F. Note that by definition, J1.(F) ~ /1(F) for every class F.
In particular, the definitions imply that no set of cardinality J1.(F) + 1 can be
shattered, and that there is at least some set of cardinality J1.(F) + 1 which cannot be
shattered. The integer J1. is usually called the Vapnik-Chervonenkis (VC) dimension
of the class F (see for instance (Baum,1988)), and appears in formalizations of
learning in the distribution-free sense.

Remarks on Interpolation and Recognition Using Neural Nets
A set may fail to be shattered by :F because it is very special (see the example
below with colinear points). In that sense, a more robust measure is useful: p,(:F)
is the largest integer I 2: 1 (possibly 00) for which the class of sets S that can be
shattered by :F is dense, in the sense that given every I-element set S {st, ... , S,}
there are points Si arbitrarily close to the respective Si'S such that S {st, ... , s,}
can be shattered by :F. Note that

=
=

p,(:F) ::; p,(:F) ::; 1l(:F)

(1)

for all :F.
To obtain an upper bound m for p,(:F) one needs to exhibit an open class of sets of
cardinality m + 1 none of which can be shattered.
Take as an example the class :F consisting of all affine functions f( x) = ax + by + c
on lR 2 ? Since any three poin ts can be shattered by an affine map provided that they
are not colinear (just choose a line ax + by + c 0 that separates any poin t which
is colored different from the rest), it follows that 3 ::; p,. On the other hand, no set
of four points can ever be dichotomized, which implies that II ::; 3 and therefore the
conclusion p, = II = 3 for this class. (The negative statement can be verified by a
case by case analysis: if the four points form the vertices of a 4-gon color them in
""XOR"" fashion, alternate vertices of the same color; if 3 form a triangle and the
remaining one is inside, color the extreme points differently from the remaining one;
if all colinear then use an alternating coloring). Finally, since there is some set of 3
points which cannot be dichotomized (any set of three colinear points is like this),
but every set of two can, !!:.. 2 .
We shall say that :F is robust if whenever S can be shattered by :F also every small
enough perturbation of S can be shattered. For a robust class and I = p,(:F), every
set in an open dense subset in the above topology, i.e. almost every set of 1 elements,
can be shattered.

=

=

3

NETS

We define a ""neural net"" as a function of a certain type , corresponding to the idea
of feedforward interconnections, via additive links, of neurons each of which has a
scalar response or activation function ().
Definition 3.1 Let () : lR ~ lR be any function. A function f : lRN ~ lR is
a single-hidden-Iayer neural net with k hidden neurons of type () and N inputs,
or just a (k , ()-net, if there are real numbers Wo, Wl, ??? , Wk, 'Tl, ??? , 'Tk and vectors
vo, Vi, ? ? ? , Vk E lRN such that, for all u E lRN ,
k

f(u)

= Wo + Vo?U + L Wi()(Vi. U -

'Ti)

(2)

i=l

where the dot indicates inner product. A net with no direct i/o connections is one
for which Vo = O.
For fixed (), and under mild assumptions on (), such neural nets can be used to
approximate uniformly arbitrary continuous functions on compacts. In particular,
they can be used to implement arbitrary dichotomies.

941

942

Sontag
In neural net practice, one often takes 9 to be the standard sigmoid u( x) = 1+!-"" or
equivalently, up to translations and change of coordinates, the hyperbolic tangent
tanh(x). Another usual choice is the hardlimiter, threshold, or Heaviside function

1t(x)

={ ~

if x ~ 0
if x> 0

which can be approximated well by u(""'(x) when the ""gain"" ""'( is large. Yet another
possibility is the use of the piecewise linear function

7r(x)

=

{

-1

!

if x ~ -1
if x> 1
otherwise.

Most analysis has been done for 1t and no direct connections, but numerical techniques typically use the standard sigmoid (or equivalently tanh). The activation
1("" will be useful as an example for which sharper bounds can be obtained.
The
examples u and 1("", but not 1t, are particular cases of the following more general
type of activation function:

Definition 3.2 A function 9 : m.
erties hold:

--+

m. will be called

a sigmoid if these two prop-

(51) t+ := limx--++oo 9(x) and L := liffix--+-oo 9(x) exist, and t+

=1=

t_.

(52) There is some point c such that 9 is differentiable at c and 9'(c)

= JL =1= o.

0

All the examples above lead to robust classes, in the sense defined earlier. More
precisely, assume that 9 is continuous except for at most finitely many points x,
and it is left continuous at such x, and let :F be the class of (k,9)-nets, for any
fixed k. Then:F is robust, and the same statement holds for nets with no direct
connections.

4

CLASSIFICATION RESULTS

We let JL(k, 9, N) denote JL(:F), where :F is the class of (k, 9)-nets in m.N with no
direct connections, and similarly for JL and JL, and a superscript d is used for the
class of arbitrary such nets (with possible direct connections from input to output).
The lower measure JL is independent of dimension:

Lemma 4.1 For each k, 9, N, !!:.(k, 9, N)

= JL(k, 9,1) and !!:.d(k, 9, N) = JLd(k, 9,1).

This justifies denoting these quantities just as JL( k, 9) and JLd( k, 9) respectively, as
we do from now on, and giving proofs only for N = 1.

Lemma 4.2 For any sigmoid 9, and for each k, N,

JL(k + 1,9, N)

> JLd(k, 1t, N)

and similarly for JL and JL.
The main results on classification will be as follows.

Remarks on Interpolation and Recognition Using Neural Nets
Theorem 1 For any sigmoid 9, and for each k,

J-L(k,1t)

-

J-Ld(k,1t)
J-L(k,9)

k +1
2k

+2

> 2k.

Theorem 2 For each k,

4l~J ~ J-L(k, 1t, 2)

<

2k + 1

J-Ld(k, 1t, 2)

<

4k

+3 .

Theorem 3 For any sigmoid 9, and for each k,

+1 <
4k + 3 <
4k - 1 <
2k

J-L(k, 1t, 2)
~(k, 1t, 2)
J-L(k, 9, 2) .

These results are proved in (Sontag, 1990a). The first inequality in Theorem 2
follows from the results in (Baum, 1988), who in fact established a lower bound
of 2N l ~ J for J-L(k, 1t, N) (and hence for J-L too), for every N, not just N = 2 as
in the ~eorem above. We conjecture, but have as yet been unable to prove, that
direct connections or sigmoids should also improve these bounds by at least a factor
of 2, just as in the two-dimensional case and in the worst-case analysis. Because
of Lemma 4.2, the last statements in Theorems 1 and 3 are consequences of the
previous two.

5

SOME PARTICULAR ACTIVATION FUNCTIONS

Consider the last inequality in Theorem 1. For arbitrary sigmoids, this is far too
conservative, as the number J-L can be improved considerably from 2k, even made
infinite (see below). We conjecture that for the important practical case 9(x) = O'(x)
it is close to optimal, but the only upper bounds that we have are still too high.
For the piecewise linear function 11"", at least, one has equality:
Lemma 5.1

~(k, 11"")

= 2k.

It is worth remarking that there are sigmoids 9, as differentiable as wanted, even
real-analytic, where all classification measures are infinite. Of course, the function
9 is so complicated that there is no reasonably ""finite"" implementation for it. This
remark is only of theoretical interest, to indicate that, unless further restrictions
are made on (S1)-(S2), much better bounds can be obtained. (If only J-L and J-L
are desired to be infinite, one may also take the simpler example 9( x)
sin( x).
Note that for any I rationally independent real numbers Xi, the vectors of the form
(sin(-YIXI), ... , sin(-y,xr), with the 'Yi'S real, form a dense subset of [-1,1]', so all
dichotomies on {Xl,"""" xd can be implemented with (1, sin)-nets.)

=

Lemma 5.2 There is some sigmoid 9, which can be taken to be an analytic function , so that J-L(1, 9) = 00.

943

944

Sontag

6

INTERPOLATION

We now consider the following approximate interpolation problem. Assume given
a sequence of k (distinct) points Xl, ??? , Xk in RN, any ? > 0, and any sequence of
real numbers YI,"""" Yk, as well as some class :F of functions from JRN to JR. We
ask if there exists some
(3)
I E :F so that I/(xd - yd < ? for each i.
Let ~(:F) be the largest integer k ~ 1, possibly infinite, so that for every set of data
as above (3) can be solved. Note that, obviously, ~(:F) ~ p,(:F). Just as in Lemma
4.1, .1. is independent of the dimension N when applied to nets. Thus we let .1.d(k, B)
and .1.(k, B) be respectively the values of .1.(:F) when applied to (k, B)-nets with or
without direct connections.
We now summarize properties of.1.. The next result -see (Sontag,1991), as well
as the full version of this paper, for a proof- should be compared with Theorem
1. The main difference is in the second equality. Note that one can prove .1.( k, B) ~
~d(k - 1,1l), in complete analogy with the case of p"" but this is not sufficient
anymore to be able to derive the last inequality in the Theorem from the second
equality.
Theorem 4 For any continuous sigmoid B, and lor each k,

.1.(k,1l)
,Ad(k,1l)
.1.(k, B)

k+1
k+2
> 2k - 1 .

Remark 6.1 Thus we can approximately interpolate any 2k - 1 points using k
sigmoidal neurons. It is not hard to prove as a corollary that, for the standard
sigmoid, this approximate interpolation property holds in the following stronger
sense: for an open dense set of 2k - 1 points, one can achieve an open dense set
of values; the proof involves looking first at points with rational coordinates, and
using that on such points one is dealing basically with rational functions (after a
diffeomorphism), plus some theory of semialgebraic sets. We conjecture that one
2 this is easy to
should be able to interpolate at 2k points. Note that for k
achieve: just choose the slope d so that some Zi - Zi+l becomes zero and the Zi are
allowed to be nonincreasing or nondecreasing. The same proof, changing the signs
if necessary, gives the wanted net. For some examples, it is quite easy to get 2k
points. For instance, .1.(k,1r) = 2k for the piecewise linear sigmoid 1r.
0

=

7

FURTHER REMARKS

The main conclusion from Theorem 1 is that sigmoids at least double recognition
power for arbitrary sets. It may be the case that p,(k, (7, N)j p,(k, 1l, N) : : : : 2 for all
N; this is true for N = 1 and is strongly suggested by Theorem 3 (the first bound
appears to be quite tight). Unfortunately the proof of this theorem is based on a
result from (Asano et. al., 1990) regarding arrangements of points in the plane, a
fact which does not generalize to dimension three or higher.
One may also compare the power of nets with and without connections, or threshold
vs sigmoidal processors, on Boolean problems. For instance, it is a trivial consequence from the given results that parity on n bits can be computed with

rni1l

Remarks on Interpolation and Recognition Using Neural Nets
hidden sigmoidal units and no direct connections, though requiring (apparently,
though this is an open problem) n thresholds. In addition, for some families of
Boolean functions, the gap between sigmoidal nets and threshols nets may be infinitely large (Sontag, 1990a). See (Sontag, 1990b) for representation properties of
two-hidden-Iayer nets
Acknow ledgements

This work was supported in part by Siemens Corporate Research, and in part by
the CAIP Center, Rutgers University.
References

Arai, M., ""Mapping abilities of three-layer neural networks,"" Proc. IJCNN Int. Joint
Conf.on Neural Networks, Washington, June 18-22, 1989, IEEE Publications, 1989,
pp. 1-419/424.
Asano,T., J. Hershberger, J. Pach, E.D. Sontag, D. Souivaine, and S. Suri, ""Separating Bi-Chromatic Points by Parallel Lines,"" Proceedings of the Second Canadian
Conference on Computational Geometry, Ottawa, Canada, 1990, p. 46-49.
Baum, E.B., ""On the capabilities of multilayer perceptrons,"" J.Complexity 4(1988):
193-215.
Chester, D., ""Why two hidden layers and better than one,"" Proc. Int. Joint Conf.
on Neural Networks, Washington, DC, Jan. 1990, IEEE Publications, 1990, p. 1.265268.
Cybenko, G., ""Approximation by superpositions of a sigmoidal function,"" Math.
Control, Signals, and Systems 2(1989): 303-314.
Funahashi, K., ""On the approximate realization of continuous mappings by neural
networks,"" Proc. Int. Joint Conf. on Neural Networks, IEEE Publications, 1988, p.
1.641-648.
Hornik, K.M., M. Stinchcombe, and H. White, ""Multilayer feedforward networks
are universal approximators,"" Neural Networks 2(1989): 359-366.
Sontag, E.D., ""Sigmoids distinguish better than Heavisides,"" Neural Computation
1(1989): 470-472.
Sontag, E.D., ""On the recognition capabilities of feedforward nets,"" Report
SYCON-90-03, Rutgers Center for Systems and Control, April 1990.
Sontag, E.D., ""Feedback Stabilization Using Two-Hidden-Layer Nets,"" Report
SYCON-90-11, Rutgers Center for Systems and Control, October 1990.
Sontag, E.D., ""Capabilities and training of feedforward nets,"" in Theory and Applications of Neural Networks (R. Mammone and J. Zeevi, eds.), Academic Press,
NY, 1991, to appear.
Sontag, E.D., and H.J. Sussmann, ""Back propagation can give rise to spurious local
minima even for networks without hidden layers,"" Complex Systems 3(1989): 91106.
Sontag, E.D., and H.J. Sussmann, ""Backpropagation separates where perceptrons
do,"" Neural Networks(1991), to appear.

945

"
544,"Making Templates Rotationally Invariant:
An Application to Rotated Digit Recognition
Shurneet Baluja
baluja@cs.cmu.edu
Justsystem Pittsburgh Research Center &
School of Computer Science, Carnegie Mellon University

Abstract
This paper describes a simple and efficient method to make template-based
object classification invariant to in-plane rotations. The task is divided into two
parts: orientation discrimination and classification. The key idea is to perform
the orientation discrimination before the classification. This can be accomplished by hypothesizing, in turn, that the input image belongs to each class of
interest. The image can then be rotated to maximize its similarity to the training images in each class (these contain the prototype object in an upright orientation). This process yields a set of images, at least one of which will have the
object in an upright position. The resulting images can then be classified by
models which have been trained with only upright examples. This approach
has been successfully applied to two real-world vision-based tasks: rotated
handwritten digit recognition and rotated face detection in cluttered scenes.

1 Introduction
Rotated text is commonly used in a variety of situations, ranging from advertisements,
logos, official post-office stamps, and headlines in magazines, to name a few. For examples, see Figure 1. We would like to be able to recognize these digits or characters, regardless of their rotation.

Figure 1: Common examples of images which contain text that is not axis aligned include logos, post-office
stamps, magazine headlines and consumer advertisements.

848

S. Baluja

The focus of this paper is on the recognition of rotated digits. The simplest method for creating a system which can recognize digits rotated within the image-plane is to employ
existing systems which are designed only for upright digit recognition [Le Cun et aI.,
1990][Le Cun et a!., 1995a][Le Cun et ai., 1995b][Lee, 1991][Guyon et a!., 1989]. By
repeatedly rotating the input image by small increments and applying the recognition system at each rotation, the digit will eventually be recognized. As will be discussed in this
paper, besides being extremely computationally expensive, this approach is also errorprone. Because the classification of each digit must occur in many orientations, the likelihood of an incorrect match is high.
The procedure presented in this paper to make templates rotationally invariant is significantly faster and more accurate than the one described above. Detailed descriptions of the
procedure are given in Section 2. Section 3 demonstrates the applicability of this approach
to a real-world vision-based task, rotated handwritten digit recognition. Section 4 closes
the paper with conclusions and suggestions for future research. It also briefly describes the
second application to which this method has been successfully applied, face detection in
cluttered scenes.

2 Making Templates Rotationally Invariant
The process to make templates rotationally invariant is easiest to describe in the context of
a binary classification problem; the extension to multiple classes is discussed later in this
section. Imagine a simplified version of the digit recognition task: we want a detector for a
single digit. Suppose we wish to tell whether the input contains the digit '3' or not. The
challenge is that the '3' can be rotated within the image plane by an arbitrary amount.
Recognizing rotated objects is a two step process. In the first step, a ""De-Rotation"" network is applied to the input image. This network analyzes the input before it is given to a
""Detection"" network. If the input contains a '3', the De-Rotation network returns the
digit's angle of rotation. The window can then be rotated by the negative of that angle to
make the '3' upright. Note that the De-Rotation network does not require a '3' as input. If
a non- ' 3' image is encountered, the De-Rotation network will return an unspecified rotation. However, a rotation of a non- '3' will yield another (perhaps different) image of a
non-'3'. When the resulting image is given to the Detection network it will not detect a
'3'. On the other hand, a rotated '3', which may not have been detected by the Detection
network alone, will be rotated to an upright position by the De-Rotation network, and will
subsequently be detected as a '3' by the Detection network.
The Detection network is trained to output a positive value only if the input contains an
upright '3', and a negative value otherwise (even if it contains a rotated '3 '). It should be
noted that the methods described here do not require neural networks. As shown in [Le
Cun et al., 1995a, Le Cun et ai., 1995b] a number of other classifiers can be used.
The De-Rotation and Detection networks are used sequentially. First, the input image is
processed by the De-Rotation network which returns an angle of rotation, assuming the
image contains a '3'. A simple geometric transformation of the image is performed to
undo this rotation. If the original image contained a '3', it would now be upright. The
resulting image is then passed to the Detection network. If the original image contained a
'3', it can now be successfully detected.
This idea can easily be extended to multiple-class classification problems: a De-Rotation
network is trained for each object class to be recognized. For the digit recognition problem, 10 De-Rotation networks are trained, one for each of the digits 0.. 9. To classify the
digits once they are upright, a single classification network is used with 10 outputs
(instead of the detection networks trained on individual digits - alternative approaches
will be described later in this paper). The classification network is used in the standard
manner; the output with the maximum value is taken as the classification. To classify a
new image, the following procedure is used:

849

Making Templates Rotationally Invariant

For each digitD (0 $; D

$;

9):

1.

Pass image through De-Rotation-network-D. This returns the rotation angle.

2.

Rotate the image by (-1.0 * returned rotation angle).

3.

Pass the de-rotated image to the classification network.

4.

If the classification network's maximum output is output D, the activation of
output D is recorded. Otherwise digit D is eliminated as a candidate.

In most cases, this will eliminate all but one of the candidates. However, in some cases
more than one candidate will remain. In these cases, the digit with the maximum recorded
activation (from Step 4) is returned. In the unlikely event that no candidates remain, either
the system can reject the sample as one it cannot classify, or it can return the maximum
value which would have been recorded in Step 4 if none of the examples were rejected.
2.1 Network Specifics
To train the De-Rotation networks, images of rotated digits were input, with the rotation
angle as the target output. Examples of rotated digits are shown in Figure 2. Each image is
28x28 pixels. The upright data sets are from the MNIST database [Le Cun et at. , 1995a].

.?-

?~~GJmmm? mm.

:

::&lIi?? ?\:? ?

IJ-~~---?.-:
.
?. :~..........
?. . .?IitIIUI:....

iB ? ? ?:R llil lll :? ?

11? ? ?? ? :? ?

n
:tM."",.m:IWMI:II.
~R.J aLIla U ~.:II.r. ~ . I!II:

~B.::mll~II:l\Wa
:...~ ...... _........~u. . . . . . . ;a . . . :. . .;aa ..........

? ? ? ?l ? ?:? ?!

? ?!:? ?

ill.?.11

Figure 2: 8 examples of each of the 10 digits to be recognized. The first example in each group of eight
is shown with no rotation; it is as it appears in the MNIST data set. The second through eighth examples
show the same digit rotated in-plane by random amounts.
In the classification network, each output represents a distinct class; therefore, the standard l-of-N output representation was used with 10 outputs. To represent a continuous
variable (the angle of rotation) in the outputs of the De-Rotation network, we used a Gaussian output encoding [Pomerleau, 1992] with 90 output units. With the Gaussian encoding, instead of only training the network to activate a single output (as is done in l-of-N
encoding), outputs close to the desired output are also activated in proportion to their distance from the desired output. This representation avoids the imposed discontinuities of
the strict l-of-N encoding for images which are similar, but have only slight differences in
rotations. Further, this representation allows finer granularity with the same number of
output units than would be possible if a l-of-N encoding was used [Pomerleau, 1992].
The network architecture for both the classification and the De-Rotation networks consists
of a single hidden layer. However, unlike a standard fully-connected network, each hidden
unit was only connected to a small patch of the 28x28 input. The De-Rotation networks
used groups of hidden units in which each hidden unit was connected to only 2x2, 3x3,
4x4 & 5x5 patches of the inputs (in each of these groups, the patches were spaced 2x2 pixels apart; therefore, the last three groups had overlapping patches). This is similar to the
networks used in [Baluja, 1997][Rowley et. at, 1998a, 1998b] for face detection. Unlike
the convolution networks used by [Le Cun et aI., 1990], the weights into the hidden units
were not shared. 1 Note that many different local receptive field configurations were tried;
almost all had equivalent performance.

S. Ba/uja

850

3 Rotated Handwritten Digit Recognition
To create a complete rotationally invariant digit recognition system, the first step is to segment each digit from the background. The second is to recognize the digit which has been
segmented. Many systems have been proposed for segmenting written digits from background clutter [Jain & Yu, 1997][Sato et ai., 1998][Satoh & Kanade, 1997]. In this paper,
we concentrate on the recognition portion of the task. Given a segmented image of a
potentially rotated digit, how do we recognize the digit?
The first experiment conducted was to establish the base-line performance. We used only
the standard, upright training set to train a classification network (this training set consists
of 60,000 digits). This network was then tested on the testing set (the testing set contains
10,000 digits) . In addition to measuring the performance on the upright testing set, the
entire testing set was also rotated. As expected, performance rapidly degrades with rotation. A graph of the performance with respect to the rotation angle is shown in Figure 3.
Perfonaaneeo' Network trained wtth t Jpript Dlpts
and Tcwhd on Rotated Dlgtu

i
]

1

J

Figure 3:

Performance of the classification
network trained only with upright images when
tested on rotated images. As the angle of
rotation increases, performance degrades. Note
the spike around 180 degrees, this is because
some digits look the same even when they are
upside-down. The peak performance is
approximately 97.5% (when the digits are
upright).

It is interesting to note that around 1800 rotation, performance slightly rises. This is
because some of the digits are symmetric across the center horizontal axis - for example
the digits '8', '1', '2' & '5' can be recognized upside-down. Therefore, at these orientations, the upright detector works well for these digits.
As mentioned earlier, the simplest method to make an upright digit classifier handle rotations is to repeatedly rotate the input image and classify it at each rotation. Thefirst drawback to this approach is the severe computational expense. The second drawback is that
because the digit is examined at many rotations, it may appear similar to numerous digits
in different orientations. One approach to avoid the latter problem is to classify the digit as
the one that is voted for most often when examined over all rotations. To ensure that this
process is not biased by the size of the increments by which the image is rotated, various
angle increments are tried. As shown in the first row of Table I, this method yields low

Table I: Exhaustive Search over all possible rotations
Number of Angle IncreQ1ents Tried
Exhaustive Search Method

360
100
50
(1 degree/increment) (3.6 degree/increment) (7.2 degreeslincrement

Most frequent vote (over all rotations)

59.5%

66.0%

65 .0%

Most frequent vote - counted onl y when votes are positi ve
(over all rotations)

75.2%

74.5%

74.0%

1. Note that in the empirical comparisons presented in [Le Cun et ai., 1995aJ, convolution networks performed
extremely well in the upright digit recognition task. However, due to limited computation resources, we were
unable to train these networks, as each takes 14-20 days to train. The network used here was trained in 3 hours,
and had approximately a 2.6% misclassification rate on the upright test set. The best networks reported in [Le
Cun et ai, 1995aJ have less than 1% error. It should be noted that the De-Rotation networks trained in this study
can easily be used in conjunction with any classification procedure, including convolutional networks.

Making Templates Rotationally Invariant

851

classification accuracies. One reason for this is that a vote is counted even when the classification network predicts all outputs to be less than 0 (the network is trained to predict
+1 when a digit is recognized, and -1 when it is not). The above experiment was repeated
with the following modification: a vote was only counted when the maximum output of
the classification network was above O. The result is shown in the second row of Table I.
The classification rate improved by more than 10%.
Given these base-line performance measures2 , we now have quantitative measurements
with which to compare the effectiveness of the approach described in this paper. The performance of the procedure used here, with 10 ""De-Rotation"" networks and a single classification network, is shown in Figure 4. Note that unlike the graph shown in Figure 3, there
is very little effect on the classification performance with the rotation angle.

Figure 4:

Performance ofthe combined DeRotation network and classification network
system proposed in this paper. Note that the
performance is largely unaffected by the
rotation. The average performance, over all
rotations, is 85.0%.

0.??

~~----------------~~

0,1.
-,*10.00

-100.00

0.00

100.110

Ito.OO

~011""""'1oft

To provide some intuition of how the De-Rotation networks perform, Figure 5 shows
examples of how each De-Rotation networks transform each digit. Each De-Rotation network suggests a rotation which makes the digit look as much like the one with which the
network was trained. For example, De-Rotation-Network-5 will suggest a rotation that
will make the input digit look as much like the digit '5' as possible; for example, see DeRotation-Network-5's effect on the digit '4'.

o

2

Original Digit
345

6

7

8

9

Digit rotated by De-Rotation-Network-O
Digit rotated by De-Rotation-Network-l
Digit rotated by De-Rotation-Network-2
Digit rotated by De-Rotation-Network-3
Digit rotated by De-Rotation-Network-4
Digit rotated by De-Rotation-Network-5
Digit rotated by De-Rotation-Network-6
Digit rotated by De-Rotation-Network-7
Digit rotated by De-Rotation-Network-8
Digit rotated by De-Rotation-Network-9

Figure 5: Digits which have been rotated by the angles specified by each of the De-rotation networks. As
expected (if the method is working), the digits on the diagonal (upper left to bottom right) appear upright.
2. Another approach is to train a single network to handle both rotation and classification by using rotated digits
as inputs, and the digit's classification as the target output. Experiments with the approach yielded results far
below the techniques presented here.

852

S. Baluja

As shown in Figure 4, the average classification accuracy is approximately 85.0%. The
performance is not as good as with the upright case alone, which had a peak performance
of approximately 97.5% (Figure 3). The high level of performance achieved in the upright
case is unlikely for rotated digits: if all rotations are admissible, some characters are
ambiguous. The problem is that when working correctly, De-Rotation-Network-D will
suggest an angle of rotation that will make any input image look as much like the digit D
as possible through rotation. In most cases when the input image is not the digit D, the
rotation will not cause the image to look like D. However, in some cases, such as those
shown in Figure 6(right), the digit will be transformed enough to cause a classification
error. Some of these errors will most likely never be correctable (for example, '6' and '9'
in some instances); however, there is hope for correcting some of the others.
Figure 6 presents the complete confusion matrix. As can be seen in the examples in Figure
6(right), the digit '4' can be rotated to appear similar to a'S'. Nonetheless, there often
remain distinctive features that allow real '5's to be differentiated from the rotated '4's.
However, the classification network is unable to make these distinctions because it was
not trained with the appropriate examples. Remember, that since the classification network was only trained with the upright digit training set, rotated '4's are never encountered during training. This reflects a fundamental discrepancy in the training/testing
procedure. The distributions of images which were used to train the classification network
is different than the distributions on which the network is tested.
To address this problem, the classification mechanism is modified. Rather than using the
single 1-oj-1O neural network classifier used previously, 10 individual Detection networks
are used. Each detection network has a single binary output that signifies whether the
input contains the digit (upright) with which the network was trained. Each De-Rotation
network is paired with the respective Detection network. The crucial point is that rather
than training the Detection-Network-D with the original upright images in the training
set, each image (whether it is a positive or negative example) is first passed through DeRotation-Network-D. Although this makes training Detection-Network-D difficult since
all the digits are rotated to appear as much like upright-D's as possible by De-RotationNetwork-D, the distribution of training images matches the testing distribution more
closely. In use, when a new image is presented, it is passed through the lO network pairs.
Candidate digits are eliminated if the binary output from the detection network does not
signal a detection. Preliminary results with this new approach are extremely promising;
the classification accuracy increases dramatically - to 93% when averaged over all rotations. This is a more than a 50% reduction in error over the previously described approach.
Predicted Digit

o
a
2

94

--

--

90

1I

--

5

88

--

3

88

--

--

4

is

3

4

89

3

3

~

5

2

87

4

2

88

3

6

74

6

3
--

5

3

7

25

2

B.

!:a.:al liBII! 'JaBEI.

c?III?BEt: ""11.'111 '-? .?:?.

3

]

10

:.! ,?.? ?!.?.?111

A. ::? ?

--

.~

7

Original Image
mage Rotated to Look Like Mistake Digit
r1mage Rotated to Look Like Correct Digit

2345678

:.1 :11.11111'.??~??

D. [? ?
3

--

89

--

--

--

64

l [? ??.la.II:.!

E?l? . ? ?
F.

::.:11:111

r.~~.:.:?.l

i?? . ?

111:11'1:: ;??11?

G. [? ? ??

Figure 6: Example errors. (LEFI) Confusion Matrix (only entries account for 2% or more entries are
filled in for ease of reading). (RlGH1) some of the errors made in classification. 3 examples of each of the
errors are shown. Row A: '4' mistaken as '5'. Row B: '5' mistaken as '6', Row C: '7' mistaken as '2'. Row
D: '7' mistaken as '6'. Row E: '8' mistaken as '4', Row F: '9' mistaken as '5', Row G: '9' mistaken as '6'.

Making Templates Rotationally Invariant

853

4 Conclusions and Future Work
This paper has presented results on the difficult problem of rotated digit recognition. First,
we presented base-line results with naive approaches such as exhaustively checking all
rotations. These approaches are both slow and have large error rates. Second, we presented results with a novel two-stage approach which is both faster and more effective
than the naive approaches. Finally, we presented preliminary results with a new approach
that more closely models the training and testing distributions.
We have recently applied the techniques presented in this paper to the detection of faces in
cluttered scenes. In previous studies, we presented methods for finding all upright frontal
faces [Rowley et aT., 1998aJ. By using the techniques presented here, we were able to
detect all frontal faces, including those which were rotated within the image plane [Baluja,
1997][Rowley et al., 1998bJ. The methods presented in this paper should also be directly
applicable to full alphabet rotated character recognition.
In this paper, we examined each digit individually. A straight-forward method to eliminate
some of the ambiguities between rotationally similar digits is to use contextual information. For example, if surrounding digits are all rotated to the same amount, this provides
strong hints about the rotation of nearby digits. Further, in most real-world cases, we
might expect digits to be close to upright; therefore, one method of incorporating this
information is to penalize matches which rely on large rotation angles.
This paper presented a general way to make template-based recognition rotation invariant.
In this study, both the rotation estimation procedures and the recognition templates were
implemented with neural-networks. Nonetheless, for classification, any technique which
implements a form of templates, such as correlation templates, support vector machines,
probabilistic networks, K-Nearest Neighbor, or principal component-based methods,
could have easily been employed.

Acknowledgements
The author would like to thank Kaari Aagstad for her reviews of many successive drafts of this paper.

References
Baluja, S. (1997) ""Face Detection with In-Plane Rotation: Early Concepts and Preliminary Results,"" Justsystem
Pittsburgh Research Center Technical Report. JPRC-TR-97-001.
Guyon, I, Poujaud, I., Personnaz, L, Dreyfus, G., Denker, J. LeCun, Y. (1989) ""Comparing Different Neural Net
Architectures for Classifying Handwritten Digits"", in IlCNN II 127-132.
Jain, A. & Yu, B. (1997) ""Automatic Text Location in Images and Video Frames"", TR: MSUCPS: TR 97-33.
Le Cun, Y., Jackel, D., Bottou, L, Cortes, c., Denker, J. Drucker, J. Guyon, I, Miller, U. Sackinger, E. Simard, P.
Vapnik, V. (1995a) ""Learning Algorithms for Classification: A Comparison on Handwritten Digit Recognition"".
Neural Networks: The Statistical Mechanics Perspective, Oh, J., Kwon, C. & Cho, S. (Ed.), pp. 261-276.
LeCun, Y., Jackel, L. D., Bottou, L., Brunot, A., Cortes, C., Denker, J. S., Drucker, H., Guyon, I., Muller, U. A.,
Sackinger, E., Simard, P. and Vapnik, V. (1995b), Comparison of learning algorithms for handwritten digit recognition,"" ICANN, Fogelman, F. and Gallinari, P., 1995, pp. 53-60.
LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. and Jackel, L. D. (1990),
""Handwritten digit recognition with a back-propagation network,"" Advances in Neural Information Processing
Systems 2 (NIPS '89), Touretzky, David (Ed.), Morgan Kaufman.
Lee, Y. (1991) ""Handwritten Digit Recognition using K-NN, RBF and Backpropagation Neural Networks"", Neural Computation, 3, 3.
Pomerleau, D.A. (1993) Neural Network Perception for Mobile Robot Guidance, Kluwer Academic
Rowley, H., Baluja, S. & Kanade, T. (1998a) ""Neural Network-Based Face Detection,"" IEEE-Transactions on
Pattern Analysis and Machine Intelligence (PAMI), Vol. 20, No.1, January, 1998.
Rowley, H., Baluja, S. & Kanade, T. (1998b) ""Rotation Invariant Neural Network-Based Face Detection,"" to
appear in Proceedings of Computer Vzsion and Pattern Recognition, 1998.
Sato, T, Kanade, T., Hughes, E. & Smith, M. (1998) ""Video OCR for Digital News Archives"" to appear in IEEE
International Workshop on Content-Based Access of Image and Vzdeo Databases.
Satoh, S. & Kanade, T. (1997) ""Name-It: Association of face and name in Video"", in Proceedings of IEEE Conference on Computer Vzsion and Pattern Recognition, 1997.

"
4276,"Structured Learning via Logistic Regression

Justin Domke
NICTA and The Australian National University
justin.domke@nicta.com.au

Abstract
A successful approach to structured learning is to write the learning objective as
a joint function of linear parameters and inference messages, and iterate between
updates to each. This paper observes that if the inference problem is ?smoothed?
through the addition of entropy terms, for fixed messages, the learning objective
reduces to a traditional (non-structured) logistic regression problem with respect
to parameters. In these logistic regression problems, each training example has a
bias term determined by the current set of messages. Based on this insight, the
structured energy function can be extended from linear factors to any function
class where an ?oracle? exists to minimize a logistic loss.

1 Introduction
The structured learning problem is to find a function F (x, y) to map from inputs x to outputs as
y ? = arg maxy F (x, y). F is chosen to optimize a loss function defined on these outputs. A
major challenge is that evaluating the loss for a given function F requires solving the inference
optimization to find the highest-scoring output y for each exemplar, which is NP-hard in general.
A standard solution to this is to write the loss function using an LP-relaxation of the inference
problem, meaning an upper-bound on the true loss. The learning problem can then be phrased as a
joint optimization of parameters and inference variables, which can be solved, e.g., by alternating
message-passing updates to inference variables with gradient descent updates to parameters [16, 9].
T
Previous work has mostly focused on linear energy functions
! F (x, y) = w ?(x, y), where a vector
of weights w is adjusted in learning, and ?(x, y) =
? ?(x, y? ) decomposes over subsets of
variables y? . While linear weights are often useful in practice [23, 16, 9, 3, 17, 12, 5], it is also
common to make use of non-linear classifiers. This is typically done by training a classifier (e.g.
ensembles of trees [20, 8, 25, 13, 24, 18, 19] or multi-layer perceptrons [10, 21]) to predict each
variable independently. Linear edge interaction weights are then learned, with unary classifiers
either held fixed [20, 8, 25, 13, 24, 10] or used essentially as ?features? with linear weights readjusted [18].
!
This paper allows the more general form F (x, y) = ? f? (x, y? ). The learning problem is to select
f? from some set of functions F? . Here, following previous work [15], we add entropy smoothing
to the LP-relaxation of the inference problem. Again, this leads to phrasing the learning problem as a
joint optimization of learning and inference variables, alternating between message-passing updates
to inference variables and optimization of the functions f? . The major result is that minimization of
the loss over f? ? F? can be re-formulated as a logistic regression problem, with a ?bias? vector
added to each example reflecting the current messages incoming to factor ?. No assumptions are
needed on the sets of functions F? , beyond assuming that an algorithm exists to optimize the logistic
loss on a given dataset over all f? ? F?

We experimentally test the results of varying F? to be the set of linear functions, multi-layer perceptrons, or boosted decision trees. Results verify the benefits of training flexible function classes
in terms of joint prediction accuracy.
1

2 Structured Prediction
The structured prediction problem can be written as seeking a function h that will predict an output
y from an input x. Most commonly, it can be written in the form
h(x; w) = arg max wT ?(x, y),

(1)

y

where ? is a fixed function of both x and y. The maximum takes place over all configurations of the
discrete vector y. It is further assumed that ? decomposes into a sum of functions evaluated over
subsets of variables y? as
!
?(x, y) =
?? (x, y? ).
?

The learning problem is to adjust set of linear weights w. This paper considers the structured learning
problem in a more general setting, directly handling nonlinear function classes. We generalize the
function h to
h(x; F ) = arg max F (x, y),
y

where the energy F again decomposes as
F (x, y) =

!

f? (x, y? ).

?

The learning problem now becomes to select {f? ? F? } for some set of functions F? . This reduces
to the previous case when f? (x, y? ) = wT ?? (x, y? ) is a linear function. Here, we do not make any
assumption on the class of functions F? other than assuming that there exists an algorithm to find
the best function f? ? F? in terms of the logistic regression loss (Section 6).

3 Loss Functions
Given a dataset (x1 , y 1 ), ..., (xN , y N ), we wish to select the energy F to minimize the empirical risk
!
R(F ) =
l(xk , y k ; F ),
(2)
k

for some loss function l. Absent computational concerns, a standard choice would be the slackrescaled loss [22]
l0 (xk , y k ; F ) = max F (xk , y) ? F (xk , y k ) + ?(y k , y),

(3)

y

where ?(y k , y) is some measure
"" of discrepancy. We assume that ? is a function that decomposes
over ?, (i.e. that ?(y k , y) = ? ?? (y?k , y ? )). Our experiments use the Hamming distance.

In Eq. 3, the maximum ranges over all possible discrete labelings y, which is in NP-hard in general.
If this inference problem must be solved approximately, there is strong motivation [6] for using
relaxations of the maximization in Eq. 1, since this yields an upper-bound on the loss. A common
solution [16, 14, 6] is to use a linear relaxation1
l1 (xk , y k ; F ) = max F (xk , ?) ? F (xk , y k ) + ?(y k , ?),

(4)

??M

where the local polytope M is defined as the set of local pseudomarginals that are normalized, and
agree when marginalized over other neighboring regions,
!
M = {?|??? (y? ) = ?? (y? ) ?? ? ?,
?? (y? ) = 1 ??, ?? (y? ) ? 1 ??, y? }.
y?

Here, ??? (y? ) = y?\? ?? (y? ) is ?? marginalized out over some region ? contained in ?. It is
easy to show that l1 ? l0 , since the two would be equivalent if ? were restricted to binary values,
and hence the maximization in l1 takes place over a larger set [6]. We also define
""

?Fk (y? ) = f? (xk , y? ) + ?? (y?k , y ? ),
1

(5)
k

F and ? are slightly generalized
to allow arguments of pseudomarginals, as F (x , ?) =
! Here,
!
! !
k
k
k
f
(x
,
y
)?(y
)
and
?(y
,
?)
=
?
?
?
y?
?
y ? ?? (y? , y ? )?(y? ).

2

which gives the equivalent representation of l1 as l1 (xk , y k ; F ) = ?F (xk , y k ) + max??M ?Fk ? ?.
The maximization in l1 is of a linear objective under linear constraints, and is thus a linear program
(LP), solvable in polynomial time using a generic LP solver. In practice, however, it is preferable to
use custom solvers based on message-passing that exploit the sparsity of the problem.
Here, we make a further approximation to the loss, replacing
the inference problem of max??M ? ??
!
with the ?smoothed? problem max??M ? ? ? + "" ? H(?? ), where H(?? ) is the entropy of the
marginals ?? . This approximation has been considered by Meshi et al. [15] who show that local
message-passing can have a guaranteed convergence rate, and by Hazan and Urtasun [9] who use it
for learning. The relaxed loss is
""
$
#
l(xk , y k ; F ) = ?F (xk , y k ) + max ?Fk ? ? + ""
H(?? ) .
(6)
??M

?

Since the entropy is positive, this is clearly a further upper-bound on the ?unsmoothed? loss, i.e.
l1 ? l. Moreover, we can bound the looseness of this approximation as in the following theorem,
proved in the appendix. A similar result was previously given [15] bounding the difference of the
objective obtained by inference with and without entropy smoothing.
Theorem 1. l and l1 are bounded by (where |y? | is the number of configurations of y? )
l1 (x, y, F ) ? l(x, y, F ) ? l1 (x, y, F ) + ""Hmax , Hmax =

#

log |y? |.

?

4 Overview
Now, the learning problem is to select the functions f? composing F to minimize R as defined in
Eq. 2. The major challenge is that evaluating R(F ) requires performing inference. Specifically, if
we define
#
A(?) = max ? ? ? + ""
H(?? ),
(7)
??M

?

then we have that

min R(F ) = min
F

F

#%
&
?F (xk , y k ) + A(?Fk ) .
k

Since A(?) contains a maximization, this is a saddle-point problem. Inspired by previous work
[16, 9], our solution (Section 5) is to introduce a vector of ?messages? ? to write A in the dual form
A(?) = min A(?, ?),
?

which leads to phrasing learning as the joint minimization
#'
(
min min
?F (xk , y k ) + A(?k , ?Fk ) .
F

{?k }

k

We propose to solve this through an alternating optimization of F and {?k }. For fixed F , messagepassing can be used to perform coordinate ascent updates to all the messages ?k (Section 5). These
updates are trivially parallelized with respect to k. However, the problem remains, for fixed messages, how to optimize the functions f? composing F . Section 7 observes that this problem can be
re-formulated into a (non-structured) logistic regression problem, with ?bias? terms added to each
example that reflect the current messages into factor ?.

5 Inference
In order to evaluate the loss, it is necessary to solve the maximization in Eq. 6. For a given ?,
consider doing inference over ?, that is, in solving the maximization in Eq. 7. Standard Lagrangian
duality theory gives the following dual representation for A(?) in terms of ?messages? ?? (x? ) from
a region ? to a subregion ? ? ?, a variant of the representation of Heskes [11].
3

Algorithm 1 Reducing structured learning to logistic regression.
For all k, ?, initialize ?k (y? ) ? 0.
Repeat until convergence:
1. For all k, for all ?, set the bias term to
?
?
#
#
1
bk? (y ? ) ? ??(y?k , y ? ) +
?k? (y? ) ?
?k? (y? )? .
#
???
???

2. For all ?, solve the logistic regression problem
&
)
K
'
(
'
(
#
#
k
k
k
k
k
k
f? ? arg max
f? (x , y? ) + b? (y? ) ? log
exp f? (x , y? ) + b? (y? ) .
f? ?F?

y?

k=1

3. For all k, for all ?, form updated parameters as
?k (y? ) ? #f? (xk , y? ) + ?(y?k , y ? ).
4. For all k, perform a fixed number of message-passing iterations to update ?k using ?k . (Eq. 10)

Theorem 2. A(?) can be represented in the dual form A(?) = min? A(?, ?), where
#
###
A(?, ?) = max ? ? ? + #
H(?? ) +
?? (x? ) (??? (y? ) ? ?? (y? )) ,
??N

?

(8)

? ??? x?

*
and N = {?| y? ?? (y? ) = 1, ?? (y? ) ? 0} is the set of locally normalized pseudomarginals.
Moreover, for a fixed ?, the maximizing ? is given by
? ?
??
#
#
1
1
?? (y? ) =
exp ? ??(y? ) +
?? (y? ) ?
?? (y? )?? ,
(9)
Z?
#
???
???

where Z? is a normalizing constant to ensure that

*

y?

?? (y? ) = 1.

Thus, for any set of messages ?, there is an easily-evaluated upper-bound A(?, ?) ? A(?), and when
A(?, ?) is minimized with respect to ?, this bound is tight. The standard approach to performing
the minimization over ? is essentially block-coordinate descent. There are variants, depending on
the size of the ?block? that is updated. In our experiments, we use blocks consisting of the set of all
messages ?? (y? ) for all regions ? containing ?. When the graph only contains regions for single
variables and pairs, this is a ?star update? of all the messages from pairs that contain a variable i. It
can be shown [11, 15] that the update is
?$? (y? ) ? ?? (y? ) +

#
#
(log ?? (y? ) +
log ??! (y? )) ? # log ?? (y? ),
1 + N?
!

(10)

? ??

for all ? ? ?, where N? = |{?|? ? ?}|. Meshi et al. [15] show that with greedy or randomized
selection of blocks to update, O( ?1 ) iterations are sufficient to converge within error ?.

6 Logistic Regression
Logistic regression is traditionally understood as defining a conditional distribution p(y|x; W ) =
exp ((W x)y ) /Z(x) where W is a matrix that maps the input features x to a vector of margins W*
x. It is easy to show that the maximum conditional likelihood training problem
maxW k log p(y k |xk ; W ) is equivalent to
)
&
#
#
k
k
max
(W x )yk ? log
exp(W x )y .
W

y

k

4

Here, we generalize this in two ways. First, rather than taking the mapping from features x to the
margin for label y as the y-th component of W x, we take it as f (x, y) for some function f in a set
of function F . (This reduces to the linear case when f (x, y) = (W x)y .) Secondly, we assume that
there is a pre-determined ?bias? vector bk associated with each training example. This yields the
learning problem
""
%
! #
!
$
#
$
k k
k k
k
k
max
f (x , y ) + b (y ) ? log
(11)
exp f (x , y) + b (y) ,
f ?F

y

k

Aside from linear logistic regression, one can see decision trees, multi-layer perceptrons, and
boosted ensembles under an appropriate loss as solving Eq. 11 for different sets of functions F
(albeit possibly to a local maximum).

7 Training
Recall that the&
learning problem is to select the functions f? ? F? so as to minimize the empirical
risk R(F ) = k [?F (xk , y k ) + A(?Fk )]. At first blush, this appears challenging, since evaluating
A(?) requires solving a message-passing optimization. However, we can use the dual representation
of A from Theorem 2 to represent minF R(F ) in the form
!'
(
min min
?F (xk , y k ) + A(?k , ?Fk ) .
(12)
F

{?k }

k

To optimize Eq. 12, we alternating between optimization of messages {?k } and energy functions
{f? }. Optimization with respect to ?k for fixed F decomposes into minimizing A(?k , ?Fk ) independently for each y k , which can be done by running message-passing updates as in Section 5 using the
parameter vector ?Fk . Thus, the rest of this section is concerned with how to optimize with respect
to F for fixed messages. Below, we will use a slight generalization of a standard result [1, p. 93].
Lemma 3. The conjugate of the entropy is the ?log-sum-exp? function. Formally,
!
!
?i
xi log xi = ? log
exp .
max ? ? x ? ?
T
?
x:x 1=1,x?0
i
i

Theorem 4. If f?? is the minimizer of Eq 12 for fixed messages ?, then
""
%
! #
!
$
#
$
?
k
k
k
k
k
k
f? = $ arg max
f? (x , y? ) + b? (y? ) ? log
exp f? (x , y? ) + b? (y? ) ,

(13)

where the set of biases are defined as
?
?
!
!
1
?? (y? ) ?
?? (y? )? .
bk? (y ? ) = ??(y?k , y ? ) +
$
???

(14)

f?

y?

k

???

Proof. Substituting A(?, ?) from Eq. 8 and ?k from Eq. 5 gives that

A(?k , ?Fk ) = max
??N

!!#
!
$
f? (xk , y? ) + ?? (y?k , y ? ) ?(y? ) + $
H(?? )
?

y?

?

+

!!!

?k? (x? ) (??? (y? )

? ?? (y? )) .

? ??? x?

Using the definition of bk from Eq. 14 above, this simplifies into
.
!
!
k k
(f? (x, y? ) + $b? (y? )) ?? (y? ) + $H(?? ) ,
A(? , ?F ) =
max
?

?? ?N?

y?

5

Fi \ Fij
Zero
Const.
Linear
Boost.
MLP

Denoising
Zero Const. Linear Boost. MLP
.502 .502 .502 .511 .502
.502 .502 .502 .510 .502
.444 .077 .059 .049 .034
.444 .034 .015 .009 .007
.445 .032 .015 .009 .008

Fi \ Fij
Zero
Const.
Linear
Boost.
MLP

Horses
Zero Const. Linear Boost. MLP
.246 .246 .247 .244 .245
.246 .246 .247 .244 .245
.185 .185 .168 .154 .156
.103 .098 .092 .084 .086
.096 .094 .087 .080 .081

Table 1: Univariate Test Error Rates (Train Errors in Appendix)
!
where N? = {?? | y? ?? (y? ) = 1, ?? (y? ) ? 0} enforces that ?? is a locally normalized set of
marginals. Applying Lemma 3 to the inner maximization gives the closed-form expression
k

A(?

, ?Fk )

=

""
?

# log

""

exp

y?

#

$
1
f? (x, y? ) + b? (y? ) .
#

Thus, minimizing Eq. 12 with respect to F is equivalent to finding (for all ?)
%
$&
#
""
""
1
k
k k
f? (x, y? ) + b? (y? )
arg max
f? (x , y? ) ? # log
exp
f?
#
y?
k
%
#
$&
""
"" 1
1
k
k
k
k
exp
f? (x , y? ) ? log
f (x , y? ) + b? (y? )
= arg max
f?
#
#
y
k

?

Observing that adding a bias term doesn?t change the maximizing f? , and using the fact that
arg max g( 1"" ?) = # arg max g(?) gives the result.
The final learning algorithm is summarized as Alg. 1. Sometimes, the local classifier f? will
depend on the input x only through some ?local features? ?? . The above framework accomodates
this situation if the set F? is considered to select these local features.
In practice, one will often wish to constrain that some of the functions f? are the same.
This is done by taking the sum in Eq. 13 not just over all data k, but also over all factors ? that should be so constrained. For example, it is common to model !
image segmentation
problems
using
a
4-connected
grid
with
an
energy
like
F
(x,
y)
=
i u(?i , yi ) +
!
v(?
,
y
,
y
),
where
?
/?
are
univariate/pairwise
features
determined
by
x,
and u and v
ij
i
j
i
ij
ij
are functions mapping
local
features
to
local
energies.
In
this
case,
u
would
be
selected
to max)
(
)*
! ! '(
!
imize k i u(?ki , yik ) + bki (yik ) ? log yi exp u(?ki , yi ) + bki (yi ) , and analogous expression exists for v. This is the framework used in the following experiments.

8 Experiments

These experiments consider three different function classes: linear, boosted decision trees, and
multi-layer perceptrons. To maximize Eq. 11 under linear functions f (x, y) = (W x)y , we simply compute the gradient with respect to W and use batch L-BFGS. For a multi-layer perceptron,
we fit the function f (x, y) = (W ?(U x))y using stochastic gradient descent with momentum2 on
mini-batches of size 1000, using a step size of .25 for univariate classifiers and .05 for pairwise.
Boosted decision trees use stochastic gradient boosting [7]: the gradient of the logistic loss is computed for each exemplar, and a regression tree is induced to fit this (one tree for each class). To
control overfitting, each leaf node must contain at least 5% of the data. Then, an optimization adjusts the values of leaf nodes to optimize the logistic loss. Finally, the tree values are multiplied by
2

At each time, the new step is a combination of .1 times the new gradient plus .9 times the old step.

6

400

400

yi=1
y =2

0.4

?i

0.6

100

0.8

?400
0

1

?200

0.2

0.4

?i

0.6

0.8

100

y =(1,1)

0.8

ij

yij=(2,1)
yij=(2,2)

0

ij

0

?100
0

1

1

y =(1,1)

50

yij=(2,1)
f

ij

f
0.6

0.8

ij

?50

?ij

0.6

yij=(2,2)

?50

0.4

?i

y =(1,2)

ij

50

0

0.2

0.4

ij

yij=(2,1)

?100
0

0.2

100

y =(1,2)

ij

yij=(2,2)
fij

?400
0

1

y =(1,1)

ij

y =(1,2)
50

0

i

0

?200

0.2

i

200

f

i

f

fi

?200

y =2

i

200

0

?400
0

yi=1

y =2

i

200

400

yi=1

?50

0.2

Linear

0.4

?ij

0.6

0.8

1

?100
0

0.2

Boosting

0.4

?ij

0.6

0.8

1

MLP

Figure 1: The univariate (top) and pairwise (bottom) energy functions learned on denoising data.
Each column shows the result of training both univariate and pairwise terms with one function class.
Denoising

Linear

Boosting

MLP Fi \ Fij

0.1

0.1

0.1

20
40
Iteration

0

20
40
Iteration

0

0.1

0
0

20
40
Iteration

MLP

MLP

0
0.2
Error

0
0.2

Boosting

0.1

Error

0
0.2

Boosting

Error

MLP Fi \ Fij
Linear

Error

0.1

0
0.2

Error

Boosting

0.2

Linear

Error

0.2

0
0

Horses

Linear

10
20
Iteration

0

10
20
Iteration

0

10
20
Iteration

Figure 2: Dashed/Solid lines show univariate train/test error rates as a function of learning iterations
for varying univariate (rows) and pairwise (columns) classifiers.

Input

True

Denoising

Linear Boosting

MLP

Input

True

Horses

Linear Boosting MLP

Figure 3: Example Predictions on Test Images (More in Appendix)

7

.25 and added to the ensemble. For reference, we also consider the ?zero? classifier, and a ?constant?
classifier that ignores the input? equivalent to a linear classifier with a single constant feature.
All examples use ! = 0.1. Each learning iteration consists of updating fi , performing 25 iterations
of message passing, updating fij , and then performing another 25 iterations of message-passing.
The first dataset is a synthetic binary denoising dataset, intended for the purpose of visualization. To
create an example, an image is generated with each pixel random in [0, 1]. To generate y, this image
is convolved with a Gaussian with standard deviation 10 and rounded to {0, 1}. Next, if yi = 0, ?ki
is sampled uniformly from [0, .9], while if yik = 1, ?ki is sampled from [.1, 1]. Finally, for a pair
(i, j), if yik = yjk , then ?kij is sampled from [0, .8] while if yik != yjk ?ij is sampled from [.2, 1]. A
constant feature is also added to both ?ki and ?kij .
There are 16 100 ? 100 images each training and testing. Test errors for each classifier combination
are in Table 1, learning curves are in Fig. 2, and example results in Fig. 3. The nonlinear classifiers
result in both lower asymptotic training and testing errors and faster convergence rates. Boosting
converges particularly quickly. Finally, because there is only a single input feature for univariate
and pairwise terms, the resulting functions are plotted in Fig. 1.
Second, as a more realistic example, we use the Weizmann horses dataset. We use 42 univariate
features fik consisting of a constant (1) the RBG values of the pixel (3), the vertical and horizontal
position (2) and a histogram of gradients [2] (36). There are three edge features, consisting of a
constant, the l2 distance of the RBG vectors for the two pixels, and the output of a Sobel edge filter.
Results are show in Table 1 and Figures 2 and 3. Again, we see benefits in using nonlinear classifiers,
both in convergence rate and asymptotic error.

9 Discussion
This paper observes that in the structured learning setting, the optimization with respect to energy
can be formulated as a logistic regression problem for each factor, ?biased? by the current messages.
Thus, it is possible to use any function class where an ?oracle? exists to optimize a logistic loss.
Besides the possibility of using more general classes of energies, another advantage of the proposed
method is the ?software engineering? benefit of having the algorithm for fitting the energy modularized from the rest of the learning procedure. The ability to easily define new energy functions for
individual problems could have practical impact.
Future work could consider convergence rates of the overall learning optimization, systematically
investigate the choice of !, or consider more general entropy approximations, such as the Bethe
approximation used with loopy belief propagation.
In related work, Hazan and Urtasun [9] use a linear energy, and alternate between updating all inference variables and a gradient descent update to parameters, using an entropy-smoothed inference
objective. Meshi et al. [16] also use a linear energy, with a stochastic algorithm updating inference
variables and taking a stochastic gradient step on parameters for one exemplar at a time, with a pure
LP-relaxation of inference. The proposed method iterates between updating all inference variables
and performing a full optimization of the energy. This is a ?batch? algorithm in the sense of making repeated passes over the data, and so is expected to be slower than an online method for large
datasets. In practice, however, inference is easily parallelized over the data, and the majority of
computational time is spent in the logistic regression subproblems. A stochastic solver can easily be
used for these, as was done for MLPs above, giving a partially stochastic learning method.
Another related work is Gradient Tree Boosting [4] in which to train a CRF, the functional gradient
of the conditional likelihood is computed, and a regression tree is induced. This is iterated to produce
an ensemble. The main limitation is the assumption that inference can be solved exactly. It appears
possible to extend this to inexact inference, where the tree is induced to improve a dual bound, but
this has not been done so far. Experimentally, however, simply inducing a tree on the loss gradient
leads to much slower learning if the leaf nodes are not modified to optimize the logistic loss. Thus,
it is likely that such a strategy would still benefit from using the logistic regression reformulation.

8

References
[1] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[2] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005.
[3] Chaitanya Desai, Deva Ramanan, and Charless C. Fowlkes. Discriminative models for multi-class object
layout. International Journal of Computer Vision, 95(1):1?12, 2011.
[4] Thomas G. Dietterich, Adam Ashenfelter, and Yaroslav Bulatov. Training conditional random fields via
gradient tree boosting. In ICML, 2004.
[5] Justin Domke. Learning graphical model parameters with approximate marginal inference. PAMI,
35(10):2454?2467, 2013.
[6] Thomas Finley and Thorsten Joachims. Training structural svms when exact inference is intractable. In
ICML, 2008.
[7] Jerome H. Friedman. Stochastic gradient boosting. Computational Statistics and Data Analysis, 38:367?
378, 1999.
[8] Stephen Gould, Jim Rodgers, David Cohen, Gal Elidan, and Daphne Koller. Multi-class segmentation
with relative location prior. IJCV, 80(3):300?316, 2008.
[9] Tamir Hazan and Raquel Urtasun. Efficient learning of structured predictors in general graphical models.
CoRR, abs/1210.2346, 2012.
[10] Xuming He, Richard S. Zemel, and Miguel ?. Carreira-Perpi??n. Multiscale conditional random fields
for image labeling. In CVPR, 2004.
[11] Tom Heskes. Convexity arguments for efficient minimization of the bethe and kikuchi free energies. J.
Artif. Intell. Res. (JAIR), 26:153?190, 2006.
[12] Sanjiv Kumar and Martial Hebert. Discriminative fields for modeling spatial dependencies in natural
images. In NIPS, 2003.
[13] Lubor Ladicky, Christopher Russell, Pushmeet Kohli, and Philip H. S. Torr. Associative hierarchical
CRFs for object class image segmentation. In ICCV, 2009.
[14] Andr? F. T. Martins, Noah A. Smith, and Eric P. Xing. Polyhedral outer approximations with application
to natural language parsing. In ICML, 2009.
[15] Ofer Meshi, Tommi Jaakkola, and Amir Globerson. Convergence rate analysis of MAP coordinate minimization algorithms. In NIPS. 2012.
[16] Ofer Meshi, David Sontag, Tommi Jaakkola, and Amir Globerson. Learning efficiently with approximate
inference via dual losses. In ICML, 2010.
[17] Sebastian Nowozin, Peter V. Gehler, and Christoph H. Lampert. On parameter learning in CRF-based
approaches to object class image segmentation. In ECCV, 2010.
[18] Sebastian Nowozin, Carsten Rother, Shai Bagon, Toby Sharp, Bangpeng Yao, and Pushmeet Kohli. Decision tree fields. In ICCV, 2011.
[19] Florian Schroff, Antonio Criminisi, and Andrew Zisserman. Object class segmentation using random
forests. In BMVC, 2008.
[20] Jamie Shotton, John M. Winn, Carsten Rother, and Antonio Criminisi. Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context.
IJCV, 81(1):2?23, 2009.
[21] Nathan Silberman and Rob Fergus. Indoor scene segmentation using a structured light sensor. In ICCV
Workshops, 2011.
[22] Benjamin Taskar, Carlos Guestrin, and Daphne Koller. Max-margin markov networks. In NIPS, 2003.
[23] Jakob J. Verbeek and Bill Triggs. Scene segmentation with crfs learned from partially labeled images. In
NIPS, 2007.
[24] John M. Winn and Jamie Shotton. The layout consistent random field for recognizing and segmenting
partially occluded objects. In CVPR, 2006.
[25] Jianxiong Xiao and Long Quan. Multiple view semantic segmentation for street view images. In ICCV,
2009.

9

"
1580,"One microphone blind dereverberation
based on quasi-periodicity of speech signals

Tomohiro Nakatani, Masato Miyoshi, and Keisuke Kinoshita
Speech Open Lab., NTT Communication Science Labs., NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan
{nak,miyo,kinoshita}@cslab.kecl.ntt.co.jp

Abstract
Speech dereverberation is desirable with a view to achieving, for example, robust speech recognition in the real world. However, it is still a challenging problem, especially when using a single microphone. Although
blind equalization techniques have been exploited, they cannot deal with
speech signals appropriately because their assumptions are not satisfied
by speech signals. We propose a new dereverberation principle based
on an inherent property of speech signals, namely quasi-periodicity. The
present methods learn the dereverberation filter from a lot of speech data
with no prior knowledge of the data, and can achieve high quality speech
dereverberation especially when the reverberation time is long.

1

Introduction

Although numerous studies have been undertaken on robust automatic speech recognition
(ASR) in the real world, long reverberation is still a serious problem that severely degrades
the ASR performance [1]. One simple way to overcome this problem is to dereverberate
the speech signals prior to ASR, but this is also a challenging problem, especially when
using a single microphone. For example, certain blind equalization methods, including
independent component analysis (ICA), can estimate the inverse filter of an unknown impulse response convolved with target signals when the signals are statistically independent
and identically distributed sequences [2]. However, these methods cannot appropriately
deal with speech signals because speech signals have inherent properties, such as periodicity and formant structure, making their sequences statistically dependent. This approach
inevitably destroys such essential properties of speech. Another approach that uses the
properties of speech has also been proposed [3]. The basic idea involves adaptively detecting time regions in which signal-to-reverberation ratios become small, and attenuating
speech signals in those regions. However, the precise separation of the signal and reverberation durations is difficult, therefore, this approach has achieved only moderate results so
far.
In this paper, we propose a new principle for estimating an inverse filter by using an essential property of speech signals, namely quasi-periodicity, as a clue. In general, voiced
segments in an utterance have approximate periodicity in each local time region while the
period gradually changes. Therefore, when a long reverberation is added to a speech signal,
signals in different time regions with different periods are mixed, thus degrading the periodicity of the signals in local time regions. By contrast, we show that we can estimate an
inverse filter for dereverberating a signal by enhancing the periodicity of the signal in each

local time region. The estimated filter can dereverberate both the periodic and non-periodic
parts of speech signals with no prior knowledge of the target signals, even though only the
periodic parts of the signals are used for the estimation.

2

Quasi-periodicity based dereverberation

We propose two dereverberation methods, referred to as Harmonicity based dEReverBeration (HERB) methods, based on the features of quasi-periodic signals: one based on an Average Transfer Function (ATF) that transforms reverberant signals into quasi-periodic components (ATF-HERB), and the other based on the Minimum Mean Squared Error (MMSE)
criterion that evaluates the quasi-periodicity of target signals (MMSE-HERB). First, we
briefly explain the features of quasi-periodic signals, and then describe the two methods.
2.1

Features of quasi-periodic signals

When a source signal s(n) is recorded in a reverberant room1 , the obtained signal x(n) is
represented as x(n) = h(n)?s(n), where h(n) is the impulse response of the room and ???
is a convolution operation. The goal of the dereverberation is to estimate a dereverberation
filter, w(n), for ?N < n < N that dereverberates x(n), and to obtain the dereverberated
signal y(n) by:
y(n) = w(n) ? x(n) = (w(n) ? h(n)) ? s(n) = q(n) ? s(n).

(1)

where q(n) = w(n) ? h(n) is referred to as a dereverberated impulse response. Here, we
assume s(n) is a quasi-periodic signal2 , which has the following features:
1. In each local time region around n0 (n0 ? ? < n < n0 + ? for ? n0 ), s(n) is
approximately a periodic signal whose period is T (n0 ).
2. Outside the region (|n ? n0 | > ?), s(n ) is also a periodic signal within its
neighboring time region, but often has another period that is different from T (n0 ).
These features make x(n) a non-periodic signal even within local time regions when h(m)
contains non-zero values for |m| > ?. This is because more than two periodic signals,
s(n) and s(n ? m), that have different periods, are added to x(n) with weights of h(0)
and h(m). Inversely, the goal of our dereverberation is to estimate w(n) that makes y(n)
a periodic signal in each local time region. Once such a filter is obtained, q(m) must have
zero values for |m| > ?, and thus, reverberant components longer than ? are eliminated
from y(n).
An important additional feature of a quasi-periodic signal is that quasi-periodic components
in a source signal can be enhanced by an adaptive harmonic filter. An adaptive harmonic
filter is a time-varying linear filter that enhances frequency components whose frequencies
correspond to multiples of the fundamental frequency (F0 ) of the target signal, while preserving their phases and amplitudes. The filter values are adaptively modified according to
F0 . For example, a filter, F (f0 (n))[?], can be implemented as follows:
x
?(n)

=
=

F (f0 (n))[x(n)],


g2 (n ? n0 )Re{x(n) ? (g1 (n)
exp(j2?kf0 (n0 )n/fs ))},
n0

(2)
(3)

k

where n0 is the center time of each frame, f0 (n0 ) is the fundamental frequency (F0 ) of
the signal at the frame, k is a harmonics index, g1 (n) and g2 (n) are analysis window
1

In this paper, time domain and frequency domain signals are represented by non-capitalized
and capitalized symbols, respectively. Arguments ?(?)? that represent the center frequencies of the
discrete Fourier transformation bins are often omitted from frequency domain signals.
2
Later, this assumption is extended so that s(n) is composed of quasi-periodic components and
non-periodic components in the case of speech signals.

S(?)

H(?)

X(?)

W(?)

F(f0)

^
E(X/X)

Y(?)

^
X(?)
Figure 1: Diagram of ATF-HERB
functions, and fs is the sampling frequency. Even when x(n) contains a long reverberation,
the reverberant components that have different frequencies from s(n) are reduced by the
harmonic filter, and thus, the quasi-periodic components can be enhanced.
2.2

ATF-HERB: average transfer function based dereverberation

Figure 1 is a diagram of ATF-HERB, which uses the average transfer function from reverberant signals to quasi-periodic signals. A speech signal, S(?), can be modeled by the
sum of the quasi-periodic components, or voiced components, Sh (?), and non-periodic
components, or unvoiced components, Sn (?), as eq. (4). The reverberant observed signal,
X(?), is then represented by the product of S and the transfer function, H(?), of a room as
eq. (5). The transfer function, H, can also be divided into two functions, D(?) and R(?).
The former transforms S into the direct signal, DS, and the latter into the reverberation
part, RS, as shown in eq. (6). X is also represented by the sum of the direct signal of the
quasi-periodic components, DSh , and the other components as eq. (7).
S(?)
X(?)

= Sh (?) + Sn (?),
= H(?)S(?),
= (D(?) + R(?))S(?),
= DSh + (RSh + HSn ).

(4)
(5)
(6)
(7)

Of these components, DSh can approximately be extracted from X by harmonic filtering.
Although the frequencies of quasi-periodic components change dynamically according to
the changes in their fundamental frequency (F0 ), their reverberation remains unchanged at
the same frequency. Therefore, direct quasi-periodic components, DSh , can be enhanced
by extracting frequency components located at multiples of its F0 . This approximated
?
direct signal X(?)
can be modeled as follows:
?
?
?
X(?)
= D(?)Sh (?) + (R(?)S
h (?) + N (?)),

(8)

?
?
where R(?)S
h (?) and N (?) are part of the reverberation of Sh and part of the direct signal
? after the harmonic filtering3 . We
and reverberation of Sn , which unexpectedly remain in X
? in eq. (8).
? are caused by RS
? h and N
assume that all the estimation errors in X
?
?
The goal of ATF-HERB is to estimate O(R(?))
= (D(?) + R(?))/H(?),
referred to as
? which can be obtained
a ?dereverberation operator.? This is because the signal DS + RS,
? by X, becomes in a sense a dereverberated signal.
by multiplying O(R)
?
?
O(R(?))X(?)
= D(?)S(?) + R(?)S(?),

(9)

? cannot be represented as a linear transformation because the reverberation
Strictly speaking, R
? depends on the time pattern of X.
? We introduce this approximation for simplicity.
included in X
3

S(?)

H(?)

X(?)

F(f0)

W(?)

Y(?)

MMSE

^
X(?)
Figure 2: Diagram of MMSE-HERB
where the right side of eq. (9) is composed of a direct signal, DS, and certain parts of the
? The rest of the reverberation included in X(= DS +RS), or (R? R)S,
?
reverberation, RS.
is eliminated by the dereverberation operator.
? SupTo estimate the dereverberation operator, we use the output of the harmonic filter, X.
?
pose a number of X values are obtained and X values are calculated from individual X
? can be approximated as the average of
values. Then, the dereverberation operator, O(R),
?
?
? by substitutX/X, or W (?) = E(X/X). W (?) is shown to be a good estimate of O(R)
?
ing E(X/X)
for eqs. (4), (5) and (8) as eq. (11).
W (?)

=

?
E(X/X),

1
1
?
) + E(
= O(R(?))E(
),
? )/N
?
1 + Sn /Sh
1 + (X ? N
?
 O(R(?))P
(|Sh (?)| > |Sn (?)|),

(10)
(11)
(12)

where P (?) is a probability function. The arguments of the two average functions in eq. (11)
have the form of a complex function, f (z) = 1/(1 + z). E(f (z)) is easily proven to equal
P (|z| < 1), using the residue theorem if it is assumed that the phase of z is uniformly
distributed, the phases of z and |z| are independent, and |z| = 1. Based on this property, the
? is a non-periodic component
second term of eq. (11) approximately equals zero because N
? almost always
that the harmonic filter unexpectedly extracts and thus the magnitude of N
?
has a smaller value than (Y ? N ) if a sufficiently long analysis window is used. Therefore,
W (?) can be approximated by eq. (12), that is, W (?) has the value of the dereverberation
operator multiplied by the probability of the harmonic components of speech with a larger
magnitude than the non-periodic components.
Once the dereverberation operator is calculated from the periodic parts of speech signals
for almost all the frequency ranges, it can dereverberate both the periodic and non-periodic
parts of the signals because the inverse transfer function is independent of the source signal
characteristics. Instead, the gain of W (?) tends to decrease with frequency when using
our method. This is because the magnitudes of the non-periodic components relative to
the periodic components tend to increase with frequency for a speech signal, and thus
the P (|Sh | > |Sn |) value becomes smaller as ? increases. To compensate for this decreasing gain, it may be useful to use the average attributes of speech on the probability,
P (|Sh | > |Sn |). In our experiments in section 4, however, W (?) itself was used as the
dereverberation operator without any compensation.
2.3

MMSE-HERB: minimum mean squared error criterion based dereverberation

As discussed in section 2.1, quasi-periodic signals can be dereverberated simply by enhancing their quasi-periodicity. To implement this principle directly, we introduce a cost

function, referred to as the minimum mean squared error (MMSE) criterion, to evaluate the
quasi-periodicity of the signals as follows:


C(w) =
(y(n) ? F (f0 (n))[y(n)])2 =
(w(n) ? x(n) ? F (f0 (n))[w(n) ? x(n)])2 ,
n

n

(13)
where y(n) = w(n) ? x(n) is a target signal that should be dereverberated by controlling
w(n), and F (f0 (n))[y(n)] is a signal obtained by applying a harmonic filter to y(n). When
y(n) is a quasi-periodic signal, y(n) approximately equals F (f0 (n))[y(n)] because of the
feature of quasi-periodic signals, and thus, the above cost function is expected to have the
minimum value. Inversely, the filter, w(n), that minimizes C(w) is expected to enhance
the quasi-periodicity of x(n). Such filter parameters can, for example, be obtained using optimization algorithms such as a hill-climbing method using the derivatives of C(w)
calculated as follows:

?C(w)
=2
(y(n) ? F (f0 (n))[y(n)])(x(n ? l) ? F (f0 (n))[x(n ? l)]),
(14)
?w(l)
n
where F (f0 (n))[x(n ? l)]) is a signal obtained by applying the adaptive harmonic filter to
x(n ? l)4 .
There are, however, several problems involved in directly using eq. (13) as the cost function.
1. As discussed in section 2.1, the values of the dereverberated impulse response,
q(n), are expected to become zero using this method where |n| > ?, however,
the values are not specifically determined where |n| < ?. This may cause unexpected spectral modification of the dereverberated signal. Additional constraints
are required in order to specify these values.
2. The cost function has a self-evident solution, that is, w(l) = 0 for all l values. This
solution means that the signal, y(n), is always zero instead of being
 dereverberated, and therefore, should be excluded. Some constraints, such as l w(l)2 = 1,
may be useful for solving this problem.
3. The complexity of the computing needed to minimize the cost function based on
repetitive estimation increases as the dereverberation filter becomes longer. The
longer the reverberation becomes, the longer the dereverberation filter should be.
To overcome these problems, we simplify the cost function in this paper. The new cost
function is defined as follows:
2
2
?
?
C(W (?)) = E((Y (?) ? X(?))
) = E((W (?)X(?) ? X(?))
),
(15)
?
where Y (?), X(?), and X(?)
are discrete Fourier transformations of y(n), x(n), and
F (f0 (n))[x(n)], respectively. The new cost function evaluates the quasi-periodicity not in
?
the time domain but in the frequency domain, and uses a fixed quasi-periodic signal X(?)
as the desired signal, instead of using the non-fixed quasi-periodic signal, F (f0 (n))[y(n)].
This modification allows us to solve the above problems. The use of the fixed desired
signals specifically provides the dereverberated impulse response, q(n), with the desired
values, even in the time region, |n| < ?. In addition, the self-evident solution, w(l) = 0, can
no longer be optimal in terms of the cost function. Furthermore, the computing complexity
is greatly reduced because the solution can be given analytically as follows:
W (?) =

?
?
E(X(?)X
(?))
.
?
E(X(?)X (?))

(16)

A diagram of this simplified MMSE-HERB is shown in Fig. 2.
4
F (f0 (n))[x(n ? l)]) is not the same signal as x
?(n ? l). When calculating F (f0 (n))[x(n ? l)],
x(n) is time-shifted with l-points while f0 (n) of the adaptive harmonic filter is not time-shifted.

STEP1:

X
F0
estimation

Input X

F0

STEP2:
^
O(R1) X

F0
estimation

X
Adaptive
harmonics
filter

^
X1

X

Dereverberation
operator
estimation

^
^
O(R1) X
O(R1) X
DereverbeAdaptive
ration
harmonics
^
F0 filter
operator
X2
estimation

Dereverberation
^
by O(R^1)
O(R1)

^
O(R1) X

^
O(R1) X
Dereverberation
^
by O(R^2)
O(R2)

^
^
O(R2) O(R1) X

Figure 3: Processing flow of dereverberation.
? S ? ) = 0,
? in eq. (8), and E(Sh Sn? ) = E(Sn S ? ) = E(N
When we assume the model of X
h
h
it is shown that the resulting W in eq. (16) again approaches the dereverberation operator,
? presented in section 2.2:
O(R),
W (?)

=


? Sn? )
1
E(Sh Sh? )
E(N
+
,
E(Sh Sh? ) + E(Sn Sn? ) H E(Sh Sh? ) + E(Sn Sn? )
E(Sh Sh? )
?
O(R(?))
.
E(Sh Sh? ) + E(Sn Sn? )
?
O(R(?))

(17)
(18)

? represents non-periodic components that are included unexpectedly and at ranBecause N
dom in the output of the harmonic filter, the absolute value of the second term in eq. (17)
is expected to be sufficiently small compared with that of the first term, therefore, we
disregard this term. Then, W (?) in eq. (16) becomes the dereverberation operator multiplied by the ratio of the expected power of the quasi-periodic components in the signals to that of whole signals. As with the speech signals discussed in section 2.2, the
E(Sh Sh? )/(E(Sh Sh? ) + E(Sn Sn? )) value becomes smaller as ? increases, and thus, the
gain of W (?) tends to decrease. Therefore, the same frequency compensation scenario as
found in section 2.2 may again be useful for the MMSE based dereverberation scheme.

3

Processing flow

Based on the above two methods, we constructed a dereverberation algorithm composed of
two steps as shown in Fig. 3. Both methods are implemented in the same processing flow
except that the methods used to calculate the dereverberation operator are different. The
flow is summarized as follows:
1. In the first step, F0 is estimated from the reverberant signal, X. Then the harmonic
? 1 based on adaptive harmonic filcomponents included in X are estimated as X
?
tering. The dereverberation operator O(R1 ) is then calculated by ATF-HERB or
MMSE-HERB for a number of reverberant speech signals. Finally, the derever? 1 ) by X.
berated signal is obtained by multiplying O(R
2. The second step employs almost the same procedures as the first step except that
the speech data dereverberated by the first step are used as the input signal. The
? 2 X2 ,
use of this dereverberated input signal means that reverberant components, R
inevitably included in eq. (8) can be attenuated. Therefore, a more effective dereverberation can be achieved in step 2.
In our preliminary experiments, however, repeating STEP 2 did not always improve the
quality of the dereverberated signals. This is because the estimation error of the dereverberation operators accumulates in the dereverberated signals when the signals are multiplied
by more than one dereverberation operator. Therefore, in our experiments, we used STEP 2
only once. A more detailed explanation of these processing steps is also presented in [4].

0

0
rtime=0.5 sec.

Power (dB)

Power (dB)

rtime=1.0 sec.
?20

?40

?60
0

0.2

0.4
0.6
Time (sec.)

?20

?40

?60
0

0.8

0

0.2

0.8

rtime=0.1 sec.

Power (dB)

Power (dB)

rtime=0.2 sec.
?20

?40

?60
0

0.4
0.6
Time (sec.)

0

0.2

0.4
0.6
Time (sec.)

0.8

?20

?40

?60
0

0.2

0.4
0.6
Time (sec.)

0.8

Figure 4: Reverberation curves of the original impulse responses (thin line) and dereverberated impulse responses (male: thick dashed line, female: thick solid line) for different
reverberation times (rtime).
Accurate F0 estimation is very important in terms of achieving effective dereverberation
with our methods in this processing flow. However, this is a difficult task, especially for
speech with a long reverberation using existing F0 estimators. To cope with this problem,
we designed a simple filter that attenuates a signal that continues at the same frequency, and
used it as a preprocessor for the F0 estimation [5]. In addition, the dereverberation operator,
? 1 ), itself is a very effective preprocessor for an F0 estimator because the reverberation
O(R
of the speech can be directly reduced by the operator. This mechanism is already included
? 1 )X.
in step 2 of the dereverberation procedure, that is, F0 estimation is applied to O(R
Therefore, more accurate F0 can be obtained in step 2 than in step 1.

4

Experimental results

We examined the performance of the proposed dereverberation methods. Almost the same
results were obtained with the two methods, and so we only describe those obtained with
ATF-HERB. We used 5240 Japanese word utterances provided by a male and a female
speaker (MAU and FKM, 12 kHz sampling) included in the ATR database as source signals,
S(?). We used four impulse responses measured in a reverberant room whose reverberation
times were about 0.1, 0.2, 0.5, and 1.0 sec, respectively. Reverberant signals, X(?), were
obtained by convolving S(?) with the impulse responses.
Figure 4 depicts the reverberation curves5 of the original impulse responses and the dereverberated impulse responses obtained with ATF-HERB. The figure shows that the proposed
methods could effectively reduce the reverberation in the impulse responses for the female
speaker when the reverberation time (rtime) was longer than 0.1 sec. For the male speaker,
the reverberation effect in the lower time region was also effectively reduced. This means
that strong reverberant components were eliminated, and we can expect the intelligibility
of the signals to be improved [6].
Figure 5 shows spectrograms of reverberant and dereverberated speech signals when rtime
was 1.0 sec. As shown in the figure, the reverberation of the signal was effectively reduced,
and the formant structure of the signal was restored. Similar spectrogram features were
observed under other reverberation conditions, and an improvement in sound quality could
clearly be recognized by listening to the dereverberated signals [7]. We also evaluated the
quality of the dereverberated speech in terms of speaker dependent word recognition rates
5

[6].

The reverberation curve shows the reduction in the energy of a room impulse response with time

2

Frequency (kHz)

Frequency (kHz)

2

1

0
0

0.4

0.8
Time (sec.)

1.2

1

0
0

0.4

0.8
Time (sec.)

1.2

Figure 5: Spectrogram of reverberant (left) and dereverberated (right) speech of a male
speaker uttering ?ba-ku-da-i?.
with an ASR system, and could achieve more than 95 % recognition rates under all the
reverberation conditions with acoustic models trained using dereverberated speech signals.
Detailed information on the ASR experiments is also provided in [4].

5

Conclusion

A new blind dereverberation principle based on the quasi-periodicity of speech signals was
proposed. We presented two types of dereverberation method, referred to as harmonicity based dereverberation (HERB) method: one estimates the average filter function that
transforms reverberant signals into quasi-periodic signals (ATF-HERB) and the other minimizes the MMSE criterion that evaluates the quasi-periodicity of signals (MMSE-HERB).
We showed that ATF-HERB and a simplified version of MMSE-HERB are both capable
of learning the dereverberation operator that can reduce reverberant components in speech
signals. Experimental results showed that a dereverberation operator trained with 5240
Japanese word utterances could achieve very high quality speech dereverberation. Future
work will include an investigation of how such high quality speech dereverberation can be
achieved with fewer speech data.

References
[1] Baba, A., Lee, A., Saruwatari, H., and Shikano, K., ?Speech recognition by reverberation adapted acoustic model,? Proc. of ASJ general meeting, pp. 27?28, Akita,
Japan, Sep., 2002.
[2] Amari, S., Douglas, S. C., Cichocki, A., and Yang, H. H., ?Multichannel blind deconvolution and equalization using the natural gradient,? Proc. IEEE Workshop on Signal
Processing Advances in Wireless Communications, Paris, pp. 101-104, April 1997.
[3] Yegnanarayana, B., and Murthy, P. S., ?Enhancement of reverberant speech using LP
residual signal,? IEEE Trans. SAP vol. 8, no. 3, pp. 267?281, 2000.
[4] Nakatani, T., Miyoshi, M., and Kinoshita, K., ?Implementation and effects of single
channel dereverberation based on the harmonic structure of speech,? Proc. IWAENC2003, Sep., 2003.
[5] Nakatani, T., and Miyoshi, M., ?Blind dereverberation of single channel speech signal
based on harmonic structure,? Proc. ICASSP-2003, vol. 1, pp. 92?95, Apr., 2003.
[6] Yegnanarayana, B., and Ramakrishna, B. S., ?Intelligibility of speech under nonexponential decay conditions,? JASA, vol. 58, pp. 853?857, Oct. 1975.
[7] http://www.kecl.ntt.co.jp/icl/signal/nakatani/sound-demos/dm/derev-demos.html

"
2283,"Generalized Maximum Margin Clustering and
Unsupervised Kernel Learning

Hamed Valizadegan
Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
valizade@msu.edu

Rong Jin
Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
rongjin@cse.msu.edu

Abstract
Maximum margin clustering was proposed lately and has shown promising
performance in recent studies [1, 2]. It extends the theory of support vector machine to unsupervised learning. Despite its good performance, there
are three major problems with maximum margin clustering that question
its efficiency for real-world applications. First, it is computationally expensive and difficult to scale to large-scale datasets because the number of
parameters in maximum margin clustering is quadratic in the number of
examples. Second, it requires data preprocessing to ensure that any clustering boundary will pass through the origins, which makes it unsuitable for
clustering unbalanced dataset. Third, it is sensitive to the choice of kernel
functions, and requires external procedure to determine the appropriate
values for the parameters of kernel functions. In this paper, we propose
?generalized maximum margin clustering? framework that addresses
the above three problems simultaneously. The new framework generalizes the maximum margin clustering algorithm by allowing any clustering
boundaries including those not passing through the origins. It significantly
improves the computational efficiency by reducing the number of parameters. Furthermore, the new framework is able to automatically determine
the appropriate kernel matrix without any labeled data. Finally, we show a
formal connection between maximum margin clustering and spectral clustering. We demonstrate the efficiency of the generalized maximum margin
clustering algorithm using both synthetic datasets and real datasets from
the UCI repository.

1

Introduction

Data clustering, the unsupervised classification of samples into groups, is an important research area in machine learning for several decades. A large number of algorithms have
been developed for data clustering, including the k-means algorithm [3], mixture models [4],
and spectral clustering [5, 6, 7, 8, 9]. More recently, maximum margin clustering [1, 2] was
proposed for data clustering and has shown promising performance. The key idea of maximum margin clustering is to extend the theory of support vector machine to unsupervised
learning. However, despite its success, the following three major problems with maximum
margin clustering has prevented it from being applied to real-world applications:
? High computational cost. The number of parameters in maximum margin clustering
is quadratic in the number of examples. Thus, it is difficult to scale to large-scale
datasets. Figure 1 shows the computational time (in seconds) of the maximum
margin clustering algorithm with respect to different numbers of examples. We

Time comparision
1600
1400
Generalized Maxmium Marging Clustering
Maximum Margin Clustering

Time (seconds)

1200
1000
800
600
400
200
0
40

60

80

100

120
140
160
Number of Samples

180

200

220

Figure 1: The scalability of the original maximum margin clustering algorithm versus the
generalized maximum margin clustering algorithm
50

2
45

1.8

40

35

Clustering error

1.6

1.4

1.2

1

30

25

20

15

10

0.8

5

0.6
0
10

0.4
0.4

0.6

0.8

1

1.2

1.4

1.6

(a) Data distribution

1.8

2

2.2

20

30

40

50

60

70

80

90

100

Kernel Width (% of data range) in RBF function

(b) Clustering error versus kernel width

Figure 2: Clustering error of spectral clustering using the RBF kernel with different kernel
width. The horizonal axis of Figure 2(b) represents the percentage of the distance range
(i.e., the difference between the maximum and the minimum distance) that is used for kernel
width.
clearly see that the computational time increases dramatically when we apply the
maximum margin clustering algorithm to even modest numbers of examples.
? Requiring clustering boundaries to pass through the origins. One important assumption made by the maximum margin clustering in [1] is that the clustering boundaries
will pass through the origins. To this end, maximum margin clustering requires
centralizing data points around the origins before clustering data. It is important
to note that centralizing data points at the origins does not guarantee clustering
boundaries to go through origins, particularly when cluster sizes are unbalanced
with one cluster significantly more popular than the other.
? Sensitive to the choice of kernel functions. Figure 2(b) shows the clustering error of
maximum margin clustering for the synthesized data of two overlapped Gaussians
clusters (Figure 2(a)) using the RBF kernel with different kernel width. We see that
the performance of maximum margin clustering depends critically on the choice
of kernel width. The same problem is also observed in spectral clustering [10].
Although a number of studies [8, 9, 10, 6] are devote to automatically identifying
appropriate kernel matrices in clustering, they are either heuristic approaches or
require additional labeled data.
In this paper, we propose ?generalized maximum margin clustering? framework that
resolves the above three problems simultaneously. In particular, the proposed framework

reformulates the problem of maximum margin clustering to include the bias term in the
classification boundary, and therefore remove the assumption that clustering boundaries
have to pass through the origins. Furthermore, the new formulism reduces the number
of parameters to be linear in the number of examples, and therefore significantly reduces
the computational cost. Finally, it is equipped with the capability of unsupervised kernel
learning, and therefore, is able to determine the appropriate kernel matrix and clustering
memberships simultaneously. More interestingly, we will show that spectral clustering, such
as the normalized cut algorithm, can be viewed as a special case of the generalized maximum
margin clustering.
The remainder of the paper is organized as follows: Section 2 reviews the work of maximum
margin clustering and kernel learning. Section 3 presents the framework of generalized
maximum margin clustering. Our empirical studies are presented in Section 4. Section 5
concludes this work.

2

Related Work

The key idea of maximum margin clustering is to extend the theory of support vector
machine to unsupervised learning. Given the training examples D = (x1 , x2 , . . . , xn ) and
their class labels y = (y1 , y2 , . . . , yn ) ? {?1, +1}n , the dual problem of support vector
machine can be written as:
1
maxn ?> e ? ?> diag(y)Kdiag(y)?
??R
2
s. t. 0 ? ? ? C, ?> y = 0
(1)
where K ? Rn?n is the kernel matrix and diag(y) stands for the diagonal matrix that
uses the vector y as its diagonal elements. To apply the above formulism to unsupervised
learning, the maximum margin clustering approach relaxes class labels y to continuous
variables, and searches for both y and ? that maximizes the classification margin. This
leads to the following optimization problem:
min

y,?,?,?

s. t.

t
?

(yy> ) ? K
(e + ? ? ? + ?y)>
? ? 0, ? ? 0

e + ? ? ? + ?y
t ? 2C? > e

?
?0

where ? stands for the element wise product between two matrices. To convert the above
problem into a convex programming problem, the authors of [1] makes two important relaxations. The first one relaxes yy> into a positive semi-definitive (PSD) matrix M ? 0 whose
diagonal elements are set to be 1. The second relaxation sets ? = 0, which is equivalent to
assuming that there is no bias term b in the expression of classification boundaries, or in
other words, classification boundaries have to pass through the origins of data. These two
assumption simplify the above optimization problem as follows:
min

M,?,?

s. t.

t
?

M ?K
(e + ? ? ?)>

e+???
t ? 2C? > e

? ? 0, ? ? 0, M ? 0

?
?0
(2)

Finally, a few additional constraints of M are added to the above optimization problem to
prevent skewed clustering sizes [1]. As a consequence of these two relaxations, the number
of parameters is increased from n to n2 , which will significantly increase the computational
cost. Furthermore, by setting ? = 0, the maximum margin clustering algorithm requires
clustering boundaries to pass through the origins of data, which is unsuitable for clustering
data with unbalanced clusters.
Another important problem with the above maximum margin clustering is the difficulty
in determining the appropriate kernel similarity matrix K. Although many kernel based
clustering algorithms set the kernel parameters manually, there are several studies devoted
to automatic selection of kernel functions, in particular the kernel width for the RBF kernel,

?
?
kx ?x k2
i.e., ? in exp ? i2?2j 2 . Shi et al. [8] recommended choosing the kernel width as 10% to
20% of the range of the distance between samples. However, in our experiment, we found
that this is not always a good choice, and in many situations it produces poor results. Ng
et al. [9] chose kernel width which provides the least distorted clusters by running the same
clustering algorithm several times for each kernel width. Although this approach seems to
generate good results, it requires running seperate experiments for each kernel width, and
therefore could be computationally intensive. Manor et al. in [10] proposed a self-tuning
spectral clustering algorithm that computes a different local kernel width for each data point
xi . In particular, the local kernel width for each xi is computed as the distance of xi to
its kth nearest neighbor. Although empirical study seems to show the effectiveness of this
approach, it is unclear how to find the optimal k in computing the local kernel width. As
we will see in the experiment section, the clustering accuracy depends heavily on the choice
of k.
Finally, we will briefly overview the existing work on kernel learning. Most previous work
focus on supervised kernel learning. The representative approaches in this category include
the kernel alignment [11, 12], semi-definitive programming [13], and spectral graph partitioning [6]. Unlike these approaches, the proposed framework is designed for unsupervised
kernel learning.

3

Generalized Maximum Margin Clustering and Unsupervised
Kernel Learning

We will first present the proposed clustering algorithm for hard margin, followed by the
extension to soft margin and unsupervised kernel learning.
3.1

Hard Margin

In the case of hard margin, the dual problem of SVM is almost identical to the problem in
Eqn. (1) except that the parameter ? does not have the upper bound C. Following [13], we
further convert the problem in (1) into its dual form:
1
(e + ? + ?y)T diag(y)K ?1 diag(y)(e + ? + ?y)
min
? ,y,? 2
s. t. ? ? 0, y ? {+1, ?1}n
(3)
where e is a vector with all its elements being one. Unlike the treatment in [13], which
rewrites the above problem as a semi-definitive programming problem, we introduce variables z that is defined as follows:
z = diag(y)(e + ?)
Given that ? ? 0, the above expression for z is essentially equivalent to the constraint
|zi | ? 1 or zi2 ? 1 for i = 1, 2, . . . , n. Then, the optimization problem in (3) is rewritten as
follows:
1
(z + ?e)T K ?1 (z + ?e)
min
z,?
2
s. t. zi2 ? 1, i = 1, 2, . . . , n
(4)
Note that the above problem may not have unique solutions for z and ? due to the translation
invariance of the objective function. More specifically, given an optimal solution z and ?,
we may be able to construct another solution z0 and ?0 such that:
z0 = z + ?e, ?0 = ? ? ?.
Evidently, both solutions result in the same value for the objective function in (4). Furthermore, with appropriately chosen ?, the new solution z0 and ?0 will be able to satisfy the
constraint zi2 ? 1. Thus, z0 and ?0 is another optimal solution for (3). This is in fact related
to the problem in SVM where the bias term b may not be unique [14]. To remove the translation invariance from the objective function, we introduce an additional term Ce (z> e)2
into the objective function, i.e.
1
(z + ?e)T K ?1 (z + ?e) + Ce (z> e)2
min
z,?
2
s. t. zi2 ? 1, i = 1, 2, . . . , n
(5)

where constant Ce weights the important of the punishment factor against the original
objective. It is set to be 10, 000 in our experiment. For the simplicity of our expression, we
further define
w = (z; ?) and P = (In , e).
Then, the problem in (4) becomes
2
min
wT P T K ?1 P w + Ce (e>
0 w)
n+1
w?R

s. t.
wi2 ? 1, i = 1, 2, . . . , n
(6)
where e0 is a vector with all its elements being 1 except its last element which is zero. We
then construct the Lagrangian as follows
n
X
2
i
L(w, ?) = wT P T K ?1 P w + Ce (e>
?i (w> In+1
w ? 1)
0 w) ?
i=1

?
= w

>

T

P K

?1

P+

C e e0 e>
0

?

n
X

i
?i In+1

!
w+

i=1

n
X

?i

i=1

i
where In+1
is an (n + 1) ? (n + 1) matrix with all the elements being zero except the ith
diagonal element which is 1. Hence, the dual problem of (6) is
n
X
?i
maxn
??R

s. t.

i=1
T

P K

?1

P+

Ce e0 e>
0

?

n
X

i
?i In+1
?0

i=1

?i ? 0, i = 1, 2, . . . , n
Finally, the solution?w can be computed using the KKT
! condition, i.e.,
n
X
i
P T K ?1 P + Ce e0 e>
?i In+1
w = 0n+1
0 ?

(7)

i=1

In
words, the Psolution w
? is proportional to the eigenvector of matrix
? T other
n
i
?
?
I
P K ?1 P + Ce e0 e>
i
n+1 for the zero eigenvalue. Since wi = (1 + ?i )yi , i =
0
i=1
1, 2, . . . , n and ?i ? 0, the class labels {yi }ni=1 can be inferred directly from the sign of
{wi }ni=1 .
Remark I It is important to realize that the problem in (5) is non-convex due to the nonconvex constraint wi2 ? 1. Thus, the optimal solution found by the dual problem in (7) is
not necessarily the optimal solution for the prime problem in (5). Our hope is that although
the solution found by the dual problem is not optimal for the prime problem, it is still a
good solution for the prime problem in (5). This is similar to the SDP relaxation made by
the maximum margin clustering algorithm in (2) that relaxes a non-convex programming
problem into a convex one. However, unlike the relaxation made in (2) that increases the
number of variables from n to n2 , the new formulism of maximum margin does not increase
the number of parameters (i.e., ?), and therefore will be computational more efficient. This
is shown in Figure 1, in which the computational time of generalized maximum margin
clustering is increased much slower than that of the maximum margin algorithm.
Remark II To avoid the high computational cost in estimating K ?1 , we replace K ?1 with
1/2
its normalized graph Laplacian L(K) [15], which is defined as L(K) = I ?D1/2
where
PKD
n
D is a diagonal matrix whose diagonal elements are computed as Di,i = j=1 Ki,j , i =
? = L(K)? where ? stands for the
1, 2, . . . , n. This is equivalent to defining a kernel matrix K
operator of pseudo inverse. More interesting, we have the following theorem showing the
relationship between generalized maximum margin clustering and the normalized cut.
Theorem 1. The normalized cut algorithm is a special case of the generalized maximum
margin clustering in (7) if the following conditions hold, i.e., (1) K ?1 is set to be the
?
normalized Laplacian L(K),
(2) all the ?s are enforced to be the same, i.e., ?i = ?0 , i =
1, 2, . . . , n, and (3) Ce ? 1.
Proof sketch: Given the conditions 1 to 3 in the theorem, the new objective function in (7)
?
becomes: max ? s.t. L(K)
? ?In and the solution for this problem is the largest eigenvector
?
of L(K).

??0

3.2

Soft Margin

We extend the formulism in (7) to the case of soft margin by considering the following
problem:
n
X
1
(e + ? ? ? + ?y)T diag(y)K ?1 diag(y)(e + ? ? ? + ?y) + C?
min
?i2
? ,y,?,? 2
i=1
s. t. ? ? 0, ? ? 0, y ? {+1, ?1}n
(8)
where C? weights the importance of the clustering errors against the clustering margin.
Similar to the previous derivation, we introduce the slack variable z and simplify the above
problem as follows:
n
X
1
(z + ?e)T K ?1 (z + ?e) + Ce (z> e)2 + C?
min
?i2
z,?,?
2
i=1
s. t. (zi + ?i )2 ? 1, ?i ? 0, i = 1, 2, . . . , n
(9)
By approximating (zi + ?i )2 as zi2 + ?i2 , we have the dual form of the above problem written
as:
n
X
maxn
?i
??R

s. t.

i=1

P > K ?1 P + Ce e0 e>
0 ?

n
X

i
?0
?i In+1

i=1

0 ? ?i ? C? , i = 1, 2, . . . , n
(10)
The main difference between the above formulism and the formulism in (7) is the introduction of the upper bound C? for ? in the case of soft margin. In the experiment, we set the
parameter C? to be 100, 000, a very large value.
3.3

Unsupervised Kernel Learning

As already pointed out, the performance of many clustering algorithms depend on the right
choice of the kernel similarity matrix. To address this problem, we extend the formulism
in (10) by including the kernel learning mechanism. In particular, we assume that a set of
m kernel similarity matrices K1 , K2 , . . . , Km
available. Our goal is to identify the linear
Pare
m
combination of kernel matrices, i.e., K = i=1 ?i Ki , that leads to the optimal clustering
accuracy. More specifically, we need to solve the following optimization problem:
n
X
max
?i
?,?

s. t.

i=1

P

>

?m
X

!?1
?i Ki

P + C e e0 e>
0 ?

i=1

n
X

i
?i In+1
?0

i=1

0 ? ?i ? C? , i = 1, 2, . . . , n,

m
X

?i = 1, ?i ? 0, i = 1, 2, . . . , m

(11)

i=1

Unfortunately,
it is difficult to solve the above problem due to the complexity introduced
Pm
by ( i=1 ?i Ki )?1 . Hence, we consider an alternative problem to the above one. We first
?1, L
?2, . . . , L
? m . Each Laplacian Li is conintroduce a set of normalized graph Laplacian L
structed from the P
kernel similarity matrix Ki . We then defined the inverse of the combined
m
? i . Then, we have the following optimization problem
matrix as K ?1 = i=1 ?i L
n
X
max
?i
?,?

s. t.

i=1
m
X
i=1

? i P + C e e0 e>
?i P > L
0 ?

n
X

i
?i In+1
?0

i=1

0 ? ?i ? C? , i = 1, 2, . . . , n,

m
X
i=1

?i = 1, ?i ? 0, i = 1, 2, . . . , m

(12)

2

0.6
1

1.8

0.4
1.6
0.5

0.2

1.4
0

0

1.2

?0.2

1

?0.5

0.8

?0.4
?1

0.6

?0.6
0.4

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

2.2

(a) Overlapped Gaussian

?1

?0.5

0

0.5

(b) Two Circles

1

?0.6

?0.4

?0.2

0

0.2

0.4

0.6

(c) Two Connected Circles

Figure 3: Data distribution of the three synthesized datasets
By solving the above problem, we are able to resolve both ? (corresponding to clustering
memberships) and ? (corresponding to kernel learning) simultaneously.

4

Experiment

We tested the generalized maximum margin clustering algorithm on both synthetic datasets
and real datasets from the UCI repository. Figure 3 gives the distribution of the synthetic
datasets. The four UCI datasets used in our study are ?Vote?, ?Digits?, ?Ionosphere?, and
?Breast?. These four datasets comprise of 218, 180, 351, and 285 examples, respectively,
and each example in these four datasets is represented by 17, 64, 35, and 32 features. Since
the ?Digits? dataset consists of multiple classes, we further decompose it into four datasets
of binary classes that include pairs of digits difficult to distinguish. Both the normalized cut
algorithm [8] and the maximum margin clustering algorithm [1] are used as the baseline.
The RBF kernel is used throughout this study to construct the kernel similarity matrices.
In our first experiment, we examine the optimal performance of each clustering algorithm
by using the optimal kernel width that is acquired through an exhaustive search. The optimal clustering errors of these three algorithms are summarized in the first three columns of
Table 1. It is clear that generalized maximum margin clustering algorithm achieve similar
or better performance than both maximum margin clustering and normlized cut for most
datasets when they are given the optimal kernel matrices. Note that the results of maximum margin clustering are reported for a subset of samples(including 80 instances) in UCI
datasets due to the out of memory problem.
Table 1: Clustering error (%) of normalized cut (NC), maximum margin clustering (MMC),
generalized maximum margin clustering (GMMC) and self-tuning spectral clustering (ST).
Dataset
Optimal Kernel Width
Unsupervised Kernel Learning
NC MMC GMMC GMMC ST (Best k) ST(Worst k)
Two Circles
2
0
0
0
0
50
7
6.25
0
0
1
45
Two Jointed Circles
1.25
2.5
1.25
3.75
5
7.5
Two Gaussian
25
15
9.6
11.90
11
40
Vote
35
10
5.6
5.6
5
50
Digits 3-8
45
31.25
2.2
3
0
47
Digits 1-7
34
1.25
.5
5.6
1.5
50
Digits 2-7
48
3.75
16
12
9
48
Digits 8-9
25
21.25
23.5
27.3
26.5
48
Ionosphere
36.5 38.75
36.1
37
37.5
41.5
Breast
In the second experiment, we evaluate the effectiveness of unsupervised kernel learning. Ten
kernel matrices are created by using the RBF kernel with the kernel width varied from 10%
to 100% of the range of distance between any two examples. We compare the proposed
unsupervised kernel learning to the self-tuning spectral clustering algorithm in [10]. One
of the problem with the self-tuning spectral clustering algorithm is that its clustering error
usually depends on the parameter k, i.e., the number of nearest neighbor used for computing
the kernel width. To provide a full picture of the self-tuning spectral clustering, we vary k
from 1 and 15 , and calculate both best and worst performance using different k. The last
three columns of Table 1 summarizes the clustering errors of generalized maximum margin

clustering and self-tuning spectral clustering with both best and worst k. First, observe
the big gap between best and worst performance of self-tuning spectral clustering with
different choice of k, which implies that this algorithm is sensitive to parameter k. Second,
for most datasets, generalized maximum margin clustering achieves similar performance as
self-tuning spectral clustering with the best k. Furthermore, for a number of datasets, the
unsupervised kernel learning method achieves the performance close to the one using the
optimal kernel width. Both results indicate that the proposed algorithm for unsupervised
kernel learning is effective in identifying appropriate kernels.

5

Conclusion

In this paper, we proposed a framework for the generalized maximum margin clustering.
Compared to the existing algorithm for maximum margin clustering, the new framework
has three advantages: 1) it reduces the number of parameters from n2 to n, and therefore
has a significantly lower computational cost, 2) it allows for clustering boundaries that do
not pass through the origin, and 3) it can automatically identify the appropriate kernel
similarity matrix through unsupervised kernel learning. Our empirical study with three
synthetic datasets and four UCI datasets shows the promising performance of our proposed
algorithm.

References
[1] L. Xu, J. Neufeld, B. Larson, and D. Schuurmans. Maximum margin clustering. In
Advances in Neural Information Processing Systems (NIPS) 17, 2004.
[2] L. Xu and D. Schuurmans. Unsupervised and semi-supervised multi-class support vector machines. In Proceedings of the 20th National Conference on Artificial Intelligence
(AAAI-05)., 2005.
[3] J. Hartigan and M. Wong. A k-means clustering algorithm. Appl. Statist., 28:100?108,
1979.
[4] R. A. Redner and H. F. Walker. Mixture densities, maximum likelihood and the em
algorithm. SIAM Review, 26:195?239, 1984.
[5] C. Ding, X. He, H. Zha, M. Gu, and H. Simon. A min-max cut algorithm for graph
partitioning and data clustering. In Proc. IEEE Int?l Conf. Data Mining, 2001.
[6] F. R. Bach and M. I. Jordan. Learning spectral clustering. In Advances in Neural
Information Processing Systems 16, 2004.
[7] R. Jin, C. Ding, and F. Kang. A probabilistic approach for optimizing spectral clustering. In Advances in Neural Information Processing Systems 18, 2006.
[8] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 22(8):888?905, 2000.
[9] A. Ng, M. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm.
In Advances in Neural Information Processing Systems 14, 2001.
[10] L. Zelnik-Manor and P. Perona. Self-tuning spectral clustering. In Advances in Neural
Information Processing Systems 17, pages 1601?1608, 2005.
[11] N. Cristianini, J. Shawe-Taylor, A. Elisseeff, and J. S. Kandola. On kernel-target
alignment. In NIPS, pages 367?373, 2001.
[12] X. Zhu, J. Kandola, Z. Ghahramani, and J. Lafferty. Nonparametric transforms of
graph kernels for semi-supervised learning. In Advances in Neural Information Processing Systems 17, pages 1641?1648, 2005.
[13] G. R. G. Lanckriet, N. Cristianini, P. L. Bartlett, Laurent El Ghaoui, and Michael I.
Jordan. Learning the kernel matrix with semidefinite programming. Journal of Machine
Learning Research, 5:27?72, 2004.
[14] C. J. C. Burges and D. J. Crisp. Uniqueness theorems for kernel methods. Neurocomputing, 55(1-2):187?220, 2003.
[15] F.R.K. Chung. Spectral Graph Theory. Amer. Math. Society, 1997.

"
3245,"Chaitin-Kolmogorov Complexity
and Generalization in Neural Networks

Barak A. Pearlmutter
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213

Ronald Rosenfeld
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213

Abstract
We present a unified framework for a number of different ways of failing
to generalize properly. During learning, sources of random information
contaminate the network, effectively augmenting the training data with
random information. The complexity of the function computed is therefore
increased, and generalization is degraded. We analyze replicated networks,
in which a number of identical networks are independently trained on the
same data and their results averaged. We conclude that replication almost
always results in a decrease in the expected complexity of the network, and
that replication therefore increases expected generalization. Simulations
confirming the effect are also presented.

1

BROKEN SYMMETRY CONSIDERED HARMFUL

Consider a one-unit backpropagation network trained on exclusive or. Without
hidden units, the problem is insoluble. One point where learning would stop is
when all weights are zero and the output is always ~, resulting in an mean squared
error of ~. But this is a saddle point; by placing the discrimination boundary
properly, one point can be gotten correctly, two with errors of ~, and one with error
of
giving an MSE of i, as shown in figure 1.

i,

Networks are initialized with small random weights, or noise is injected during training to break symmetries of this sort. But in breaking this symmetry, something has
been lost. Consider a kNN classifier, constructed from a kNN program and the
training data. Anyone who has a copy of the kNN program can construct an identical classifier if they receive the training data. Thus, considering the classification

925

926

Pearlmutter and Rosenfeld
as an abstract entity, we know its complexity cannot exceed that of the training
data plus the overhead of the complexity of the program, which is fixed.
But this is not necessarily the case for the backpropagation network we saw! Because of the introduction of randomly broken symmetries, the complexity of the
classification itself can exceed that of the training data plus the learning procedure.
Thus an identical classifier can no longer be constructed just from the program and
the training data, because random factors have been introduced. For a striking
example, consider presenting a ""32 bit parity with 10,000 exceptions"" stochastic
learner with one million exemplars. The complexity of the resulting function will
be high, since in order to specify it we must specify not only the regularities of
training set, which we just did in a couple words, but also which of the 4 billion
possibilities are among the 10,000 exceptions.
Applying this idea to undertraining and overtraining, we see that there are two kinds
of symmetries that can be broken. First, if not all the exemplars can be loaded,
which of the outliers are not loaded can be arbitrary. Second, underconstrained
networks that behave the same on the training set may behave differently on other
inputs. Both phenomena can be present simultaneously.

2

A COMPLEXITY BOUND

The expected value of the complexity of the function implemented by a network b
trained on data d, where b is a potentially stochastic mapping, satisfies

E(C(b(d)))

~

C(d)

+ C(b) + I(b(d)ld)

where I(b(d)ld) is the negative of the entropy of the bias distribution of b trained
on d,
I(b(d)ld) -H(b(d))
log P(b(d)
f)

=

=- L

=

f

where f ranges over functions that the network could end up performing, with
the network regarded as a black box. This in turn is bounded by the information
contained in the random internal parameters, or by the entropy of the watershed
structure; but these are both potentially unbounded.
A number of techniques for improving generalization, when viewed in this light,
work because they tighten this bound.
? Weight decay [2] and the statistical technique of ridge regression impose an
extra constraint on the parameters, reducing their freedom to arbitrarily break
symmetry when underconstrained.
? Cross validation attempts to stop training before too many symmetries have
been broken.
? Efforts to find the perfect number of hidden units attempt to minimize the
number of symmetries that must be broken .
These techniques strike a balance between undertraining and overtraining. Since in
any realistic domain both of these effects will be simultaneously present, it would
seem advantageous to attack the problem at the root. One approach that has been

Chaitin-Kolmogorov Complexity and Generalization in Neural Networks

3

:

0

@

0

@

+

t!

+
+

+

?
? ?

+:.+ + + +
......:.+++.
?
+.... + +

~""*

?

........"" ""

@

??

?

+

112

t? .~+,:. t

?? +

,

.. ,

.,

?? ++...,+ + +
? +

o ~ .....?_... ~_,;.;.~~_/:.:....t:~ .. _... ~ .. _
?? 1+:

@

+

0

.....

??t+~:. +++++.;....

??

? ..: *.+

+ +
+

?? tot""'+.+

0

?

..:

??

~
~
Figure 1: The bifurcation of a perceptron trained on xor.

?

it

?
-3

+
+

+

++

? ? + : +*

?

+

+

+
+

++
+

~----------~----------~

-3

o

3

Figure 2: The training set. Crosses
are negative examples and diamonds
are positive examples.

rediscovered a number of times [1, 3], and systematically explored in its pure form
by Lincoln and Skrzypek [4], is that of replicated networks.

3

REPLICATED NETWORKS

One might think that the complexity of the average of a collection of networks would
be the sum of the complexities of the components; but this need not be the case.
Consider an ensemble network, in which an infinite number of networks are taught
the training data simultaneously, each making its random decisions according to
whatever distributions the training procedure calls for, and their output averaged.
We have seen that the complexity of a single network can exceed that of its training
data plus the training program. But this is not the case with ensemble networks,
since the ensemble network output can be determined solely from the program and
the training data, i.e. C(E(b(d))) ~ C(b)+C(d)+C(""replicate"") where C(""replicate"")
is the complexity of the instruction to replicate and average (a small constant).
A simple way to approximate the ensemble machine is to train a number of networks
simultaneously and average the results. As the number of networks is increased,
the composite model approaches the ensemble network, which cannot have higher
complexity than the training data plus the program plus the instruction to replicate.
Note that even if one accidentally stumbles across the perfect architecture and

927

928

Pearlmutter and Rosenfeld
training regime, resulting in a net that always learns the training set perfectly but
with no leftover capacity, and which generalizes as well as anything could, then
making a replicated network can't hurt, since all the component networks would do
exactly the same thing anyway.
A number of researchers seem to have inadvertently exploited this fact. For instance,
Hampshire et al. [1] train a number of networks on a speech task, where the networks
differed in choice of objective function. The networks' outputs were averaged to
form the answer used in the recognition phase, and the generalization performance
of the composite network was significantly higher than that of any of its component
networks. Replicated implementations programmed from identical specifications is
a common technique in software engineering of highly reliable systems.

4

THE ISSUE OF INDUCTIVE BIAS

The representational power of an ensemble is greater that that of a single network.
By the usual logic, one would expect the ensemble to have worse generalization,
since its inductive bias is weaker. Counterintuitively, this is not the case. For
instance, the VC dimension of an ensemble of perceptrons is infinite, because it can
implement an arbitrary three layer network, using replication to implement weights.
This is much greater than the finite VC dimension of a single perceptron within the
ensemble, but our analysis predicts better generalization for the ensemble than for
a single stochastic perceptron when the bounds are tight, that is, when
H(b(d)) ~ C(""replicate"").
(1)
This leads to the conclusion that just knowing the inductive bias of a learner is
not enough information to make strong conclusions about its expected generalization. Thus, distribution free results based purely on the inductive bias, such as VC
dimension based PAC learning theory [5], may sometimes be unduly pessimistic.
As to replicated networks, we have seen that they can not help but improve generalization when (1) holds. Thus, if one is training the same network over and over,
perhaps with slightly different training regimes, and getting worse generalization
than was hoped for, but on different cases each time, then one can improve generalization in a seemingly principled manner by putting all the trained networks in a
box and calling it a finite sample of the ensemble network (and perhaps buying a
bigger computer to run it on).

5

EMPIRICAL SUPPORT

We conducted the following experiment: 17 standard backpropagation networks
(Actually 20, but 3 were lost to a disk failure) were trained on a binary classification
task. The nets all had identical architectures (2-20-1) but different initial weights,
chosen uniformly from the interval [-1, 1]. The same training set was used to train
all the networks. The fl:nctions implemented by each of the networks were then
calculated in detail, and the performance of individual networks compared to that
of their ensemble.
The classification task was a stochastic 2D linear discriminator. Each point was
obtained from a Gaussian centered at (0.0) with stdev 1. A classification of 1 was

Chaitin-Kolmogorov Complexity and Generalization in Neural Networks

I

...? J

,
,

?

.. t h,{??>

I ?

I
. ~

/
.'

~

l ?~
Figure 3: The functions implemented by the 17 trained networks, and by their
average (bottom right). Both the x and y axes run from -3 to 3, and grey levels are
used to represent intermediate values in the interval [0,1].

929

930

Pearlmutter and Rosenfeld
Table 1: Mean squared error and number of mislabeled exemplars for each network
on the training set of 200.
net
12
9
16
5
7
10
13
17
19
6
15
18
8
11
20
14
4
mean
ensemble
nohidden

MSE
0.0150837
0.0200039
0.0200026
0.0250207
0.0250213
0.0228319
0.0250156
0.0250018
0.0175466
0.0300099
0.0300075
0.0300060
0.0350609
0.0350006
0.0400013
0.0305254
0.0408391
0.027469 ? 0.007226
0.016286
0.060314

errors
3
4
4
5
5
5
5
5
5
6
6
6
7
7
8
9
13
6.058824
4
31

***

****
****
*****
*****
*****
*****
*****
*****
******
******
******
*******
*******
********
*********
*************
? 2.261457
**""''''

assigned to points with z ~ 0, and 0 to points with z < 0, but reversed with an
independent probability of 0.1. The final position of each point was then determined
by adding a zero mean Gaussian with stdev .25. 200 points were so generated for
the training set (shown in figure 2) and another 1000 points for the test set.
Looking at figure 3, each net appears to correctly classify as many of the inputs
as possible, within the bounds imposed on it by its inductive bias. Each function
implemented by such a net is roughly equivalent to a linear combination of 20
independent linear discriminators. It is therefore clear why each map consists of
regions delineated by up to 20 straight lines. Since the initial conditions were
different for each net, so were the resultant regions. All networks misclassified some
of the exemplars (see table 1), but the missclassifications were different for each
network, illustrating symmetry breaking due to an overconstraining data set.
Note that the ensemble's performance on the training set is comparable to that of
the best of the trained networks, while its performance on the test set is far superior.
The MSE error of the ensemble is much much better than the bound obtained from
Jensen's inequality, the average MSE. In fact, the ensemble network gets a lower
MSE than all but one individual network on the training sets, and a much lower
MSE than any individual network on the test set; and it generalizes much better
than any of the individual networks by a misclassification count metric.

Chaitin-Kolmogorov Complexity and Generalization in Neural Networks
Table 2: Mean squared error and number of
mislabeled samples for each network on the
test set of 1000. The performance of a theoretically perfect classifier (sign x) on the test set
is 170 misclassifications, which is about what
the network without hidden units gets.
net
16
9
4
5
11
15
6
19
7
8
12
17
18
20
13
14
10
mean
ensemble
nohidden

MSE
0.201
0.207
0.206
0.209
0.208
0.207
0.212
0.213
0.214
0.214
0.212
0.219
0.220
0.223
0.223
0.227
0.226
0.214 ? 0.007
0.160
0.0715

errors
205
213
215
216
216
216
219
220
222
224
225
225
227
229
231
237
254
223 ? 10.7
200
169

Table 3: Histogram of the networks' performance by number of
misclassified training exemplars.

I error count I networks I
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

*
**
******
***
**

*

*

*

References
[1] J. Hampshire and A. Waibel. A novel objective function for improved phoneme
recognition using time delay neural networks. Technical Report CMU-CS-89118, Carnegie Mellon University School of Computer Science, March 1989.
[2] Geoffrey E. Hinton, Terrence J. Sejnowski, and David H. Ackley. Boltzmann
Machines: Constraint satisfaction networks that learn. Technical Report CMUCS-84-119, Carnegie-Mellon University, May 1984.
[3] Nathan Intrator. A neural network for feature extraction. In D. S. Touretzky,
editor, Advances in Neural Information Processing Systems 2, pages 719-726,
San Mateo, CA, 1990. Morgan Kaufmann.
[4] Willian P. Lincoln and Josef Skrzypek. Synergy of clustering multiple back propagation networks. In D. S. Touretzky, editor, Advances in Neural Information
Processing Systems 2: pages 650-657, San Mateo, CA, 1990. Morgan Kaufmann.
[5] L. G. Valiant. A theory of the learnable. Communications of the ACM,
27(11):1134-1142, 19~4.

931

"
4316,"Convex Calibrated Surrogates for Low-Rank Loss
Matrices with Applications to Subset Ranking Losses
Harish G. Ramaswamy
Shivani Agarwal
Computer Science & Automation Computer Science & Automation
Indian Institute of Science
Indian Institute of Science
harish gurup@csa.iisc.ernet.in
shivani@csa.iisc.ernet.in

Ambuj Tewari
Statistics and EECS
University of Michigan
tewaria@umich.edu

Abstract
The design of convex, calibrated surrogate losses, whose minimization entails
consistency with respect to a desired target loss, is an important concept to have
emerged in the theory of machine learning in recent years. We give an explicit
construction of a convex least-squares type surrogate loss that can be designed to
be calibrated for any multiclass learning problem for which the target loss matrix
has a low-rank structure; the surrogate loss operates on a surrogate target space
of dimension at most the rank of the target loss. We use this result to design
convex calibrated surrogates for a variety of subset ranking problems, with target
losses including the precision@q, expected rank utility, mean average precision,
and pairwise disagreement.

1

Introduction

There has been much interest in recent years in understanding consistency properties of learning
algorithms ? particularly algorithms that minimize a surrogate loss ? for a variety of ?nite-output
learning problems, including binary classi?cation, multiclass classi?cation, multi-label classi?cation, subset ranking, and others [1?17]. For algorithms minimizing a surrogate loss, the question
of consistency reduces to the question of calibration of the surrogate loss with respect to the target
loss of interest [5?7, 16]; in general, one is interested in convex surrogates that can be minimized
ef?ciently. In particular, the existence (and lack thereof) of convex calibrated surrogates for various
subset ranking problems, with target losses including for example the discounted cumulative gain
(DCG), mean average precision (MAP), mean reciprocal rank (MRR), and pairwise disagreement
(PD), has received signi?cant attention recently [9, 11?13, 15?17].
In this paper, we develop a general result which allows us to give an explicit convex, calibrated
surrogate de?ned on a low-dimensional surrogate space for any ?nite-output learning problem for
which the loss matrix has low rank. Recently, Ramaswamy and Agarwal [16] showed the existence
of such surrogates, but their result involved an unwieldy surrogate space, and moreover did not give
an explicit, usable construction for the mapping needed to transform predictions in the surrogate
space back to the original prediction space. Working in the same general setting as theirs, we give
an explicit construction that leads to a simple least-squares type surrogate. We then apply this
result to obtain several new results related to subset ranking. Speci?cally, we ?rst obtain calibrated,
score-based surrogates for the Precision@q loss, which includes the winner-take-all (WTA) loss as
a special case, and the expected rank utility (ERU) loss; to the best of our knowledge, consistency
with respect to these losses has not been studied previously in the literature. When there are r
documents to be ranked for each query, the score-based surrogates operate on an r-dimensional
surrogate space. We then turn to the MAP and PD losses, which are both widely used in practice, and
for which it has been shown that no convex score-based surrogate can be calibrated for all probability
distributions [11, 15, 16]. For the PD loss, Duchi et al. [11] gave certain low-noise conditions on the
probability distribution under which a convex, calibrated score-based surrogate could be designed;
1

we are unaware of such a result for the MAP loss. A straightforward application of our low-rank
result to these losses yields convex calibrated surrogates de?ned on O(r2 )-dimensional surrogate
spaces, but in both cases, the mapping needed to transform back to predictions in the original space
involves solving a computationally hard problem. Inspired by these surrogates, we then give a
convex score-based surrogate with an ef?cient mapping that is calibrated with respect to MAP under
certain conditions on the probability distribution; this is the ?rst such result for the MAP loss that
we are aware of. We also give a family of convex score-based surrogates calibrated with the PD
loss under certain noise conditions, generalizing the surrogate and conditions of Duchi et al. [11].
Finally, we give an ef?cient mapping for the O(r2 )-dimensional surrogate for the PD loss, and show
that this leads to a convex surrogate calibrated with the PD loss under a more general condition, i.e.
over a larger set of probability distributions, than those associated with the score-based surrogates.
Paper outline. We start with some preliminaries and background in Section 2. Section 3 gives our
primary result, namely an explicit convex surrogate calibrated for low-rank loss matrices, de?ned on
a surrogate space of dimension at most the rank of the matrix. Sections 4?7 then give applications
of this result to the Precision@q, ERU, MAP, and PD losses, respectively. All proofs not included
in the main text can be found in the appendix.

2

Preliminaries and Background

Setup. We work in the same general setting as that of Ramaswamy and Agarwal [16]. There is an
instance space X , a ?nite set of class labels Y = [n] = {1, . . . , n}, and a ?nite set of target labels
(possible predictions) T = [k] = {1, . . . , k}. Given training examples (X1 , Y1 ), . . . , (Xm , Ym )
drawn i.i.d. from a distribution D on X ?Y, the goal is to learn a prediction model h : X ?T . Often,
T = Y, but this is not always the case (for example, in the subset ranking problems we consider,
the labels in Y are typically relevance vectors or preference graphs over a set of r documents, while
the target labels in T are permutations over the r documents). The performance of a prediction
model h : X ?T is measured via a loss function ? : Y ? T ?R+ (where R+ = [0, ?)); here ?(y, t)
denotes the loss incurred on predicting t ? T when the label is y ? Y. Speci?cally, the goal is
to learn a model h with low expected loss or ?-error er?D [h] = E(X,Y )?D [?(Y, h(X))]; ideally, one
?
wants the ?-error of the learned model to be close to the optimal ?-error er?,?
D = inf h:X ?T erD [h].
An algorithm which when given a random training sample as above produces a (random) model
hm : X ?T is said to be consistent w.r.t. ? if the ?-error of the learned model hm converges in
P
1
probability to the optimal: er?D [hm ] ?
? er?,?
D .
Typically, minimizing the discrete ?-error directly is computationally dif?cult; therefore one uses
? + (where R
? + = [0, ?]), de?ned on the continuous
instead a surrogate loss function ? : Y ? Rd ?R
d
surrogate target space R for some d ? Z+ instead of the discrete target space T , and learns a
model f : X ?Rd by minimizing (approximately, based on the training sample) the ?-error er?
D [f ] =
E(X,Y )?D [?(Y, f (X))]. Predictions on new instances x ? X are then made by applying the learned
model f and mapping back to predictions in the target space T via some mapping pred : Rd ?T ,
giving h(x) = pred(f (x)). Under suitable conditions, algorithms that approximately minimize the
?-error based on a training sample are known to be consistent with respect to ?, i.e. to converge in
?
probability to the optimal ?-error er?,?
D = inf f :X ?Rd erD [f ]. A desirable property of ? is that it be
calibrated w.r.t. ?, in which case consistency w.r.t. ? also guarantees consistency w.r.t. ?; we give a
formal de?nition of calibration and statement of this result below.
?
In what follows, we will denote by ?n the probability simplex in Rn : ?n = {p ? Rn+ : i pi = 1}.
For z ? R, let (z)+ = max(z, 0). We will ?nd it convenient to view the loss function ? : Y ?T ?R+
as an n ? k matrix with elements ?yt = ?(y, t) for y ? [n], t ? [k], and column vectors ?t =
?+
(?1t , . . . , ?nt )? ? Rn+ for t ? [k]. We will also represent the surrogate loss ? : Y ? Rd ?R
d
n
d
?
as a vector function ? : R ?R+ with ?y (u) = ?(y, u) for y ? [n], u ? R , and ?(u) =
? n for u ? Rd .
(?1 (u), . . . , ?n (u))? ? R
+
?+
De?nition 1 (Calibration). Let ? : Y ? T ?R+ and let P ? ?n . A surrogate loss ? : Y ? Rd ?R
d
is said to be calibrated w.r.t. ? over P if there exists a function pred : R ?T such that
p? ?(u) > inf p? ?(u) .
?p ? P :
inf
?
u?Rd :pred(u)?argmin
/
t p ?t

1

P

P

u?Rd

Here ?
? denotes convergence in probability: Xm ?
? a if ?? > 0, P(|Xm ? a| ? ?) ? 0 as m ? ?.

2

In this case we also say (?, pred) is (?, P)-calibrated, or if P = ?n , simply ?-calibrated.
? + . Then ? is calibrated w.r.t. ?
Theorem 2 ( [6, 7, 16]). Let ? : Y ? T ?R+ and ? : Y ? Rd ?R
over ?n iff ? a function pred : Rd ?T such that for all distributions D on X ? Y and all sequences
of random (vector) functions fm : X ?Rd (depending on (X1 , Y1 ), . . . , (Xm , Ym )),
P

? er?,?
er?
D [fm ] ?
D

P

er?D [pred ? fm ] ?
? er?,?
D .

implies

For any instance x ? X , let p(x) ? ?n denote the conditional label probability vector at x, given by
p(x) = (p1 (x), . . . , pn (x))? where py (x) = P(Y = y | X = x). Then one can extend the above
result to show that for P ? ?n , ? is calibrated w.r.t. ? over P iff ? a function pred : Rd ?T such
that the above implication holds for all distributions D on X ? Y for which p(x) ? P ?x ? X .

Subset ranking. Subset ranking problems arise frequently in information retrieval applications.
In a subset ranking problem, each instance in X consists of a query together with a set of say
r documents to be ranked. The label space Y varies from problem to problem: in some cases,
labels consist of binary or multi-level relevance judgements for the r documents, in which case
Y = {0, 1}r or Y = {0, 1, . . . , s}r for some appropriate s ? Z+ ; in other cases, labels consist
of pairwise preference graphs over the r documents, represented as (possibly weighted) directed
acyclic graphs (DAGs) over r nodes. Given examples of such instance-label pairs, the goal is to
learn a model to rank documents for new queries/instances; in most cases, the desired ranking takes
the form of a permutation over the r documents, so that T = Sr (where Sr denotes the group of
permutations on r objects). As noted earlier, various loss functions are used in practice, and there
has been much interest in understanding questions of consistency and calibration for these losses in
recent years [9?15, 17]. The focus so far has mostly been on designing r-dimensional surrogates,
which operate on a surrogate target space of dimension d = r; these are also termed ?score-based?
surrogates since the resulting algorithms can be viewed as learning one real-valued score function
for each of the r documents, and in this case the pred mapping usually consists of simply sorting
the documents according to these scores. Below we will apply our result on calibrated surrogates
for low-rank loss matrices to obtain new calibrated surrogates ? both r-dimensional, score-based
surrogates and, in some cases, higher-dimensional surrogates ? for several subset ranking losses.

3

Calibrated Surrogates for Low Rank Loss Matrices

The following is the primary result of our paper. The result gives an explicit construction for a
convex, calibrated, least-squares type surrogate loss de?ned on a low-dimensional surrogate space
for any target loss matrix that has a low-rank structure.
Theorem 3. Let ? : Y ? T ?R+ be a loss function such that there exist d ? Z+ , vectors
?1 , . . . , ?n ? Rd , ? 1 , . . . , ? k ? Rd and c ? R such that
d
?

?(y, t) =

?yi ?ti + c .

i=1

? + be de?ned as
Let ??? : Y ? Rd ?R
??? (y, u) =

d
?
i=1

and let pred?? : Rd ?T be de?ned as
?
?
Then ??? , pred?? is ?-calibrated.

(ui ? ?yi )2

pred?? (u) ? argmint?[k] u? ? t .

Proof. Let p ? ?n . De?ne up ? Rd as up
i =
p? ? ?? (u) =

?n

y=1

d ?
n
?
i=1 y=1

py ?yi ?i ? [d]. Now for any u ? Rd , we have
py (ui ? ?yi )2 .

Minimizing this over u ? R yields that u is the unique minimizer of p? ? ?? (u). Also, for any
t ? [k], we have
d

p

3

?

p ?t =

n
?

py

y=1

Now, for each t ? [k], de?ne

d
??

?yi ?ti + c

i=1

?

= (up )? ? t + c .

?

regret?p (t) = p? ?t ? min
p? ?t? = (up )? ? t ? min
(up )? ? t? .
?
?
t ?[k]

t ?[k]

Clearly, by de?nition of pred?? , we have regret?p (pred?? (up )) = 0. Also, if regret?p (t) = 0 for all
t ? [k], then trivially pred?? (u) ? argmint p? ?t ?u ? Rd (and there is nothing to prove in this case).
Therefore assume ?t ? [k] : regret?p (t) > 0, and let
regret?p (t) .
? =
min
t?[k]:regret?p (t)>0
Then we have
inf

?
u?Rd :pred?
/
t p ?t
? (u)?argmin

p? ? ?? (u)

=
=

inf

u?Rd :regret?p (pred?
? (u))??

p? ? ?? (u)

inf

?
?
p
u?Rd :regret?p (pred?
? (u))?regretp (pred? (u ))+?

p? ? ?? (u) .

Now, we claim that the mapping u ?? regret?p (pred?? (u)) is continuous at u = up . To see this,
suppose the sequence {um } converges to up . Then we have
regret?p (pred?? (um ))

=

(up )? ? pred?? (um ) ? min
(up )? ? t?
?

=

(u ? um ) ? pred?? (um ) + u?
? min
(up )? ? t?
m ? pred?
? (um )
?

=

p

?

p

?

t ?[k]

(u ? um ) ? pred?? (um ) + min
?

t ?[k]

holds by de?nition of pred?? . It is easy to see
to up . Thus regret?p (pred?? (um )) converges to

The last equality
as um converges
continuity at up . In particular, this implies ?? > 0 such that
This gives

u?
m ? t?

t ?[k]

? min
(up )? ? t?
?
t ?[k]

the term on the right goes to zero
regret?p (pred?? (up )) = 0, yielding

?u ? up ? < ? =? regret?p (pred?? (u)) ? regret?p (pred?? (up )) < ? .
inf

?
?
p
u?Rd :regret?p (pred?
? (u))?regretp (pred? (u ))+?

p? ? ?? (u)

?
>

inf

u?Rd :?u?up ???

p? ? ?? (u)

inf p? ? ?? (u) ,

u?Rd

where the last inequality holds since p? ? ?? (u) is a strictly convex function of u and up is its unique
minimizer. The above sequence of inequalities give us that
inf

?
u?Rd :pred?
/
t p ?t
? (u)?argmin

p? ? ?? (u)

>

inf p? ? ?? (u) .

u?Rd

Since this holds for all p ? ?n , we have that (??? , pred?? ) is ?-calibrated.
We note that Ramaswamy and Agarwal [16] showed a similar least-squares type surrogate calibrated
for any loss ? : Y ? T ?R+ ; indeed our proof technique above draws inspiration from the proof
technique there. However, the surrogate they gave was de?ned on a surrogate space of dimension
n ? 1, where n is the number of class labels in Y. For many practical problems, this is an intractably
large number. For example, as noted above, in the subset ranking problems we consider, the number
of class labels is typically exponential in r, the number of documents associated with each query.
On the other hand, as we will see below, many subset ranking losses have a low-rank structure,
with rank linear or quadratic in r, allowing us to use the above result to design convex calibrated
surrogates on an O(r) or O(r2 )-dimensional space. Ramaswamy and Agarwal also gave another
result in which they showed that any loss matrix of rank d has a d-dimensional convex calibrated
surrogate; however the surrogate there was de?ned such that it took values < ? on an awkward
space in Rd (not the full space Rd ) that would be dif?cult to construct in practice, and moreover,
their result did not yield an explicit construction for the pred mapping required to use a calibrated
surrogate in practice. Our result above combines the bene?ts of both these previous results, allowing
explicit construction of low-dimensional least-squares type surrogates for any low-rank loss matrix.
The following sections will illustrate several applications of this result.
4

4

Calibrated Surrogates for Precision@q

The Precision@q is a popular performance measure for subset ranking problems in information
retrieval. As noted above, in a subset ranking problem, each instance in X consists of a query
together with a set of r documents to be ranked. Consider a setting with binary relevance judgement
labels, so that Y = {0, 1}r with n = 2r . The prediction space is T = Sr (group of permutations on
r objects) with k = r!. For y ? {0, 1}r and ? ? Sr , where ?(i) denotes the position of document i
under ?, the Precision@q loss for any integer q ? [r] can be written as follows:
q

?P@q (y, ?)

=

1?

=

1?

1?
y ?1
q i=1 ? (i)
r

1?
yi ? 1(?(i) ? q) .
q i=1

?
? + and pred? :
Therefore, by Theorem 3, for the r-dimensional surrogate ?P@q
: {0, 1}r ? Rr ?R
P@q
r
R ?Sr de?ned as
r
?
?
?P@q
(y, u) =
(ui ? yi )2
i=1

pred?P@q (u)

argmax??Sr

?

r
?
i=1

ui ? 1(?(i) ? q) ,

?
, pred?P@q ) is ?P@q -calibrated. It can easily be seen that for any u ? Rr , any
we have that (?P@q
permutation ? which places the top q documents sorted in decreasing order of scores ui in the top
q positions achieves the maximum in pred?P@q (u); thus pred?P@q (u) can be implemented ef?ciently
using a standard sorting or selection algorithm. Note that the popular winner-take-all (WTA) loss,
which assigns a loss of 0 if the top-ranked item is relevant (i.e. if y??1 (1) = 1) and 1 otherwise,
is simply a special case of the above loss with q = 1; therefore the above construction also yields
a calibrated surrogate for the WTA loss. To our knowledge, this is the ?rst example of convex,
calibrated surrogates for the Precision@q and WTA losses.

5

Calibrated Surrogates for Expected Rank Utility

The expected rank utility (ERU) is a popular subset ranking performance measure used in recommender systems displaying short ranked lists [18]. In this case the labels consist of multi-level
relevance judgements (such as 0 to 5 stars), so that Y = {0, 1, . . . , s}r for some appropriate s ? Z+
with n = (s + 1)r . The prediction space again is T = Sr with k = r!. For y ? {0, 1, . . . , s}r and
? ? Sr , where ?(i) denotes the position of document i under ?, the ERU loss is de?ned as
r
?
1??(i)
max(yi ? v, 0) ? 2 w?1 ,
?ERU (y, ?) = z ?
i=1

where z is a constant to ensure the positivity of the loss, v ? [s] is a constant that indicates a
neutral score, and w ? R is a constant indicating the viewing half-life. Thus, by Theorem 3, for the
?
? + and pred? : Rr ?Sr de?ned as
: {0, 1, . . . , s}r ? Rr ?R
r-dimensional surrogate ?ERU
ERU
r
?
?
?ERU
(y, u) =
(ui ? max(yi ? v, 0))2
i=1

pred?ERU (u)

?

argmax??Sr

r
?
i=1

ui ? 2

1??(i)
w?1

,

?
, pred?ERU ) is ?ERU -calibrated. It can easily be seen that for any u ? Rr , any
we have that (?ERU
permutation ? satisfying the condition

ui > uj
?
predERU (u), and

=? ?(i) < ?(j)

therefore pred?ERU (u) can be implemented ef?ciently
achieves the maximum in
by simply sorting the r documents in decreasing order of scores ui . As for Precision@q, to our
knowledge, this is the ?rst example of a convex, calibrated surrogate for the ERU loss.
5

6

Calibrated Surrogates for Mean Average Precision

The mean average precision (MAP) is a widely used ranking performance measure in information
retrieval and related applications [15, 19]. As with the Precision@q loss, Y = {0, 1}r and T = Sr .
For y ? {0, 1}r and ? ? Sr , where ?(i) denotes the position of document i under ?, the MAP loss
is de?ned as follows:
?(i)
? 1 ?
1
?MAP (y, ?) = 1 ?
y ?1 .
|{? : y? = 1}| i:y =1 ?(i) j=1 ? (j)
i

It was recently shown that there cannot exist any r-dimensional convex, calibrated surrogates for
the MAP loss [15]. We now re-write the MAP loss above in a manner that allows us to show the
existence of an O(r2 )-dimensional convex, calibrated surrogate. In particular, we can write
r ?
r ?
i
i
?
?
y??1 (i) y??1 (j)
1
yi yj
1
. = 1 ? ?r
?MAP (y, ?) = 1 ? ?r
i
?=1 y? i=1 j=1
?=1 y? i=1 j=1 max(?(i), ?(j))
?
? + and
-dimensional surrogate ?MAP
: {0, 1}r ? Rr(r+1)/2 ?R
Thus, by Theorem 3, for the r(r+1)
2
?
r(r+1)/2
?Sr de?ned as
predMAP : R
r ?
i ?
?
y i y j ?2
?
(y, u) =
?MAP
uij ? ?r
?=1 y?
i=1 j=1

pred?MAP (u)

we have that

?
(?MAP
, pred?MAP )

?

argmax??Sr

r ?
i
?
i=1 j=1

is ?MAP -calibrated.

uij ?

1
,
max(?(i), ?(j))

Note however that the optimization problem associated with computing pred?MAP (u) above can be
written as a quadratic assignment problem (QAP), and most QAPs are known to be NP-hard. We
conjecture that the QAP associated with the mapping pred?MAP above is also NP-hard. Therefore,
?
while the surrogate loss ?MAP
is calibrated for ?MAP and can be minimized ef?ciently over a training
sample to learn a model f : X ?Rr(r+1)/2 , for large r, evaluating the mapping required to transform
predictions in Rr(r+1)/2 back to predictions in Sr is likely to be computationally infeasible. Below
we describe an alternate mapping in place of pred?MAP which can be computed ef?ciently, and show
?
that under certain conditions on the probability distribution, the surrogate ?MAP
together with this
mapping is still calibrated for ?MAP .
Speci?cally, de?ne predMAP : Rr(r+1)/2 ?Sr as follows:
?
?
predMAP (u) ? ? ? Sr : uii > ujj =? ?(i) < ?(j) .

Clearly, predMAP (u) can be implemented ef?ciently by simply sorting the ?diagonal? elements uii
for i ? [r]. Also, let ?Y denote the probability simplex over Y, and for each p ? ?Y , de?ne
up ? Rr(r+1)/2 as follows:
?
?
? ? yi y j ?
Yi Yj
?
=
E
py ? r
up
=
?i, j ? [r] : i ? j .
Y
?p
r
ij
?=1 Y?
?=1 y?
y?Y

Now de?ne Preinforce ? ?Y as follows:
?
p
p
p
Preinforce = p ? ?Y : up
ii ? ujj =? uii ? ujj +

?

??[r]\{i,j}

p
where we set up
ij = uji for i < j. Then we have the following result:

?
p
(up
?
u
)
,
+
j?
i?

?
Theorem 4. (?MAP
, predMAP ) is (?MAP , Preinforce )-calibrated.

The ideal predictor pred?MAP uses the entire u matrix, but the predictor predMAP , uses only the diagonal elements. The noise conditions Preinforce can be viewed as basically enforcing that the diagonal
elements dominate and enforce a clear ordering themselves.
In fact, since the mapping predMAP depends on only the diagonal elements of u, we can equivalently
de?ne an r-dimensional surrogate that is calibrated w.r.t. ?MAP over Preinforce . Speci?cally, we have
the following immediate corollary:
6

r
? + and pred
?
Corollary 5. Let ??MAP : {0, 1}r ? Rr ?R
MAP : R ?Sr be de?ned as
r ?
?
?
2
yi
?) =
u
?i ? ?r
??MAP (y, u
?=1 y?
i=1
?
?
?
pred
u) ?
? ? Sr : u
?i > u
?j =? ?(i) < ?(j) .
MAP (?

?
Then (??MAP , pred
MAP ) is (?MAP , Preinforce )-calibrated.

r
?
?
Looking at the
?rform of ?MAP and predMAP , we can see that the function s : Y?R de?ned as
si (y) = yi /( ?=1 yr ) is a ?standardization function? for the MAP loss over Preinforce , and therefore
it follows that any ?order-preserving surrogate? with this standardization function is also calibrated
with the MAP loss over Preinforce [13]. To our knowledge, this is the ?rst example of conditions on
the probability distribution under which a convex calibrated (and moreover, score-based) surrogate
can be designed for the MAP loss.

7

Calibrated Surrogates for Pairwise Disagreement

The pairwise disagreement (PD) loss is a natural and widely used loss in subset ranking [11, 17].
The label space Y here consists of a ?nite number of (possibly weighted) directed acyclic graphs
r(r?1)
(DAGs) over r nodes; we can represent each such label as a vector y ? R+
where at least one
of yij or yji is 0 for each i ?= j, with yij > 0 indicating a preference for document i over document
j and yij denoting the weight of the preference. The prediction space as usual is T = Sr with
k = r!. For y ? Y and ? ? Sr , where ?(i) denotes the position of document i under ?, the PD loss
is de?ned as follows:
r ?
?
?
?
yij 1 ?(i) > ?(j) .
?PD (y, ?) =
i=1 j?=i

It was recently shown that there cannot exist any r-dimensional convex, calibrated surrogates for the
?
?+
PD loss [15, 16]. By Theorem 3, for the r(r ? 1)-dimensional surrogate ?PD
: Y ? Rr(r?1) ?R
?
and predPD : Rr(r?1) ?Sr de?ned as
r ?
?
?
?PD
(y, u) =
(uij ? yij )2
(1)
i=1 j?=i

pred?PD (u)

?

argmin??Sr

r ?
?
i=1 j?=i

?
?
uij ? 1 ?(i) > ?(j)

?
we immediately have that (?PD
, pred?PD ) is ?PD -calibrated (in fact the loss matrix ?PD has rank at most
r(r?1)
r(r?1)
, allowing for an 2 -dimensional surrogate; we use r(r ?1) dimensions for convenience).
2
In this case, the optimization problem associated with computing pred?PD (u) above is a minimum

weighted feedback arc set (MWFAS) problem, which is known to be NP-hard. Therefore, as with the
?
MAP loss, while the surrogate loss ?PD
is calibrated for ?PD and can be minimized ef?ciently over
a training sample to learn a model f : X ?Rr(r?1) , for large r, evaluating the mapping required to
transform predictions in Rr(r?1) back to predictions in Sr is likely to be computationally infeasible.
Below we give two sets of results. In Section 7.1, we give a family of score-based (r-dimensional)
surrogates that are calibrated with the PD loss under different conditions on the probability distribution; these surrogates and conditions generalize those of Duchi et al. [11]. In Section 7.2, we
give a different condition on the probability distribution under which we can actually avoid ?dif?cult? graphs being passed to pred?PD . This condition is more general (i.e. encompasses a larger set
of probability distributions) than those associated with the score-based surrogates; this gives a new
(non-score-based, r(r ?1)-dimensional) surrogate with an ef?ciently computable pred mapping that
is calibrated with the PD loss over a larger set of probability distributions than previous surrogates
for this loss.
7.1

Family of r-Dimensional Surrogates Calibrated with ?PD Under Noise Conditions

The following gives a family of score-based surrogates, parameterized by functions f : Y?Rr , that
are calibrated with the PD loss under different conditions on the probability distribution:
7

Theorem 6. Let f : Y?Rr be any function that maps DAGs y ? Y to score vectors f (y) ? Rr . Let
? + , pred : Rr ?Sr and Pf ? ?Y be de?ned as
? f : Y ? R r ?R
r
?
?2
?
?f (y, u) =
ui ? fi (y)
?
?
i=1
pred(u) ? ? ? Sr : ui > uj =? ?(i) < ?(j)
?
?
Pf = p ? ?Y : EY ?p [Yij ] > EY ?p [Yji ] =? EY ?p [fi (Y )] > EY ?p [fj (Y )] .
Then (?f , pred) is (?PD , Pf )-calibrated.

The noise conditions Pf state that the expected value of function f must decide the ?right? ordering.
We note that the surrogate given by Duchi et al. [11] can be written in our notation as
r ?
r
?
?
?DMJ (y, u) =
yij (uj ? ui ) + ?
?(ui ) ,
i=1 j?=i

i=1

where ? is a strictly convex and 1-coercive function and ? > 0. Taking ?(z) = z 2 and ? = 12 gives
a special case of the family of score-based surrogates
? in Theorem 6 above obtained by taking f as
(yij ? yji ) .
fi (y) =
j?=i

Indeed, the set of noise conditions under which the surrogate ?DMJ is shown to be calibrated with
the PD loss in Duchi et al. [11] is exactly the set Pf above with this choice of f . We also note that f
can be viewed as a ?standardization function? [13] for the PD loss over Pf .
7.2

An O(r2 )-dimensional Surrogate Calibrated with ?PD Under More General Conditions

?
: Y ? Rr(r?1) de?ned in Eq. (1). We noted
Consider now the r(r ? 1)-dimensional surrogate ?PD
?
the corresponding mapping predPD involved an NP-hard optimization problem. Here we give an
alternate mapping predPD : Rr(r?1) ?Sr that can be computed ef?ciently, and show that under
?
certain conditions on the probability distribution , the surrogate ?PD
together with this mapping
predPD is calibrated for ?PD . The mapping predPD is described by Algorithm 1 below:

Algorithm 1 predPD (Input: u ? Rr(r?1) ; Output: Permutation ? ? Sr )
Construct a directed graph over [r] with edge (i, j) having weight (uij ? uji )+ . If this graph is
acyclic, return any topological sorted order. If the graph has cycles, sort the edges in ascending
order by weight and delete them one by one (smallest weight ?rst) until the graph becomes acyclic;
return any topological sorted order of the resulting acyclic graph.
For each p ? ?Y , de?ne E p = {(i, j) ? [r] ? [r] : EY ?p [Yij ] > EY ?p [Yji ]}, and de?ne
?
?
?
?
PDAG = p ? ?Y : [r], E p is a DAG .

Then we have the following result:
?
, predPD ) is (?PD , PDAG )-calibrated.
Theorem 7. (?PD

It is easy to see that PDAG ? Pf ?f (where Pf is as de?ned in Theorem 6), so that the above result
yields a low-dimensional, convex surrogate with an ef?ciently computable pred mapping that is
calibrated for the PD loss under a broader set of conditions than the previous surrogates.

8

Conclusion

Calibration of surrogate losses is an important property in designing consistent learning algorithms.
We have given an explicit method for constructing calibrated surrogates for any learning problem
with a low-rank loss structure, and have used this to obtain several new results for subset ranking,
including new calibrated surrogates for the Precision@q, ERU, MAP and PD losses.
Acknowledgments
The authors thank the anonymous reviewers, Aadirupa Saha and Shiv Ganesh for their comments. HGR acknowledges a Tata Consultancy Services (TCS) PhD fellowship and the Indo-US Virtual Institute for Mathematical and Statistical Sciences (VIMSS). SA thanks the Department of Science & Technology (DST) and
Indo-US Science & Technology Forum (IUSSTF) for their support. AT gratefully acknowledges the support of
NSF under grant IIS-1319810.

8

References
[1] G?abor Lugosi and Nicolas Vayatis. On the Bayes-risk consistency of regularized boosting
methods. Annals of Statistics, 32(1):30?55, 2004.
[2] Wenxin Jiang. Process consistency for AdaBoost. Annals of Statistics, 32(1):13?29, 2004.
[3] Tong Zhang. Statistical behavior and consistency of classi?cation methods based on convex
risk minimization. Annals of Statistics, 32(1):56?134, 2004.
[4] Ingo Steinwart. Consistency of support vector machines and other regularized kernel classi?ers. IEEE Transactions on Information Theory, 51(1):128?142, 2005.
[5] Peter L. Bartlett, Michael Jordan, and Jon McAuliffe. Convexity, classi?cation and risk bounds.
Journal of the American Statistical Association, 101(473):138?156, 2006.
[6] Tong Zhang. Statistical analysis of some multi-category large margin classi?cation methods.
Journal of Machine Learning Research, 5:1225?1251, 2004.
[7] Ambuj Tewari and Peter L. Bartlett. On the consistency of multiclass classi?cation methods.
Journal of Machine Learning Research, 8:1007?1025, 2007.
[8] Ingo Steinwart. How to compare different loss functions and their risks. Constructive Approximation, 26:225?287, 2007.
[9] David Cossock and Tong Zhang. Statistical analysis of bayes optimal subset ranking. IEEE
Transactions on Information Theory, 54(11):5140?5154, 2008.
[10] Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. Listwise approach to learning
to rank: Theory and algorithm. In International Conference on Machine Learning, 2008.
[11] John Duchi, Lester Mackey, and Michael Jordan. On the consistency of ranking algorithms. In
International Conference on Machine Learning, 2010.
[12] Pradeep Ravikumar, Ambuj Tewari, and Eunho Yang. On NDCG consistency of listwise ranking methods. In International Conference on Arti?cial Intelligence and Statistics, 2011.
[13] David Buffoni, Cl?ement Calauz`enes, Patrick Gallinari, and Nicolas Usunier. Learning scoring
functions with order-preserving losses and standardized supervision. In International Conference on Machine Learning, 2011.
[14] Wei Gao and Zhi-Hua Zhou. On the consistency of multi-label learning. In Conference on
Learning Theory, 2011.
[15] Cl?ement Calauz`enes, Nicolas Usunier, and Patrick Gallinari. On the (non-)existence of convex,
calibrated surrogate losses for ranking. In Advances in Neural Information Processing Systems
25, pages 197?205. 2012.
[16] Harish G. Ramaswamy and Shivani Agarwal. Classi?cation calibration dimension for general
multiclass losses. In Advances in Neural Information Processing Systems 25, pages 2087?
2095. 2012.
[17] Yanyan Lan, Jiafeng Guo, Xueqi Cheng, and Tie-Yan Liu. Statistical consistency of ranking
methods in a rank-differentiable probability space. In Advances in Neural Information Processing Systems 25, pages 1241?1249. 2012.
[18] Quoc V. Le and Alex Smola. Direct optimization of ranking measures, arXiv:0704.3359, 2007.
[19] Yisong Yue, Thomas Finley, Filip Radlinski, and Thorsten Joachims. A support vector method
for optimizing average precision. In Proceedings of the 30th ACM SIGIR International Conference on Research and Development in Information Retrieval, 2007.

9

"
3414,"Structural epitome: A way to summarize one?s visual
experience

Nebojsa Jojic
Microsoft Research

Alessandro Perina
Microsoft Research
University of Verona

Vittorio Murino
Italian Institute of Technology
University of Verona

Abstract
In order to study the properties of total visual input in humans, a single subject
wore a camera for two weeks capturing, on average, an image every 20 seconds.
The resulting new dataset contains a mix of indoor and outdoor scenes as well
as numerous foreground objects. Our first goal is to create a visual summary
of the subject?s two weeks of life using unsupervised algorithms that would automatically discover recurrent scenes, familiar faces or common actions. Direct
application of existing algorithms, such as panoramic stitching (e.g., Photosynth)
or appearance-based clustering models (e.g., the epitome), is impractical due to
either the large dataset size or the dramatic variations in the lighting conditions.
As a remedy to these problems, we introduce a novel image representation, the
?structural element (stel) epitome,? and an associated efficient learning algorithm.
In our model, each image or image patch is characterized by a hidden mapping T
which, as in previous epitome models, defines a mapping between the image coordinates and the coordinates in the large ?all-I-have-seen? epitome matrix. The
limited epitome real-estate forces the mappings of different images to overlap
which indicates image similarity. However, the image similarity no longer depends on direct pixel-to-pixel intensity/color/feature comparisons as in previous
epitome models, but on spatial configuration of scene or object parts, as the model
is based on the palette-invariant stel models. As a result, stel epitomes capture
structure that is invariant to non-structural changes, such as illumination changes,
that tend to uniformly affect pixels belonging to a single scene or object part.

1

Introduction

We develop a novel generative model which combines the powerful invariance properties achieved
through the use of hidden variables in epitome [2] and stel (structural element) models [6, 8]. The
latter set of models have a hidden stel index si for each image pixel i. The number of discrete states
si can take is small, typically 4-10, as the stel indices point to a small palette of distributions over local measurements, e.g., color. The actual local measurement xi (e.g. color) for pixel i is assumed to
have been generated from the appropriate palette entry. This constrains the pixels with the same stel
index s to have similar colors or whatever local measurements xi represent. The indexing scheme is
further assumed to change little accross different images of the same scene/object, while the palettes
can vary significantly. For example, two images of the same scene captured in different levels of
overall illumination would still have very similar stel partitions, even though their palettes may be
vastly different. In this way, the image representation rises above a matrix of local measurements in
favor of a matrix of stel indices which can survive remarkable non-structural image changes, as long
as these can be explained away by a change in the (small) palette. For example, in Fig. 1B, images
of pedestrians are captured by a model that has a prior distribution of stel assignments shown in the
first row. The prior on stel probabilities for each pixel adds up to one, and the 6 images showing
these prior probabilities add up to a uniform image of ones. Several pedestrian images are shown
1

A) GRAPHICAL MODELS

B) PROBABILISTIC INDEX MAP

i

i

p(S )
S

e

?

S

?

t

T

X

X

Epitome

PIM

C) STEL EPITOME

Stel 2

Stel 3

Stel 4

Stel 5

Stel 6

q(s=1)

q(s=2)

q(s=3)

q(s=4)

q(s=5)

q(s=6)

p(s)

e

T
i

Stel 1

Palette

?

Palette

?

Palette

?

X

t

Stel Epitome

q(s=3)

q(s=4)

q(s=1)

q(s=2)

q(s=3)

q(s=4)

q(s=1)

q(s=2)

q(s=3)

q(s=4)

Inference

x

G) REGULAR
EPITOME [2]

q(s=2)

q(s=1)

e(s)

D) FOUR
FRAMES

F) ALIGNMENT
OF INTENSITY
IMAGES

s=4

s=2

E) ALIGNMENT
WITH STEL

s=3

s=1

Figure 1: A) Graphical model of Epitome, Probabilistic index map (PIM) and Stel epitome. B)
Examples of PIM parameters. C) Example of stel epitome parameters. D) Four frames aligned with
stel epitome E-F). In G) we show the original epitome model [2] trained on these four frames.

below with their posterior distributions over stel assignments, as well as the mean color of each stel.
This illustrates that the different parts of the pedestrian images are roughly matched. Torso pixels,
for instance, are consistently assigned to stel s = 3, despite the fact that different people wore shirts
or coats of very different colors. Such a consistent segmentation is possible because torso pixels
tend to have similar colors within any given image and because the torso is roughly in the same
position across images (though misalignment of up to half the size of the segments is largely tolerated). While the figure shows the model with S=6 stels, larger number of stels were shown to lead
to further segmentation of the head and even splitting of the left from right leg [6]. Motivated by
similar insights as in [6], a number of models followed, e.g. [7, 13, 14, 8], as the described addition
of hidden variables s achieves the remarkable level of intensity invariance first demonstrated through
the use of similarity templates [12], but at a much lower computational cost.
In this paper, we embed the stel image representation within a large stel epitome: a stel prior matrix,
like the one shown in the top row of Fig. 1B, but much larger so that it can contain representations
of multiple objects or scenes. This requires the additional transformation variables T for each image
whose role is to align it with the epitome. The model is thus qualitatively enriched in two ways: 1)
the model is now less sensitive to misalignment of images, as through alignment to the epitome, the
images are aligned to each other, and 2) interesting structure emerges when the epitome real estate
is limited so that though it is much larger than the size of a single image, it is till much smaller
than the real estate needed to simply tile all images without overlap. In that case, a large collection
of images must naturally undergo an unsupervised clustering in order for this real estate to be used
as well as possible (or as well as the local minimum obtained by the learning algorithm allows).
This clustering is quite different from the traditional notion of clustering. As in the original epitome
models, the transformation variables play both the alignment and cluster indexing roles. Different
2

models over the typical scenes/objects have to compete over the positions in the epitome, with a
panoramic version of each scene emerging in different parts of the epitome, finally providing a rich
image indexing scheme. Such a panoramic scene submodel within the stel epitome is illustrated
in Fig. 1C. A portion of the larger stel epitome is shown with 3 images that map into this region.
The region represents one of two home offices in the dataset analyzed in the experiments. Stel s=1
captures the laptop screen, while the other stels capture other parts of the scene, as well as large
shadowing effects (while the overall changes in illumination and color changes in object parts rarely
affect stel representations, the shadows can break the stel invariance, and so the model learned to
cope with them by breaking the shadows across multiple stels). The three images shown, mapping
to different parts of this region, have very different colors as they were taken at different times of
day and across different days, and yet their alignment is not adversely affected, as it is evident in
their posterior stel segmentation aligned to the epitome.
To further illustrate the panoramic alignment, we used the epitome mapping to show for the 4 different images in Fig. 1D how they overlap with stel s=4 of another office image (Fig. 1E), as well as
how multiple images of this scene, including these 4, look when they are aligned and overlapped as
intensity images in Fig. 1F. To illustrate the gain from palette-invariance that motivated this work,
we show in Fig. 1G the original epitome model [2] trained on images of this scene. Without the
invariances afforded by the stel representation, the standard color epitome has to split the images of
the scene into two clusters, and so the laptop screen is doubled there.
Qualitatively quite different from both epitomes and previous stel models, the stel epitome is a
model flexible enough to be applied to a very diverse set of images. In particular, we are interested
in datasets that might represent well a human?s total visual input over a longer period of time,
and so we captured two weeks worth of SenseCam images, taken at a frequency of roughly one
image every 20 seconds during all waking hours of a human subject over a period of two weeks
(www.research.microsoft.com/?jojic/aihs).

2

Stel epitome

The graphical model describing the dependencies in stel epitomes is provided in Fig. 1A. The parametric forms for the conditional distributions are standard multinomial and Gaussian distributions
just as the ones used in [8]. We first consider the generation of a single image or an image patch (depending on which visual scale we are epitomizing), and, for brevity, temporarily omit the subscript
t indexing different images.
The epitome is a matrix of multinomial distributions over S indices s ? {1, 2, ..., S}, associated
with each two-dimensional epitome location i:
p(si = s) = ei (s).

(1)

Thus each location in the epitome contains S probabilities (adding to one) for different indices.
Indices for the image are assumed to be generated from these distributions. The distribution over
the entire collection of pixels (either from an entire image, or a patch), p({xi }|{si }, T, ?), depends
on the parametrization of the transformations T . We adopt the discrete transformation model used
previously in graphical models e.g. [1, 2], where the shifts are separated from other transformations
such as scaling or rotation, T = (`, r), with ` being a 2-dimensional shift and r being the index into
the set of other transformations, e.g., combinations of rotation and scaling:
Y
Y
p({xi }|{si }, T, ?) =
p(xri?` |si , ?) =
p(xri?` |?si ),
(2)
i

i

where superscript r indicates transformation of the image x by the r-th transformation, and i ? ` is
the mod-difference between the two-dimensional variables with respect to the edges of the epitome
(the shifts wrap around). ? is the palette associated with the image, and ?s is its s ? th entry.
Various palette models for probabilistic index / structure element map models have been reviewed
in [8]. For brevity, in this paper we focus on the simplest case where the image measurements are
simply pixel colors, and the palette entries are simply Gaussians with parameters ?s = (?s , ?s ).
In this case, p(xri?` |?si ) = N (xri?` ; ?si , ?si ), and the joint likelihood over observed and hidden
variables can be written as
YY
[si =s]
P = p(?)p(`, r)
N (xri?` ; ?s , ?s )ei (s)
,
(3)
i

s

3

where [] is the indicator function.
To derive the inference and leaning algorithms
P for Qthe mode, we start with a posterior distribution
model Q and the appropriate free energy
Q log P . The standard variational approach, however,
is not as straightforward as we might hope as major obstacles need to be overcome to avoid local
minima and slow convergence. To focus on these important issues, we further simplify the problem
and omit both the non-shift part of the transformations (r) and palette priors p(?), and for consistency, we also omit these parts of the model in the experiments. These two elements of the model
can be dealt with in the manner proposed previously: The R discrete transformations (scale/rotation
combinations, for example) can be inferred in a straight-forward way that makes the entire algorithm
that follows R times slower (see [1] for using such transformations in a different context), and the
various palette models from [8] can all be inserted here with the update rules adjusted appropriately.
A large stel epitome is difficult to learn because decoupling of all hidden variables in the posterior leads to severe local minima, with all images either mapped to a single spot in the epitome,
or mapped everywhere in the epitome so that the stel distribution is flat. This problem becomes
particularly evident in larger epitomes, due to the imbalance in the cardinalities of the three types of
hidden variables. To resolve this, we either need a very high numerical precision (and considerable
patience), or the severe variational approximations need to be avoided as much as possible. It is
indeed possible to tractably use a rather expressive posterior
Y
Y
Q = q(`)
q(?s |`)
q(si ),
(4)
s

i

further setting q(?s |`) = ?(?s ? ?
?s,` )?(?s ? ??s,` ), where ? is the Dirac function. This leads to
F

= H(Q) +

X

q(`)q(si = s)

s,`,i

+

X

q(`)q(si = s)

s,`,i

X
x2i?`
?
?s,` xi?`
?
q(`)q(si = s)
+
?
2?s,`
??s,`
s,`,i

?
?2s,`
2??s,`

?

XX
s

q(si = s) log ei (s),

(5)

i

where H(Q) is the entropy of the posterior distribution. Setting to zero the derivatives of this free
energy with respect to the variational parameters ? the probabilities q(si = s), q(`), and the palette
means and variance estimates ?
?s,` , ??s,` ? we obtain a set of updates for iterative inference.
2.1

E STEP

The following steps are iterated for a single image x on an m ? n grid and for a given epitome
distributions e(s) on an M ? N grid. Index i corresponds to the epitome coordinates and masks
m are used to describe which of all M ? N coordinates correspond to image coordinates. In the
variational EM learning on a collection of images index by t, these steps are done for each image,
yielding posterior distributions indexed by t and then the M step is performed as described below.
We initialize q(si = s) = e(si ) and then iterate the following steps in the following order.
Palette updates
P P
`
i mi?` q(si = s)q(`)xi?`
P
= P
`
i q(si = s)q(`)mi?`

(6)

P P
2 
`
i mi?` q(si = s)q(`)xi?`
P
P
??
?2s,`
=
q(s
=
s)q(`)m
i
i?`
`
i

(7)

?
?s,`

??s,`

Epitome mapping update
log q(`) = const +

1X
q(sti = s) log 2??i?`
2 i,s

(8)

This update is derived from the free energy and from the expression for ? above). This equation
can be used as is when the epitome e(s) is well defined (that is the entropy of component stel
4

distribution is low in the latter iterations), as long as the usual care is taken in exponentiation before
normalization - the maximum log q(`) should be subtracted from all elements of the M ? N matrix
log q(`) before exponentiation.
In the early iterations of EM, however, when distributions ei (s) have not converged yet, numerical
imprecision can stop the convergence, leaving the algorithm at a point which is not even a local
minimum. The reason for this is that after the normalization step we described, q(`) will still be very
peaky, even for relatively flat e(s) due to the large number of pixels in the image. The consequence is
that low alignment probabilities are rounded down to zero, as after exponentiation and normalization
their values go below numerical precision. If there are areas of the epitome where no single image is
mapped with high probability, then the update in those areas in the M step would have to depend on
the low-probability mappings for different images, and their relative probabilities would determine
which of the images contribute more and which less to updating these areas of the epitome. To
preserve the numerical precision needed for this, we set k thresholds ?k , and compute log q?(`)k , the
distributions at the k different precision levels:
log q?(`)k = [log q(`) ? ?k ] ? ?k + [log q(`) < ?k ] ? log q(`),
where [] is the indicator function. This limits how high the highest probability in the map is allowed
to be. The k ? th distribution sets all values above ?k to be equal to ?k .
We can now normalize these k distributions as discussed above:
exp {log q?(`)k ? maxi log q?(`)k }
q?(`)k = P
?(`)k ? maxi log q?(`)k }
` exp {log q
To keep track of which precision level is needed for different `, we calculate the masks
X
m
? i,k =
q?(`)k ? mi?` ,
`

where mask m is the mask discussed in the main text with ones in the upper left corner?s m ? n
entries and zeros elsewhere, designating the default image position for a shift of ` = 0 (or given that
shifts are defined with a wrap-around, the shift of ` = (M, N )). Masks m
? i,k provide total weight of
the image mapping at the appropriate epitome location at different precision levels.
Posterior stel distribution q(s) update at multiple precision levels

log q?(si = s)k =

const ?

X X
`

?

i|i?`?C

q?(`)k

X X
x2i?`
?
?s,` xi?`
+
?
q?(`)k
2??s,`
??s,`
`

i|i?`?C

X X
`

q?(`)k
?
?2s,`
2??s,`

i|i?`?C

+m
? i,k ? log e(si = s).

(9)

To keep track of these different precision levels, we also define a mask M so that Mi = k indicates
that the k-th level of detail should be used for epitome location i. The k-th level is reserved for those
locations that have only the values from up to the k-th precision band of q(`) mapped there (we will
have m ? n mappings of the original image to each epitome location, as this many different shifts
will align the image so as to overlap with any given
P epitome location). One simple, though not most
efficient way to define this matrix is Mi = 1 + b k m
? i,k c.
We now normalize log q?(si = s)k to compute the distribution at k different precision levels, q?(si =
s)k , and
P compute q(s) integrating the results from different numerical precision levels as q(si =
s) = k [Mi = k] ? q?(si = s)k .
2.2

M STEP

The highest k for each epitome location Di = maxt {Mit }, is determined over all images xt in
the dataset, so that we know the appropriate precision level at which to perform summation and
normalization. Then the epitome update consists of:
P
X
[M t = k] ? q t (si )
e(si = s) =
[Di = k] t P
.
t
t [M = k]
k

5

Bike

Kitchen

Car

Work
office

Dining
room

Outside
home

Home
office

Tennis
field

Laptop
room

Living
room

Figure 2: Some examples from the dataset (www.resaerch.microsoft.com/?jojic/aihs)

Note that most of the summations can be performed by convolution operations and as the result, the
complexity of the algorithm is of the O(SM N log M N ) for M X N epitomes.

3

Experiments

Using a SenseCam wearable camera, we have obtained two weeks worth of images, taken at the rate
of one frame every 20 seconds during all waking hours of a human subject. The resulting image
dataset captures the subject?s (summer) life rather completely in the following sense: Majority of
images can be assigned to one of the emergent categories (Fig. 2) and the same categories represent
the majority of images from any time period of a couple of days. We are interested in appropriate
summarization, browsing, and recognition tasks on this dataset. This dataset also proved to be
fundamental for testing stel epitomes, as the illumination and viewing angle variations are significant
across images and we found that the previous approaches to scene recognition provide only modest
recognition rates. For the purposes of evaluation, we have manually labeled a random collection
of 320 images and compared our method with other approaches on supervised and unsupervised
classification. We divided this reduced dataset in 10 different recurrent scenes (32 images per class);
some examples are depicted in Fig. 2. In all the experiments with the reduced dataset we used an
epitome area 14 times larger than the image area and five stels (S=5). The numerical results reported
in the tables are averaged over 4 train/test splits.
In supervised learning the scene labels are available during the stel epitome learning. We used this
information to aid both the original epitome [9] and the stel epitome modifying the models by the
addition of an observed scene class variable c in two ways: i) by linking c in the Bayesian network
with e, and so learning p(e|c), and ii) by linking c with T inferring p(T |c). In the latter strategy,
where we model p(T |c), we learn a single epitome, but we assume that the epitome locations are
linked with certain scenes, and this mapping is learned for each epitome pixel. Then, the distribution
p(c|`) over scene labels can be used for inference of the scene label for the test data. For a previt
ously unseen
is achieved by computing the label posterior p(ct |xt ) using
Ptest image x , recognition
t t
t
p(c |x ) = ` p(c|`) ? p(`|x ).
We compared our approach with the epitomic location recognition method presented in [9], with
Latent Dirichlet allocation (LDA) [4], and with the Torralba approach [11]. We also compared
with baseline discriminative classifiers and with the pyramid matching kernel approach [5], using
SIFT features [3]. For the above techniques that are based on topic models, representing images
as spatially disorganized bags of features, the codebook of SIFT features was based 16x16 pixel
patches computed over a grid spaced by 8 pixels. We chose a number of topics Z = 45 and 200
codewords (W = 200). The same quantized dictionary has been employed in [5].
To provide a fair comparison between generative and discriminative methods, we also used the
free energy optimization strategy presented in [10], which provides an extra layer of discriminative
training for an arbitrary generative model. The comparisons are provided in Table 1. Accuracies
achieved using the free energy optimization strategy [10] are reported in the Opt. column.
6

Table 1: Classification accuracies.
Method
Stel epitome
Stel epitome
Epitome [9]
Epitome [9]

p(T |c)
p(e|c)
p(T |c)
p(e|c)

Accuracy
70,06%
88,67%
74,36%
69,80%

[10] Opt.
n.a.
98,70%
n.a.
79,14%

Method
LDA [4]
GMM [11]
SIFT + K-NN
[5]

C=3

Accuracy
74,23%
56,81%
79,42%
96,67%

[10] Opt.
80,11%
n.a.
n.a.
n.a.

We also trained both the regular epitome and the stel epitome in an unsupervised way. An illustration of the resulting stel epitome is provided in Fig. 3. The 5 panels marked s = 1, . . . , 5 show the
stel epitome distribution. Each of these panels is an image ei (s) for an appropriate s. On the top
of the stel epitome, four enlarged epitome regions are shown to highlight panoramic reconstructions
of a few classes. We also show the result of averaging all images according to their mapping to the
stel epitome (Fig. 3D) for comparison with the traditional epitome (Fig.3C) which models colors
rather than stels. As opposed to the stel epitome, the learned color epitome [2] has to have multiple
versions of the same scene in different illumination conditions. Furthermore, many different scenes
tend to overlap in the color epitome, especially indoor scenes which all look equally beige. Finally,
in Fig. 3B we show examples of some images of different scenes mapped onto the stel epitome,
whose organization is illustrated by a rendering of all images averaged into the appropriate location
(similarly to the original color epitomes). Note that the model automatically clusters images using
the structure, and not colors, even in face of variation of colors present in the exemplars of the ?Car?,
or the ?Work office? classes (See also the supplemental video that illustrates the mapping dynamically). The regular epitome cannot capture these invariances, and it clusters images based on overall
intensity more readily than based on the structure of the scene. We evaluated the two models numerically in the following way. Using the two types of unsupervised epitomes, and the known labels
for the images in the training set, we assigned labels to the test set using the same classification rule
explained in the previous paragraph. This semi-supervised test reveals how consistent the clustering
induced by epitomes is with the human labeling. The stel epitome accuracy, 73,06%, outperforms
the standard epitome model [9], 69,42%, with statistical significance.
We have also trained both types of epitomes over a real estate 35 times larger than the original image
size using different random sets of 5000 images taken from the dataset. The stel epitomes trained in
an unsupervised way are qualitatively equivalent, in that they consistently capture around six of the
most prominent scenes from Fig. 2, whereas the traditional epitomes tended to capture only three.

4

Conclusions

The idea of recording our experiences is not new. (For a review and interesting research directions
see [15]). It is our opinion that recording, summarizing and browsing continuous visual input is particularly interesting. With the recent substantial increases in radio connectiviy, battery life, display
size, and computing power of small devices, and the avilability of even greater computing power
off line, summarizing one?s total visual input is now both a practically feasible and scientifically
interesting target for vision research. In addition, a variety of applications may arise once this functionality is provided. As a step in this direction, we provide a new dataset that contains a mix of
indoor and outdoor scenes as a result of two weeks of continuous image acquisition, as well as a
simple algorithm that deals with some of the invariances that have to be incorporated in a model of
such data. However, it is likely that modeling the geometry of the imaging process will lead to even
more interesting results. Although straightforward application of panoramic stitching algorithms,
such as Photosynth, did not work on this dataset, because of both the sheer number of images and
the significant variations in the lighting conditions, such methods or insights from their development
will most likely be very helpful in further development of unsupervised learning algorithms for such
types of datasets. The geometry constraints may lead to more reliable background alignments for
the next logical phase in modeling for ?All-I-have-seen? datasets: The learning of the foreground
object categories such as family members? faces. As this and other such datasets grow in size, the
unsupervised techniques for modeling the data in a way where interesting visual components emerge
over time will become both more practically useful and scientifically interesting.
7

Figure 3: Stel epitome of images captured by a wearable camera

8

s=1

s=2

B) IMAGE MAPPINGS ON THE STEL EPITOME

Work office stel-panorama

Car stel-panorama

10,4 pt

A) STEL EPITOME

C) EPITOME

Home office stel-panorama

Kitchen stel-panorama

D) STEL EPITOME
RECONSTRUCTION

References
[1] B. Frey and N. Jojic, ?Transformation-invariant clustering using the EM algorithm ?, TPAMI
2003, vol. 25, no. 1, pp. 1-17.
[2] N. Jojic, B. Frey, A. Kannan, ?Epitomic analysis of appearance and shape?, ICCV 2003.
[3] D. Lowe, ?Distinctive Image Features from Scale-Invariant Keypoints,? IJCV, 2004, vol. 60,
no. 2, pp. 91-110.
[4] L. Fei-Fei, P. Perona, ?A Bayesian Hierarchical Model for Learning Natural Scene Categories,?
IEEE CVPR 2005, pp. 524-531.
[5] S. Lazebnik, C. Schmid, J. Ponce, ?Beyond Bags of Features: Spatial Pyramid Matching for
Recognizing Natural Scene Categories,? IEEE CVPR, 2006, pp. 2169-2178.
[6] N. Jojic and C. Caspi, ?Capturing image structure with probabilistic index maps,? IEEE CVPR
2004, pp. 212-219.
[7] J. Winn and N. Jojic, ?LOCUS: Learning Object Classes with Unsupervised Segmentation?
ICCV 2005.
[8] N. Jojic, A.Perina, M.Cristani, V.Murino and B. Frey, ?Stel component analysis: modeling
spatial correlation in image class structure,? IEEE CVPR 2009.
[9] K. Ni, A. Kannan, A. Criminisi and J. Winn, ?Epitomic Location Recognition,? IEEE CVPR
2008.
[10] A. Perina, M. Cristani, U. Castellani, V. Murino and N. Jojic, ?Free energy score-space,? NIPS
2009.
[11] A. Torralba, K.P. Murphy, W.T. Freeman and M.A. Rubin, ?Context-based vision system for
place and object recognition,? ICCV 2003, pp. 273-280.
[12] C. Stauffer, E. Miller, and K. Tieu, ?Transform invariant image decomposition with similarity
templates,? NIPS 2003.
[13] V. Ferrari , A. Zisserman, ?Learning Visual Attributes,? NIPS 2007.
[14] B. Russell, A. Efros, J. Sivic, B. Freeman, A. Zisserman ?Segmenting Scenes by Matching
Image Composites,? NIPS 2009.
[15] G. Bell and J. Gemmell, Total Recall. Dutton Adult 2009.

9

"
1906,"Synergies between Intrinsic and Synaptic
Plasticity in Individual Model Neurons

Jochen Triesch
Dept. of Cognitive Science, UC San Diego, La Jolla, CA, 92093-0515, USA
Frankfurt Institute for Advanced Studies, Frankfurt am Main, Germany
triesch@ucsd.edu

Abstract
This paper explores the computational consequences of simultaneous intrinsic and synaptic plasticity in individual model neurons. It proposes
a new intrinsic plasticity mechanism for a continuous activation model
neuron based on low order moments of the neuron?s firing rate distribution. The goal of the intrinsic plasticity mechanism is to enforce a sparse
distribution of the neuron?s activity level. In conjunction with Hebbian
learning at the neuron?s synapses, the neuron is shown to discover sparse
directions in the input.

1

Introduction

Neurons in the primate visual system exhibit a sparse distribution of firing rates. In particular, neurons in different visual cortical areas show an approximately exponential distribution of their firing rates in response to stimulation with natural video sequences [1]. The
brain may do this because the exponential distribution maximizes entropy under the constraint of a fixed mean firing rate. The fixed mean firing rate constraint is often considered
to reflect a desired level of metabolic costs. This view is theoretically appealing. However,
it is currently not clear how neurons adjust their firing rate distribution to become sparse.
Several different mechanisms seem to play a role: First, synaptic learning can change a
neuron?s response to a distribution of inputs. Second, intrinsic learning may change conductances in the dendrites and soma to adapt the distribution of firing rates [7]. Third,
non-linear lateral interactions in a network can make a neuron?s responses more sparse [8].
In the extreme case this leads to winner-take-all networks, which form a code where only
a single unit is active for any given stimulus. Such ultra-sparse codes are considered inefficient, however. This paper investigates the interaction of intrinsic and synaptic learning
processes in individual model neurons in the learning of sparse codes.
We consider an individual continuous activation model neuron with a non-linear transfer
function that has adjustable parameters. We are proposing a simple intrinsic learning mechanism based on estimates of low-order moments of the activity distribution that allows the
model neuron to adjust the parameters of its non-linear transfer function to obtain an approximately exponential distribution of its activity. We then show that if combined with a
standard Hebbian learning rule employing multiplicative weight normalization, this leads
to the extraction of sparse features from the input. This is in sharp contrast to standard
Hebbian learning in linear units with multiplicative weight normalization, which leads to

the extraction of the principal Eigenvector of the input correlation matrix. We demonstrate
the behavior of the combined intrinsic and synaptic learning mechanisms on the classic
bars problem [4], a non-linear independent component analysis problem.
The remainder of this paper is organized as follows. Section 2 introduces our scheme for intrinsic plasticity and presents experiments demonstrating the effectiveness of the proposed
mechanism for inducing a sparse firing rate distribution. Section 3 studies the combination
of intrinsic plasticity with Hebbian learning at the synapses and demonstrates how it gives
rise to the discovery of sparse directions in the input. Finally, Sect. 4 discusses our findings
in the context of related work.

2

Intrinsic Plasticity Mechanism

Biological neurons do not only adapt synaptic properties but also change their excitability through the modification of voltage gated channels. Such intrinsic plasticity has been
observed across many species and brain areas [9]. Although our understanding of these
processes and their underlying mechanisms remains quite unclear, it has been hypothesized
that this form of plasticity contributes to a neuron?s homeostasis of its mean firing rate level.
Our basic hypothesis is that the goal of intrinsic plasticity is to ensure an approximately exponential distribution of firing rate levels in individual neurons. To our knowledge, this
idea was first investigated in [7], where a Hodgkin-Huxley style model with a number of
voltage gated conductances was considered. A learning rule was derived that adapts the
properties of voltage gated channels to match the firing rate distribution of the unit to a
desired distribution. In order to facilitate the simulation of potentially large networks we
choose a different, more abstract level of modeling employing a continuous activation unit
with a non-linear transfer function. Our model neuron is described by:
Y = S? (X) ,

X = wT u ,

(1)

where Y is the neuron?s output (firing rate), X is the neuron?s total synaptic current, w
is the neuron?s weight vector representing synaptic strengths, the vector u represents the
pre-synaptic input, and S? (.) is the neuron?s non-linear transfer function (activation function), parameterized by a vector of parameters ?. In this section we will not be concerned
with synaptic mechanism changing the weight vector w, so we will just consider a particular distribution p(X = x) ? p(x) of the net synaptic current and consider the resulting
distribution of firing rates p(Y = y) ? p(y). Intrinsic plasticity is modeled as inducing
changes to the non-linear transfer function with the goal of bringing the distribution of
activity levels p(y) close to an exponential distribution.
In general terms, the problem is that of matching a distribution to another. Given a signal
with a certain distribution, find a non-linear transfer function that converts the signal to
one with a desired distribution. In image processing, this is typically called histogram
matching. If there are no restrictions on the non-linearity then a solution can always be
found. The standard example is histogram equalization, where a signal is passed through
its own cumulative density function to give a uniform distribution over the interval [0, 1].
While this approach offers a general solution, it is unclear how individual neurons could
achieve this goal. In particular, it requires that the individual neuron can change its nonlinear transfer function arbitrarily, i.e. it requires infinitely many degrees of freedom.
2.1

Intrinsic Plasticity Based on Low Order Moments of Firing Rate

In contrast to the general scheme outlined above the approach proposed here utilizes a
simple sigmoid non-linearity with only two adjustable parameters a and b:
1
Sab (X) =
.
(2)
1 + exp (? (X ? b) /a)

Parameter a > 0 changes the steepness of the sigmoid, while parameter b shifts it
left/right1 . Qualitatively similar changes in spike threshold and slope of the activation
function have been observed in cortical neurons. Since the non-linearity has only two degrees of freedom it is generally not possible to ascertain an exponential activity distribution
for an arbitrary input distribution. A plausible alternative goal is to just match low order
moments of the activity distribution to those of a specific target distribution. Since our
sigmoid non-linearity has two parameters, we consider the first and second moments.
For a random variable T following an exponential distribution with mean ? we have:
p(T = t) =

1
exp (?t/?) ;
?

MT1 ? hT i = ? ;


 
MT2 ? T 2 = 2?2 ,

(3)

where h.i denotes the expected value operator. Our intrinsic plasticity rule is formulated as
a set of simple

  proportional control laws for a and b that drive the first and second moments
hY i and Y 2 of the output distributions to the values of the corresponding moments of an
exponential distribution MT1 and MT2 :


 
(4)
a? = ? Y 2 ? 2?2 , b? = ? (hY i ? ?) ,

where ? and ? are learning rates. The mean ? of the desired exponential distribution is
a free parameter which may vary across cortical areas. Equations (4) describe a system
of coupled integro-differential equations where

  the integration is implicit in the expected
value operations. Note that both hY i and Y 2 depend on the sigmoid parameters a and
b. From (4) it is obvious that there is a stationary point of these dynamics if the first and
second moment of Y equal the desired values of ? and 2?2 , respectively.
The first and second moments of Y 
needto be estimated online. In our model, we calculate
? 1 and M
? 2 of hY i and Y 2 according to:
estimates M
Y
Y
?? 1 = ?(y ? M
?1) ,
M
Y
Y

?? 2 = ?(y 2 ? M
?2) ,
M
Y
Y

(5)

where ? is a small learning rate.
2.2

Experiments with Intrinsic Plasticity Mechanism

We tested the proposed intrinsic plasticity mechanism for a number of distributions of
the synaptic current X (Fig. 1). Consider the case where this current follows a Gaussian
distribution with zero mean and 
unit variance: X ? N (0, 1). Under this assumption we can
calculate the moments hY i and Y 2 (although only numerically) for any particular values
of a and b. Panel a in Fig. 1 shows a phase diagram of this system. Its flow field is sketched
and two sample trajectories converging to a stationary point are
The stationary point

 given.

is at the intersection of the nullclines where hY i = ? and Y 2 = 2?2 . Its coordinates
are a? ? 0.90, b? ? 2.38. Panel b compares the theoretically optimal transfer function
(dotted), which would lead to an exactly exponential distribution of Y , with the learned
sigmoidal transfer function (solid). The learned transfer function gives a very good fit.
The resulting distribution of Y is in fact very close to the desired exponential distribution.
For the general case of a Gaussian input distribution with mean ?G and standard deviation
?G , the sigmoid parameters will converge to a ? a? ?G and b ? b? ?G + ?G under the
intrinsic plasticity rule. If the input to the unit can be assumed to be Gaussian, this relation
can be used to calculate the desired parameters of the sigmoid non-linearity directly.
1
Note that while we view adjusting a and b as changing the shape of the sigmoid non-linearity,
an equivalent view is that a and b are used to linearly rescale the signal X before it is passed through
a ?standard? logistic function. In general, however, intrinsic plasticity may give rise to non-linear
changes that cannot be captured by such a linear re-scaling of all weights.

a

b

10

1

8

input distribution
optimal transfer fct.
learned transfer fct.

0.8
6
b

0.6
4

0.4

2

0.2

0
0

1

2

3

4

0
?4

5

?2

0

2

4

a

c

d

input distribution
optimal transfer fct.
learned transfer fct.

1

1

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

0

0.2

0.4

0.6

0.8

1

0
0

input distribution
optimal transfer fct.
learned transfer fct.

0.2

0.4

0.6

0.8

1

Figure 1: Dynamics of intrinsic plasticity mechanism for various input distributions. a,b:
Gaussian input distribution. Panel a shows the phase plane diagram. Arrows indicate
the flow field of the system. Dotted lines indicate approximate locations of the nullclines
(found numerically).Two example trajectories are exhibited which converge to the stationary point (marked with a circle). Panel b shows the optimal (dotted) and learned transfer function (solid). The Gaussian input distribution (dashed, not drawn to scale) is also
shown. c,d: same as b but for uniform and exponential input distribution. Parameters were
? = 0.1, ? = 5 ? 10?4 , ? = 2 ? 10?3 , ? = 10?3 .
Panels c and d show the result of intrinsic plasticity for two other input distributions. In
the case of a uniform input distribution in the interval [0, 1] (panel c) the optimal transfer
function becomes infinitely steep for x ? 1. For an exponentially distributed input (panel
d), the ideal transfer function would simply be the identity function. In both cases the
intrinsic plasticity mechanism adjusts the sigmoid non-linearity in a sensible fashion and
the output distribution is a fair approximation of the desired exponential distribution.
2.3

Discussion of the Intrinsic Plasticity Mechanism

The proposed mechanism for intrinsic plasticity is effective in driving a neuron to exhibit
an approximately exponential distribution of firing rates as observed in biological neurons
in the visual system. The general idea is not restricted to the use of a sigmoid non-linearity.
The same adaptation mechanism can also be used in conjunction with, say, an adjustable
threshold-linear activation function. An interesting alternative to the proposed mechanism
can be derived by directly minimizing the KL divergence between the output distribution
and the desired exponential distribution through stochastic gradient descent. The resulting
learning rule, which is closely related to a rule for adapting a sigmoid nonlinearity to max-

imize the output entropy derived by Bell and Sejnowski[2], will be discussed elsewhere. It
leads to very similar results to the ones presented here.
A biological implementation of the proposed mechanism is plausible. All that is needed are
estimates of the first and second moment of the firing rate distribution. A specific, testable
prediction of the simple model is that changes to the distribution of a neuron?s firing rate
levels that keep the average firing rate of the neuron unchanged but alter the second moment
of the firing rate distribution should lead to measurable changes in the neuron?s excitability.

3

Combination of Intrinsic and Synaptic Plasticity

In this Section we want to study the effects of simultaneous intrinsic and synaptic learning for an individual model neuron. Synaptic learning is typically modeled with Hebbian
learning rules, of which a large number are being used in the literature. In principle, any
Hebbian learning rule can be combined with our scheme for intrinsic plasticity. Due to
space limitations, we only consider the simplest of all Hebbian learning rules:
?w = ?uY (u) = ?uSab (wT u) ,

(6)

where the notation is identical to that of Sec. 2 and ? is a learning rate. This learning rule
is unstable and needs to be accompanied by a scheme limiting weight growth. We simply
adopt a multiplicative normalization scheme that after each update re-scales the weight
vector to unit length: w ? w/|| w ||.
3.1

Analysis for the Limiting Case of Fast Intrinsic Plasticity

Under a few assumptions, an interesting intuition about the simultaneous intrinsic and Hebbian learning can be gained. Consider the limit of intrinsic plasticity being much faster than
Hebbian plasticity. This may not be very plausible biologically, but it allows for an interesting analysis. In this case we may assume that the non-linearity has adapted to give an
approximately exponential distribution of the firing rate Y before w can change much.
Thus, from (6), ?w can be seen as a weighted sum of the inputs u, with the activities
Y acting as weights that follow an approximately exponential distribution. Since similar
inputs u will produce similar outputs Y , the expected value of the weight update h?wi
will be dominated by a small set of inputs that produce the highest output activities. The
remainder of the inputs will ?pull? the weight vector back to the average input hui. Due
to the multiplicative weight normalization, the stationary states of the weight vector are
reached if ?w is parallel to w, i.e., if h?wi = kw for some constant k.
A simple example shall illustrate the effect of intrinsic plasticity on Hebbian learning in
more detail. Consider the case where there are only two clusters of inputs at the locations
c1 and c2 . Let us also assume that both clusters account for exactly half of the inputs. If
the weight vector is slightly closer to one of the two clusters, inputs from this cluster will
activate the unit more strongly and will exert a stronger ?pull? on the weight vector. Let
m = ? ln(2) denote the median of the exponential firing rate distribution with mean ?.
Then inputs from the closer cluster, say c1 , will be responsible for all activities above m
while the inputs from the other cluster will be responsible for all activities below m. Hence,
the expected value of the weight update h?wi will be given by:
Z ?
Z m
y
y
h?wi ? ?c1
exp(?y/?)dy + ?c2
exp(?y/?)dy
(7)
?
?
m
0
??
=
((1 + ln 2) c1 + (1 ? ln 2) c2 ) .
(8)
2
Taking the multiplicative weight normalization into account, we see that the weight vector

?3

x 10

0

10

6

?1

frequency

contribution to weight vector fi

8

4

10

?2

10

2
?3

0
0

10
200

400
600
cluster number i

800

1000

0

2
4
6
contribution to weight vector f

i

8
?3

x 10

Figure 2: Left: relative contributions to the weight vector fi for N = 1000 input clusters
(sorted). Right: the distribution of the fi is approximately exponential.
will converge to either of the following two stationary states:
(1 ? ln 2)c1 + (1 ? ln 2)c2
w=
.
(9)
|| (1 ? ln 2)c1 + (1 ? ln 2)c2 ||
The weight vector moves close to one of the two clusters but does not fully commit to it.
For the general case of N input clusters, only a few clusters will strongly contribute to the
final weight vector. Generalizing the result from above, it is not difficult to derive that the
weight vector w will be proportional to a weighted sum of the cluster centers:
w?

N
X

fi ci ; with fi = 1 + log(N ) ? i log(i) + (i ? 1) log(i ? 1) ,

(10)

i=1

where we define 0 log(0) ? 0. Here, fi denotes the relative contribution of the i-th closest
input cluster to the final weight vector. There can be at most N ! resulting weight vectors
owing to the number of possible assignments of the fi to the clusters. Note that the final
weight vector does not depend on the desired mean activity level ?. Fig. 2 plots (10)
for N = 1000 (left) and shows that the resulting distribution of the fi is approximately
exponential (right).
We can see why such a weight vector may correspond to a sparse direction in the input space
as follows: consider the case where the input cluster centers are random vectors of unit
length in a high-dimensional space. It is a property of high-dimensional spaces that random
vectors are approximately orthogonal, so that cTi cj ? ?ij , where ?ij is the Kronecker delta.
If we consider the projection ofPan input from an arbitrary cluster, say cj , onto the weight
T
T
vector, we see that wT cj ?
i fi ci cj ? fj . The distribution of X = w u follows
the distribution of the fi , which is approximately exponential. Thus, the projection of all
inputs onto the weight vector has an approximately exponential distribution. Note that this
behavior is markedly different from Hebbian learning in a linear unit which leads to the
extraction of the principal eigenvector of the input correlation matrix.
It is interesting to note that in this situation the optimal transfer function S ? that will make
the unit?s activity Y have an exponential distribution of a desired mean ? is simply a multiplication with a constant k, i.e. S ? (X) = kX. Thus, depending on the initial weight vector
and the resulting distribution of X, the neuron?s activation function may transiently adapt
to enforce an approximately exponential firing rate distribution, but the simultaneous Hebbian learning drives it back to a linear form. In the end, a simple linear activation function
may result from this interplay of intrinsic and synaptic plasticity. In fact, the observation
of approximately linear activation functions in cortical neurons is not uncommon.

activity

1

0.5

0
0

0.5

1

1.5

2

2.5

3

3.5

4

4.5

5
4

x 10
input patterns/10000

Figure 3: Left: example stimuli from the ?bars? problem for a 10 by 10 pixel retina. Right:
the activity record shows the unit?s response to every 10th input pattern. Below, we show
the learned weight vector after presentation of 10,000, 20,000, and 30,000 training patterns.
3.2

Application to the ?Bars? Problem

The ?bars? problem is a standard problem for unsupervised learning architectures [4]. It
is a non-linear ICA problem for which traditional ICA approaches have been shown to
fail [5]. The input domain consists of an N -by-N retina. On this retina, all horizontal
and vertical bars (2N in total) can be displayed. The presence or absence of each bar is
determined independently, with every bar occurring with the same probability p (in our
case p = 1/N ). If a horizontal and a vertical bar overlap, the pixel at the intersection point
will be just as bright as any other pixels on the bars, rather than twice as bright. This makes
the problem a non-linear ICA problem. Example stimuli from the bars dataset are shown
in Fig. 3 (left). Note that we normalize input vectors to unit length. The goal of learning in
the bars problem is to find the independent sources of the images, i.e., the individual bars.
Thus, the neural learning system should develop filters that represent the individual bars.
We have trained an individual sigmoidal model neuron on the bars input domain. The
theoretical analysis above assumed that intrinsic plasticity is much faster than synaptic
plasticity. Here, we set the intrinsic plasticity to be slower than the synaptic plasticity,
which is more plausible biologically, to see if this may still allow the discovery of sparse
directions in the input. As illustrated in Fig. 3 (right) the unit?s weight vector aligns with
one of the individual bars as soon as the intrinsic plasticity has pushed the model neuron
into a regime where its responses are sparse: the unit has discovered one of the independent
sources of the input domain. This result is robust if the desired mean activity ? of the unit
is changed over a wide range. If ? is reduced from its default value (1/2N = 0.05)
over several orders of magnitude (we tried down to 10?5 ) the result remains unchanged.
However, if ? is increased above about 0.15, the unit will fail to represent an individual
bar but will learn a mixture of two or more bars, with different bars being represented with
different strengths. Thus, in this example ? in contrast to the theoretical result above ?
the desired mean activity ? does influence the weight vector that is being learned. The
reason for this is that the intrinsic plasticity only imperfectly adjusts the output distribution
to the desired exponential shape. As can be seen in Fig. 3 the output has a multimodal
structure. For low ?, only the highest mode, which corresponds to a specific single bar
presented in isolation, contributes strongly to the weight vector.

4

Discussion

Biological neurons are highly adaptive computation devices. While the plasticity of a neuron?s synapses has always been a core topic of neural computation research, there has been
little work investigating the computational properties of intrinsic plasticity mechanisms and

the relation between intrinsic and synaptic learning. This paper has investigated the potential role of intrinsic learning mechanisms operating at the soma when used in conjunction
with Hebbian learning at the synapses. To this end, we have proposed a new intrinsic plasticity mechanism that adjusts the parameters of a sigmoid nonlinearity to move the neuron?s
firing rate distribution to a sparse regime. The learning mechanism is effective in producing approximately exponential firing rate distributions as observed in neurons in the visual
system of cats and primates. Studying simultaneous intrinsic and synaptic learning, we
found a synergistic relation between the two. We demonstrated how the two mechanisms
may cooperate to discover sparse directions in the input. When applied to the classic ?bars?
problem, a single unit was shown to discover one of the independent sources as soon as the
intrinsic plasticity moved the unit?s activity distribution into a sparse regime. Thus, this research is related to other work in the area of Hebbian projection pursuit and Hebbian ICA,
e.g., [3, 6]. In such approaches, the ?standard? Hebbian weight update rule is modified to
allow the discovery of non-gaussian directions in the input. We have shown that the combination of intrinsic plasticity with the standard Hebbian learning rule can be sufficient for
the discovery of sparse directions in the input. Future work will analyze the combination
of intrinsic plasticity with other Hebbian learning rules. Further, we would like to consider
networks of such units and the formation of map-like representations. The nonlinear nature
of the transfer function may facilitate the construction of hierarchical networks for unsupervised learning. It will also be interesting to study the effects of intrinsic plasticity in the
context of recurrent networks, where it may contribute to keeping the network in a certain
desired dynamic regime.
Acknowledgments
The author is supported by the National Science Foundation under grants NSF 0208451
and NSF 0233200. I thank Erik Murphy-Chutorian and Emanuel Todorov for discussions
and comments on earlier drafts.

References
[1] R. Baddeley, L. F. Abbott, M.C. Booth, F. Sengpiel, and T. Freeman. Responses of neurons in
primary and inferior temporal visual cortices to natural scenes. Proc. R. Soc. London, Ser. B,
264:1775?1783, 1998.
[2] A. J. Bell and T. J. Sejnowski. An information-maximization approach to blind separation and
blind deconvolution. Neural Computation, 7:1129?1159, 1995.
[3] B. S. Blais, N. Intrator, H. Shouval, and L. N. Cooper. Receptive field formation in natural scene
environments. Neural Computation, 10:1797?1813, 1998.
[4] P. F?oldi?ak. Forming sparse representations by local anti-hebbian learning. Biological Cybernetics, 64:165?170, 1990.
[5] S. Hochreiter and J. Schmidhuber. Feature extraction through LOCOCODE. Neural Computation, 11(3):679?714, 1999.
[6] A. Hyv?arinen and E. Oja. Independent component analysis by general nonlinear hebbian-like
learning rules. Signal Processing, 64(3):301?313, 1998.
[7] M. Stemmler and C. Koch. How voltage-dependent conductances can adapt to maximize the
information encoded by neuronal firing rate. Nature Neuroscience, 2(6):521?527, 1999.
[8] W. E. Vinje and J. L. Gallant. Sparse coding and decorrelation in primary visual cortex during
natural vision. Science, 287:1273?1276, 2000.
[9] W. Zhang and D. J. Linden. The other side of the engram: Experience-driven changes in neuronal
intrinsic excitability. Nature Reviews Neuroscience, 4:885?900, 2003.

"
2109,"Variable KD-Tree Algorithms for Spatial Pattern
Search and Discovery

Jeremy Kubica
Robotics Institute
Carnegie Mellon University
Pittsburgh, PA 15213

Joseph Masiero
Institute for Astronomy
University of Hawaii
Honolulu, HI 96822

Andrew Moore
Robotics Institute
Carnegie Mellon University
Pittsburgh, PA 15213

jkubica@ri.cmu.edu

masiero@ifa.hawaii.edu

awm@cs.cmu.edu

Robert Jedicke
Institute for Astronomy
University of Hawaii
Honolulu, HI 96822

Andrew Connolly
Physics & Astronomy Department
University of Pittsburgh
Pittsburgh, PA 15213

jedicke@ifa.hawaii.edu

ajc@phyast.pitt.edu

Abstract
In this paper we consider the problem of finding sets of points that conform to a given underlying model from within a dense, noisy set of observations. This problem is motivated by the task of efficiently linking
faint asteroid detections, but is applicable to a range of spatial queries.
We survey current tree-based approaches, showing a trade-off exists between single tree and multiple tree algorithms. To this end, we present a
new type of multiple tree algorithm that uses a variable number of trees
to exploit the advantages of both approaches. We empirically show that
this algorithm performs well using both simulated and astronomical data.

1 Introduction
Consider the problem of detecting faint asteroids from a series of images collected on a
single night. Inherently, the problem is simply one of connect-the-dots. Over a single
night we can treat the asteroid?s motion as linear, so we want to find detections that, up
to observational errors, lie along a line. However, as we consider very faint objects, several difficulties arise. First, objects near our brightness threshold may oscillate around this
threshold, blinking into and out-of our images and providing only a small number of actual
detections. Second, as we lower our detection threshold we will begin to pick up more spurious noise points. As we look for really dim objects, the number of noise points increases
greatly and swamps the number of detections of real objects.
The above problem is one example of a model based spatial search. The goal is to identify
sets of points that fit some given underlying model. This general task encompasses a wide
range of real-world problems and spatial models. For example, we may want to detect
a specific configuration of corner points in an image or search for multi-way structure in
scientific data. We focus our discussion on problems that have a high density of both true

and noise points, but which may have only a few points actually from the model of interest.
Returning to the asteroid linking example, this corresponds to finding a handful of points
that lie along a line within a data set of millions of detections.
Below we survey several tree-based approaches for efficiently solving this problem. We
show that both single tree and conventional multiple tree algorithms can be inefficient and
that a trade-off exists between these approaches. To this end, we propose a new type of
multiple tree algorithm that uses a variable number of tree nodes. We empirically show
that this new algorithm performs well using both simulated and real-world data.

2 Problem Definition
Our problem consists of finding sets of points that fit a given underlying spatial model. In
doing so, we are effectively looking for known types of structure buried within the data. In
general, we are interested in finding sets with k or more points, thus providing a sufficient
amount of support to confirm the discovery. Finding this structure within the data may
either be our end goal, such as in asteroid linkage, or may just be a preprocessor for a more
sophisticated statistical test, such as renewal strings [1]. We are particularly interested in
high-density, low-support domains where there may be many hundreds of thousands of
points, but only a handful actually support our model.
Formally, the data consists of N unique D-dimensional points. We assume that the underlying model can be estimated from c unique points. Since k ? c, the model may overconstrained. In these cases we divide the points into two sets: Model Points and Support
Points. Model points are the c points used to fully define the underlying model. Support
points are the remaining points used to confirm the model. For example, if we are searching for sets of k linear points, we could use a set?s endpoints as model points and treat the
middle k ? 2 as support points. Or we could allow any two points to serve as model points,
providing an exhaustive variant of the RANSAC algorithm [2].
The prototypical example used in this paper is the (linear) asteroid linkage problem:
For each pair of points find the k ? 2 best support points for the line that
they define (such that we use at most one point at each time step).
In addition, we place restrictions on the validity of the initial pairs by providing velocity
bounds. It is important to note that although we use this problem as a running example, the
techniques described can be applied to a range of spatial problems.

3 Overview of Previous Approaches
3.1 Constructive Algorithms
Constructive algorithms ?build up? valid sets of points by repeatedly finding additional
points that are compatible with the current set. Perhaps the simplest approach is to perform
a two-tiered brute force search. First, we exhaustively test all sets of c points to determine
if they define a valid model. Then, for each valid set we test all of the remaining points for
support. For example in the asteroid linkage problem, we can initially search over all O(N 2 )
pairs of points and for each of the resulting lines test all O(N) points to determine if they
support that line. A similar approach within the domain of target tracking is sequential
tracking (for a good introduction see [3]), where points at early time steps are used to
estimate a track that is then projected to later time steps to find additional support points.
In large-scale domains, these approaches can often be made tractable by using spatial structure in the data. Again returning to our asteroid example, we can place the points in a

KD-tree [4]. We can then limit the number of initial pairs examined by using this tree to
find points compatible with our velocity constraints. Further, we can use the KD-tree to
only search for support points in localized regions around the line, ignoring large numbers
of obviously infeasible points. Similarly, trees have been used in tracking algorithms to
efficiently find points near predicted track positions [5]. We call these adaptations single
tree algorithms, because at any given time the algorithm is searching at most one tree.
3.2 Parameter Space Methods
Another approach is to search for valid sets of points by searching the model?s parameter
space, such as in the Hough transform [6]. The idea behind these approaches is that we can
test whether each point is compatible with a small set of model parameters, allowing us to
search parameter space to find the valid sets. However, this method can be expensive in
terms of both computation and memory, especially for high dimensional parameter spaces.
Further, if the model?s total support is low, the true model occurrences may be effectively
washed out by the noise. For these reasons we do not consider parameter space methods.
3.3 Multiple Tree Algorithms
The primary benefit of tree-based algorithms is that they are able to use spatial structure
within the data to limit the cost of the search. However, there is a clear potential to push
further and use structure from multiple aspects of the search at the same time. In doing
so we can hopefully avoid many of the dead ends and wrong turns that may result from
exploring bad initial associations in the first few points in our model. For example, in the
domain of asteroid linkage we may be able to limit the number of short, initial associations
that we have to consider by using information from later time steps. This idea forms the
basis of multiple tree search algorithms [7, 8, 9].
Multiple tree methods explicitly search for the entire set of points at once by searching
over combinations of tree nodes. In standard single tree algorithms, the search tries to find
individual points satisfying some criteria (e.g. the next point to add) and the search state
is represented by a single node that could contain such a point. In contrast, multiple tree
algorithms represent the current search state with multiple tree nodes that could contain
points that together conform to the model. Initially, the algorithm begins with k root nodes
from either the same or different tree data structures, representing the k different points that
must be found. At each step in the search, it narrows in on a set of mutually compatible
spatial regions and thus a set of individual points that fit the model by picking one of the
model nodes and recursively exploring its children. As with a standard ?single tree? search,
we constantly check for opportunities to prune the search.
There are several important drawbacks to multiple tree algorithms. First, additional trees
introduce a higher branching factor in the search and increase the potential for taking deep
?wrong turns.? Second, care must be taken in order to deal with missing or a variable
number of support points. Kubica et. al. discuss the use of an additional ?missing? tree
node to handle these cases [9]. However, this approach can effectively make repeated
searches over subsets of trees, making it more expensive both in theory and practice.

4 Variable Tree Algorithms
In general we would like to exploit structural information from all aspects of our search
problem, but do so while branching the search on just the parameters of interest. To this
end we propose a new type of search that uses a variable number of tree nodes. Like a
standard multiple tree algorithm, the variable tree algorithm searches combinations of tree
nodes to find valid sets of points. However, we limit this search to just those points required

(A)

(B)

Figure 1: The model nodes? bounds (1 and 2) define a region of feasible support (shaded)
for any combination of model points from those nodes (A). As shown in (B), we can classify
entire support tree nodes as feasible (node b) or infeasible (nodes a and c).
to define, and thus bound, the models currently under consideration. Specifically, we use M
model tree nodes,1 which guide the recursion and thus the search. In addition, throughout
the search we maintain information about other potential supporting points that can be used
to confirm the final track or prune the search due to a lack of support.
For example in the asteroid linking problem each line is defined by only 2 points, thus we
can efficiently search through the models using a multiple tree search with 2 model trees.
As shown in Figure 1.A, the spatial bounds of our current model nodes immediately limit
the set of feasible support points for all line segments compatible with these nodes. If we
track which support points are feasible, we can use this information to prune the search due
to a lack of support for any model defined by the points in those nodes.
The key idea behind the variable tree search is that we can use a dynamic representation of
the potential support. Specifically, we can place the support points in trees and maintain
a dynamic list of currently valid support nodes. As shown in Figure 1.B, by only testing
entire nodes (instead of individual points), we are using spatial coherence of the support
points to remove the expense of testing each support point at each step in the search. And
by maintaining a list of support tree nodes, we are no longer branching the search over
these trees. Thus we remove the need to make a hard ?left or right? decision. Further, using
a combination of a list and a tree for our representation allows us to refine our support
representation on the fly. If we reach a point in the search where a support node is no
longer valid, we can simply drop it off the list. And if we reach a point where a support
node provides too coarse a representation of the current support space, we can simply
remove it and add both of its children to the list.
This leaves the question of when to split support nodes. If we split them too soon, we may
end up with many support nodes in our list and mitigate the benefits of the nodes? spatial
coherence. If we wait too long to split them, then we may have a few large support nodes
that cannot efficiently be pruned. Although we are still investigating splitting strategies, the
experiments in this paper use a heuristic that seeks to provide a small number of support
nodes that are a reasonable fit to the feasible region. We effectively split a support node
if doing so would allow one of its two children to be pruned. For KD-trees this roughly
means checking whether the split value lies outside the feasible region.
The full variable tree algorithm is given in Figure 2. A simple example of finding linear
tracks while using the track?s endpoints (earliest and latest in time) as model points and
1 Typically M = c, although in some cases it may be beneficial to use a different number of model
nodes.

1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.

Variable Tree Model Detection
Input: A set of M current model tree nodes M
A set of current support tree nodes S
Output: A list Z of feasible sets of points
S? ? {} and Scurr ? S
IF we cannot prune based on the mutual compatibility of M:
FOR each s ? Scurr
IF s is compatible with M:
IF s is ?too wide?:
Add s?s left and right child to the end of Scurr .
ELSE
Add s to S? .
IF we have enough valid support points:
IF all of m ? M are leaves:
Test all combinations of points owned by the model nodes, using
the support nodes? points as potential support.
Add valid sets to Z.
ELSE
Let m? be the non-leaf model tree node that owns the most points.
Search using m? ?s left child in place of m? and S? instead of S.
Search using m? ?s right child in place of m? and S? instead of S.

Figure 2: A simple variable tree algorithm for spatial structure search. This algorithm
shown uses simple heuristics such as: searching the model node with the most points and
splitting a support node if it is too wide. These heuristics can be replaced by more accurate,
problem-specific ones.

using all other points for support is illustrated in Figure 3. The first column shows all
the tree nodes that are currently part of the search. The second and third columns show
the search?s position on the two model trees and the current set of valid support nodes
respectively. Unlike the pure multiple tree search, the variable tree search does not ?branch
off? on the support trees, allowing us to consider multiple support nodes from the same
time step at any point in the search. Again, it is important to note that by testing the
support points as we search, we are both incorporating support information into the pruning
decisions and ?pruning? the support points for entire sets of models at once.

5 Results on the Asteroid Linking Domain
The goal of the single-night asteroid linkage problem is to find sets of 2-dimensional point
detections that correspond to a roughly linear motion model. In the below experiments we
are interested in finding sets of at least 7 detections from a sequence of 8 images. The
movements were constrained to have a speed between 0.05 and 0.5 degrees per day and
were allowed an observational error threshold of 0.0003 degrees. All experiments were run
on a dual 2.5 GHz Apple G5 with 4 GB of RAM.
The asteroid detection data consists of detections from 8 images of the night sky separated
by half-hour intervals. The images were obtained with the MegaCam instrument on the
3.6-meter Canada-France-Hawaii Telescope. The detections, along with confidence levels,
were automatically extracted from the images. We can pre-filter the data to pull out only
those observations above a given confidence threshold ?. This allows us to examine how
the algorithms perform as we begin to look for increasingly faint asteroids. It should be
noted that only limited preprocessing was done to the data, resulting in a very high level

Search Step 1:

Search Step 2:

Search Step 5:

Figure 3: The variable tree algorithm performs a depth first search over the model nodes.
At each level of the search the model nodes are checked for mutual compatibility and each
support node on the list is check for compatibility with the set of model nodes. Since we
are not branching on the support nodes, we can split a support node and add both children
to our list. This figure shows the current model and support nodes and their spatial regions.

Table 1: The running times (in seconds) for the asteroid linkers with different detection
thresholds ? and thus different numbers N and density of observations.
?
N
Single Tree
Multiple Tree
Variable Tree

10.0
3531
2
1
<1

8.0
5818
7
3
1

6.0
12911
61
30
4

5.0
24068
488
607
40

4.0
48646
2442
4306
205

of false detections. While future data sets will contain significantly reduced noise, it is
interesting to examine the performance of the algorithms on this real-world high noise,
high density data.
The results on the intra-night asteroid tracking domain, shown in Table 1, illustrate a clear
advantage to using a variable tree approach. As the significance threshold ? decreases,
the number and density of detections increases, allowing the support tree nodes to capture
feasibility information for a large number of support points. In contrast, neither the full
multiple tree algorithm nor the single-tree algorithm performed well. For the multiple tree
algorithm, this decrease in performance is likely due to a combination of the high number
of time steps, the allowance of a missing observation, and the high density. In particular,
the increased density can reduce opportunities for pruning, causing the algorithm to explore
deeper before backtracking.

Table 2: Average running times (in seconds) for a 2-dimensional rectangle search with
different numbers of points N. The brute force algorithm was only run to N = 2500.
N
Brute Force
Single Tree
Multi-Tree
Variable-Tree

500
0.37
0.02
0.01
0.01

1000
2.73
0.07
0.02
0.02

2000
21.12
0.30
0.06
0.05

2500
41.03
0.51
0.09
0.07

5000
n/a
2.15
0.30
0.22

10000
n/a
10.05
1.11
0.80

25000
n/a
66.24
6.61
4.27

50000
n/a
293.10
27.79
16.30

Table 3: Average running times (in seconds) for a rectangle search with different numbers
of required corners k. For this experiment N = 10000 and D = 3.
k
Single Tree
Multi-Tree
Variable-Tree

8
4.71
3.96
0.65

7
4.72
19.45
0.75

6
4.71
45.02
0.85

5
4.71
67.50
0.92

4
4.71
78.81
1.02

6 Experiments on the Simulated Rectangle Domain
We can apply the above techniques to a range of other model-based spatial search problems.
In this section we consider a toy template matching problem, finding axis-aligned hyperrectangles in D-dimensional space by finding k or more corners that fit a rectangle. We
use this simple, albeit artificial, problem both to demonstrate potential pattern recognition
applications and to analyze the algorithms as we vary the properties of the data.
Formally, we restrict the model to use the upper and lower corners as the two model points.
Potential support points are those points that fall within some threshold of the other 2D ? 2
corners. In addition, we restrict the allowable bounds of the rectangles by providing a
maximum width.
To evaluate the algorithms? relative performance, we used random data generated from a
uniform distribution on a unit hyper-cube. The threshold and maximum width were fixed
for all experiments at 0.0001 and 0.2 respectively. All experiments were run on a dual 2.5
GHz Apple G5 with 4 GB of RAM.
The first factor that we examined was how each algorithm scales with the number of points.
We generated random data with 5 known rectangles and N additional random points and
computed the average wall-clock running time (over ten trials) for each algorithm. The
results, shown in Table 2, show a graceful scaling of all of the multiple tree algorithms. In
contrast, the brute force and single tree algorithms run into trouble as the number of points
becomes moderately large. The variable tree algorithm consistently performs the best, as it
is able to avoid significant amounts of redundant computation.
One potential drawback of the full multiple tree algorithm is that since it branches on all
points, it may become inefficient as the allowable number of missing support points grows.
To test this we looked at 3-dimensional data and varied the minimum number of required
support points k. As shown in Table 3, all multiple tree methods become more expensive
as the number of required support points decreases. This is especially the case for the
multi-tree algorithm, which has to perform several almost identical searches to account for
missing points. However, the variable-tree algorithm?s performance degrades gracefully
and is the best for all trials.

7 Conclusions
Tree-based spatial algorithms provide the potential for significant computational savings
with multiple tree algorithms providing further opportunities to exploit structure in the
data. However, a distinct trade-off exists between ignoring structure from all aspects of
the problem and increasing the combinatorics of the search. We presented a variable tree
approach that exploits the advantages of both single tree and multiple tree algorithms. A
combinatorial search is carried out over just the minimum number of model points, while
still tracking the feasibility of the various support points. As shown in the above experiments, this approach provides significant computational savings over both the traditional
single tree and and multiple tree searches. Finally, it is interesting to note that the dynamic
support technique described in this paper is general and may be applied to a range of other
algorithms, such as the Fast Hough Transform [10], that maintain information on which
points support a given model.
Acknowledgments
Jeremy Kubica is supported by a grant from the Fannie and John Hertz Foundation. Andrew
Moore and Andrew Connolly are supported by a National Science Foundation ITR grant
(CCF-0121671).

References
[1] A.J. Storkey, N.C. Hambly, C.K.I. Williams, and R.G. Mann. Renewal Strings for
Cleaning Astronomical Databases. In UAI 19, 559-566, 2003.
[2] M.A. Fischler and R.C. Bolles. Random Sample Consensus: A Paradigm for Model
Fitting with Applications to Image Analysis and Automated Cartography. Comm. of
the ACM, 24:381?395, 1981.
[3] S. Blackman and R. Popoli. Design and Analysis of Modern Tracking Systems. Artech
House, 1999.
[4] J.L. Bentley . Multidimensional Binary Search Trees Used for Associative Searching.
Comm. of the ACM, 18 (9), 1975.
[5] J. K. Uhlmann. Algorithms for multiple-target tracking.
80(2):128?141, 1992.

American Scientist,

[6] P. V. C. Hough. Machine analysis of bubble chamber pictures. In International Conference on High Energy Accelerators and Instrumentation. CERN, 1959.
[7] A. Gray and A. Moore. N-body problems in statistical learning. In T. K. Leen and
T. G. Dietterich, editors, Advances in Neural Information Processing Systems. MIT
Press, 2001.
[8] G. R. Hjaltason and H. Samet. Incremental distance join algorithms for spatial
databases. In Proc. of the 1998 ACM-SIGMOD Conference, 237?248, 1998.
[9] J. Kubica, A. Moore, A. Connolly, and R. Jedicke. A Multiple Tree Algorithm for the
Efficient Association of Asteroid Observations. In KDD?05. August 2005.
[10] H. Li, M.A. Lavin, and R.J. Le Master. Fast Hough Transform: A Hierarchical
Approach. In Computer Vision, Graphics, and Image Processing, 36(2-3):139?161,
November 1986.

"
5156,"Variational Information Maximisation for
Intrinsically Motivated Reinforcement Learning
Shakir Mohamed and Danilo J. Rezende
Google DeepMind, London
{shakir, danilor}@google.com

Abstract
The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple
data modalities, in maximising the efficiency of noisy transmission channels, or
when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the
Blahut-Arimoto algorithm ? an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper
provides a new approach for scalable optimisation of the mutual information by
merging techniques from variational inference and deep learning. We develop our
approach by focusing on the problem of intrinsically-motivated learning, where
the mutual information forms the definition of a well-known internal drive known
as empowerment. Using a variational lower bound on the mutual information,
combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information
maximisation and empowerment-based reasoning directly from pixels to actions.

1 Introduction
The problem of measuring and harnessing dependence between random variables is an inescapable
statistical problem that forms the basis of a large number of applications in machine learning, including rate distortion theory [4], information bottleneck methods [28], population coding [1], curiositydriven exploration [26, 21], model selection [3], and intrinsically-motivated reinforcement learning
[22]. In all these problems the core quantity that must be reasoned about is the mutual information.
In general, the mutual information (MI) is intractable to compute and few existing algorithms are
useful for realistic applications. The received algorithm for estimating mutual information is the
Blahut-Arimoto algorithm [31] that effectively solves for the MI by enumeration ? an approach
with exponential complexity that is not suitable for modern machine learning applications. By combining the best current practice from variational inference with that of deep learning, we bring the
generality and scalability seen in other problem domains to information maximisation problems.
We provide a new approach for maximisation of the mutual information that has significantly lower
complexity, allows for computation with high-dimensional sensory inputs, and that allows us to
exploit modern computational resources.
The technique we derive is generally applicable, but we shall describe and develop our approach
by focussing on one popular and increasingly topical application of the mutual information: as a
measure of ?empowerment? in intrinsically-motivated reinforcement learning. Reinforcement learning (RL) has seen a number of successes in recent years that has now established it as a practical,
scalable solution for realistic agent-based planning and decision making [16, 13]. A limitation of
the standard RL approach is that an agent is only able to learn using external rewards obtained from
its environment; truly autonomous agents will often exist in environments that lack such external
rewards or in environments where rewards are sparsely distributed. Intrinsically-motivated reinforcement learning [25] attempts to address this shortcoming by equipping an agent with a number
of internal drives or intrinsic reward signals, such as hunger, boredom or curiosity that allows the
agent to continue to explore, learn and act meaningfully in a reward-sparse world. There are many
1

State Representation

External Environment
Action

Decoder

Observation

z

q(a1 , . . . , aK |s, s0 )
z

Environment

z

?

Agent

x?

Internal Environment

a1

a2

aK

s?
Source ?(A|s)

Option KB

Critic

h(a1 , . . . , aK |s)

State Repr.
z

State
Embedding

Option

z

z

?
x

Planner

s

a1

a2

aK

?(s)

Figure 1: Perception-action loop separating environment into internal and external facets.

Figure 2: Computational graph for variational information maximisation.

ways in which to formally define internal drives, but what all such definitions have in common is
that they, in some unsupervised fashion, allow an agent to reason about the value of information in
the action-observation sequences it experiences. The mutual information allows for exactly this type
of reasoning and forms the basis of one popular intrinsic reward measure, known as empowerment.
Our paper begins by describing the framework we use for online and self-motivated learning (section 2) and then describes the general problem associated with mutual information estimation and
empowerment (section 3). We then make the following contributions:
? We develop stochastic variational information maximisation, a new algorithm for scalable estimation of the mutual information and channel capacity that is applicable to both discrete and
continuous settings.
? We combine variational information optimisation and tools from deep learning to develop a scalable algorithm for intrinsically-motivated reinforcement learning, demonstrating a new application of the variational theory for problems in reinforcement learning and decision making.
? We demonstrate that empowerment-based behaviours obtained using variational information maximisation match those using the exact computation. We then apply our algorithms to a broad range
of high-dimensional problems for which it is not possible to compute the exact solution, but for
which we are able to act according to empowerment ? learning directly from pixel information.

2 Intrinsically-motivated Reinforcement Learning
Intrinsically- or self-motivated learning attempts to address the question of where rewards come
from and how they are used by an autonomous agent. Consider an online learning system that
must model and reason about its incoming data streams and interact with its environment. This
perception-action loop is common to many areas such as active learning, process control, black-box
optimisation, and reinforcement learning. An extended view of this framework was presented by
Singh et al. [25], who describe the environment as factored into external and internal components
(figure 1). An agent receives observations and takes actions in the external environment. Importantly, the source and nature of any reward signals are not assumed to be provided by an oracle in
the external environment, but is moved to an internal environment that is part of the agent?s decisionmaking system; the internal environment handles the efficient processing of all input data and the
choice and computation of an appropriate internal reward signal.
There are two important components of this framework: the state representation and the critic. We
are principally interested in vision-based self-motivated systems, for which there are no solutions
currently developed. To achieve this, our state representation system is a convolutional neural network [14]. The critic in figure 1 is responsible for providing intrinsic rewards that allow the agent to
act under different types of internal motivations, and is where information maximisation enters the
intrinsically-motivated learning problem.
The nature of the critic and in particular, the reward signal it provides is the main focus of this
paper. A wide variety of reward functions have been proposed, and include: missing information or
Bayesian surprise, which uses the KL divergence to measure the change in an agents internal belief
after the observation of new data [8, 24]; measures based on prediction errors of future states such
predicted L1 change, predicted mode change or probability gain [17], or salient event prediction
[25]; and measures based on information-theoretic quantities such as predicted information gain
(PIG) [15], causal entropic forces [30] or empowerment [23]. The paper by Oudeyer & Kaplan [19]
2

currently provides the widest singular discussion of the breadth of intrinsic motivation measures.
Although we have a wide choice of intrinsic reward measures, none of the available informationtheoretic approaches are efficient to compute or scalable to high-dimensional problems: they require
either knowledge of the true transition probability or summation over all configurations of the state
space, which is not tractable for complex environments or when the states are large images.

3 Mutual Information and Empowerment
The mutual information is a core information-theoretic quantity that acts as a general measure of
dependence between two random variables x and y, defined as:
? ?
?
p(x, y)
I(x, y) = Ep(y|x)p(x) log
,
(1)
p(x)p(y)

where the p(x, y) is a joint distribution over the random variables, and p(x) and p(y) are the corresponding marginal distributions. x and y can be many quantities of interest: in computational
neuroscience they are the sensory inputs and the spiking population code; in telecommunications
they are the input signal to a channel and the received transmission; when learning exploration
policies in RL, they are the current state and the action at some time in the future, respectively.
For intrinsic motivation, we use an internal reward measure referred to as empowerment [12, 23]
that is obtained by searching for the maximal mutual information I(?, ?), conditioned on a starting
state s, between a sequence of K actions a and the final state reached s0 :
? ?
?
p(a, s0 |s)
!
0
E(s) = max I (a, s |s) = max Ep(s0 |a,s)!(a|s) log
,
(2)
!
!
!(a|s)p(s0 |s)

where a = {a1 , . . . , aK } is a sequence of K primitive actions ak leading to a final state s0 , and
p(s0 |a, s) is the K-step transition probability of the environment. p(a, s0 |s) is the joint distribution
of action sequences and the final state, !(a|s) is a distribution over K-step action sequences, and
p(s0 |s) is the joint probability marginalised over the action sequence.
Equation (2) is the definition of the channel capacity in information theory and is a measure of the
amount of information contained in the action sequences a about the future state s0 . This measure
is compelling since it provides a well-grounded, task-independent measure for intrinsic motivation
that fits naturally within the framework for intrinsically motivated learning described by figure 1.
Furthermore, empowerment, like the state- or action-value function in reinforcement learning, assigns a value E(s) to each state s in an environment. An agent that seeks to maximise this value will
move towards states from which it can reach the largest number of future states within its planning
horizon K. It is this intuition that has led authors to describe empowerment as a measure of agent
?preparedness?, or as a means by which an agent may quantify the extent to which it can reliably
influence its environment ? motivating an agent to move to states of maximum influence [23].

An empowerment-based agent generates an open-loop sequence of actions K steps into the future
? this is only used by the agent for its internal planning using !(a|s). When optimised using (2),
the distribution !(a|s) becomes an efficient exploration policy that allows for uniform exploration
of the state space reachable at horizon K, and is another compelling aspect of empowerment (we
provide more intuition for this in appendix A). But this policy is not what is used by the agent for
acting: when an agent must act in the world, it follows a closed-loop policy obtained by a planning
algorithm using the empowerment value (e.g., Q-learning); we expand on this in sect. 4.3. A further
consequence is that while acting, the agent is only ?curious? about parts of its environment that can
be reached within its internal planning horizon K. We shall not explore the effect of the horizon in
this work, but this has been widely-explored and we defer to the insights of Salge et al. [23].

4 Scalable Information Maximisation
The mutual information (MI) as we have described it thus far, whether it be for problems in empowerment, channel capacity or rate distortion, hides two difficult statistical problems. Firstly, computing the MI involves expectations over the unknown state transition probability. This can be seen by
rewriting the MI in terms of the difference between conditional entropies H(?) as:
I(a, s0 |s) = H(a|s)

H(a|s0 , s),

(3)

where H(a|s) = E!(a|s) [log !(a|s)] and H(a|s , s) = Ep(s0 |a,s)!(a|s) [log p(a|s , s)]. This computation requires marginalisation over the K-step transition dynamics of the environment p(s0 |a, s),
0

3

0

which is unknown in general. We could estimate this distribution by building a generative model
of the environment, and then use this model to compute the MI. Since learning accurate generative
models remains a challenging task, a solution that avoids this is preferred (and we also describe one
approach for model-based empowerment in appendix B).
Secondly, we currently lack an efficient algorithm for MI computation. There exists no scalable
algorithm for computing the mutual information that allows us to apply empowerment to highdimensional problems and that allow us to easily exploit modern computing systems. The current
solution is to use the Blahut-Arimoto algorithm [31], which essentially enumerates over all states,
thus being limited to small-scale problems and not being applicable to the continuous domain. More
scalable non-parametric estimators have been developed [7, 6]: these have a high memory footprint
or require a very large number of observations, any approximation may not be a bound on the
MI making reasoning about correctness harder, and they cannot easily be composed with existing
(gradient-based) systems that allow us to design a unified (end-to-end) system. In the continuous
domain, Monte Carlo integration has been proposed [10], but applications of Monte Carlo estimators
can require a large number of draws to obtain accurate solutions and manageable variance. We
have also explored Monte Carlo estimators for empowerment and describe an alternative importance
sampling-based estimator for the MI and channel capacity in appendix B.1.
4.1 Variational Information Lower Bound
The MI can be made more tractable by deriving a lower bound to it and maximising this instead ?
here we present the bound derived by Barber & Agakov [1]. Using the entropy formulation of the MI
(3) reveals that bounding the conditional entropy component is sufficient to bound the entire mutual
information. By using the non-negativity property of the KL divergence, we obtain the bound:
KL[p(x|y)kq(x|y)] 0 ) H(x|y) ? Ep(x|y) [log q? (x|y)]
I ! (s) = H(a|s)

H(a|s0 , s)

H(a) +Ep(s0 |a,s)!? (a|s) [log q? (a|s0 , s)] = I !,q (s)

(4)

where we have introduced a variational distribution q? (?) with parameters ?; the distribution !? (?)
has parameters ?. This bound becomes exact when q? (a|s0 , s) is equal to the true action posterior
distribution p(a|s0 , s). Other lower bounds for the mutual information are also possible: Jaakkola &
Jordan [9] present a lower bound by using the convexity bound for the logarithm; Brunel & Nadal
[2] use a Gaussian assumption and appeal to the Cramer-Rao lower bound.
The bound (4) is highly convenient (especially when compared to other bounds) since the transition
probability p(s0 |a, s) appears linearly in the expectation and we never need to evaluate its probability
? we can thus evaluate the expectation directly by Monte Carlo using data obtained by interaction
with the environment. The bound is also intuitive since we operate using the marginal distribution on
action sequences !? (a|s), which acts as a source (exploration distribution), the transition distribution
p(s0 |a, s) acts as an encoder (transition distribution) from a to s0 , and the variational distribution
q? (a|s0 , s) conveniently acts as a decoder (planning distribution) taking us from s0 to a.
4.2 Variational Information Maximisation
A straightforward optimisation procedure based on (4) is an alternating optimisation for the parameters of the distributions q? (?) and !? (?). Barber & Agakov [1] made the connection between this
approach and the generalised EM algorithm and refer to it as the IM (information maximisation)
algorithm and we follow the same optimisation principle. From an optimisation perspective, the
maximisation of the bound I !,q (s) in (4) w.r.t. !(a|s) can be ill-posed (e.g., in Gaussian models,
the variances can diverge). We avoid such divergent solutions by adding a constraint on the value of
the entropy H(a), which results in the constrained optimisation problem:
? = max I !,q (s) s.t. H(a|s) < ?, E(s)
? = max Ep(s0|a,s)!(a|s) [ 1 ln !(a|s)+ln q? (a|s0, s)] (5)
E(s)
!,q

!,q

where a is the action sequence performed by the agent when moving from s to s0 and
temperature (which is a function of the constraint ?).

is an inverse

At all times we use very general source and decoder distributions formed by complex non-linear
functions using deep networks, and use stochastic gradient ascent for optimisation. We refer to
our approach as stochastic variational information maximisation to highlight that we do all our
computation on a mini-batch of recent experience from the agent. The optimisation for the decoder
q? (?) becomes a maximum likelihood problem, and the optimisation for the source !? (?) requires
computation of an unnormalised energy-based model, which we describe next. We summmarise the
overall procedure in algorithm 1.
4

4.2.1 Maximum Likelihood Decoder
The first step of the alternating optimisation is the optimisation of equation (5) w.r.t. the decoder q,
and is a supervised maximum likelihood problem. Given a set of data from past interactions with
the environment, we learn a distribution from the start and termination states s, s0 , respectively, to
the action sequences a that have been taken. We parameterise the decoder as an auto-regressive
distribution over the K-step action sequence:
q? (a|s0 , s) = q(a1 |s, s0 )

K
Y

k=2

q(ak |f? (ak

1 , s, s

0

(6)

)),

We are free to choose the distributions q(ak ) for each action in the sequence, which we choose as
categorical distributions whose mean parameters are the result of the function f? (?) with parameters
?. f is a non-linear function that we specify using a two-layer neural network with rectified-linear
activation functions. By maximising this log-likelihood, we are able to make stochastic updates to
the variational parameters ? of this distribution. The neural network models used are expanded upon
in appendix D.
4.2.2 Estimating the Source Distribution
Given a current estimate of the decoder q, the variational solution for the distribution !(a|s) computed
I ! (s)/ !(a|s) = 0 under
P by solving the functional derivative
1
?
the constraint that
u(s, a)) , where
a !(a|s) = 1, is given by ! (a|s) =
Z(s) exp (?
P
0
u
?
(s,a)
u(s, a) = Ep(s0 |s,a) [ln q? (a|s, s )], u
?(s, a) = u(s, a) and Z(s) =
is a normalisation
ae
term. By substituting this optimal distribution into the original objective (5) we find that it can be
expressed in terms of the normalisation function Z(s) only, E(s) = 1 log Z(s).
The distribution ! ? (a|s) is implicitly defined as an unnormalised distribution ? there are no direct
mechanisms for sampling actions or computing the normalising function Z(s) for such distributions.
We could use Gibbs or importance sampling, but these solutions are not satisfactory as they would
require several evaluations of the unknown function u(s, a) per decision per state. We obtain a
more convenient problem by approximating the unnormalised distribution ! ? (a|s) by a normalised
(directed) distribution h? (a|s). This is equivalent to approximating the energy term u
?(s, a) by a
function of the log-likelihood of the directed model, r? :
! ? (a|s) ? h? (a|s) ) u
?(s, a) ? r? (s, a);

r? (s, a) = ln h? (a|s) +

? (s).

(7)

We introduced a scalar function ? (s) into the approximation, but since this is not dependent on
the action sequence a it does not change the approximation (7), and can be verified by substituting
(7) into ! ? (a|s). Since h? (a|s) is a normalised distribution, this leaves ? (s) to account for the
normalisation term log Z(s), verified by substituting ! ? (a|s) and (7) into (5). We therefore obtain a
cheap estimator of empowerment E(s) ? 1 ? (s).
To optimise the parameters ? of the directed model h? and the scalar function ? we can minimise
any measure of discrepancy between the two sides of the approximation (7). We minimise the
squared error, giving the loss function L(h? , ? ) for optimisation as:
?
?
L(h? , ? ) = Ep(s0 |s,A) ( ln q? (a|s,s0 ) r? (s, a))2 .
(8)
At convergence of the optimisation, we obtain a compact function with which to compute the empowerment that only requires forward evaluation of the function . h? (a|s) is parameterised using
an auto-regressive distribution similar to (18), with conditional distributions specified by deep networks. The scalar function ? is also parameterised using a deep network. Further details of these
networks are provided in appendix D.

4.3 Empowerment-based Behaviour policies
Using empowerment as an intrinsic reward measure, an agent will seek out states of maximal empowerment. We can treat the empowerment value E(s) as a state-dependent reward and can then
utilise any standard planning algorithm, e.g., Q-learning, policy gradients or Monte Carlo search.
We use the simplest planning strategy by using a one-step greedy empowerment maximisation. This
amounts to choosing actions a = arg maxa C(s, a), where C(s, a) = Ep(s0 |s,a) [E(s)]. This policy
does not account for the effect of actions beyond the planning horizon K. A natural enhancement is
to use value iteration [27] to allow the agent to take actions by maximising its long term (potentially
5

Bound MI

True MI

Approximate
(nats)
Approximate Empowerment
Empowerment (nats)

3.0

True MI

Parameters: ? variational, convolutional, ?
source
while not converged do
x
{Read current state}
s = ConvNet (x) {Compute state repr.}
A ? !(a|s) {Draw action sequence.}
Obtain data (x, a, x0 ) {Acting in env. }
s0 = ConvNet (x0 ) {Compute state repr.}
? / r? log q? (a|s, s0 ) (18)
? / r? L(h? , ? ) (8)
/ r log q? (a|s, s0 ) + r L(h? , ? )
end while
E(s) = 1 ? (s)
{Empowerment}

y = ?0.083 + 1 ? x, r 2 = 0.903

2.5

1.5

?

?
?

?

?
?

?
?
??
? ?
? ? ?

2.0

Bound MI

Algorithm 1: Stochastic Variational Information
Maximisation for Empowerment

?

?
?

? ?

?
?

?
?
?
?

?

?

?
?

1.5

2.0

2.5

Empowerment(nats)
(nats)
TrueTrue
Empowerment

3.0

Figure 3: Comparing exact vs approximate empowerment. Heat maps: empowerment in 3 environments: two
rooms, cross room, two-rooms; Scatter
plot: agreement for two-rooms.

discounted) empowerment. A third approach would be to use empowerment as a potential function
and the difference between the current and previous state?s empowerment as a shaping function with
in the planning [18]. A fourth approach is one where the agent uses the source distribution !(a|s)
as its behaviour policy. The source distribution has similar properties to the greedy behaviour policy and can also be used, but since it effectively acts as an empowered agents internal exploration
mechanism, it has a large variance (it is designed to allow uniform exploration of the state space).
Understanding this choice of behaviour policy is an important line of ongoing research.
4.4

Algorithm Summary and Complexity

The system we have described is a scalable and general purpose algorithm for mutual information
maximisation and we summarise the core components using the computational graph in figure 2 and
in algorithm 1. The state representation mechanism used throughout is obtained by transforming raw
observations x, x0 to produce the start and final states s, s0 , respectively. When the raw observations
are pixels from vision, the state representation is a convolutional neural network [14, 16], while for
other observations (such as continuous measurements) we use a fully-connected neural network ?
we indicate the parameters of these models using . Since we use a unified loss function, we can
apply gradient descent and backpropagate stochastic gradients through the entire model allowing for
joint optimisation of both the information and representation parameters. For optimisation we use a
preconditioned optimisation algorithm such as Adagrad [5].
The computational complexity of empowerment estimators involves the planning horizon K, the
number of actions N , and the number of states S. For the exact computation we must enumerate over
the number of states, which for grid-worlds is S / D2 (for D?D grids), or for binary images is S =
2
2D . The complexity of using the Blahut-Arimoto (BA) algorithm is O(N K S 2 ) = O(N K D4 ) for
2
grid worlds or O(N K 22D ) for binary images. The BA algorithm, even in environments with a small
number of interacting objects becomes quickly intractable, since the state space grows exponentially
with the number of possible interactions, and is also exponential in the planning horizon. In contrast,
our approach deals directly on the image dimensions. Using visual inputs, the convolutional network
produces a vector of size P , upon which all subsequent computation is based, consisting of an Llayer neural network. This gives a complexity for state representation of O(D2 P + LP 2 ). The
autoregressive distributions have complexity of O(H 2 KN ), where H is the size of the hidden layer.
Thus, our approach has at most quadratic complexity in the size of the hidden layers used and linear
in other quantities, and matches the complexity of any currently employed large-scale vision-based
models. In addition, since we use gradient descent throughout, we are able to leverage the power of
GPUs and distributed gradient computations.

5 Results
We demonstrate the use of empowerment and the effectiveness of variational information maximisation in two types of environments. Static environments consists of rooms and mazes in different
configurations in which there are no objects with which the agent can interact, or other moving ob6

Figure 5: Left: empowerment landscape for
Figure 4: Empowerment for a room environagent and key scenario. Yellow is the key and
ment, showing a) an empty room, b) room with
green is the door. Right: Agent in a corridor
an obstacle c) room with a moveable box, d)
with flowing lava. The agent places a bricks to
room with row of moveable boxes.
stem the flow of lava.
jects. The number of states in these settings is equal to the number of locations in the environment,
so is still manageable for approaches that rely on state enumeration. In dynamic environments, aspects of the environment change, such as flowing lava that causes the agent to reset, or a predator
that chases the agent. For the most part, we consider discrete action settings in which the agent has
five actions (up, down, left, right, do nothing). The agent may have other actions, such as picking
up a key or laying down a brick. There are no external rewards available and the agent must reason
purely using visual (pixel) information. For all these experiments we used a horizon of K = 5.
5.1

Effectiveness of the MI Bound

We first establish that the use of the variational information lower bound results in the same behaviour as that obtained using the exact mutual information in a set of static environments. We
consider environments that have at most 400 discrete states and compute the true mutual information using the Blahut-Arimoto algorithm. We compute the variational information bound on the
same environment using pixel information (on 20 ? 20 images). To compare the two approaches we
look at the empowerment landscape obtained by computing the empowerment at every location in
the environment and show these as heatmaps. For action selection, what matters is the location of the
maximum empowerment, and by comparing the heatmaps in figure 3, we see that the empowerment
landscape matches between the exact and the variational solution, and hence, will lead to the same
agent-behaviour.
In each image in figure 3, we show a heat-map of the empowerment for each location in the environment. We then analyze the point of highest empowerment: for the large room it is in the centre of
the room; for the cross-shaped room it is at the centre of the cross, and in a two-rooms environment,
it is located near both doors. In addition, we show that the empowerment values obtained by our
method constitute a close approximation to the true empowerment for the two-rooms environment
(correlation coeff = 1.00, R2 =0.90). These results match those by authors such as Klyubin et al.
[12] (using empowerment) and Wissner-Gross & Freer [30] (using a different information-theoretic
measure ? the causal entropic force). The advantage of the variational approach is clear from this
discussion: we are able to obtain solutions of the same quality as the exact computation, we have far
more favourable computational scaling (one that is not exponential in the size of the state space and
planning horizon), and we are able to plan directly from pixel information.
5.2

Dynamic Environments

Having established the usefulness of the bound and some further understanding of empowerment,
we now examine the empowerment behaviour in environments with dynamic characteristics. Even
in small environments, the number of states becomes extremely large if there are objects that can
be moved, or added and removed from the environment, making enumerative algorithms (such as
BA) quickly infeasible, since we have an exponential explosion in the number of states. We first
reproduce an experiment from Salge et al. [23, ?4.5.3] that considers the empowered behaviour of
an agent in a room-environment, a room that: is empty, has a fixed box, has a moveable box, has
a row of moveable boxes. Salge et al. [23] explore this setup to discuss the choice of the state
representation, and that not including the existence of the box severely limits the planning ability of
the agent. In our approach, we do not face this problem of choosing the state representation, since
the agent will reason about all objects that appear within its visual observations, obviating the need
for hand-designed state representations. Figure 4 shows that in an empty room, the empowerment is
uniform almost everywhere except close to the walls; in a room with a fixed box, the fixed box limits
the set of future reachable states, and as expected, empowerment is low around the box; in a room
where the box can be moved, the box can now be seen as a tool and we have high empowerment
near the box; similarly, when we have four boxes in a row, the empowerment is highest around the
7

Box+Up

Box+Down Box+Left

Box+Right

Up

Down

Left

Right

Stay

1

1

2

3

4

5

6

6

t=4

t=3

t=2

t=1

C(s, a)

t=5

Figure 7: Predator (red) and agent (blue) scenario. Panels 1, 6 show the 3D simulation.
Other panels show a trace of the path that the
Figure 6: Empowerment planning in a lava-filled predator and prey take at points on its trajectory.
maze environment. Black panels show the path The blue/red shows path history; cyan shows the
direction to the maximum empowerment.
taken by the agent.
boxes. These results match those of Salge et al. [23] and show the effectiveness of reasoning from
pixel information directly.
Figure 6 shows how planning with empowerment works in a dynamic maze environment, where lava
flows from a source at the bottom that eventually engulfs the maze. The only way the agent is able to
safeguard itself, is to stem the flow of lava by building a wall at the entrance to one of the corridors.
At every point in time t, the agent decides its next action by computing the expected empowerment
after taking one action. In this environment, we show the planning for all 9 available actions and a
bar graph with the empowerment values for each resulting state. The action that leads to the highest
empowerment is taken and is indicated by the black panels1 .
Figure 5(left) shows two-rooms separated by a door. The agent is able to collect a key that allows
it to open the door. Before collecting the key, the maximum empowerment is in the region around
the key, once the agent has collected the key, the region of maximum empowerment is close to
the door2 . Figure 5(right) shows an agent in a corridor and must protect itself by building a wall of
bricks, which it is able to do successfully using the same empowerment planning approach described
for the maze setting.
5.3 Predator-Prey Scenario
We demonstrate the applicability of our approach to continuous settings, by studying a simple 3D
physics simulation [29], shown in figure 7. Here, the agent (blue) is followed by a predator (red) and
is randomly reset to a new location in the environment if caught by the predator. Both the agent and
the predator are represented as spheres in the environment that roll on a surface with friction. The
state is the position, velocity and angular momentum of the agent and the predator, and the action is
a 2D force vector. As expected, the maximum empowerment lies in regions away from the predator,
which results in the agent learning to escape the predator3 .

6 Conclusion
We have developed a new approach for scalable estimation of the mutual information by exploiting
recent advances in deep learning and variational inference. We focussed specifically on intrinsic
motivation with a reward measure known as empowerment, which requires at its core the efficient
computation of the mutual information. By using a variational lower bound on the mutual information, we developed a scalable model and efficient algorithm that expands the applicability of
empowerment to high-dimensional problems, with the complexity of our approach being extremely
favourable when compared to the complexity of the Blahut-Arimoto algorithm that is currently the
standard. The overall system does not require a generative model of the environment to be built,
learns using only interactions with the environment, and allows the agent to learn directly from visual information or in continuous state-action spaces. While we chose to develop the algorithm in
terms of intrinsic motivation, the mutual information has wide applications in other domains, all
which stand to benefit from a scalable algorithm that allows them to exploit the abundance of data
and be applied to large-scale problems.
Acknowledgements: We thank Daniel Polani for invaluable guidance and feedback.
1

2

Video:
http://youtu.be/eA9jVDa7O38
Video:
http://youtu.be/eSAIJ0isc3Y
http://youtu.be/tMiiKXPirAQ; http://youtu.be/LV5jYY-JFpE

8

3

Videos:

References
[1] Barber, D. and Agakov, F. The IM algorithm: a variational approach to information maximization. In NIPS, volume 16, pp. 201, 2004.
[2] Brunel, N. and Nadal, J. Mutual information, Fisher information, and population coding.
Neural Computation, 10(7):1731?1757, 1998.
[3] Buhmann, J. M., Chehreghani, M. H., Frank, M., and Streich, A. P. Information theoretic
model selection for pattern analysis. Workshop on Unsupervised and Transfer Learning, 2012.
[4] Cover, T. M. and Thomas, J. A. Elements of information theory. John Wiley & Sons, 1991.
[5] Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and
stochastic optimization. The Journal of Machine Learning Research, 12:2121?2159, 2011.
[6] Gao, S., Steeg, G. V., and Galstyan, A. Efficient estimation of mutual information for strongly
dependent variables. arXiv:1411.2003, 2014.
[7] Gretton, A., Herbrich, R., and Smola, A. J. The kernel mutual information. In ICASP, volume 4, pp. IV?880, 2003.
[8] Itti, L. and Baldi, P. F. Bayesian surprise attracts human attention. In NIPS, pp. 547?554, 2005.
[9] Jaakkola, T. S. and Jordan, M. I. Improving the mean field approximation via the use of mixture
distributions. In Learning in graphical models, pp. 163?173. 1998.
[10] Jung, T., Polani, D., and Stone, P. Empowerment for continuous agent-environment systems.
Adaptive Behavior, 19(1):16?39, 2011.
[12] Klyubin, A. S., Polani, D., and Nehaniv, C. L. Empowerment: A universal agent-centric
measure of control. In IEEE Congress on Evolutionary Computation, pp. 128?135, 2005.
[13] Koutn??k, J., Schmidhuber, J., and Gomez, F. Evolving deep unsupervised convolutional networks for vision-based reinforcement learning. In GECCO, pp. 541?548, 2014.
[14] LeCun, Y. and Bengio, Y. Convolutional networks for images, speech, and time series. The
handbook of brain theory and neural networks, 3361:310, 1995.
[15] Little, D. Y. and Sommer, F. T. Learning and exploration in action-perception loops. Frontiers
in neural circuits, 7, 2013.
[16] Mnih, V., Kavukcuoglu, K., and Silver, D., et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529?533, 2015.
[17] Nelson, J. D. Finding useful questions: on Bayesian diagnosticity, probability, impact, and
information gain. Psychological review, 112(4):979, 2005.
[18] Ng, Andrew Y, Harada, Daishi, and Russell, Stuart. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, 1999.
[19] Oudeyer, P. and Kaplan, F. How can we define intrinsic motivation? In International conference on epigenetic robotics, 2008.
[21] Rubin, J., Shamir, O., and Tishby, N. Trading value and information in MDPs. In Decision
Making with Imperfect Decision Makers, pp. 57?74. 2012.
[22] Salge, C., Glackin, C., and Polani, D. Changing the environment based on empowerment as
intrinsic motivation. Entropy, 16(5):2789?2819, 2014.
[23] Salge, C., Glackin, C., and Polani, D. Empowerment?an introduction. In Guided SelfOrganization: Inception, pp. 67?114. 2014.
[24] Schmidhuber, J. Formal theory of creativity, fun, and intrinsic motivation (1990?2010). IEEE
Trans. Autonomous Mental Development, 2(3):230?247, 2010.
[25] Singh, S. P., Barto, A. G., and Chentanez, N. Intrinsically motivated reinforcement learning.
In NIPS, 2005.
[26] Still, S. and Precup, D. An information-theoretic approach to curiosity-driven reinforcement
learning. Theory in Biosciences, 131(3):139?148, 2012.
[27] Sutton, R. S. and Barto, A. G. Introduction to reinforcement learning. MIT Press, 1998.
[28] Tishby, N., Pereira, F. C., and Bialek, W. The information bottleneck method. In Allerton
Conference on Communication, Control, and Computing, 1999.
[29] Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In
Intelligent Robots and Systems (IROS), pp. 5026?5033, 2012.
[30] Wissner-Gross, A. D. and Freer, C. E. Causal entropic forces. Phys. Rev. Let., 110(16), 2013.
[31] Yeung, R. W. The Blahut-Arimoto algorithms. In Information Theory and Network Coding,
pp. 211?228. 2008.

9

"
571,"SMEM Algorithm for Mixture Models

N aonori U eda Ryohei Nakano
{ueda, nakano }@cslab.kecl.ntt.co.jp
NTT Communication Science Laboratories
Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237 Japan
Zoubin Ghahramani Geoffrey E. Hinton
zoubin@gatsby.uc1.ac.uk g.hinton@ucl.ac.uk
Gatsby Computational Neuroscience Unit, University College London
17 Queen Square, London WC1N 3AR, UK

Abstract
We present a split and merge EM (SMEM) algorithm to overcome the local
maximum problem in parameter estimation of finite mixture models. In the
case of mixture models, non-global maxima often involve having too many
components of a mixture model in one part of the space and too few in another, widely separated part of the space. To escape from such configurations
we repeatedly perform simultaneous split and merge operations using a new
criterion for efficiently selecting the split and merge candidates. We apply
the proposed algorithm to the training of Gaussian mixtures and mixtures of
factor analyzers using synthetic and real data and show the effectiveness of
using the split and merge operations to improve the likelihood of both the
training data and of held-out test data.

1

INTRODUCTION

Mixture density models, in particular normal mixtures, have been extensively used
in the field of statistical pattern recognition [1]. Recently, more sophisticated mixture density models such as mixtures of latent variable models (e.g., probabilistic
PCA or factor analysis) have been proposed to approximate the underlying data
manifold [2]-[4]. The parameter of these mixture models can be estimated using the
EM algorithm [5] based on the maximum likelihood framework [3] [4]. A common
and serious problem associated with these EM algorithm is the local maxima problem. Although this problem has been pointed out by many researchers, the best
way to solve it in practice is still an open question.
Two of the authors have proposed the deterministic annealing EM (DAEM) algorithm [6], where a modified posterior probability parameterized by temperature is
derived to avoid local maxima. However, in the case of mixture density models,
local maxima arise when there are too many components of a mixture models in
one part of the space and too few in another. It is not possible to move a component from the overpopulated region to the underpopulated region without passing

N Ueda, R. Nakano, Z. Ghahramani and G. E. Hinton

600

through positions that give lower likelihood. We therefore introduce a discrete move
that simultaneously merges two components in an overpopulated region and splits
a component in an underpopulated region.
The idea of split and merge operations has been successfully applied to clustering
or vector quantization (e.g., [7]). To our knowledge, this is the first time that
simultaneous split and merge operations have been applied to improve mixture
density estimation. New criteria presented in this paper can efficiently select the
split and merge candidates. Although the proposed method, unlike the DAEM
algorithm, is limited to mixture models, we have experimentally comfirmed that our
split and merge EM algorithm obtains better solutions than the DAEM algorithm.

2

Split and Merge EM (SMEM) Algorithm

The probability density function (pdf) of a mixture of M density models is given by
M

p(x; 8) =

L amP(xlwm;Om),

where

am 2: 0 and

(1)

m=l
The p(xlwm ; Om) is a d-dimensional density model corresponding to the component
The EM algorithm, as is well known, iteratively estimates the parameters 8 =
{(am, Om), m = 1, ... , M} using two steps. The E-step computes the expectation
of the complete data log-likelihood.

Wm .

Q(818(t?) =

L L P(wmlx; 8(t?) logamP(xlwm;Om),
X

(2)

m

where P(wmlx; 8(t?) is the posterior probability which can be computed by

P(

Wm

I'
8(t?) =
X,

M

a~p(xlwm;
o~;h
(t)

(t)

Lm'=l am,p(xlwm,;Om')

.

(3)

Next, the M-step maximizes this Q function with respect to 8 to estimate the new
parameter values 8(t+1).
Looking at (2) carefully, one can see that the Q function can be represented in
the form of a direct sum; i.e., Q(818(t?) = L~=l qm(818(t?), where qm(818(t?) =
LXEx P(wmlx ; 8(t?) logamP(xlw m; Om) and depends only on am and Om . Let 8*
denote the parameter values estimated by the usual EM algorithm. Then after the
EM algorithm has converged, the Q function can be rewritten as

Q* = q7

+ q; + qk +

L

q:n.

(4)

m,m,/i,j,k
We then try to increase the first three terms of the right-hand side of (4) by merging
two components Wi and Wj to produce a component Wi', and splitting the component
Wk into two components Wj' and Wk"" To reestimate the parameters of these new
components, we have to initialize the parameters corresponding to them using 8*.
The initial parameter values for the merged component Wi' can be set as a linear
combination of the original ones before merge:
and

Oi'

P(wJ
?l x,? 8*)
= O*~
wX P(w ? lx?8*)+O*~
J wX
Lx P(wil x ; 8*) + Lx P(wjlx; 8*)
t

t,

(5)

SMEM Algorithm for Mixture Models

601

On the other hand, as for two components Wj' and

Wk',

we set
(6)

where t is some small random perturbation vector or matrix (i.e., Iltll?IIOk 11)1.
The parameter reestimation for m = i', j' and k' can be done by using EM steps,
but note that the posterior probability (3) should be replaced with (7) so that this
reestimation does not affect the other components.

o~;h
La~p(xlwm;
(I
am,p x m ,
(t)

W

.O(t))
m'

x

L

m'=i,j,k

P(wm'lx; 8*),

m = i',j', k'.

m'=i',j',k'

(7)

Clearly Lm'=i',j',k' P(wm, Ix; 8(t)) = Lm=i,j,k P{wmlx; 8*) always holds during
the reestimation process. For convenience, we call this EM procedure the partial
EM procedure. After this partial EM procedure, the usual EM steps, called the full
EM procedure, are performed as a post processing. After these procedures, if Q
is improved, then we accept the new estimate and repeat the above after setting
the new paramters to 8*. Otherwise reject and go back to 8* and try another
candidate. We summarize these procedures as follows:

[SMEM Algorithm]
1. Perform the usual EM updates. Let 8* and Q* denote the estimated parameters
and corresponding Q function value, respectively.
2. Sort the split and merge candidates by computing split and merge criteria (described in the next section) based on 8*. Let {i, j, k}c denote the cth candidate.
3. For c = 1, ... , C max , perform the following: After initial parameter settings
based on 8 *, perform the partial EM procedure for {i, j, k }c and then perform
the full EM procedure. Let 8** be the obtained parameters and Q** be the
corresponding Q function value. If Q** > Q*, then set Q* f - Q**, 8* f - 8**
and go to Step 2.
4. Halt with 8* as the final parameters.
Note that when a certain split and merge candidate which improves the Q function
value is found at Step 3, the other successive candidates are ignored. There is
therefore no guarantee that the split and the merge candidates that are chosen will
give the largest possible improvement in Q. This is not a major problem, however,
because the split and merge operations are performed repeatedly. Strictly speaking,
C max = M(M -1)(M - 2)/2, but experimentally we have confirmed that C max '"" 5
may be enough.

3

Split and Merge Criteria

Each of the split and merge candidates can be evaluated by its Q function value
after Step 3 of the SMEM algorithm mentioned in Sec.2. However, since there
are so many candidates, some reasonable criteria for ordering the split and merge
candidates should be utilized to accelerate the SMEM algorithm.
In general, when there are many data points each of which has almost equal posterior
probabilities for any two components, it can be thought that these two components
1 In the case of mixture Gaussians, covariance matrices E i , and E k , should be positive
definite. In this case, we can initialize them as E i , = E k , = det(Ek)l/d Id indtead of (6).

N. Ueda. R. Nakano. Z. Ghahramani and G. E. Hinton

602

might be merged.
criterion:

To numerically evaluate this, we define the following merge

Jmerge(i,j; 8*) = P i (8*fp j (8*),

(8)

where Pi(8*) = (P(wilxl; 8*), ... , P(wilxN; 8*))T E nN is the N-dimensional
vector consisting of posterior probabilities for the component Wi. Clearly, two components Wi and Wj with larger Jmerge(i,j; 8*) should be merged.
As a split criterion (Jsplit), we define the local Kullback divergence as:

J split (k ; 8- *) =

J(

Pk x; 8*)
- Iog Pk(X;
(I 8*)
e) d x,
P x Wk; k

(9)

which is the distance between two distributions: the local data density Pk(X) around
the component Wk and the density of the component Wk specified by the current
parameter estimate ILk and ~k' The local data density is defined as:
(10)
This is a modified empirical distribution weighted by the posterior probability so
that the data around the component Wk are focused. Note that when the weights
are equal, i.e., P(wklx; 8*) = 11M, (10) is the usual empirical distribution, i.e.,
Pk(X; 8*) = (liN) E~=l 6(x - xn). Since it can be thought that the component
with the largest Jspl it (k; 8*) has the worst estimate of the local density, we should
try to split it. Using Jmerge and Jsp/it, we sort the split and merge candidates as
follows. First, merge candidates are sorted based on Jmerge. Then, for each sorted
merge ' candidate {i,j}e, split candidates excluding {i,j}e are sorted as {k}e. By
combining these results and renumbering them, we obtain {i, j , k }e.

4
4.1

Experiments
Gaussian mixtures

First, we show the results of two-dimensional synthetic data in Fig. 1 to visually
demonstrate the usefulness of the split and merge operations. Initial mean vectors
and covariance matrices were set to near mean of all data and unit matrix, respectively. The usual EM algorithm converged to the local maximum solution shown in
Fig. l(b), whereas the SMEM algorithm converged to the superior solution shown
in Fig. led) very close to the true one. The split of the 1st Gaussian shown in
Fig. l(c) seems to be redundant, but as shown in Fig. led) they are successfully
merged and the original two Gaussians were improved. This indicates that the split
and merge operations not only appropriately assign the number of Gaussians in a
local data space, but can also improve the Gaussian parameters themselves.
Next, we tested the proposed algorithm using 20-dimensional real data (facial images) where the local maxima make the optimization difficult. The data size was 103
for training and 103 for test. We ran three algorithms (EM, DAEM, and SMEM)
for ten different initializations using the K-means algorithm. We set M = 5 and
used a diagonal covariance for each Gaussian. As shown in Table 1, the worst solution found by the SMEM algorithm was better than the best solutions found by
the other algorithms on both training and test data.

603

SMEM Algorithmfor Mixture Models

:'(:0: .

.~.~
. ' "" .; ':.. .

\.';,,:

,"" ...... .
. :~~1 co
-

...';q\'
~.t./

?""{J??
.... O?:6
'..!' "" ~:. ""

2 .:

""

? i,:""'.....

....:...:
. ,,' :. '

(a) True Gaussians and
generated data

(b) Result by EM (t=72)

.

,' ..

(c) Example of split
and merge (t=141)

?
I
.'
? :: . : :,?? t
, ' ??

(d) Final result by SMEM
(t=212)

Figure 1: Gaussian mixture estimation results.

Table 1: Log-likelihood I data point
-145

Training
data

Test
data

initiall value

EM

DAEM

SMEM

min

-159.1
1.n
-157.3
-163.2

-148.2
0.24
-147.7
-148.6

-147.9
0.04
-147.8
-147.9

-145.1
0.08
-145.0
-145.2

mean
std
max
min

-168.2
2.80
-165.5
-174.2

-159.8
1.00
-158.0
-160.8

-159.7
0.37
-159.6
-159.8

-155.9
0.09
-155.9
-156.0

mean
std

max

'~-150
~

'0

8-'55
?

~
g.-,,,

...J

Table 2: No. of iterations
mean
sId

max
min

EM

DAEM

SMEM

47
16
65
37

147
39
189
103

155
44
219
109

,

EM

-1&5
I

ste~
:

,

,

,

I

with split and'merge

I

:

:

I-------'--~ ,-.----~--~~

,

~

~

..

.. ~ m ~
No. of iterations

~

____

~

~

=

Figure 2: Trajectories of loglikelihood. Upper (lower)
corresponds to training (test) data.

Figure 2 shows log-likelihood value trajectories accepted at Step 3 of the SMEM
algorithm during the estimation process 2. Comparing the convergence points at
Step 3 marked by the '0' symbol in Fig. 2, one can see that the successive split
and merge operations improved the log-likelihood for both the training and test
data, as we expected. Table 2 compares the number of iterations executed by the
three algorithms. Note that in the SMEM algorithm, the EM-steps corresponding
to rejected split and merge operations are not counted. The average rank of the
accepted split and merge candidates was 1.8 (STD=O.9) , which indicates that the
proposed split and merge criteria work very well . Therefore, the SMEM algorithm
was about 155 x 1.8/47 c::: 6 times slower than the original EM algorithm.
4.2

Mixtures of factor analyzers

A mixture of factor analyzers (MFA) can be thought of as a reduced dimension
mixture of Gaussians [4]. That is, it can extract locally linear low-dimensional
manifold underlying given high-dimensional data. A single FA model assumes that
an observed D-dimensional variable x are generated as a linear transformation of
some lower K-dimensionallatent variable z rv N(O, I) plus additive Gaussian noise
v rv N(O, w). w is diagonal. That is, the generative model can be written as
2Dotted lines in Fig. 2 denote the starting points of Step 2. Note that it is due to the
initialization at Step 3 that the log-likelihood decreases just after the split and merge.

N. Ueda, R. Nakano, Z. Ghahrarnani and G. E. Hinton

604

??~~~~--~~?-X~l""?

?~?~~~~--~'""O':',,-x-:.,.

?~~~~--~~?-x~;

(a) Initial values

(b) Result by EM

(c) Result by SMEM

Rgure 3: Extraction of 1D manifold by using a mixture of factor analyzers.

x = Az + v + J-L. Here J-L is a mean vector. Then from simple calculation, we
can see that x
N(J-L, AAT + '11). Therefore, in the case of a M mixture of FAs,
x
L~=l omN(J-Lm, AmA~ + 'lim). See [4] for the details. Then, in this case,
the Q function is also decomposable into M components and therefore the SMEM
algorithm is straightforwardly applicable to the parameter estimation of the MFA
models.
t'V

t'V

Figure 3 shows the results of extracting a one-dimensional manifold from threedimensional data (nOisy shrinking spiral) using the EM and the SMEM algorithms 3.
Although the EM algorithm converged to a poor local maxima, the SMEM algorithm successfully extracted data manifold. Table 3 compares average log-likelihood
per data point over ten different initializations. The log-likelihood values were drastically improved on both training and test data by the SMEM algorithm.
The MFA model is applicable to pattern recognition tasks [2][3] since once an MFA
model is fitted to each class, we can compute the posterior probabilities for each data
point. We tried a digit recognition task (10 digits (classes))4 using the MFA model.
The computed log-likelihood averaged over ten classes and recognition accuracy for
test data are given in Table 4. Clearly, the SMEM algorithm consistently improved
the EM algorithm on both log-likelihood and recognition accuracy. Note that the
recognition accuracy by the 3-nearest neighbor (3NN) classifier was 88.3%. It is
interesting that the MFA approach by both the EM and SMEM algorithms could
outperform the nearest neighbor approach when K = 3 and M = 5. This suggests
that the intrinsic dimensionality of the data would be three or so.
'
3In this case, each factor loading matrix Am becomes a three dimensional column
vector corresponding to each thick line in Fig. 3. More correctly, the center position and
the direction of each thick line are f..Lm and Am, respectively. And the length of each thick
line is 2 IIAmll.
4The data were created using the degenerate Glucksman's feature (16 dimensional data)
by NTT labs.[8]. The data size was 200/class for training and 200/class for test.

605

SMEM Algorithm/or Mixture Models
Table 4: Digit recognition results
Table 3: Log-likelihood I data point
EM

Training -7.68 (0.151)
Test

-7.75 (0.171)

-7.26 (0.017)

EM

SMEM

EM

SMEM

K=3

M=5
M=10

-3.18
-3.09

-3.15
-3.05

89.0
87.5

91.3
88.7

K=8

M=5
M=10

-3.14
-3.04

-3.11
-3.01

85.3
82.5

87.3
85.1

-7.33 (0.032)

O:STD

5

Log-likelihood / data point Recognition rate (""!o)

SMEM

Conclusion

We have shown how simultaneous split and merge operations can be used to move
components of a mixture model from regions of the space in which there are too
many components to regions in which there are too few. Such moves cannot be
accomplished by methods that continuously move components through intermediate
locations because the likelihood is lower at these locations. A simultaneous split and
merge can be viewed as a way of tunneling through low-likelihood barriers, thereby
eliminating many non-global optima. In this respect, it has some similarities with
simulated annealing but the moves that are considered are long-range and are very
specific to the particular problems that arise when fitting a mixture model. Note
that the SMEM algorithm is applicable to a wide variety of mixture models, as long
as the decomposition (4) holds. To make the split and merge method efficient we
have introduced criteria for deciding which splits and merges to consider and have
shown that these criteria work well for low-dimensional synthetic datasets and for
higher-dimensional real datasets. Our SMEM algorithm conSistently outperforms
standard EM and therefore it would be very useful in practice.

References
[1] MacLachlan, G. and Basford K., ""Mixture models: Inference and application
to clustering,"" Marcel Dekker, 1988.
[2] Hinton G. E., Dayan P., and Revow M., ""Modeling the minifolds of images of
handwritten digits,"" IEEE Trans. PAMI, vol.8, no.1, pp. 65-74, 1997.
[3] Tipping M. E. and Bishop C. M., ""Mixtures of probabilistic principal component analysers,"" Tech. Rep. NCRG-97-3, Aston Univ. Birmingham, UK, 1997.
[4] Ghahramani Z. and Hinton G. E., ""The EM algorithm for mixtures of factor
analyzers,"" Tech. Report CRG-TR-96-1, Univ. of Toronto, 1997.
[5] Dempster A. P., Laird N. M. and Rubin D. B., ""Maximum likelihood from
incomplete data via the EM algorithm,"" Journal of Royal Statistical Society
B, vol. 39, pp. 1-38, 1977.
[6] Ueda N. and Nakano R., ""Deterministic annealing EM algorithm,"" Neural Networks, voLl1, no.2, pp.271-282, 1998.
[7] Ueda N. and Nakano R., ""A new competitive learning approach based on an
equidistortion principle for designing optimal vector quantizers,"" Neural Networks, vol.7, no.8, pp.1211-1227, 1994.
[8] Ishii K., ""Design of a recognition dictionary using artificially distorted characters,"" Systems and computers in Japan, vol.21, no.9, pp. 669-677, 1989.

"
1160,"K-Local Hyperplane and Convex Distance
Nearest Neighbor Algorithms

Pascal Vincent and Yoshua Bengio
Dept. IRO, Universit?e de Montr?eal
C.P. 6128, Montreal, Qc, H3C 3J7, Canada
vincentp,bengioy @iro.umontreal.ca
http://www.iro.umontreal.ca/ vincentp




Abstract
Guided by an initial idea of building a complex (non linear) decision
surface with maximal local margin in input space, we give a possible
geometrical intuition as to why K-Nearest Neighbor (KNN) algorithms
often perform more poorly than SVMs on classification tasks. We then
propose modified K-Nearest Neighbor algorithms to overcome the perceived problem. The approach is similar in spirit to Tangent Distance, but
with invariances inferred from the local neighborhood rather than prior
knowledge. Experimental results on real world classification tasks suggest that the modified KNN algorithms often give a dramatic improvement over standard KNN and perform as well or better than SVMs.

1 Motivation
The notion of margin for classification tasks has been largely popularized by the success
of the Support Vector Machine (SVM) [2, 15] approach. The margin of SVMs has a nice
geometric interpretation1: it can be defined informally as (twice) the smallest Euclidean
distance between the decision surface and the closest training point. The decision surface
produced by the original SVM algorithm is the hyperplane that maximizes this distance
while still correctly separating the two classes. While the notion of keeping the largest possible safety margin between the decision surface and the data points seems very reasonable
and intuitively appealing, questions arise when extending the approach to building more
complex, non-linear decision surfaces.
Non-linear SVMs usually use the ?kernel trick? to achieve their non-linearity. This conceptually corresponds to first mapping the input into a higher-dimensional feature space
with some non-linear transformation and building a maximum-margin hyperplane (a linear
decision surface) there. The ?trick? is that this mapping is never computed directly, but implicitly induced by a kernel. In this setting, the margin being maximized is still the smallest
Euclidean distance between the decision surface and the training points, but this time measured in some strange, sometimes infinite dimensional, kernel-induced feature space rather
than the original input space. It is less clear whether maximizing the margin in this new
space, is meaningful in general (see [16]).
1
for the purpose of this discussion, we consider the original hard-margin SVM algorithm for two
linearly separable classes.

A different approach is to try and build a non-linear decision surface with maximal distance to the closest data point as measured directly in input space (as proposed in [14]). We
could for instance restrict ourselves to a certain class of decision functions and try to find
the function with maximal margin among this class. But let us take this even further. Extending the idea of building a correctly separating non-linear decision surface as far away
as possible from the data points, we define the notion of local margin as the Euclidean distance, in input space, between a given point on the decision surface and the closest training
point. Now would it be possible to find an algorithm that could produce a decision surface
which correctly separates the classes and such that the local margin is everywhere maximal
along its surface? Surprisingly, the plain old Nearest Neighbor algorithm (1NN) [5] does
precisely this!
So why does 1NN in practice often perform worse than SVMs? One typical explanation, is
that it has too much capacity, compared to SVM, that the class of function it can produce is
too rich. But, considering it has infinite capacity (VC-dimension), 1NN is still performing
quite well... This study is an attempt to better understand what is happening, based on
geometrical intuition, and to derive an improved Nearest Neighbor algorithm from this
understanding.

2 Fixing a broken Nearest Neighbor algorithm
2.1 Setting and definitions

 (the input space).
	



 

We are given
corresponding
  
 set
 atraining



 
of  points
   
!  #""$
%""  '&( 

 and

)!their
*
)!*
class label
where
is the
+
, 
number of different classes.
The
pairs
are
assumed
to
be
samples
drawn
from
an

./
0
unknown
. Barring
76 class labels associated to
1 distribution 1 inputs,
4 5the
*  duplicate
 define a partition of  : let 
32
each
.
""
97:
The problem is to find
that will generalize well on new
.=a
>decision
0  9 function 8  <;
points drawn from . 8 should ideally minimize the expected classification.=
error,

0 
M O$P where ?A@ denotes the expectation with respect to i.e. minimize ?A@CB DFG'E HJILKNF
T


S




&
9
M R denotes the indicator function, whose value is if 8
and DFG(E HJQKN	
and U otherwise.
The setting is that of a classical classification problem in








In the previous and following discussion, we often refer to the concept of decision surface,
9
also known as decision
The function 8 corresponding to a given
de6 #boundary.
W
""
*  algorithm

2
fines for any class
two regions of the input space: the region V

6
*
9  5X6
and its complement  Y V . The decision
surface
for
class
is
the
?boundary?
8
&
*
between those two regions, i.e. the contour of V , and can be seen as a Z Y
dimensional
manifold (a ?surface? in   ) possibly made of several disconnected components. For simplicity, when we mention the decision surface in our discussion we consider only the case
of two class discrimination, in which there is a single decision surface.


1

  that
When we mention a test point, we mean a point
 not belong to the training
9  does
set  and for which the algorithm is to decide on a class 8
.
By distance, we mean the usual Euclidean
in input-space
 distance

 
tween two points [ and \ will be written ] [ \ or alternatively ^[ Y



The distance between
ba<cJdfepoint
g'h +
and
+
 a`single
i  a set of points
point of the set: ]
_
]
.
The
 K-neighborhood jLk
to is smallest.

 

of a test point

*  

The
 K-c-neighborhood jmk
to is smallest.



of a test point

_

is the set of the l



is the set of l

 .
\(^ .

The distance be-

is the distance to the closest
points of  whose distance
points of 

*

whose distance

By Nearest
Neighbor algorithm (1NN) we mean the following algorithm: the class of a test

point is decided to be the same as the class of its closest neighbor in _ .
By K-Nearest
 Neighbor algorithm (KNN) we mean the following algorithm: the class of
a test point is decided
to be the same as the class appearing most frequently among the

K-neighborhood of .
2.2 The intuition

Figure 1: A local view of the decision surface produced by the Nearest Neighbor (left) and
SVM (center) algorithms, and how the Nearest Neighbor solution gets closer to the SVM
solution in the limit, if the support for the density of each class is a manifold which can be
considered locally linear (right).
Figure 1 illustrates a possible intuition about why SVMs outperforms 1NNs when we have
a finite number of samples. For classification tasks where the classes are considered to
be mostly separable,2 we often like to think of each class as residing close to a lowerdimensional manifold (in the high dimensional input space) which can reasonably be considered locally linear3 . In the case of a finite number of samples, ?missing? samples would
appear as ?holes? introducing artifacts in the decision surface produced by classical Nearest Neighbor algorithms. Thus the decision surface, while having the largest possible local
margin with regard to the training points, is likely to have a poor small local margin with
respect to yet unseen samples falling close to the locally linear manifold, and will thus result in poor generalization performance. This problem fundamentally remains with the K
Nearest Neighbor (KNN) variant of the algorithm, but, as can be seen on the figure, it does
not seem to affect the decision surface produced by SVMs (as the surface is constrained to
a particular smooth form, a straight line or hyperplane in the case of linear SVMs). It is
interesting to notice, if the assumption of locally linear class manifolds holds, how the 1NN
solution approaches the SVM solution in the limit as we increase the number of samples.
To fix this problem, the idea is to somehow fantasize the missing points, based on a local linear approximation of the manifold of each class. This leads to modified Nearest
Neighbor algorithms described in the next sections.4
2
By ?mostly separable? we mean that the Bayes error is almost zero, and the optimal decision
surface has not too many disconnected components.
3
i.e. each class has a probability density with a ?support? that is a lower-dimensional manifold,
and with the probability quickly fading, away from this support.
4
Note that although we never generate the ?fantasy? points explicitly, the proposed algorithms are
really equivalent to classical 1NN with fantasized points.

2.3 The basic algorithm

Given a test point , we are really interested in finding the closest neighbor, not among
the training set  , but among an abstract, virtually enriched training set that would contain
all the fantasized ?missing? points of the manifold of each6 class, locally approximated by
an affine subspace. We shall thus consider, for each class
 , the local affine subspace that
passes& through the l points of the K-c neighborhood of . This affine subspace is typically
l Y dimensional or less, and we will somewhat abusively call it the ?local hyperplane?.5
Formally, the local hyperplane can be defined as



 
 * k  5

i i   k

 )  
   
 
   
 

	

N
N 



)  



>)   *  
.
j k

where





(1)





&

, ) is to
Another way to define this hyperplane, that gets rid of the constraint   

take
point within the hyperplane as an origin, for instance the centroid 6 
 a reference
)

 . This same hyperplane can then be expressed as
 k N

k

 * k  5
;Y    )  ) 
Y

where



i i  
)  k

 ;Y   
   
 
	


N



.

(2)

6



Our modified nearest
then associates
a<c d point

  whose
9  Aa$test
 *   neighbor algorithm
* g ] +to
 the * kclass
k
is
closest
to
.
Formally
, where
hyperplane
8
F
  *  ,
]
k
is logically called K-local Hyperplane Distance, hence the name K-local
Hyperplane Distance Nearest Neighbor algorithm (HKNN in short).
Computing, for each class

]

6

F
  *  ,
k


eg	!a!#c ""d H QK ^  Y i ^
$

a<cJd  )   k  ;Y   +
(3)
Y
% 'g & (*),+++ Y
++
 N 	
++
++
amounts to solving a linear system in  , that can be easily expressed in matrix form as:
 .-	/0  /  1-	/  )  
(4)
Y





)

   




where and  are Z dimensional column vectors, 

;Y 
matrix whose columns are the  vectors defined earlier.7

 k

-

, and



is a Z2

l

Strictly speaking a hyperplane in an 3 dimensional input space is an 35476 affine subspace, while
our ?local hyperplanes? can have fewer dimensions.
6
We could be using one of the 8 neighbors as the reference point, but this formulation with the
centroid will prove useful later.
94 :7;
7
Actually there is an infinite number of solutions to this system since the
are linearly dependent: remember that the initial formulation had an equality constraint and thus only 8<4=6 effective
degrees of freedom. But we are interested
94 :7; in >@?BADCFEHGJK I ?BA	LFL not in M so any solution will do. Alternatively, we can remove one of the
from the system so that it has a unique solution.
5

2.4 Links with other paradigms
The proposed
 HKNN
*   algorithm is very similar in spirit to the Tangent Distance Algok
rithm [13].
can be seen as a tangent hyperplane representing a set of local di;Y 
rections of transformation (any linear combination of the  vectors) that do not affect
the class identity. These are invariances. The main difference is that in HKNN these invariances are inferred directly from the local neighborhood in the training set, whereas in
Tangent Distance, they are based on prior knowledge. It should be interesting (and relatively easy) to combine both approaches for improved performance when prior knowledge
is available.
Previous work on nearest-neighbor variations based on other locally-defined metrics can
be found in [12, 9, 6, 7], and is very much related to the more general paradigm of Local
Learning Algorithms [3, 1, 10].
We should also mention close similarities between our approach and the recently proposed
Local Linear Embedding [11] method for dimensionality reduction.
The idea of fantasizing points around the training points in order to define the decision
surface is also very close to methods based on estimating the class-conditional input density [14, 4].
Besides, it is interesting to look at HKNN from a different, less geometrical angle: it can be
understood as choosing the class that achieves the best reconstruction (the smallest reconstruction error) of the test pattern through a linear combination of l particular prototypes
of that class (the l neighbors). From this point of view, the algorithm is very similar to
the Nearest Feature Line (NFL) [8] method. They differ in the fact that NFL considers all
pairs of points for its search rather than the local l neighbors, thus looking
at many (  )
&
lines (i.e. 2 dimensional affine subspaces), rather than at a single l Y
dimensional one.

3 Fixing the basic HKNN algorithm
3.1 Problem arising for large K
One problem with the basic HKNN algorithm, as previously described, arises as we increase the value of l , i.e. the number of points considered in the neighborhood of the test
point. In a typical high dimensional setting, exact colinearities between input patterns are
rare, which means that as soon as l Z , any pattern of   (including nonsensical ones)
can be produced by a linear combination of the l neighbors. The ?actual? dimensionality
of the manifold may be much less than l . This is due to ?near-colinearities?
di - /0 that producing
rections associated to small eigenvalues of the covariance matrix
are but noise,
that can lead the algorithm to mistake those noise directions for ?invariances?, and may
hurt its performance even for smaller values of l . Another related issue is that the linear
approximation of the class manifold by a hyperplane is valid only locally, so we might
want to restrict the ?fantasizing? of class members to a smaller region of the hyperplane.
We considered two ways of dealing with these problems.8
3.2 The convex hull solution
One way to avoid the above mentioned problems is to restrict ourselves to considering
the convex hull of the neighbors, rather than the whole hyperplane they support (of 
which
	
the convex hull is a subset). This corresponds to adding a constraint of   U
to
equation (1). Unlike the problem of computing the distance to the hyperplane, the distance
to the convex hull cannot be found by solving a simple linear system, but typically requires
solving a quadratic programming problem (very similar to the one of SVMs). While this
8
A third interesting avenue, which : we did not have time to explore, would be to keep only the
most relevant principal components of , ignoring those corresponding to small eigenvalues.

is more complex to implement, it should be mentioned that the problems to be solved are
of a relatively small dimension of order l , and that the time of the whole algorithm will
very likely still be dominated by the search of the l nearest neighbors within each class.
This algorithm will be referred to as K-local Convex Distance Nearest Neighbor Algorithm
(CKNN in short).
3.3 The ?weight decay? penalty solution
This consists in incorporating a penalty term to equation (3) to penalize large values of
(i.e. it penalizes moving away from the centroid, especially in non essential directions):



- +
  * k    a<g'cJ& (0d )+  Y )  Y  k  ;Y   +   k 
(5)
%
++
++
 N 
 N 
++
  - /  ++   /   - /  )  
The solution for  is given by solving the linear system
D 
Y
2 Z identity matrix. This is equation (4) with an additional diagonal term.
where D  is the Z7T
]

The resulting algorithm is a generalization of HKNN (basic HKNN corresponds to

U

).

4 Experimental results



We performed a number of experiments, to highlight different properties of the algorithms:
A first 2D toy example (see Figure 2) graphically illustrates the qualitative differences in
the decision surfaces produced by KNN, linear SVM and CKNN.
Table 1 gives quantitative results on two real-world digit OCR tasks, allowing to compare
the performance of the different old and new algorithms.
Figure 3 illustrates the problem arising with large l , mentioned in Section 3, and shows
that the two proposed solutions: CKNN and HKNN with an added weight decay , allow
to overcome it.
In our final experiment, we wanted to see if the good performance of the new algorithms
absolutely depended on having all the training points at hand, as this has a direct impact
on speed. So we checked what performance we could get out of HKNN and CKNN when
using only a small but representative subset of the training points, namely the set of support
vectors found by a Gaussian Kernel SVM. The results obtained for MNIST are given in
Table 2, and look very encouraging. HKNN appears to be able to perform as well or better
than SVMs without requiring more data points than SVMs.





Figure 2: 2D illustration of the decision surfaces produced by KNN (left, K=1), linear SVM
(middle), and CKNN (right, K=2). The ?holes? are again visible in KNN. CKNN doesn?t
suffer from this, but keeps the objective of maximizing the margin locally.

5 Conclusion
From a few geometrical intuitions, we have derived two modified versions of the KNN
algorithm that look very promising. HKNN is especially attractive: it is very simple to
implement on top of a KNN system, as it only requires the additional step of solving a
small and simple linear system, and appears to greatly boost the performance of standard
KNN even above the level of SVMs. The proposed algorithms share the advantages of
KNN (no training required, ideal for fast adaptation, natural handling of the multi-class
case) and its drawbacks (requires large memory, slow testing). However our latest result
also indicate the possibility of substantially reducing the reference set in memory without
loosing on accuracy. This suggests that the algorithm indeed captures essential information
in the data, and that our initial intuition on the nature of the flaw of KNN may well be at
least partially correct.

References
[1] C. G. Atkeson, A. W. Moore, and S. Schaal. Locally weighted learning. Artificial Intelligence
Review, 1996.
[2] B. Boser, I. Guyon, and V. Vapnik. An algorithm for optimal margin classifiers. In Fifth Annual
Workshop on Computational Learning Theory, pages 144?152, Pittsburgh, 1992.
[3] L. Bottou and V. Vapnik. Local learning algorithms. Neural Computation, 4(6):888?900, 1992.
[4] Olivier Chapelle, Jason Weston, L?eon Bottou, and Vladimir Vapnik. Vicinal risk minimization.
In T.K. Leen, T.G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing
Systems, volume 13, pages 416?422, 2001.
[5] T.M. Cover and P.E. Hart. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1):21?27, 1967.
[6] J. Friedman. Flexible metric nearest neighbor classification. Technical Report 113, Stanford
University Statistics Department, 1994.
[7] Trevor Hastie and Robert Tibshirani. Discriminant adaptive nearest neighbor classification and
regression. In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, Advances in Neural Information Processing Systems, volume 8, pages 409?415. The MIT Press,
1996.
[8] S.Z. Li and J.W. Lu. Face recognition using the nearest feature line method. IEEE Transactions
on Neural Networks, 10(2):439?443, 1999.
[9] J. Myles and D. Hand. The multi-class measure problem in nearest neighbour discrimination
rules. Pattern Recognition, 23:1291?1297, 1990.
[10] D. Ormoneit and T. Hastie. Optimal kernel shapes for local linear regression. In S. A. Solla,
T. K. Leen, and K-R. Mller, editors, Advances in Neural Information Processing Systems, volume 12. MIT Press, 2000.
[11] Sam Roweis and Lawrence Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323?2326, Dec. 2000.
[12] R. D. Short and K. Fukunaga. The optimal distance measure for nearest neighbor classification.
IEEE Transactions on Information Theory, 27:622?627, 1981.
[13] P. Y. Simard, Y. A. LeCun, J. S. Denker, and B. Victorri. Transformation invariance in pattern
recognition ? tangent distance and tangent propagation. Lecture Notes in Computer Science,
1524, 1998.
[14] S. Tong and D. Koller. Restricted bayes optimal classifiers. In Proceedings of the 17th National
Conference on Artificial Intelligence (AAAI), pages 658?664, Austin, Texas, 2000.
[15] V.N. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, 1995.
[16] Bin Zhang. Is the maximal margin hyperplane special in a feature space? Technical Report
HPL-2001-89, Hewlett-Packards Labs, 2001.

Table 1: Test-error obtained on the USPS and MNIST digit classification tasks by KNN,
SVM (using a Gaussian Kernel), HKNN and CKNN. Hyper parameters were tuned on
a separate validation set. Both HKNN and CKNN appear to perform much better than
original KNN, and even compare favorably to SVMs.
Data Set
USPS
(6291 train,
1000 valid.,
2007 test points)
MNIST
(50000 train,
10000 valid.,
10000 test points)

Algorithm
KNN
SVM
HKNN
CKNN
KNN
SVM
HKNN
CKNN

Test Error
4.98%
4.33%
3.93%
3.98%
2.95%
1.30%
1.26%
1.46%

Parameters used

l

l
l
l
l

 &

 
  & U(U
 &f
 
U

	
 U
  
 
  &
  
  & U'U
U
 
U

l 

0.032
CKNN
basic HKNN
HKNN, lambda=1
HKNN, lambda=10

0.03
0.028

error rate

0.026
0.024
0.022
0.02
0.018
0.016
0.014
0.012
0

20

40

60

80

100

120

K

Figure 3: Error rate on MNIST as a function of l for CKNN, and HKNN with different
values of . As can be seen the basic HKNN algorithm performs poorly for large values of
l . As expected, CKNN is relatively unaffected by this problem, and HKNN can be made
robust through the added ?weight decay? penalty controlled by .

Table 2: Test-error obtained on MNIST with HKNN and CKNN when using a reduced
training set made of the 16712 support vectors retained by the best Gaussian Kernel SVM.
This corresponds to 28% of the initial 60000 training patterns. Performance is even better
than when using the whole dataset. But here, hyper parameters l and were chosen with
the test set, as we did not have a separate validation set in this setting. It is nevertheless
remarkable that comparable performances can be achieved with far fewer points.
Data Set
MNIST (16712 train s.v.,
10000 test points)

Algorithm
HKNN
CKNN

Test Error
1.23%
1.36%

 U


Parameters


  used
&

l

l 

U

"
3145,"A Connectionist Learning Control
Architecture for Navigation

Jonathan R. Bachrach
Department of Computer and Information Science
University of Massachusetts
Amherst, MA 01003

Abstract
A novel learning control architecture is used for navigation. A sophisticated test-bed is used to simulate a cylindrical robot with a sonar belt
in a planar environment. The task is short-range homing in the presence of obstacles. The robot receives no global information and assumes
no comprehensive world model. Instead the robot receives only sensory
information which is inherently limited. A connectionist architecture is
presented which incorporates a large amount of a priori knowledge in the
form of hard-wired networks, architectural constraints, and initial weights.
Instead of hard-wiring static potential fields from object models, myarchitecture learns sensor-based potential fields, automatically adjusting them
to avoid local minima and to produce efficient homing trajectories. It does
this without object models using only sensory information. This research
demonstrates the use of a large modular architecture on a difficult task.

1

OVERVIEW

I present a connectionist learning control architecture tailored for simulated shortrange homing in the presence of obstacles. The kinematics of a cylindrical robot
(shown in Figure 1) moving in a planar environment is simulated. The robot has
wheels that propel it independently and simultaneously in both the x and y directions with respect to a fixed orientation. It can move up to one radius per discrete
time step. The robot has a 360 degree sensor belt with 16 distance sensors and
16 grey-scale sensors evenly placed around its perimeter. These 32 values form the
robot's view.
Figure 2 is a display created by the navigation simulator. The bottom portion of

457

458

Bachrach

Figure 1: Simulated robot.

Figure 2: Navigation simulator.
the figure shows a bird's-eye view of the robot's environment. In this display, the
bold circle represents the robot's uhome"" position, with the radius line indicating
the home orientation. The other circle with radius line reprelent. the robot's current position and orientation. The top panel shows the grey-scale view from the
home position, and the next panel down shows the grey-scale view from the robot's
current position. For better viewing, the distance and grey-scale sensor values are
superimposed, and the height of the profile is 1/distance instead of distance. Thus
as the robot gets closer to objects they get taller, and when the robot gets farther
away from objects they get shorter in the display.
The robot cannot move through nor usee"" through obstacles (i.e., obstacles are
opaque). The task is for the robot to align itself with the home position from
arbitrary starting positions in the environment while not colliding with obstacles.
This task is performed using only the sensory information-the robot does not have
access to the bird's-eye view.
This is a difficult control task. The sensory information forms a high-dimensional

A Connectionist Learning Control Architecture for Navigation

;. -. '~I~')
:. -

1""'""

.. "",:.'

.it

""

Figure 3: The potential field method. This figure shows a contour plot of a terrain
created using potential fields generated from object models. The contour diagram
shows level curves where the grey level of the line depicts the height of the line: the
maximum height is depicted in black, and the minimum height is depicted in white.
continuous space, and successful homing generally requires a nonlinear mapping
from this space to the space of real-valued actions. Further, training networks
is not easily achieved on this space. The robot assumes no comprehensive world
model and receives no global information, but receives only sensory information
that is inherently limited. Furthermore, it is difficult to reach home using random
exploration thereby making simple trial-and-error learning intractable. In order to
handle this task an architecture was designed that facilitates the coding of domain
knowledge in the form of hard-wired networks, architectural constraints, and initial
weights.

1.1

POTENTIAL FIELDS

Before I describe the architecture, I briefly discuss a more traditional technique for
navigation that uses potential fields. This technique involves building explicit object
model. repre.enting the extent and position of object. in the robot'. environment.
Repelling potential fields are then placed around obstacle. using the object models,
and an attracting potential field is placed on the goal. This can be visualized as
a terrain where the global minimum is located at the goal, and where there are
bumps around the obstacles. The robot goes home by descending the terrain. The
contour diagram in Figure 3 shows such a terrain. The task is to go from the top
room to the bottom through the door. Unfortunately, there can be local minima.
In this environment there are two prime examples of minima: the right-hand wall
between the home location and the upper room-opposing forces exactly counteract
each other to produce a local minimum in the right-hand side of the upper room,
and the doorway-the repelling fields on the door frame create an insurmountable
bump in the center of the door.
In contrast, my technique learns a sensor-based potential field model. Instead of
hard-wiring static potential fields from the object models, the proposed architecture

459

460

Bachrach
Evaluation

Evaluation

State

State

2-Net

3-Net

Figure 4: Control architectures.

learns potential fields, automatically adjusting them to both avoid local minima
and produce efficient trajectories. Furthermore, it does this without object models,
using only sensory information.

1.2

2-NET /3-NET ARCHITECTURES

I shall begin by introducing two existing architectures: the 2-net and 3-net architectures. These architectures were proposed by Werbos [9] and Jordan and Jacobs [4]
and are also based on the ideas of Barto, Sutton, Watkins [2, 1, 8], and Jordan
and Rumelhart [3]. The basic idea is to learn an evaluation function and then
train the controller by differentiating this function with respect to the controller
weights. These derivatives indicate how to change the controller's weights in order
to minimize or maximize the evaluation function. The 2-net architecture consists
of a controller and a critic. The controller maps states to actions, and the 2-net
critic maps state/action pairs to evaluations. The 3-net architecture consists of a
controller, a forward model, and a critic. The controller maps states to actions, the
forward model maps state/action pairs to next states, and the 3-net critic maps
states to evaluations.
It has been said that it is easier to train a 2-net architecture because there is no
forward model [5]. The forward model might be very complicated and difficult to
train. With a 2-net architecture, only a 2-net critic is trained based on state/action
input pairs. But what if a forward model already exists or even a priori knowledge
exists to aid in explicit coding of a forward model? Then it might be simpler to
use the 3-net architecture because the 3-net critic would be easier to train. It is
based on state-only input and not state/action pairs, and it includes more domain
knowledge.

A Connectionist Learning Control Architecture for Navigation
Total Path Length Home
Stralgrht-Llne
Path Length

+

I

Avoldanee
Path L ength

Obstacle
Avofd...ee
CrItic

Homing
CrItic

I
I

Next View

Forward
Model
Action

ControRer

I
Current VIew

Figure 5: My architecture.

2

THE NAVIGATION ARCHITECTURE

The navigation architecture is a version of a 3-net architecture tailored for navigation, where the state is the robot's view and the evaluation is an estimate of the
length of the shortest path for the robot's current location to home. It consists of a
controller, a forward model, and two adaptive critics. The controller maps views to
actions, the forward model maps view/action pairs to next views, the homing critic
maps views to path length home using a straight line trajectory, and the obstacle
avoidance critic maps views to additional path length needed to avoid obstacles. The
sum of the outputs of the homing critic and the obstacle avoidance critic equals the
total path length home. The forward model is a hard-wired differentiable network
incorporating geometrical knowledge about the sensors and space. Both critics and
the controller are radial basis networks using Gaussian hidden units.
2.1

TRAINING

Initially the controller is trained to produce straight-line trajectories home. With
the forward model fixed, the homing critic and the controller are trained using deadreckoning. Dead-reckoning is a technique for keeping track of the distance home
by accumulating the incremental displacements. This distance provides a training
signal for training the homing critic via supervised learning.
Next, the controller is further trained to avoid obstacles. In this phase, the obstacle
avoidance critic is added while the weights of the homing critic and forward model
are frozen. Using the method of temporal differences [7] the controller and obstacle
avoidance qitic are adjusted so that the expected path length decreases by one
radius per time step. After training, the robot takes successive one-radius steps

461

462

Bachrach

""

I

~

.

'

,~

~ .

_.~

:

j'

~

?

1

""

,',; '..~., ""',.~. '""i'
? - ? ."" ??? 'I' .....

'

I""

Figure 6: An example.
toward its home location.

3

AN EXAMPLE

I applied this architecture to the environment shown in Figure 2. Figure 6 shows
the results of training. The left panel is a contour plot of the output of the homing
critic and reflects only the straight-line distance to the home location. The right
panel is a contour plot of the combined output of the homing critic and the obstacle
avoidance critic and now reflects the actual path length home. After training the
robot is able to form efficient homing trajectories starting from anywhere in the
environment.

4

DISCUSSION

The homing task represents a difficult control task requiring the solution of a number
of problems. The first problem is that there is a small chance of getting home using
random exploration. The solution to this problem involves building a nominal initial
controller that chooses straight-line trajectories home. Next, because the state
space is high-dimensional and continuous it is impractical to evenly place Gaussian
units, and it is difficult to learn continuous mappings using logistic hidden units.
Instead I use Gaussian units whose initial weights are determined using expectation
maximization. This is a soft form of competitive learning [6] that, in my case,
creates spatially tuned units. Next, the forward model for the robot's environments
is very difficult to learn. For this reason I used a hard-wired forward model whose
performance is good in a wide range of environments. Here the philosophy is to
learn only things that are difficult to hard-wire. Finally, the 2-net critic is difficult
to train. Therefore, I split the 2-net critic into a 3-net critic and a hard-wired
forward model.
There are many directions for extending this work. First, I would like to apply this
architecture to real robots using realistic sensors and dynamics. Secondly, I want to

A Connectionist Learning Control Architecture for Navigation
to look at long range homing. Lastly, I would like to investigate navigation tasks
involving multiple goals.
Acknowledgements

This material is based upon work supported by the Air Force Office of Scientific
Research, Bolling AFB, under Grant AFOSR-89-0526 and by the National Science
Foundation under Grant ECS-8912623. I would like to thank Richard Durbin, David
Rumelhart, Andy Barto, and the UMass Adaptive Networks Group for their help
on this project.

References
[1] A. G. Barto, R. S. Sutton, and C. Watkins. Sequential decision problems and
neural networks. In David S. Touretzky, editor, Advance"" in Neural Information Proce16ing Sy""tem"", P.O. Box 50490, Palo Alto, CA 94303, 1989. Morgan
Kaufmann Publishers.
[2] Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson. Neuron like adaptive elements that can solve difficult learning control problems.
IEEE Tran""action"" on Sy""tem"", Man, and Cybernetic"", SMC-13(15), September/October 1985.
[3] M. I. Jordan and D. E. Rumelhart. Supervised learning with a distal teacher.
1989. Submitted to: Cognitive Science.
[4] Michael I. Jordan and Robert Jacobs. Learning to control an unstable system
with forward modeling. In David S. Touretzky, editor, Advance"" in Neural
Information Proce16ing Sy6tem"", P.O. Box 50490, Palo Alto, CA 94303, 1989.
Morgan Kaufmann Publishers.
[5] Sridhar Mahadevan and Jonathan Connell.
Automatic programming of
behavior-based robots using reinforcemnt learning. Technical report, IBM Research Division, T.J. Watson Research Center, Box 704, Yorktown Heights, NY
10598, 1990.
[6] S. J. Nowlan. A generative framework for unsupervised learning. Denver,
Colorado, 1989. IEEE Conference on Neural Information Processing SystemsNatural and Synthetic.
[7] Richard Sutton. Learning to predict by the methods of temporal differences.
Technical report, GTE Laboratories, 1987.
[8] Richard S. Sutton. Temporal Credit A16ignment in Reinforcement Learning.
PhD thesis, Department of Computer and Information Science, University of
Massachusetts at Amherst, 1984.
[9] Paul J. Werbos. Reinforcement learning over time. In T. Miller, R. S. Sutton, and P. J. Werbos, editors, Neural Network"" for Control. The MIT Press,
Cambridge, MA, In press.

463

"
3583,"The Fixed Points of Off-Policy TD

J. Zico Kolter
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139
kolter@csail.mit.edu

Abstract
Off-policy learning, the ability for an agent to learn about a policy other than the
one it is following, is a key element of Reinforcement Learning, and in recent
years there has been much work on developing Temporal Different (TD) algorithms that are guaranteed to converge under off-policy sampling. It has remained
an open question, however, whether anything can be said a priori about the quality
of the TD solution when off-policy sampling is employed with function approximation. In general the answer is no: for arbitrary off-policy sampling the error
of the TD solution can be unboundedly large, even when the approximator can
represent the true value function well. In this paper we propose a novel approach
to address this problem: we show that by considering a certain convex subset of
off-policy distributions we can indeed provide guarantees as to the solution quality
similar to the on-policy case. Furthermore, we show that we can efficiently project
on to this convex set using only samples generated from the system. The end result is a novel TD algorithm that has approximation guarantees even in the case of
off-policy sampling and which empirically outperforms existing TD methods.

1

Introduction

In temporal prediction tasks, Temporal Difference (TD) learning provides a method for learning
long-term expected rewards (the ?value function?) using only trajectories from the system. The
algorithm is ubiquitous in Reinforcement Learning, and there has been a great deal of work studying
the convergence properties of the algorithm. It is well known that for a tabular value function
representation, TD converges to the true value function [3, 4]. For linear function approximation
with on-policy sampling (i.e., when the states are drawn from the stationary distribution of the policy
we are trying to evaluate), the algorithm converges to a well-known fixed point that is guaranteed
to be close to the optimal projection of the true value function [17]. When states are sampled offpolicy, standard TD may diverge when using linear function approximation [1], and this has led in
recent years to a number of modified TD algorithms that are guaranteed to convergence even in the
presence of off-policy sampling [16, 15, 9, 10].
Of equal importance, however, is the actual quality of the TD solution under off-policy sampling.
Previous work, as well as an example we present in this paper, show that in general little can be said
about this question: the solution found by TD can be arbitrarily poor in the case of off-policy sampling, even when the true value function is well-approximated by a linear basis. Pursing a slightly
different approach, other recent work has looked at providing problem dependent bounds, which use
problem-specific matrices to obtain tighter bounds than previous approaches [19]; these bounds can
apply to the off-policy setting, but depend on problem data, and will still fail to provide a reasonable
bound in the cases mentioned above where the off-policy approximation is arbitrarily poor. Indeed,
a long-standing open question in Reinforcement Learning is whether any a priori guarantees can be
made about the solution quality for off-policy methods using function approximation.
In this paper we propose a novel approach that addresses this question: we present an algorithm that
looks for a subset of off-policy sampling distributions where a certain relaxed contraction property
1

holds; for distributions in this set, we show that it is indeed possible to obtain error bounds on the
solution quality similar to those for the on-policy case. Furthermore, we show that this set of feasible
off-policy sampling distributions is convex, representable via a linear matrix inequality (LMI), and
we demonstrate how the set can be approximated and projected onto efficiently in the finite sample
setting. The resulting method, which we refer to as TD with distribution optimization (TD-DO),
is thus able to guarantee a good approximation to the best possible projected value function, even
for off-policy sampling. In simulations we show that the algorithm can improve significantly over
standard off-policy TD.

2

Preliminaries and Background

A Markov chain is a tuple, (S, P, R, ?), where S is a set of states, P : S ? S ? R+ is a transition
probability function, R : S ? R is a reward function, and ? ? [0, 1) is a discount factor. For
simplicity of presentation we will assume the state space is countable, and so can be indexed by
the set S = {1, . . . , n}, which allows us to use matrix rather than operator notation. The value
function for a Markov chain, P
V : S ? R maps states to their long term discounted sum of rewards,
?
and is defined as V (s) = E [ t=0 ? t R(st )|s0 = s]. The value function may also be expressed via
Bellman?s equation (in vector form)
V = R + ?P V
(1)
where R, V ? Rn represent vectors of all rewards and values respectively, and P ? Rn?n is a
matrix of probability transitions Pij = P (s? = j|s = i).
In linear function approximation, the value function is approximated as a linear combination of
some features describing the state: V (s) ? wT ?(s), where w ? Rk is a vector of parameters, and
? : S ? Rk is a function mapping states to k-dimensional feature vectors; or, again using vector
notation, V ? ?w, where ? ? Rn?k is a matrix of all feature vectors. The TD solution is a fixed
point of the Bellman operator followed by a projection, i.e.,
?
?
?wD
= ?D (R + ?P ?wD
)
(2)
where ?D = ?T (?T D?)?1 ?T D is a projection matrix weighted by the diagonal matrix D ?
Rn?n . Rearranging terms gives the analytical solution
?1 T
?
wD
= ?T D(? ? ?P ?)
? DR.
(3)
Although we cannot expect to form this solution exactly when P is unknown and too large to represent, we can approximate the solution via stochastic iteration (leading to the original TD algorithm),
or via the least-squares TD (LSTD) algorithm, which forms the matrices
m
m


1 X
1 X
(i)
w
?D = A??1?b, A? =
?(s(i) ) ?(s(i) ) ? ??(s? ) , ?b =
?(s(i) )r(i)
(4)
m i=1
m i=1
(i)

(i)
? D. When D
given a sequence of states, rewards, and next states {s(i) , r(i) , s? }m
i=1 where s
is not the stationary distribution of the Markov chain (i.e., we are employing off-policy sampling),
then the original TD algorithm may diverge (LSTD will still be able to compute the TD fixed point
in this case, but has a greater computational complexity of O(k 2 )). Thus, there has been a great deal
of work on developing O(k) algorithms that are guaranteed to converge to the LSTD fixed point
even in the case of off-policy sampling [16, 15].

We note that the above formulation avoids any explicit mention of a Markov Decision Process
(MDP) or actual policies: rather, we just have tuples of the form {s, r, s? } where s is drawn from
an arbitrary distribution but s? still follows the ?policy? we are trying to evaluate. This is a standard
formulation for off-policy learning (see e.g. [16, Section 2]); briefly, the standard way to reach this
setting from the typical notion of off-policy learning (acting according to one policy in an MDP, but
evaluating another) is to act according to some original policy in an MDP, and then subsample only
those actions that are immediately consistent with the policy of interest. We use the above notation as it avoids the need for any explicit notation of actions and still captures the off-policy setting
completely.
2.1

Error bounds for the TD fixed point

Of course, in addition to the issue of convergence, there is the question as to whether we can say anything about the quality of the approximation at this fixed point. For the case of on-policy sampling,
the answer here is an affirmative one, as formalized in the following theorem.
2

4

Approximation Error

10

TD Solution

2

10

Optimal Approximation

0

10

?2

10

?4

10

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

p

Figure 1: Counter example for off-policy TD learning: (left) the Markov chain considered for the
counterexample; (right) the error of the TD estimate for different off-policy distributions (plotted on
a log scale), along with the error of the optimal approximation.
?
?
Theorem 1. (Tsitsiklis and Van Roy [17], Lemma 6) Let wD
be the unique solution to ?wD
=
?
?D (R + ?P ?wD ) where D is the stationary distribution of P . Then
1
?
k?wD
? V kD ?
k?D V ? V kD .
(5)
1??

Thus, for on-policy sampling with linear function approximation, not only does TD converge to its
fixed point, but we can also bound the error of its approximation relative to k?D V ? V kD , the
lowest possible approximation error for the class of function approximators.1
Since this theorem plays an integral role in the remainder of this paper, we want to briefly give the
intuition of its proof. A fundamental property of Markov chains [17, Lemma 1] is that transition
matrix P is non-expansive in the D norm when D is the stationary distribution
kP xkD ? kxkD , ?x.
(6)
From this it can be shown that the Bellman operator is a ?-contraction in the D norm and Theorem 1
follows. When D is not the stationary distribution of the Markov chain, then (6) need not hold, and
it remains to be seen what, if anything, can be said a priori about the TD fixed point in this situation.

3

An off-policy counterexample

Here we present a simple counter-example which shows, for general off-policy sampling, that the
TD fixed point can be an arbitrarily poor approximator of the value function, even if the chosen
bases can represent the true value function with low error. The same intuition has been presented
previously [11]. though we here present a concrete numerical example for illustration.
Example 1. Consider the two-state Markov chain shown in Figure 1, with transition probability
matrix P = (1/2)11T , discount factor ? = 0.99, and value function V = [1 1.05]T (with R =
(I ? ?P )V ). Then for any ? > 0 and C > 0, there exists an off-policy distribution D such that
using bases ? = [1 1.05 + ?]T gives
?
k?D V ? V k ? ?, and k?wD
? V k ? C.
(7)
Proof. (Sketch) The fact that k?D V ? V k ? ? is obvious from the choice of basis. To show that
the TD error can be unboundedly large, let D = diag(p, 1 ? p); then, after some simplification, the
TD solution is given analytically by
?2961 + 4141p ? 2820? + 2820p?
?
(8)
wD
=
?2961 + 4141p ? 45240? + 84840p? ? 40400?2 + 40400p?2
which is infinite, (1/w = 0), when
2961 + 45240? + 40400?2
.
(9)
4141 + 84840? + 40400?2
?
Since this solution is in (0, 1) for all epsilon, by choosing p close to this value, we can make wD
arbitrarily large, which in turn makes the error of the TD estimate arbitrarily large.
p=

1

The approximation factor can be sharpened to ?

1
1?? 2

in some settings [18], though the analysis does not

carry over to our off-policy case, so we present here the simpler version.

3

Figure 1 shows a plot of k?w? ? V k2 for the example above with ? = 0.001, varying p from 0 to
1. For p ? 0.715 the error of the TD solution approaches infinity; the essential problem here is that
when D is not the stationary distribution of P , A = ?T D(? ? ?P ?) can become close to zero (or
for the matrix case, one of its eigenvalues can become zero), and the TD value function estimate can
grow unboundedly large. Thus, we argue that simple convergence for an off-policy algorithm is not
a sufficient criterion for a good learning system, since even for a convergent algorithm the quality of
the actual solution could be arbitrarily poor.

4

A convex characterization of valid off-policy distributions

Although it may seem as though the above example would imply that very little could be said about
the quality of the TD fixed point under off-policy sampling, in this section we show that by imposing
additional constraints on the sampling distribution, we can find a convex family of distributions for
which it is possible to make guarantees.
To motivate the approach, we again note that error bounds for the on-policy TD algorithm follow
from the Markov chain property that kP xkD ? kxkD for all x when D is the stationary distribution. However, finding a D that satisfies this condition is no easier than computing the stationary
distribution directly and thus is not a feasible approach. Instead, we consider a relaxed contraction
property: that the transition matrix P followed by a projection onto the bases will be non-expansive
for any function already in the span of ?. Formally, we want to consider distributions D for which
k?D P ?wkD ? k?wkD

(10)

k

for any w ? R . This defines a convex set of distributions, since
k?D P ?wk2D ? k?wk2D
?
?

wT ?T P T D?(?T D?)?1 ?T D?(?T D?)?1 ?DP ?T w ? wT ?T D?w

wT ?T P T D?(?T D?)?1 ?DP ?T ? ?T D? w ? 0.

(11)

This holds for all w if and only if2

?T P T D?(?T D?)?1 ?DP ?T ? ?T D?  0
which in turn holds if and only if

(12)

3

F ?



?T D?
?T DP ?
T T
? P D? ?T D?



0

(13)

This is a matrix inequality (LMI) in D, and thus describes a convex set. Although the distribution D
is too high-dimensional to optimize directly, analogous to LSTD, the F matrix defined above is of a
representable size (2k ? 2k), and can be approximated from samples. We will return to this point in
the subsequent section, and for now will continue to use the notation of the true distribution D for
simplicity. The chief theoretical result of this section is that if we restrict our attention to off-policy
distributions within this convex set, we can prove non-trivial bounds about the approximation error
of the TD fixed point.
Theorem 2. Let w? be the unique solution to ?w? = ?D (R+?P ?w? ) where D is any distribution
? ? D?1/2 D?1/2 Then4
satisfying (13). Further, let D? be the stationary distribution of P , and let D
?
1 + ??(D)
?
k?wD
? V kD ?
k?D V ? V kD .
(14)
1??
The bound here is of a similar form to the previously stated bound for on-policy TD, it bounds
the error of the TD solution relative to the error of the best possible approximation, except for
? term, which measures how much the chosen distribution deviates from the
the additional ??(D)
? = 1, so we recover the original bound up to a
stationary distribution. When D = D? , ?(D)
constant factor. Even though the bound does include this term that depends on the distance from
the stationary distribution, no such bound is possible for D that do not satisfy the convex constraint
(13), as illustrated by the previous counter-example.
2

A  0 (A  0) denotes that A is negative (positive)
semidefinite.
?
?
A B
Using the Schur complement property that
 0 ? B T AB ? C  0 [2, pg 650-651].
T
B
C
4
?(A) denotes the condition number of A, the ratio of the singular values ?(A) = ?max (A)/?min (A).
3

4

4

10

Approximation Error

TD Solution
Optimal Approximation

2

10

Feasible Region

0

10

?2

10

?4

10

0

0.1

0.2

0.3

0.4

0.5
p

0.6

0.7

0.8

0.9

1

Figure 2: Counter example from Figure 1 shown with the set of all valid distributions for which
F  0. Restricting the solution to this region avoids the possibility of the high error solution.
Proof. (of Theorem 2) By the triangle inequality and the definition of the TD fixed point,
?
?
k?wD
? V kD ? k?wD
? ?D V kD + k?D V ? V kD
?
= k?D (R + ?P ?wD
) ? ?D (R + ?P V )kD + k?D V ? V kD
?
= ?k?D P ?wD ? ?D P V kD + k?D V ? V kD
?
? ?k?D P ?wD
? ?D P ?D V kD + ?k?D P ?D V ? ?D P V kD + k?D V ? V kD .
(15)

Since ?D V = ?w
? for some w,
? we can use the definition of our contraction k?D P ?wkD ? k?wkD
to bound the first term as
?
?
?
k?D P ?wD
? ?D P ?D V kD ? k?wD
? ?D V kD ? k?wD
? V kD .

(16)

Similarly, the second term in (15) can be bounded as
k?D P ?D V ? ?D P V kD ? kP ?D V ? P V kD ? kP kD k?D V ? V kD

(17)

where kP kD denotes the matrix norm kAkD ? maxkxkD ?1 kAxkD . Substituting these bounds
back into (15) gives
?
(1 ? ?)k?wD
? V kD ? (1 + ?kP kD )k?D V ? V kD ,
(18)
?
so all the remains is to show that kP kD ? ?(D). To show this, first note that kP kD? = 1, since

max kP xkD? ?

kxkD? ?1

max kxkD? = 1,

kxkD? ?1

(19)

and for any nonsingular D,
kP kD = max kP xkD = max
kxkD ?1

kyk2 ?1

p
y T D?1/2 P T DP D?1/2 y = kD1/2 P D?1/2 k2 .

(20)

Finally, since D? and D are both diagonal (and thus commute),

kD1/2 P D?1/2 k2 = kD??1/2 D1/2 D?1/2 P D??1/2 D?1/2 D?1/2 k2
? kD??1/2 D1/2 k2 kD?1/2 P D??1/2 k2 kD?1/2 D?1/2 k2
?
= kD?1/2 D1/2 k2 kD?1/2 D1/2 k2 = ?(D)
?

?

The final form of the bound can be quite loose of, course, as many of the steps involved in the proof
used substantial approximations and discarded problem specific data (such as the actual k?D P kD
? term, for instance). This is in constrast to the previously mentioned
term in favor of the generic ?(D)
work of Yu and Bertsekas [19] that uses these and similar terms to obtain much tigher, but data
dependent, bounds. Indeed, applying a theorem from this work we can arrive as a slight improvement
of the bound above [13], but the focus here is just on the general form and possibility of the bound.
Returning to the counter-example from the previous section, we can visualize the feasible region
for which F  0, shown as the shaded portion in Figure 2, and so constraining the solution to
this feasible region avoids the possibility of the high error solution. Moreover, in this example the
optimal TD error occurs exactly at the point where ?min (F ) = 0, so that projecting an off-policy
distribution onto this set will give an optimal solution for initially infeasible distributions.
5

4.1

Estimation from samples

Returning to the issue of optimizing this distribution only using samples from the system, we note
(i)
that analogous to LSTD, for samples {s(i) , r(i) , s? }m
i=1
#
""
m
m
X
(i)
(i)
T
(i)
? (i) T
1
1 X?
?(s
)?(s
)
?(s
)?(s
)
F? =
?
Fi
(21)
(i)
m i=1 ?(s? )?(s(i) )T ?(s(i) )?(s(i) )T
m i=1

will be an unbiased estimate of the LMI matrix F (for a diagonal matrix D given the our sampling
distribution over s(i) ). Placing a weight di on each sample, we could optimize the sum F? (d) =
Pm
?
i=1 di Fi and obtain a tractable optimization problem. However, optimizing these weights freely is
not permissible, since this procedure allows us to choose di 6= dj even if s(i) = s(j) , which violates
the weights in the original LMI. However, if we additionally require that s(i) = s(j) ? di = dj
(or more appropriately for continuous features and states, for example that kdi ? dj k ? 0 as
k?(s(i) ) ? ?(s(j) )k ? 0 according to some norm) then we are free to optimize over these empirical
distribution weights. In practice, we want to constrain this distribution in a manner commensurate
with the complexity of the feature space and the number of samples. However, determining the best
such distributions to use in practice remains an open problem for future work in this area.
Finally, since many empirical distributions satisfy F? (d)  0, we propose to ?project? the empirical
distribution onto this set by minimizing the KL divergence between the observed and optimized
distributions, subject to the constraint that F? (d)  0. Since this constraint is guaranteed to hold at
the stationary distribution, the intuition here is that by moving closer to this set, we will likely obtain
a better solution. Formally, the final optimization problem, which we refer to as the TD-DO method
(Temporal Difference Distribution Optimization), is given by
m
X

min
d

??
pi log di

s.t. , 1T d = 0, F? (d)  0, d ? C.

(22)

i=1

where C is some convex set that respects the metric constraints described above. This is a convex optimization problem in d, and thus can be solved efficiently, though off-the-shelf solvers can perform
quite poorly, especially for large dimension m.
4.2

Efficient Optimization

Here we present a first-order optimization method based on solving the dual of (22). By properly
exploiting the decomposability of the objective and low-rank structure of the dual problem, we
develop an iterative optimization method where each gradient step can be computed very efficiently.
The presentation here is necessarily brief due to space constraints, but we also include a longer
description and an implementation of the method in the supplementary material. For simplicity we
present the algorithm ignoring the constraint set C, though we discuss possible additonal constraints
briefly in supplementary material.
We begin by forming the Lagrangian of (22), introducing Lagrange multipliers Z ? R2k?2k for the
constraint F? (d)  0 and ? ? R for the constraint 1T d = 1. This leads to the dual optimization
problem
)
( m
X
T ?
T
p?i log di ? tr(Z F (d)) + ?(1 d ? 1) .
(23)
max min ?
Z0,?

d

i=1

Treating Z as fixed, we maximize over ? and minimize over d in (23) using an equality-constrained,
feasible start Newton method [2, pg 528]. Since the objective is separable over the di ?s the Hessian
matrix is diagonal, and the Newton step can be computed in O(m) time; furthermore, since we
solve this subproblem for each update of dual variables Z, we can warm-start Newton?s method
from previous solutions, leading to a number of Newton steps that is virtually constant in practice.
Considering now the maximization over Z, the gradient of
(
)
X
?
T ? ?
?
T ?
g(Z) ?
?p?i log di (Z) ? trZ F (d (Z)) + ? (Z)(1 d (Z) ? 1)
i

6

(24)

0.4
Off?policy TD
Off?policy TD?DO
On?policy TD
Optimal Projection

0.8

Normalized Approximation Error

Normalized Approximation Error

1

0.6

0.4

0.2

0
2

3

4

5
6
7
Number of Bases

8

9

Off?policy TD
Off?policy TD?DO
On?policy TD
Optimal Projection

0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
2

10

3

4

5
6
7
Number of Bases

8

9

10

Figure 3: Average approximation error of the TD methods, using different numbers of bases functions, for the random Markov chain (left) and diffusion chain (right).
0.25
Off?policy TD
Off?policy TD?DO
On?policy TD
Optimal Projection

1.5

Normalized Approximation Error

Normalized Approximation Error

2

1

0.5

0 0
10

1

2

10
10
Closeness to Stationary Distribution, C

Off?policy TD
Off?policy TD?DO
On?policy TD
Optimal Projection

0.2

0.15

0.1

0.05

0 0
10

3

10
mu

1

2

10
10
Closeness to Stationary Distribution, C

3

10
mu

Figure 4: Average approximation error, using off-policy distributions closer or further from the
stationary distribution (see text) for the random Markov chain (left) and diffusion chain (right).
0.8
Off?policy TD
Off?policy TD?DO
On?policy TD

Normalized Approximation Error

Normalized Approximation Error

1.5

1

0.5

0 2
10

3

10
Number of Samples

0.6
0.5
0.4
0.3
0.2
0.1
0 2
10

4

10

Off?policy TD
Off?policy TD?DO
On?policy TD

0.7

3

10
Number of Samples

4

10

Figure 5: Average approximation error for TD methods computed via sampling, for different numbers of samples, for random Markov chain (left) and diffusion chain (right).
is given simply by ?Z g(Z) = ?F? (d? (Z)). We then exploit the fact that we expect Z to typically be
low-rank: by the KKT conditions for a semidefinite program F? (d) and Z will have complementary
ranks, and since we expect F? (d) to be nearly full rank at the solution, we factor Z = Y Y T for
Y ? Rk?p with p ? k. Although this is now a non-convex problem, local optimization of this
objective is still guaranteed to give a global solution to the original semidefinite problem, provided
we choose the rank of Y to be sufficient to represent the optimal solution [5]. The gradient of this
transformed problem is ?Z g(Y Y T ) = ?2F? (d)Y , which can be computed in time O(mkp) since
each F?i term is a low-rank matrix, and we optimize the dual objective via an off-the-shelf LBFGS
solver [12, 14]. Though it is difficult to bound p aprirori, we can check after the solution that our
chosen value was sufficient for the global solution, and we have found that very low values (p = 1
or p = 2) were sufficient in our experiments.

5

Experiments

Here we present simple simulation experiments illustrating our proposed approach; while the evaluation is of course small scale, the results highlight the potential of TD-DO to improve TD algorithms
both practically as well as theoretically. Since the benefits of the method are clearest in terms of
7

0.19

0.22
Off?policy TD?DO
Normalized Approximation Error

Normalized Approximation Error

Off?policy TD?DO
0.18
0.17
0.16
0.15
0.14
0.13
0.12
20

30

40

50
60
70
Number of Clusters

80

90

0.2
0.18
0.16
0.14
0.12
0.1
0.08
5

100

10

15

20
25
30
35
40
Number of LBFGS Iterations

45

50

Figure 6: (Left) Effect of the number of clusters for sample-based learning on diffusion chain,
(Right) performance of algorithm on diffusion chain versus number of LBFGS iterations
the mean performance over many different environments we focus on randomly generated Markov
chains of two types: a random chain and a diffusion process chain.5
Figure 3 shows the average approximation error of the different algorithms with differing numbers
of basis function, over 1000 domains. In this and all experiments other than those evaluating the
effect of sampling, we use the full ? and P matrices to compute the convex set, so that we are
evaluating the performance of the approach in the limit of large numbers of samples. We evaluate
the approximation error kV? ? V kD where D is the off-policy sampling distribution (so as to be
as favorable as possible to off-policy TD). In all cases the TD-DO algorithm improves upon the
off-policy TD, though the degree of improvement can vary from minor to quite significant.
Figure 4 shows a similar result for varying the closeness of the sampling distribution to the
stationary distribution; in our experiments, the off-policy distribution is sampled according to
D ? Dir(1 + C? ?) where ? denotes the stationary distribution. As expected, the off-policy approaches perform similarly for larger C? (approaching the stationary distribution), with TD-DO
having a clear advantage when the off-policy distribution is far from the stationary distribution.
In Figure 5 we consider the effect of sampling on the algorithms. For these experiments we employ a
simple clustering method to compute a distribution over states d that respects the fact that ?(s(i) ) =
?(s(j) ) ? di = dj : we group the sampled states into k clusters via k-means clustering on the
feature vectors, and optimize over the reduced distribution d ? Rk . In Figure 6 we vary the number
of clusters k for the sampled diffusion chain, showing that the algorithm is robust to a large number
of different distributional representations; we also show the performance of our method varying the
number of LBFGS iterations, illustrating that performance generally improves monotonically.

6

Conclusion

The fundamental idea we have presented in this paper is that by considering a convex subset of
off-policy distributions (and one which can be computed efficiently from samples), we can provide
performance guarantees for the TD fixed point. While we have focused on presenting error bounds
for the analytical (infinite sample) TD fixed point, a huge swath of problems in TD learning arise
from this same off-policy issue: the convergence of the original TD method, the ability to find the
?1 regularized TD fixed point [6], the on-policy requirement of the finite sample analysis of LSTD
[8], and the convergence of TD-based policy iteration algorithms [7]. Although left for future work,
we suspect that the same techniques we present here can also be extending to these other cases,
potentially providing a wide range of analogous results that still apply under off-policy sampling.
Acknowledgements. We thank the reviewers for helpful comments and Bruno Scherrer for pointing
out a potential improvement to the error bound. J. Zico Kolter is supported by an NSF CI Fellowship.
5

Experimental details: For the random Markov Chain rows of P are drawn IID from a Dirichlet distribution,
and the reward and bases are random normal, with |S| = 11. For the diffusion-based chain, we sample
|S| = 100 points from a 2D unit cube xi ? [0, 1]2 and set p(s? = j|s = i) ? exp(?kxi ? xj k2 /(2? 2 ))
for bandwidth ? = 0.4. Similarly, rewards are sampled from a zero-mean Gaussian Process with covariance
Kij = exp(?kxi ? xj k2 /(2? 2 )), and for basis vectors we use the principle eigenvectors of Cov(V ) =
E[(I ? ?P )RRT (I ? ?P )T ] = (I ? ?P )K(I ? ?P )T , which are the optimal bases for representing value
functions (in expectation). Some details of the domains are omitted due to space constraints, but MATLAB
code for all the experiments is included in the supplementary files.

8

References
[1] L. C. Baird. Residual algorithms: Reinforcement learning with function approximation. In
Proceedings of the International Conference on Machine Learning, 1995.
[2] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[3] P. Dayan. The convergence of TD(?) for general ?. Machine Learning, 8(3?4), 1992.
[4] T. Jaakkola, M. I. Jordan, and S. P. Singh. On the convergence of stochastic iterative dynamic
programming algorithms. Neural Computation, 6(6), 1994.
[5] M. Journee, F. Bach, P.A. Absil, and R. Sepulchre. Low-rank optimization on the cone of
positive semidefinite matrices. SIAM Journal on Optimization, 20(5):2327?2351, 2010.
[6] J.Z. Kolter and A.Y. Ng. Regularization and feature selection in least-squares temporal difference learning. In Proceedings of the International Conference on Machine Learning, 2009.
[7] M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning
Research, 4:1107?1149, 2003.
[8] A. Lazaric, M. Ghavamzadeh, and R. Munos. Finite-sample analysis of LSTD. In Proceedings
of the International Conference on Machine Learning, 2010.
[9] H.R. Maei and R.S. Sutton. GQ(?): A general gradient algorithm for temporal-difference
prediction learning with eligibility traces. In Proceedings of the Third Conference on Artificial
General Intelligence, 2010.
[10] H.R. Maei, Cs. Szepesvari, S. Bhatnagar, and R.S. Sutton. Toward off-policy learning control
with function approximation. In Proceedings of the International Conference on Machine
Learning, 2010.
[11] R. Munos. Error bounds for approximate policy iteration. In Proceedings of the International
Conference on Machine Learning, 2003.
[12] J. Nocedal and S.J. Wright. Numerical Optimization. Springer, 1999.
[13] B. Scherrer. Personal communication, 2011.
[14] M. Schmidt. minfunc, 2005. Available at http://www.cs.ubc.ca/?schmidtm/
Software/minFunc.html.
[15] R.S. Sutton, H.R. Maei, D. Precup, S. Bhatnagar, D. Silver, Cs. Szepesvari, and E. Wiewiora.
Fast gradient-descent methods for temporal-difference learning with linear function approximation. In Proceedings of the International Conference on Machine Learning, 2009.
[16] R.S. Sutton, Cs. Szepesvari, and H.R. Maei. A convergent O(n) algorithm for off-policy
temporal-different learning with linear function approximation. In Advances in Neural Information Processing, 2008.
[17] J.N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function
approximation. IEEE Transactions and Auotomatic Control, 42:674?690, 1997.
[18] J.N. Tsitsiklis and B. Van Roy. Average cost temporal difference learning. Automatica,
35(11):1799?1808, 1999.
[19] H. Yu and D. P. Bertsekas. Error bounds for approximations from projected linear equations.
Mathematics of Operations Research, 35:306?329, 2010.

9

"
293,"A mean field algorithm for Bayes learning
in large feed-forward neural networks

Manfred Opper
Institut fur Theoretische Physik
Julius-Maximilians-Universitat, Am Hubland
D-97074 Wurzburg, Germany
opperOphysik.Uni-Wuerzburg.de

Ole Winther
CONNECT
The Niels Bohr Institute
Blegdamsvej 17
2100 Copenhagen, Denmark
wintherGconnect.nbi.dk

Abstract
We present an algorithm which is expected to realise Bayes optimal
predictions in large feed-forward networks. It is based on mean field
methods developed within statistical mechanics of disordered systems. We give a derivation for the single layer perceptron and show
that the algorithm also provides a leave-one-out cross-validation
test of the predictions. Simulations show excellent agreement with
theoretical results of statistical mechanics.

1

INTRODUCTION

Bayes methods have become popular as a consistent framework for regularization
and model selection in the field of neural networks (see e.g. [MacKay,1992]). In
the Bayes approach to statistical inference [Berger, 1985] one assumes that the prior
uncertainty about parameters of an unknown data generating mechanism can be
encoded in a probability distribution, the so called prior. Using the prior and
the likelihood of the data given the parameters, the posterior distribution of the
parameters can be derived from Bayes rule. From this posterior, various estimates
for functions ofthe parameter, like predictions about unseen data, can be calculated.
However, in general, those predictions cannot be realised by specific parameter
values, but only by an ensemble average over parameters according to the posterior
probability.
Hence, exact implementations of Bayes method for neural networks require averages
over network parameters which in general can be performed by time consuming

M. Opper and O. Winther

226

Monte Carlo procedures. There are however useful approximate approaches for
calculating posterior averages which are based on the assumption of a Gaussian
form of the posterior distribution [MacKay,1992]. Under regularity conditions on
the likelihood, this approximation becomes asymptotically exact when the number
of data is large compared to the number of parameters. This Gaussian ansatz
for the posterior may not be justified when the number of examples is small or
comparable to the number of network weights. A second cause for its failure would
be a situation where discrete classification labels are produced from a probability
distribution which is a nonsmooth function of the parameters. This would include
the case of a network with threshold units learning a noise free binary classification
problem.
In this contribution we present an alternative approximate realization of Bayes
method for neural networks, which is not based on asymptotic posterior normality. The posterior averages are performed using mean field techniques known from
the statistical mechanics of disordered systems. Those are expected to become exact
in the limit of a large number of network parameters under additional assumptions on
the statistics of the input data. Our analysis follows the approach of [Thouless, Anderson& Palmer,1977] (TAP) as adapted to the simple percept ron by [Mezard,1989].
The basic set up of the Bayes method is as follows: We have a training set consisting
of m input-output pairs Dm = {(sll,ull),m = 1, ... ,/J}, where the outputs are
generated independently from a conditional probability distribution P( u ll Iw, sll).
This probability is assumed to describe the output u ll to an input sll of a neural
network with weights w subject to a suitable noise process. If we assume that the
unknown parameters w are randomly distributed with a prior distribution p(w),
then according to Bayes theorem our knowledge about w after seeing m examples
is expressed through the posterior distribution
m

p(wIDm) = Z-lp(w)

II P(ulllw,sll)

( 1)

11=1

=J

n;=l

where Z
dwp(w)
P(ulllw, sll) is called the partition function in statistical
mechanics and the evidence in Bayesian terminology. Taking the average with respect to the posterior eq. (1), which in the following will be denoted by angle brackets, gives Bayes estimates for various quantities. For example the optimal predictive
probability for an output u to a new input s is given by pBayes(uls) = (P(ulw, s?.
In section 2 exact equations for the posterior averaged weights (w) are derived for
arbitrary networks. In 3 we specialize these equations to a perceptron and develop
a mean field ansatz in section 4. The resulting system of mean field equations equations is presented in section 5. In section 6 we consider Bayes optimal predictions
and a leave-one-out estimator for the generalization error. We conclude in section 7
with a discussion of our results.

2

A RESULT FOR POSTERIOR AVERAGES FROM
GAUSSIAN PRIORS

In this section we will derive an interesting equation for the posterior mean of the
weights for arbitrary networks when the prior is Gaussian. This average of the

227

Mean Field Algorithm/or Bayes Learning

weights can be calculated for the distribution (1) by using the following simple and
well known result for averages over Gaussian distributions.
Let v be a Gaussian random variable with zero means. Then for any function f(v),
we have
2
df(v)
(vf(v?a = (v )a? (~)a .

(2)

Here ( .. .)a denotes the average over the Gaussian distribution of v. The relation is
easily proved from an integration by parts.

=

In the following we will specialize to an isotropic Gaussian prior p(w)
~Ne-!w.w. In [Opper & Winter ,1996] anisotropic priors are treated as well.
v 21r

Applying (2) to each component of wand the function
the following equations
(w)

=Z-l

Here ( . . .)Il =

= Z-l

J

tJ

Ii

1l=1

""icll

dwp(w)

dw wp(w)

Ii

n;=l P(o-Illw,sll), we get

P(o-""Iw, s"")

,,=1

P(o-""Iw, s"")\7 w P(o-lllw, sll)

(3)

JJdWp(w) ...nn""1t' P(a""lw ,s"") is a reduced average over a posterior where
dwp(w)

""~t'

P(a""lw ,s"")

the Jl-th example is kept out of the training set and \7 w denotes the gradient with
respect to w.

3

THE PERCEPTRON

In the following , we will utilize the fact that for neural networks, the probability (1)
depends only on the so called internal fields 8 = JNw . s .
A simple but nontrivial example is the perceptron with N dimensional input vector s
and output 0-( W, s) = sign( 8). We will generalize the noise free model by considering
label noise in which the output is flipped, i.e. 0-8 < 0 with a probability (1 +e.B)-l.
(For simplicity, we will assume that f3 is known such that no prior on f3 is needed .)
The conditional probability may thus be written as
P(0-1l8 1l ) = P(o-Illw sll) ,-

e-.B9( -at' At')

1 + e-.B

'

(4)

where 9(x) = 1 for x > 0 and 0 otherwise. Obviously, this a nonsmooth function of
the weights w, for which the posterior will not become Gaussian asymptotically.
For this case (3) reads

t

(w) = _1_
(P'(0-1l8 1l ?1l o-Ilsll =
.jN 1l=1 (P(0-1l8 1l ?1l
_1_

f JJ

.jN 1l=1

d8fll (8)P'(0-1l8) o-Ilsll
d8fll(A)P(0-1l8)

(5)

M. Opper and O. Winther

228

IIJ (~) is the density of -dNw . glJ, when the weights ware randomly drawn from a
posterior, where example (glJ , (TIJ) was kept out of the training set. This result states
that the weights are linear combinations of the input vectors. It gives an example
of the ability of Bayes method to regularize a network model: the effective number
of parameters will never exceed the number of data points.

4

MEAN FIELD APPROXIMATION

Sofar, no approximations have been made to obtain eqs. (3,5). In general IIJ(~)
depends on the entire set of data Dm and can not be calculated easily. Hence, we
look for a useful approximation to these densities.
We split the internal field into its average and fluctuating parts, i.e. we set ~IJ =
(~IJ)IJ + v lJ , with vlJ = IN(w - (w)lJ)glJ. Our mean field approximation is based
on the assumption of a central limit theorem for the fluctuating part of the internal
field, vlJ which enters in the reduced average of eq. (5). This means, we assume
that the non-Gaussian fluctuations of Wi around (Wi)IJ' when mulitplied by sr will
sum up to make vlJ a Gaussian random variable. The important point is here that
for the reduced average, the Wi are not correlated to the sr! 1
We expect that this Gaussian approximation is reasonable, when N, the number
of network weights is sufficiently large.Following ideas of [Mezard, Parisi & Virasoro,1987] and [Mezard,1989]' who obtained mean field equations for a variety of
disordered systems in statistical mechanics, one can argue that in many cases this
assumption may be exactly fulfilled in the 'thermodynamic limit' m, N ~ 00 with
a = ~ fixed. According to this ansatz, we get

in terms of the second moment of vlJ AIJ := ~ 2:i,j

srsj (WiWj)1J -

(Wi)IJ(Wj)IJ).

To evaluate (5) we need to calculate the mean (~IJ)IJ and the variance AIJ. The first
problem is treated easily within the Gaussian approximation.

(6)

In the third line (2) has been used again for the Gaussian random variable vlJ .
Sofar, the calculation of the variance AIJ for general inputs is an open problem.
However, we can make a further reasonable ansatz, when the distribution of the
inputs is known. The following approximation for AIJ is expected to become exact
in the thermodynamic limit if the inputs of the training set are drawn independently
1 Note that the fluctuations of the internal field with respect to the full posterior mean
(which depends on the input si-') is non Gaussian, because the different terms in the sum
become slightly correlated.

229

Mean Field Algorithm/or Bayes Learning

from a distribution, where all components Si are uncorrelated and normalized i.e.
Si
0 and Si Sj
dij. The bars denote expectation over the distribution of inputs.
For the generalisation to a correlated input distribution see [Opper& Winther,1996].
Our basic mean field assumption is that the fluctuations of the All with the data
set can be neglected so that we can replace them by their averages All. Since the
reduced posterior averages are not correlated with the data sf, we obtain All ~
2:i(wl}1l - (Wi)!). Finally, we replace the reduced average by the expectation
over the full posterior, neglecting terms of order liN. Using 2:i(wl)
N, which
2:i (Wi)2 . This
follows from our choice of the Gaussian prior, we get All ~ A = 1 depends only on known quantities.

=

=

tr

tr

5

=

MEAN FIELD EQUATIONS FOR THE PERCEPTRON

(5) and (6) give a selfconsistent set of equations for the variable xll
We finally get

== \~{::::N:

.

(7)

(8)
with

(9)

These mean field equations can be solved by iteration. It is useful to start with a
small number of data and then to increase the number of data in steps of 1 - 10.
Numerical work show that the algorithm works well even for small systems sizes,
N ~ 15.

6

BAYES PREDICTIONS AND LEAVE-ONE-OUT

After solving the mean field equations we can make optimal Bayesian classifications
for new data s by chosing the output label with the largest predictive probability.
In case of output noise this reduces to uBayes(s) = sign(u(w, s? Since the posterior
distribution is independent of the new input vector we can apply the Gaussian assumption again to the internal field, d. and obtain uBayes(s) = u( (w), s), i.e for the
simple perceptron the averaged weights implement the Bayesian prediction. This
will not be the case for multi-layer neural networks.
We can also get an estimate for the generalization error which occurs on the prediction of new data. The generalization error for the Bayes prediction is defined
by {Bayes = (8 (-u(s)(u(w,s??s, where u(s) is the true output and ( ... )s denotes
average over the input distribution. To obtain the leave-one-out estimator of { one

M. Opper and O. Winther

230
0.50

0 .40
.""-

""-

. >-

0 .30
...

0 .20

I

0.10

o. 00 '---""'----'-.J' - ----'_ _--'---_

o

---L_

_

-'--_~_

1- I I

J
_

_ L __

___""__

_

L__ _

- ' - -_ - - ' - -_

----'

6

4

2

=

=

Figure 1: Error vs. a mj N for the simple percept ron with output noise f3 0.5
and N = 50 averaged over 200 runs. The full lines are the simulation results (upper
curve shows prediction error and the lower curve shows training error). The dashed
line is the theoretical result for N -+ 00 obtained from statistical mechanics [Opper &
Haussler, 1991] . The dotted line with larger error bars is the moving control estimate.
removes the p-th example from the training set and trains the network using only
the remaining m - 1 examples. The p'th example is used for testing. Repeating
this procedure for all p an unbiased estimate for the Bayes generalization error with
m-1 training data is obtained as the mean value f~~r8 =
EI' e (-ul'(O'(w, 81'?1')
which is exactly the type of reduced averages which are calculated within our approach. Figure 1 shows a result of simulations of our algorithm when the inputs are
uncorrelated and the outputs are generated from a teacher percept ron with fixed
noise rate f3.

!

7

CONCLUSION

In this paper we have presented a mean field algorithm which is expected to implement a Bayesian optimal classification well in the limit of large networks. We have
explained the method for the single layer perceptron. An extension to a simple multilayer network, the so called committee machine with a tree architecture is discussed
in [Opper& Winther,1996]. The algorithm is based on a Gaussian assumption for
the distribution of the internal fields, which seems reasonable for large networks.
The main problem sofar is the restriction to ideal situations such as a known distri-

Mean FieLd Algorithm/or Bayes Learning

231

bution of inputs which is not a realistic assumption for real world data. However,
this assumption only entered in the calculation of the variance of the Gaussian field.
More theoretical work is necessary to find an approximation to the variance which
is valid in more general cases. A promising approach is a derivation of the mean
field equations directly from an approximation to the free energy -In(Z). Besides a
deeper understanding this would also give us the possibility to use the method with
the so called evidence framework , where the partition function (evidence) can be
used to estimate unknown (hyper-) parameters of the model class [Berger, 1985]. It
will further be important to extend the algorithm to fully connected architectures.
In that case it might be necessary to make further approximations in the mean field
method.

ACKNOWLEDGMENTS
This research is supported by a Heisenberg fellowship of the Deutsche Forschrmgsgemeinschaft and by the Danish Research Councils for the Natural and Technical
Sciences through the Danish Computational Neural Network Center (CONNECT) .

REFERENCES
Berger, J. O. (1985) Statistical Decision theory and Bayesian Analysis, SpringerVerlag, New York.
MacKay, D. J. (1992) A practical Bayesian framework for backpropagation networks,
Neural Compo 4 448.
Mezard , M., Parisi G. & Virasoro M. A. (1987) Spin Glass Theory and Beyond,
Lecture Notes in Physics , 9, World Scientific, .
Mezard, M. (1989) The space of interactions in neural networks: Gardner's calculation with the cavity method J. Phys. A 22, 2181 .
Opper, M. & Haussler, D. (1991) in IVth Annual Workshop on Computational
Learning Theory (COLT91) , Morgan Kaufmann.
Opper M. & Winther 0 (1996) A mean field approach to Bayes learning in feedforward neural networks, Phys. Rev. Lett. 76 1964.
Thouless, D .J ., Anderson, P. W . & Palmer, R .G. (1977), Solution of 'Solvable model
of a spin glass' Phil. Mag. 35, 593.

"
5648,"Unsupervised Domain Adaptation with Residual
Transfer Networks
Mingsheng Long? , Han Zhu? , Jianmin Wang? , and Michael I. Jordan]
?
KLiss, MOE; TNList; School of Software, Tsinghua University, China
]
University of California, Berkeley, Berkeley, USA
{mingsheng,jimwang}@tsinghua.edu.cn, zhuhan10@gmail.com, jordan@berkeley.edu

Abstract
The recent success of deep neural networks relies on massive amounts of labeled
data. For a target task where labeled data is unavailable, domain adaptation can
transfer a learner from a different source domain. In this paper, we propose a new
approach to domain adaptation in deep networks that can jointly learn adaptive
classifiers and transferable features from labeled data in the source domain and
unlabeled data in the target domain. We relax a shared-classifier assumption made
by previous methods and assume that the source classifier and target classifier
differ by a residual function. We enable classifier adaptation by plugging several
layers into deep network to explicitly learn the residual function with reference
to the target classifier. We fuse features of multiple layers with tensor product
and embed them into reproducing kernel Hilbert spaces to match distributions for
feature adaptation. The adaptation can be achieved in most feed-forward models by
extending them with new residual layers and loss functions, which can be trained
efficiently via back-propagation. Empirical evidence shows that the new approach
outperforms state of the art methods on standard domain adaptation benchmarks.

1

Introduction

Deep networks have significantly improved the state of the art for a wide variety of machine-learning
problems and applications. Unfortunately, these impressive gains in performance come only when
massive amounts of labeled data are available for supervised training. Since manual labeling of
sufficient training data for diverse application domains on-the-fly is often prohibitive, for problems
short of labeled data, there is strong incentive to establishing effective algorithms to reduce the
labeling consumption, typically by leveraging off-the-shelf labeled data from a different but related
source domain. However, this learning paradigm suffers from the shift in data distributions across
different domains, which poses a major obstacle in adapting predictive models for the target task [1].
Domain adaptation [1] is machine learning under the shift between training and test distributions. A
rich line of approaches to domain adaptation aim to bridge the source and target domains by learning
domain-invariant feature representations without using target labels, so that the classifier learned from
the source domain can be applied to the target domain. Recent studies have shown that deep networks
can learn more transferable features for domain adaptation [2, 3], by disentangling explanatory factors
of variations behind domains. Latest advances have been achieved by embedding domain adaptation
in the pipeline of deep feature learning which can extract domain-invariant representations [4, 5, 6, 7].
The previous deep domain adaptation methods work under the assumption that the source classifier can
be directly transferred to the target domain upon the learned domain-invariant feature representations.
This assumption is rather strong as in practical applications, it is often infeasible to check whether
the source classifier and target classifier can be shared or not. Hence we focus in this paper on a more
general, and safe, domain adaptation scenario in which the source classifier and target classifier differ
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

by a small perturbation function. The goal of this paper is to simultaneously learn adaptive classifiers
and transferable features from labeled data in the source domain and unlabeled data in the target
domain by embedding the adaptations of both classifiers and features in a unified deep architecture.
Motivated by the state of the art deep residual learning [8], winner of the ImageNet ILSVRC 2015
challenge, we propose a new Residual Transfer Network (RTN) approach to domain adaptation in
deep networks which can simultaneously learn adaptive classifiers and transferable features. We relax
the shared-classifier assumption made by previous methods and assume that the source and target
classifiers differ by a small residual function. We enable classifier adaptation by plugging several
layers into deep networks to explicitly learn the residual function with reference to the target classifier.
In this way, the source classifier and target classifier can be bridged tightly in the back-propagation
procedure. The target classifier is tailored to the target data better by exploiting the low-density
separation criterion. We fuse features of multiple layers with tensor product and embed them into
reproducing kernel Hilbert spaces to match distributions for feature adaptation. The adaptation can be
achieved in most feed-forward models by extending them with new residual layers and loss functions,
and can be trained efficiently using standard back-propagation. Extensive evidence suggests that the
RTN approach outperforms several state of art methods on standard domain adaptation benchmarks.

2

Related Work

Domain adaptation [1] builds models that can bridge different domains or tasks, which mitigates
the burden of manual labeling for machine learning [9, 10, 11, 12], computer vision [13, 14, 15]
and natural language processing [16]. The main technical problem of domain adaptation is that the
domain discrepancy in probability distributions of different domains should be formally reduced.
Deep neural networks can learn abstract representations that disentangle different explanatory factors
of variations behind data samples [17] and manifest invariant factors underlying different populations
that transfer well from original tasks to similar novel tasks [3]. Thus deep neural networks have been
explored for domain adaptation [18, 19, 15], multimodal and multi-task learning [16, 20], where
significant performance gains have been witnessed relative to prior shallow transfer learning methods.
However, recent advances show that deep networks can learn abstract feature representations that
can only reduce, but not remove, the cross-domain discrepancy [18, 4]. Dataset shift has posed a
bottleneck to the transferability of deep features, resulting in statistically unbounded risk for target
tasks [21, 22]. Some recent work addresses the aforementioned problem by deep domain adaptation,
which bridges the two worlds of deep learning and domain adaptation [4, 5, 6, 7]. They extend deep
convolutional networks (CNNs) to domain adaptation either by adding one or multiple adaptation
layers through which the mean embeddings of distributions are matched [4, 5], or by adding a fully
connected subnetwork as a domain discriminator whilst the deep features are learned to confuse
the domain discriminator in a domain-adversarial training paradigm [6, 7]. While performance was
significantly improved, these state of the art methods may be restricted by the assumption that under
the learned domain-invariant feature representations, the source classifier can be directly transferred
to the target domain. In particular, this assumption may not hold when the source classifier and target
classifier cannot be shared. As theoretically studied in [22], when the combined error of the ideal
joint hypothesis is large, then there is no single classifier that performs well on both source and target
domains, so we cannot find a good target classifier by directly transferring from the source domain.
This work is primarily motivated by He et al. [8], the winner of the ImageNet ILSVRC 2015 challenge.
They present deep residual learning to ease the training of very deep networks (hundreds of layers),
termed residual nets. The residual nets explicitly reformulate the layers as learning residual functions
?F (x) with reference to the layer inputs x, instead of directly learning the unreferenced functions
F (x) = ?F (x) + x. The method focuses on standard deep learning in which training data and test
data are drawn from identical distributions, hence it cannot be directly applied to domain adaptation.
In this paper, we propose to bridge the source classifier fS (x) and target classifier fT (x) by the
residual layers such that the classifier mismatch across domains can be explicitly modeled by the
residual functions ?F (x) in a deep learning architecture. Although the idea of adapting source
classifier to target domain by adding a perturbation function has been studied by [23, 24, 25], these
methods require target labeled data to learn the perturbation function, which cannot be applied to
unsupervised domain adaptation, the focus of this study. Another distinction is that their perturbation
function is defined in the input space x, while the input to our residual function is the target classifier
fT (x), which can capture the connection between the source and target classifiers more effectively.
2

3

Residual Transfer Networks

s
In unsupervised domain adaptation problem, we are given a source domain Ds = {(xsi , yis )}ni=1
of ns
t
labeled examples and a target domain Dt = {xtj }nj=1
of nt unlabeled examples. The source domain
and target domain are sampled from different probability distributions p and q respectively, and p 6= q.
The goal of this paper is to design a deep neural network that enables learning of transfer classifiers
y = fs (x) and y = ft (x) to close the source-target discrepancy, such that the expected target risk
Rt (ft ) = E(x,y)?q [ft (x) 6= y] can be bounded by leveraging the source domain supervised data.

The challenge of unsupervised domain adaptation arises in that the target domain has no labeled
data, while the source classifier fs trained on source domain Ds cannot be directly applied to the
target domain Dt due to the distribution discrepancy p(x, y) 6= q(x, y). The distribution discrepancy
may give rise to mismatches in both features and classifiers, i.e. p(x) 6= q(x) and fs (x) 6= ft (x).
Both mismatches should be fixed by joint adaptation of features and classifiers to enable effective
domain adaptation. Classifier adaptation is more difficult than feature adaptation because it is directly
related to the labels but the target domain is fully unlabeled. Note that the state of the art deep feature
adaptation methods [5, 6, 7] generally assume classifiers can be shared on adaptive deep features. This
paper assumes fs 6= ft and presents an end-to-end deep learning framework for classifier adaptation.
Deep networks [17] can learn distributed, compositional, and abstract representations for natural data
such as image and text. This paper addresses unsupervised domain adaptation within deep networks
for jointly learning transferable features and adaptive classifiers. We extend deep convolutional
networks (CNNs), i.e. AlexNet [26], to novel residual transfer networks (RTNs) as shown in Figure 1.
Denote by fs (x) the source classifier, and the empirical error of CNN on source domain data Ds is
min
fs

ns
1 X
L (fs (xsi ) , yis ),
ns i=1

(1)

where L(?, ?) is the cross-entropy loss function. Based on the quantification study of feature transferability in deep convolutional networks [3], convolutional layers can learn generic features that are
transferable across domains [3]. Hence we opt to fine-tune, instead of directly adapt, the features of
convolutional layers when transferring pre-trained deep models from source domain to target domain.
3.1

Feature Adaptation

Deep features learned by CNNs can disentangle explanatory factors of variations behind data distributions to boost knowledge transfer [19, 17]. However, the latest literature findings reveal that deep
features can reduce, but not remove, the cross-domain distribution discrepancy [3], which motivates
the state of the art deep feature adaptation methods [5, 6, 7]. Deep features in standard CNNs must
eventually transition from general to specific along the network, and the transferability of features and
classifiers will decrease when the cross-domain discrepancy increases [3]. In other words, the shifts
in the data distributions linger even after multilayer feature abstractions. In this paper, we perform
feature adaptation by matching the feature distributions of multiple layers ` ? L across domains. We
reduce feature dimensions by adding a bottleneck layer f cb on top of the last feature layer of CNNs,
and then fine-tune CNNs on source labeled examples such that the feature distributions of the source
and target are made similar under new feature representations in multiple layers L = {f cb, f cc}, as
shown in Figure 1. To adapt multiple feature layers effectively, we propose the tensor product between
features of multiple layers to perform lossless multi-layer feature fusion, i.e. zsi , ?`?L xs`
i and
ztj , ?`?L xt`
.
We
then
perform
feature
adaptation
by
minimizing
the
Maximum
Mean
Discrepancy
j
(MMD) [27] between source and target domains using the fused features (dubbed tensor MMD) as



ns X
ns
ns X
nt
nt X
nt
X
X
X
k zsi , zsj
k zsi , ztj
k zti , ztj
min DL (Ds , Dt ) =
+
?2
(2)
fs ,ft
n2s
n2t
ns nt
i=1 j=1
i=1 j=1
i=1 j=1
0

2

where the characteristic kernel k(z, z0 ) = e?kvec(z)?vec(z )k /b is the Gaussian kernel function
defined on the vectorization of tensors z and z0 with bandwidth parameter b. Different from DAN [5]
that adapts multiple feature layers using multiple MMD penalties, this paper adapts multiple feature
layers by first fusing them and then adapting the fused features. The advantage of our method against
DAN [5] is that our method facilitates easier model selection, since our method has only one MMD
penalty for adapting multiple layers while DAN [5] needs |L| MMD penalties for adapting |L| layers.
3

Xs

fcb
AlexNet,
ResNet?

Xt

fcb

?

fcc

MMD

?

fc1

fc2

?f ( x )

fS ( x ) = fT ( x ) + ?f ( x )
fcc

?f

S

(x)

Ys

fcb

Xs

?

fcc

Xs

Zs

weight layer
?F ( x )
x

MMD

fT ( x )

entropy minimization

Yt

fcb

Xt

Zt

?

weight layer

fcc

Xt

F (x) =

+

?F ( x ) + x

Figure 1: (left) Residual Transfer Network (RTN) for domain adaptation, based on well-established
architectures. Due to dataset shift, (1) the last-layer features are tailored to domain-specific structures
that are not safely transferable, hence we add a bottleneck layer f cb that is adapted jointly with the
classifier layer f cc by the tensor MMD module; (2) Supervised classifiers are not safely transferable,
hence we bridge them by the residual layers f c1?f c2 so that fS (x) = fT (x)+?f (x). (middle) The
tensor MMD module for multi-layer feature adaptation. (right) The building block for deep residual
learning; Instead of using the residual block to model feature mappings, we use it to bridge the source
classifier fS (x) and target classifier fT (x) with x , fT (x), F (x) , fS (x), and ?F (x) , ?f (x).
3.2

Classifier Adaptation

As feature adaptation cannot remove the mismatch in classification models, we further perform
classifier adaptation to learn transfer classifiers that make domain adaptation more effective. Although
the source classifier fs (x) and target classifier ft (x) are different, fs (x) 6= ft (x), they should be
related to ensure the feasibility of domain adaptation. It is reasonable to assume that fs (x) and ft (x)
differ only by a small perturbation function ?f (x). Prior work on classifier adaptation [23, 24, 25]
assumes that ft (x) = fs (x) + ?f (x), where the perturbation ?f (x) is a function of input feature x.
However, these methods require target labeled data to learn the perturbation function, which cannot
be applied to unsupervised domain adaptation where target domain has no labeled data. How to
bridge fs (x) and ft (x) in a framework is a key challenge of unsupervised domain adaptation. We
postulate that the perturbation function ?f (x) can be learned jointly from the source labeled data
and target unlabeled data, given that the source classifier and target classifier are properly connected.
To enable classifier adaptation, consider fitting F (x) as an original mapping by a few stacked layers
(convolutional or fully connected layers) in Figure 1 (right), where x denotes the inputs to the first of
these layers [8]. If one hypothesizes that multiple nonlinear layers can asymptotically approximate
complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate
the residual functions, i.e. F (x) ? x. Rather than expecting stacked layers to approximate F (x), one
explicitly lets these layers approximate a residual function ?F (x) , F (x) ? x, with the original
function being ?F (x) + x. The operation ?F (x) + x is performed by a shortcut connection and an
element-wise addition, while the residual function is parameterized by residual layers within each
residual block. Although both forms are able to asymptotically approximate the desired functions, the
ease of learning is different. In reality, it is unlikely that identity mappings are optimal, but it should
be easier to find the perturbations with reference to an identity mapping, than to learn the function
as new. The residual learning is the key to the successful training of very deep networks. The deep
residual network (ResNet) framework [8] bridges the inputs and outputs of the residual layers by the
shortcut connection (identity mapping) such that F (x) = ?F (x) + x, which eases the learning of
residual function ?F (x) (similar to the perturbation function across the source and target classifiers).
Based on this key observation, we extend the CNN architectures (Figure 1, left) by plugging in the
residual block (Figure 1, right). We reformulate the residual block to bridge the source classifier
fS (x) and target classifier fT (x) by letting x , fT (x), F (x) , fS (x), and ?F (x) , ?f (x). Note
that fS (x) is the outputs of the element-wise addition operator and fT (x) is the outputs of the targetclassifier layer f cc, both before softmax activation ?(?), fs (x) , ? (fS (x)) , ft (x) , ? (fT (x)).
We can connect the source classifier and target classifier (before activation) by the residual block as
fS (x) = fT (x) + ?f (x) ,

(3)

where we have used classifiers fS and fT before softmax activation to ensure that the final classifiers
fs and ft will output probabilities. Residual layers f c1?f c2 are fully-connected layers with c ? c
units, where c is the number of classes. We set the source classifier fS as the outputs of the residual
block to make it better trainable from the source-labeled data by deep residual learning [8]. In other
words, if we set fT as the outputs of the residual block, then we may be unable to learn it successfully
as we do not have target labeled data and thus standard back-propagation will not work. Deep residual
learning [8] ensures to output valid classifiers |?f (x)|  |fT (x)| ? |fS (x)|, and more importantly,
4

makes the perturbation function ?f (x) dependent on both the target classifier fT (x) (due to the
functional dependency) as well as the source classifier fS (x) (due to the back-propagation pipeline).
Although we successfully cast the classifier adaptation into the residual learning framework while
the residual learning framework tends to make the target classifier ft (x) not deviate much from the
source classifier fs (x), we still cannot guarantee that ft (x) will fit the target-specific structures well.
To address this problem, we further exploit the entropy minimization principle [28] for refining the
classifier adaptation, which encourages the low-density separation between classes by minimizing
the entropy of class-conditional distribution fjt (xti ) = p(yit = j|xti ; ft ) on target domain data Dt as
min
ft

nt

1 X
H ft xti ,
nt i=1

(4)

where ft (xti ) is the conditional probability that target classifier ft (x) assigns unlabeled target point
xti P
to pseudo-label yit (predicted value), and H(?) is the entropy function defined as H (ft (xti )) =
c
? j=1 fjt (xti ) log fjt (xti ), c is the number of classes, and fjt (xti ) is the probability of predicting
point xti to class j. By minimizing entropy penalty (4), the target classifier ft (x) is made directly
accessible to target-unlabeled data and will amend itself to pass through the target low-density regions.
3.3

Residual Transfer Network

To enable effective unsupervised domain adaptation, we propose Residual Transfer Network (RTN),
which jointly learns transferable features and adaptive classifiers by integrating deep feature learning
(1), feature adaptation (2), and classifier adaptation (3)?(4) in an end-to-end deep learning framework,
min

fS =fT +?f

+

ns
1 X
L (fs (xsi ) , yis )
ns i=1
nt

? X
H ft xti
nt i=1

(5)

+ ? DL (Ds , Dt ),
where ? and ? are the tradeoff parameters for the tensor MMD penalty (2) and entropy penalty (4)
respectively. The proposed RTN model (5) is enabled to learn both transferable features and adaptive
classifiers. As classifier adaptation proposed in this paper and feature adaptation studied in [5, 6] are
tailored to adapt different layers of deep networks, they can complement each other to establish better
performance. Since training deep CNNs requires a large amount of labeled data that is prohibitive for
many domain adaptation applications, we start with the CNN models pre-trained on ImageNet 2012
data and fine-tune it as [5]. The training of RTN mainly follows standard back-propagation, including
the residual transfer layers for classifier adaptation [8]. Note that, the optimization of tensor MMD
penalty (2) requires carefully-designed algorithm to establish linear-time training, as detailed in [5].

4

Experiments

We evaluate the residual transfer network against state of the art transfer learning and deep learning
methods on unsupervised domain adaptation benchmarks. Codes and datasets will be available online.
4.1

Setup

Office-31 [13] is a benchmark for domain adaptation, comprising 4,110 images in 31 classes collected
from three distinct domains: Amazon (A), which contains images downloaded from amazon.com,
Webcam (W) and DSLR (D), which contain images taken by web camera and digital SLR camera
with different photographical settings, respectively. To enable unbiased evaluation, we evaluate all
methods on all six transfer tasks A ? W, D ? W, W ? D, A ? D, D ? A and W ? A as in [5, 7].
Office-Caltech [14] is built by selecting the 10 common categories shared by Office-31 and Caltech256 (C), and is widely used by previous methods [14, 29]. We can build 12 transfer tasks: A ? W,
D ? W, W ? D, A ? D, D ? A, W ? A, A ? C, W ? C, D ? C, C ? A, C ? W, and C
? D. While Office-31 has more categories and is more difficult for domain adaptation algorithms,
5

Office-Caltech provides more transfer tasks to enable an unbiased look at dataset bias [30]. We adopt
DeCAF7 [2] features for shallow transfer methods and original images for deep adaptation methods.
We compare with both conventional and the state of the art transfer learning and deep learning methods:
Transfer Component Analysis (TCA) [9], Geodesic Flow Kernel (GFK) [14], Deep Convolutional
Neural Network (AlexNet [26]), Deep Domain Confusion (DDC) [4], Deep Adaptation Network
(DAN) [5], and Reverse Gradient (RevGrad) [6]. TCA is a conventional transfer learning method
based on MMD-regularized Kernel PCA. GFK is a manifold learning method that interpolates across
an infinite number of intermediate subspaces to bridge domains. DDC is the first method that
maximizes domain invariance by adding to AlexNet an adaptation layer using linear-kernel MMD
[27]. DAN learns more transferable features by embedding deep features of multiple task-specific
layers to reproducing kernel Hilbert spaces (RKHSs) and matching different distributions optimally
using multi-kernel MMD. RevGrad improves domain adaptation by making the source and target
domains indistinguishable for a discriminative domain classifier via an adversarial training paradigm.
To go deeper with the efficacy of classifier adaptation (residual transfer block) and feature adaptation
(tensor MMD module), we perform ablation study by evaluating several variants of RTN: (1) RTN
(mmd), which adds the tensor MMD module to AlexNet; (2) RTN (mmd+ent), which further adds
the entropy penalty to AlexNet; (3) RTN (mmd+ent+res), which further adds the residual module to
AlexNet. Note that RTN (mmd) improves DAN [5] by replacing the multiple MMD penalties in DAN
by a single tensor MMD penalty in RTN (mmd), which facilitates much easier parameter selection.
We follow standard protocols and use all labeled source data and all unlabeled target data for domain
adaptation [5]. We compare average classification accuracy of each transfer task using three random
experiments. For MMD-based methods (TCA, DDC, DAN, and RTN), we use Gaussian kernel with
bandwidth b set to median pairwise squared distances on training data, i.e. median heuristic [27]. As
there are no target labeled data in unsupervised domain adaptation, model selection proves difficult.
For all methods, we perform cross-valuation on labeled source data to select candidate parameters,
then conduct validation on transfer task A ? W by requiring one labeled example per category from
target domain W as the validation set, and fix the selected parameters throughout all transfer tasks.
We implement all deep methods based on the Caffe deep-learning framework, and fine-tune from
Caffe-trained models of AlexNet [26] pre-trained on ImageNet. For RTN, We fine-tune all the
feature layers, train bottleneck layer f cb, classifier layer f cc and residual layers f c1?f c2, all through
standard back-propagation. Since these new layers are trained from scratch, we set their learning rate
to be 10 times that of the other layers. We use mini-batch stochastic gradient descent (SGD) with
momentum of 0.9 and the learning rate annealing strategy implemented in RevGrad [6]: the learning
rate is not selected through a grid search due to high computational cost?it is adjusted during SGD
?0
using the following formula: ?p = (1+?p)
? , where p is the training progress linearly changing from
0 to 1, ?0 = 0.01, ? = 10 and ? = 0.75, which is optimized for low error on the source domain. As
RTN works very stably across different transfer tasks, the MMD penalty parameter ? and entropy
penalty ? are first selected on A ? W and then fixed as ? = 0.3, ? = 0.3 for all other transfer tasks.
4.2

Results

The classification accuracy results on the six transfer tasks of Office-31 are shown in Table 1, and the
results on the twelve transfer tasks of Office-Caltech are shown in Table 2. The RTN model based
on AlexNet (Figure 1) outperforms all comparison methods on most transfer tasks. In particular,
RTN substantially improves the accuracy on hard transfer tasks, e.g. A ? W and C ? W, where the
source and target domains are very different, and achieves comparable accuracy on easy transfer tasks,
D ? W and W ? D, where source and target domains are similar [13]. These results suggest that
RTN is able to learn more adaptive classifiers and more transferable features for domain adaptation.
tensor From the results, we can make interesting observations. (1) Standard deep-learning methods
(AlexNet) perform comparably with traditional shallow transfer-learning methods with deep DeCAF7
features as input (TCA and GFK). The only difference between these two sets of methods is that
AlexNet can take the advantage of supervised fine-tuning on the source-labeled data, while TCA and
GFK can take benefits of their domain adaptation procedures. This result confirms the current practice
that supervised fine-tuning is important for transferring source classifier to target domain [19], and
sustains the recent discovery that deep neural networks learn abstract feature representation, which
can only reduce, but not remove, the cross-domain discrepancy [3]. This reveals that the two worlds of
6

Table 1: Accuracy on Office-31 dataset using standard protocol [5] for unsupervised adaptation.
Method
TCA [9]
GFK [14]
AlexNet [26]
DDC [4]
DAN [5]
RevGrad [6]
RTN (mmd)
RTN (mmd+ent)
RTN (mmd+ent+res)

A?W
59.0?0.0
58.4?0.0
60.6?0.4
61.0?0.5
67.8?0.3
73.0?0.6
66.0?0.4
68.0?0.3
70.2?0.3

D?W
90.2?0.0
93.6?0.0
95.4?0.2
95.0?0.3
96.0?0.1
96.4?0.4
96.1?0.3
96.4?0.2
96.8?0.2

W?D
88.2?0.0
91.0?0.0
99.0?0.1
98.5?0.3
99.0?0.1
99.2?0.3
99.2?0.3
99.2?0.1
99.3?0.1

A?D
57.8?0.0
58.6?0.0
64.2?0.3
64.9?0.4
66.8?0.2
67.6?0.4
68.8?0.2
69.3?0.2

D?A
51.6?0.0
52.4?0.0
45.5?0.5
47.2?0.5
50.0?0.4
49.8?0.4
50.2?0.3
50.5?0.3

W?A
47.9?0.0
46.1?0.0
48.3?0.5
49.4?0.6
49.8?0.3
50.0?0.3
50.7?0.2
51.0?0.1

Avg
65.8
66.7
68.8
69.3
71.6
71.5
72.2
72.9

Table 2: Accuracy on Office-Caltech dataset using standard protocol [5] for unsupervised adaptation.
Method
A?W D?W W?D A?D D?A W?A A?C W?C D?C C?A C?W C?D Avg
TCA [9]
84.4 96.9 99.4 82.8 90.4 85.6 81.2 75.5 79.6 92.1 88.1 87.9 87.0
GFK [14]
89.5 97.0 98.1 86.0 89.8 88.5 76.2 77.1 77.9 90.7 78.0 77.1 85.5
AlexNet [26]
79.5 97.7 100.0 87.4 87.1 83.8 83.0 73.0 79.0 91.9 83.7 87.1 86.1
DDC [4]
83.1 98.1 100.0 88.4 89.0 84.9 83.5 73.4 79.2 91.9 85.4 88.8 87.1
DAN [5]
91.8 98.5 100.0 91.7 90.0 92.1 84.1 81.2 80.3 92.0 90.6 89.3 90.1
RTN (mmd)
93.2 98.5 100.0 91.7 88.0 90.7 84.0 81.3 80.4 91.0 89.8 90.4 90.0
RTN (mmd+ent)
93.8 98.6 100.0 92.9 93.6 92.7 87.8 84.8 83.4 93.2 96.6 93.9 92.6
RTN (mmd+ent+res) 95.2 99.2 100.0 95.5 93.8 92.5 88.1 86.6 84.6 93.7 96.9 94.2 93.4

deep learning and domain adaptation are not compatible well with each other in the two-step pipeline,
which motivates carefully-designed deep adaptation architectures to unify them. (2) Deep-transfer
learning methods that reduce the domain discrepancy by domain-adaptive deep networks (DDC, DAN
and RevGrad) substantially outperform standard deep learning methods (AlexNet) and traditional
shallow transfer-learning methods with deep features as the input (TCA and GFK). This confirms
that incorporating domain-adaptation modules into deep networks can improve domain adaptation
performance. By adapting source-target distributions in multiple task-specific layers using optimal
multi-kernel two-sample matching, DAN performs the best in general among the prior deep-transfer
learning methods. (3) The proposed residual transfer network (RTN) method performs the best and
sets up a new state of the art result on these datasets. Different from all the previous deep-transfer
learning methods that only adapt the feature layers of deep neural networks to learn more transferable
features, RTN further adapts the classifier layers to bridge the source and target classifiers in an
end-to-end residual learning framework, which can correct the classifier mismatch more effectively.
To go deeper into different modules of RTN, we show the results of RTN variants in Tables 1 and 2. (1)
RTN (mmd) achieves comparable results as DAN, but it has only one MMD penalty parameter while
DAN has two or three. Thus the proposed tensor MMD module is effective for adapting multiple
feature layers using a single MMD penalty, which is important for easy model selection. (2) RTN
(mmd+ent) performs substantially better than RTN (mmd). This highlights the importance of entropy
minimization for low-density separation, which exploits the cluster structure of target-unlabeled
data such that the target-classifier can be better adapted to the target data. (3) RTN (mmd+ent+res)
performs the best across all variants. This highlights the importance of residual transfer of classifier
layers for learning more adaptive classifiers. This is critical as in practical applications, there is no
guarantee that the source classifier and target classifier can be safely shared. It is worth noting that,
the entropy penalty and the residual module should be used together, otherwise the residual function
tends to learn useless zero mapping such that the source and target classifiers are nearly identical [8].
4.3

Discussion

Predictions Visualization: We respectively visualize in Figures 2(a)?2(d) the t-SNE embeddings [2]
of the predictions by DAN and RTN on transfer task A ? W. We can make the following observations.
(1) The predictions made by DAN in Figure 2(a)?2(b) show that the target categories are not well
discriminated by the source classifier, which implies that target data is not well compatible with the
source classifier. Hence the source and target classifiers should not be assumed to be identical, which
has been a common assumption made by all prior deep domain adaptation methods [4, 5, 6, 7]. (2)
The predictions made by RTN in Figures 2(c)?2(d) show that the target categories are discriminated
7

(a) DAN: Source=A

(b) DAN: Target=W

(c) RTN: Source=A

(d) RTN: Target=W

6

f S (x )

f T (x )

?f(x )

4
2
0

mean

deviation

Statistics

(a) Layer Responses

-0.5

-1
fs

-1.5

10

ft

20

Class

(b) Classifier Shift

30

Average Accuracy (%)

Layer Responses

8

Weight Parameters

Figure 2: Visualization: (a)-(b) t-SNE of DAN predictions; (c)-(d) t-SNE of RTN predictions.
100
90
80
70
60

A ?W

50
0.01 0.04 0.07 0.1

C?W

0.4

0.7

1

?

(c) Accuracy w.r.t. ?

Figure 3: (a) layer responses; (b) classifier shift; (c) sensitivity of ? (dashed lines show best baselines).
better by the target classifier (larger class-to-class distances), which suggests that residual transfer of
classifiers is a reasonable extension to previous deep feature adaptation methods. RTN simultaneously
learns more adaptive classifiers and more transferable features to enable effective domain adaptation.
Layer Responses: We show in Figure 3(a) the means and standard deviations of the layer responses
[8], which are the outputs of fT (x) (f cc layer), ?f (x) (f c2 layer), and fS (x) (after element-wise
sum operator), respectively. This exposes the response strength of the residual functions. The results
show that the residual function ?f (x) have generally much smaller responses than the shortcut
function fT (x). These results support our motivation that the residual functions are generally smaller
than the non-residual functions, as they characterize the small gap between the source classifier and
target classifier. The small residual function can be learned effectively via deep residual learning [8].
Classifier Shift: To justify that there exists a classifier shift between source classifier fs and target
classifier ft , we train fs on source domain and ft on target domain, both provided with labeled data.
By taking A as source domain and W as target domain, the weight parameters of the classifiers (e.g.
softmax regression) are shown in Figure 3(b), which shows that fs and ft are substantially different.
Parameter Sensitivity: We check the sensitivity of entropy parameter ? on transfer tasks A ? W
(31 classes) and C ? W (10 classes) by varying the parameter in {0.01, 0.04, 0.07, 0.1, 0.4, 0.7, 1.0}.
The results are shown in Figures 3(c), with the best results of the baselines shown as dashed lines.
The accuracy of RTN first increases and then decreases as ? varies and demonstrates a desirable
bell-shaped curve. This justifies our motivation of jointly learning transferable features and adaptive
classifiers by the RTN model, as a good trade-off between them can promote transfer performance.

5

Conclusion

This paper presented a novel approach to unsupervised domain adaptation in deep networks, which
enables end-to-end learning of adaptive classifiers and transferable features. Similar to many prior
domain adaptation techniques, feature adaptation is achieved by matching the distributions of features
across domains. However, unlike previous work, the proposed approach also supports classifier
adaptation, which is implemented through a new residual transfer module that bridges the source
classifier and target classifier. This makes the approach a good complement to existing techniques. The
approach can be trained by standard back-propagation, which is scalable and can be implemented by
most deep learning package. Future work constitutes semi-supervised domain adaptation extensions.

Acknowledgments
This work was supported by the National Natural Science Foundation of China (61325008, 61502265),
National Key R&D Program of China (2016YFB1000701, 2015BAF32B01), and TNList Key Project.
8

References
[1] S. J. Pan and Q. Yang. A survey on transfer learning. TKDE, 22(10):1345?1359, 2010.
[2] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional
activation feature for generic visual recognition. In ICML, 2014.
[3] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In
NIPS, 2014.
[4] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell. Deep domain confusion: Maximizing for
domain invariance. 2014.
[5] M. Long, Y. Cao, J. Wang, and M. I. Jordan. Learning transferable features with deep adaptation networks.
In ICML, 2015.
[6] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, 2015.
[7] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell. Simultaneous deep transfer across domains
and tasks. In ICCV, 2015.
[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.
[9] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain adaptation via transfer component analysis.
TNNLS, 22(2):199?210, 2011.
[10] L. Duan, I. W. Tsang, and D. Xu. Domain transfer multiple kernel learning. TPAMI, 34(3):465?479, 2012.
[11] K. Zhang, B. Sch?lkopf, K. Muandet, and Z. Wang. Domain adaptation under target and conditional shift.
In ICML, 2013.
[12] X. Wang and J. Schneider. Flexible transfer learning under support and model shift. In NIPS, 2014.
[13] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In ECCV,
2010.
[14] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow kernel for unsupervised domain adaptation. In
CVPR, 2012.
[15] J. Hoffman, S. Guadarrama, E. Tzeng, R. Hu, J. Donahue, R. Girshick, T. Darrell, and K. Saenko. LSDA:
Large scale detection through adaptation. In NIPS, 2014.
[16] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing
(almost) from scratch. JMLR, 12:2493?2537, 2011.
[17] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. TPAMI,
35(8):1798?1828, 2013.
[18] X. Glorot, A. Bordes, and Y. Bengio. Domain adaptation for large-scale sentiment classification: A deep
learning approach. In ICML, 2011.
[19] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and transferring mid-level image representations
using convolutional neural networks. In CVPR, June 2013.
[20] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. Multimodal deep learning. In ICML, 2011.
[21] Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In
COLT, 2009.
[22] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory of learning
from different domains. MLJ, 79(1-2):151?175, 2010.
[23] J. Yang, R. Yan, and A. G. Hauptmann. Cross-domain video concept detection using adaptive svms. In
MM, pages 188?197. ACM, 2007.
[24] Lixin Duan, Ivor W Tsang, Dong Xu, and Tat-Seng Chua. Domain adaptation from multiple sources via
auxiliary classifiers. In ICML, pages 289?296. ACM, 2009.
[25] L. Duan, D. Xu, I. W. Tsang, and J. Luo. Visual event recognition in videos by learning from web data.
TPAMI, 34(9):1667?1680, 2012.
[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural
networks. In NIPS, 2012.
[27] A. Gretton, K. Borgwardt, M. Rasch, B. Sch?lkopf, and A. Smola. A kernel two-sample test. JMLR,
13:723?773, March 2012.
[28] Y. Grandvalet and Y. Bengio. Semi-supervised learning by entropy minimization. In NIPS, 2004.
[29] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy domain adaptation. In AAAI, 2016.
[30] A. Torralba and A. A. Efros. Unbiased look at dataset bias. In CVPR, 2011.

9

"
1905,"Experts in a Markov Decision Process

Eyal Even-Dar
Computer Science
Tel-Aviv University
evend@post.tau.ac.il

Sham M. Kakade
Computer and Information Science
University of Pennsylvania
skakade@linc.cis.upenn.edu

Yishay Mansour ?
Computer Science
Tel-Aviv University
mansour@post.tau.ac.il

Abstract
We consider an MDP setting in which the reward function is allowed to
change during each time step of play (possibly in an adversarial manner),
yet the dynamics remain fixed. Similar to the experts setting, we address
the question of how well can an agent do when compared to the reward
achieved under the best stationary policy over time. We provide efficient
algorithms, which have regret bounds with no dependence on the size of
state space. Instead, these bounds depend only on a certain horizon time
of the process and logarithmically on the number of actions. We also
show that in the case that the dynamics change over time, the problem
becomes computationally hard.

1

Introduction

There is an inherent tension between the objectives in an expert setting and those in a reinforcement learning setting. In the experts problem, during every round a learner chooses
one of n decision making experts and incurs the loss of the chosen expert. The setting is
typically an adversarial one, where Nature provides the examples to a learner. The standard objective here is a myopic, backwards looking one ? in retrospect, we desire that our
performance is not much worse than had we chosen any single expert on the sequence of
examples provided by Nature. In contrast, a reinforcement learning setting typically makes
the much stronger assumption of a fixed environment, typically a Markov decision process (MDP), and the forward looking objective is to maximize some measure of the future
reward with respect to this fixed environment.
The motivation of this work is to understand how to efficiently incorporate the benefits of
existing experts algorithms into a more adversarial reinforcement learning setting, where
certain aspects of the environment could change over time. A naive way to implement an
experts algorithm is to simply associate an expert with each fixed policy. The running time
of such algorithms is polynomial in the number of experts and the regret (the difference
from the optimal reward) is logarithmic in the number of experts. For our setting the number of policies is huge, namely #actions#states , which renders the naive experts approach
computationally infeasible.
Furthermore, straightforward applications of standard regret algorithms produce regret
bounds which are logarithmic in the number of policies, so they have linear dependence
?

This work was supported in part by the IST Programme of the European Community, under the
PASCAL Network of Excellence, IST-2002-506778, by a grant from the Israel Science Foundation
and an IBM faculty award. This publication only reflects the authors? views.

on the number of states. We might hope for a more effective regret bound which has no
dependence on the size of state space (which is typically large).
The setting we consider is one in which the dynamics of the environment are known to
the learner, but the reward function can change over time. We assume that after each time
step the learner has complete knowledge of the previous reward functions (over the entire
environment), but does not know the future reward functions.
As a motivating example one can consider taking a long road-trip over some period of
time T . The dynamics, namely the roads, are fixed, but the road conditions may change
frequently. By listening to the radio, one can get (effectively) instant updates of the road
and traffic conditions. Here, the task is to minimize the cost during the period of time T .
Note that at each time step we select one road segment, suffer a certain delay, and need to
plan ahead with respect to our current position.
This example is similar to an adversarial shortest path problem considered in Kalai and
Vempala [2003]. In fact Kalai and Vempala [2003], address the computational difficulty of
handling a large number of experts under certain linear assumptions on the reward functions. However, their algorithm is not directly applicable to our setting, due to the fact that
in our setting, decisions must be made with respect to the current state of the agent (and
the reward could be changing frequently), while in their setting the decisions are only made
with respect to a single state.
McMahan et al. [2003] also considered a similar setting ? they also assume that the reward
function is chosen by an adversary and that the dynamics are fixed. However, they assume
that the cost functions come from a finite set (but are not observable) and the goal is to find
a min-max solution for the related stochastic game.
In this work, we provide efficient ways to incorporate existing best experts algorithms into
the MDP setting. Furthermore, our loss bounds (compared to the best constant policy) have
no dependence on the number of states and depend only on on a certain horizon time of
the environment and log(#actions). There are two sensible extensions of our setting. The
first is where we allow Nature to change the dynamics of the environment over time. Here,
we show that it becomes NP-Hard to develop a low regret algorithm even for oblivious
adversary. The second extension is to consider one in which the agent only observes the
rewards for the states it actually visits (a generalization of the multi-arm bandits problem).
We leave this interesting direction for future work.

2

The Setting

We consider an MDP with state space S; initial state distribution d1 over S; action space
A; state transition probabilities {Psa (?)} (here, Psa is the next-state distribution on taking action a in state s); and a sequence of reward functions r1 , r2 , . . . rT , where rt is the
(bounded) reward function at time step t mapping S ? A into [0, 1].

The goal is to maximize the sum of undiscounted rewards over a T step horizon. We assume
the agent has complete knowledge of the transition model P , but at time t, the agent only
knows the past reward functions r1 , r2 , . . . rt?1 . Hence, an algorithm A is a mapping from
S and the previous reward functions r1 , . . . rt?1 to a probability distribution over actions,
so A(a|s, r1 , . . . rt?1 ) is the probability of taking action a at time t.
We define the return of an algorithm A as:

"" T
#

X
1

rt (st , at )d1 , A
Vr1 ,r2 ,...rT (A) = E
T
t=1

where at ? A(a|st , r1 , . . . rt?1 ) and st is the random variable which represents the state

at time t, starting from initial state s1 ? d1 and following actions a1 , a2 , . . . at?1 . Note
that we keep track of the expectation and not of a specific trajectory (and our algorithm
specifies a distribution over actions at every state and at every time step t).
Ideally, we would like to find an A which achieves a large reward Vr1 ,...rT (A) regardless of
how the adversary chooses the reward functions. In general, this of course is not possible,
and, as in the standard experts setting, we desire that our algorithm competes favorably
against the best fixed stationary policy ?(a|s) in hindsight.

3

An MDP Experts Algorithm

3.1

Preliminaries

Before we provide our algorithm a few definitions are in order. For every stationary policy ?(a|s), we define P ? to be the transition matrix induced by ?, where the component
[P ? ]s,s? is the transition probability from s to s? under ?. Also, define d?,t to be the state
distribution at time t when following ?, ie
d?,t = d1 (P ? )t
where we are treating d1 as a row vector here.
Assumption 1 (Mixing) We assume the transition model over states, as determined by ?,
has a well defined stationary distribution, which we call d? . More formally, for every
initial state s, d?,t converges to d? as t tends to infinity and d? P ? = d? . Furthermore, this
implies there exists some ? such that for all policies ?, and distributions d and d? ,
kdP ? ? d? P ? k1 ? e?1/? kd ? d? k1
where kxk1 denotes the l1 norm of a vector x. We refer to ? as the mixing time and assume
that ? > 1.
The parameter ? provides a bound on the planning horizon timescale, since it implies that
every policy achieves close to its average reward in O(? ) steps 1 . This parameter also
governs how long it effectively takes to switch from one policy to another (after time O(? )
steps there is little information in the state distribution about the previous policy).
This assumption allows us to define the average reward of policy ? in an MDP with reward
function r as:
?r (?) = Es?d? ,a??(a|s) [r(s, a)]
and the value, Q?,r (s, a), is defined as
""?
#

X

Q?,r (s, a) ? E
(r(st , at ) ? ?r (?)) s1 = s, a1 = a, ?
t=1

where and st and at are the state and actions at time t, after starting from state s1 = s
then deviating with an immediate action of a1 = a and following ? onwards. We slightly
abuse notation by writing Q?,r (s, ? ? ) = Ea??? (a|s) [Q?,r (s, a)]. These values satisfy the
well known recurrence equation:
?

Q?,r (s, a) = r(s, a) ? ?r (?) + Es? ?Psa [Q? (s? , ?)]

(1)

where Q? (s , ?) is the next state value (without deviation).
1

If this timescale is unreasonably large for some specific MDP, then one could artificially impose
some horizon time and attempt to compete with those policies which mix in this horizon time, as
done Kearns and Singh [1998].

If ? ? is an optimal policy (with respect to ?r ), then, as usual, we define Q?r (s, a) to be the
value of the optimal policy, ie Q?r (s, a) = Q?? ,r (s, a).
We now provide two useful lemmas. It is straightforward to see that the previous assumption implies a rate of convergence to the stationary distribution that is O(? ), for all policies.
The following lemma states this more precisely.
Lemma 2 For all policies ?,
kd?,t ? d? k1 ? 2e?t/? .
Proof. Since ? is stationary, we have d? P ? = d? , and so
kd?,t ? d? k1 = kd?,t?1 P ? ? d? P ? k1 ? kd?,t?1 ? d? k1 e?1/?

which implies kd?,t ? d? k1 ? kd1 ? d? k1 e?t/? . The claim now follows since, for all
distributions d and d? , kd ? d? k1 ? 2.

The following derives a bound on the Q values as a function of the mixing time.

Lemma 3 For all reward functions r, Q?,r (s, a) ? 3? .
Proof. First, let us bound Q?,r (s, ?), where ? is used on the first step. For all t, including
t = 1, let d?,s,t be the state distribution at time t starting from state s and following ?.
Hence, we have
?
X

Q?,r (s, ?) =
Es? ?d?,s,t ,a?? [r(s? , a)] ? ?r (?))
t=1

?
=

? 
X
t=1

?
X
t=1

Es? ?d? ,a?? [r(s? , a)] ? ?r (?) + 2e?t/?

2e?t/? ?

Z

?



2e?t/? = 2?

0

Using the recurrence relation for the values, we know Q?,r (s, a) could be at most 1 more
than the above. The result follows since 1 + 2? ? 3?

3.2

The Algorithm

Now we provide our main result showing how to use any generic experts algorithm in our
setting. We associate each state with an experts algorithm, and the expert for each state
is responsible for choosing the actions at that state. The immediate question is what loss
function should we feed to each expert. It turns out Q?t ,rt is appropriate. We now assume
that our experts algorithm achieves a performance comparable to the best constant action.
Assumption 4 (Black Box Experts) We assume access to an optimized best expert algorithm which guarantees that for any sequence of loss functions c1 , c2 , . . . cT over actions
A, the algorithm selects a distribution qt over A (using only the previous loss functions
c1 , c2 , . . . ct?1 ) such that
T
X
t=1

Ea?qt [ct (a)] ?

T
X
t=1

ct (a) + M

p
T log |A|,

where kct (a)k ? M . Furthermore, we also assume that decision distributions do not
change quickly:
r
log |A|
kqt ? qt+1 k1 ?
t

These assumptions are satisfied by the multiplicative weights algorithms. For instance, the
algorithm in Freund and Schapire
[1999] is such that the for each decision a, | log qt (a) ?
q
log qt+1 (a)| changes by O(

log |A|
),
t

which implies the weaker l1 condition above.

In our setting, we have an experts algorithm associated with every state s, which is fed the
loss function Q?t ,rt (s, ?) at time t. The above assumption then guarantees that at every
state s for every action a we have that
T
X
t=1

Q?t ,rt (s, ?t ) ?

T
X

Q?t ,rt (s, a) + 3?

t=1

p
T log |A|

since the loss function Q?t ,rt is bounded by 3? , and that
r
log |A|
|?t (?|s) ? ?t+1 (?|s)|1 ?
t
As we shall see, it is important that this ?slow change? condition be satisfied. Intuitively,
our experts algorithms will be using a similar policy for significantly long periods of time.
Also note that since the experts algorithms are associated with each state and each of the
N experts chooses decisions out of A actions, the algorithm is efficient (polynomial in N
and A, assuming that that the black box uses a reasonable experts algorithm).
We now state our main theorem.
Theorem 5 Let A be the MDP experts algorithm. Then for all reward functions
r1 , r2 , . . . rT and for all stationary policies ?,
r
r
log |A|
log |A| 4?
2
Vr1 ,r2 ,...rT (A) ? Vr1 ,r2 ,...rT (?) ? 8?
? 3?
?
T
T
T
?
As expected, the regret goes to 0 at the rate O(1/ T ), as is the case with experts algorithms. Importantly, note that the bound does not depend on the size of the state space.
3.3

The Analysis

The analysis is naturally divided into two parts. First, we analyze the performance of the
algorithm in an idealized setting, where the algorithm instantaneously obtains the average
reward of its current policy at each step. Then we take into account the slow change of the
policies to show that the actual performance is similar to the instantaneous performance.
An Idealized Setting: Let us examine the case in which at each time t, when the algorithm uses ?t , it immediately obtains reward ?rt (?t ). The following theorem compares the
performance of our algorithms to that of a fixed constant policy in this setting.
Theorem 6 For all sequences r1 , r2 , . . . rT , the MDP experts algorithm have the following
performance bound. For all ?,
T
X
t=1

?rt (?t ) ?

T
X
t=1

?rt (?) ? 3?

p

T log |A|

where ?1 , ?2 , . . . ?T is the sequence of policies generated by A in response to r1 , r2 , . . . rT .
Next we provide a technical lemma, which is a variant of a result in Kakade [2003]
Lemma 7 For all policies ? and ? ? ,
?r (? ? ) ? ?r (?) = Es?d?? [Q?,r (s, ? ? ) ? Q?,r (s, ?)]

Proof. Note that by definition of stationarity, if the state distribution is at d?? , then the
next state distribution is also d?? if ? ? is followed. More formally, if s ? d?? , a ? ? ? (a|s),
and s? ? Psa , then s? ? d?? . Using this and equation 1, we have:
Es?d?? [Q?,r (s, ? ? )] = Es?d?? ,a??? [Q?,r (s, a)]
= Es?d?? ,a??? [r(s, a) ? ?r (?) + Es? ?Psa [Q? (s? , ?)]
= Es?d?? ,a??? [r(s, a) ? ?r (?)] + Es?d?? [Q? (s, ?)]
= ?r (? ? ) ? ?r (?) + Es?d?? [Q? (s, ?)]



Rearranging terms leads to the result.

The lemma shows why our choice to feed each experts algorithm Q?t ,rt was appropriate.
Now we complete the proof of the above theorem.
Proof. Using the assumed regret in assumption 4,
T
X
t=1

?rt (?) ?

T
X

?rt (?t )

=

t=1

T
X
t=1

=

Es?d? [Q?t ,rt (s, ?) ? Q?t ,rt (s, ?t )]

T
X
Es?d? [ Q?t ,rt (s, ?) ? Q?t ,rt (s, ?t )]
t=1

p
? Es?d? [3? T log A]
p
= 3? T log A
where we used the fact that d? does not depend on the time in the second step.



Taking Mixing Into Account: This subsection relates the values V to the sums of average
reward used in the idealized setting.
Theorem 8 For all sequences r1 , r2 , . . . rT and for all A
r
T
log |A| 2?
1X
2
?r (?t )| ? 4?
+
|Vr1 ,r2 ,...rT (A) ?
T t=1 t
T
T

where ?1 , ?2 , . . . ?T is the sequence of policies generated by A in response to r1 , r2 , . . . rT .
Since the above holds for all A (including those A which are the constant policy ?), then
combining this with Theorem 6 (once with A and once with ?) completes the proof of
Theorem 5. We now prove the above.
The following simple lemma is useful and we omit the proof. It shows how close are the
next state distributions when following ?t rather than ?t+1 .
Lemma 9 Let ? and ? ? be such that k?(?|s)?? ? (?|s)k1 ? ?. Then for any state distribution
?
d, we have kdP ? ? dP ? k1 ? ?.
Analogous to the definition of d?,t , we define dA,t
dA,t = Pr[st = s|d1 , A]
which is the probability that the state at time t is s given that A has been followed.
Lemma 10 Let ?1 , ?2 , . . . ?T be the sequence of policies generated by A in response to
r1 , r2 , . . . rT . We have
r
log |A|
2
kdA,t ? d?t k1 ? 2?
+ 2e?t/?
t

Proof. Let k ? t. Using our experts assumption, it is straightforward
p to see that that the
change in the policy over k steps is |?k (?|s) ? ?t (?|s)|1 ? (t ? k) log |A|/t. Using this
with dA,k = dA,k?1 P (?k ) and d?t P ?t = d?t , we have
kdA,k ? d?t k1

= kdA,k?1 P ?k ? d?t k1
? kdA,k?1 P ?t ? d?t k1 + kdA,k?1 P ?k ? dA,k?1 P ?t k1
p
? kdA,k?1 P ?t ? d?t P ?t k1 + 2(t ? k) log |A|/t
p
? e?1/? kdA,k?1 ? d?t k1 + 2(t ? k) log |A|/t

where we have used the last lemma in the third step and our contraction assumption 1 in
the second to last step. Recursing on the above equation leads to:
kdA,t ? d?t k

2
X
p
? 2 log |A|/t
(t ? k)e?(t?k)/? + e?t/? kd1 ? d?t k
k=t

?
X
p
ke?k/? + 2e?t/?
? 2 log |A|/t
k=1

The sum is bounded by an integral from 0 to ?, which evaluates to ? 2 .



We are now ready to complete the proof of Theorem 8.
Proof. By definition of V ,
Vr1 ,r2 ,...rT (A)

=
?
?
?

T
1X
Es?dA,t ,a??t [rt (s, a)]
T t=1

T
T
1X
1X
Es?d?t ,a??t [rt (s, a)] +
kdA,t ? d?t k1
T t=1
T t=1
!
r
T
T
1X
log |A|
1X
2
?t/?
?r (?t ) +
2?
+ 2e
T t=1 t
T t=1
t
r
T
1X
log |A| 2?
2
?r (?t ) + 4?
+
T t=1 t
T
T

where we have bounded the sums by integration in the second to last step. A symmetric
argument leads to the result.


4

A More Adversarial Setting

In this section we explore a different setting, the changing dynamics model. Here, in each
timestep t, an oblivious adversary is allowed to choose both the reward function rt and
the transition model Pt ? the model that determines the transitions to be used at timestep
t. After each timestep, the agent receives complete knowledge of both rt and Pt . Furthermore, we assume that Pt is deterministic, so we do not concern ourselves with mixing
issues. In this setting, we have the following hardness result. We let Rt? (M ) be the optimal
average reward obtained by a stationary policy for times [1, t].
Theorem 11 In the changing dynamics model, if there exists a polynomial time online
algorithm (polynomial in the problem parameters) such that, for any MDP, has an expected
average reward larger than (0.875 + ?)Rt? (M ), for some ? > 0 and t, then P = N P .

The following lemma is useful in the proof and uses the fact that it is hard to approximate
MAX3SAT within any factor better than 0.875 (Hastad [2001]).
Lemma 12 Computing a stationary policy in the changing dynamics model with average
reward larger than (0.875 + ?)R? (M ), for some ? > 0, is NP-Hard.
Proof: We prove it by reduction from 3-SAT. Suppose that the 3-SAT formula, ? has m
clauses, C1, . . . , Cm , and n literals, x1 , . . . , xn then we reduce it to MDP with n + 1
states,s1 , . . . sn , sn+1 , two actions in each state, 0, 1 and fixed dynamic for 3m steps which
will be described later. We prove that a policy with average reward p/3 translates to an
assignment that satisfies p fraction of ? and vice versa. Next we describe the dynamics.
Suppose that C1 is (x1 ? ?x2 ? x7 ) and C2 is (x4 ? ?x1 ? x7 ). The initial state is s1 and the
reward for action 0 is 0 and the agent moves to state s2 , for action 1 the reward is 1 and it
moves to state sn+1 . In the second timestep the reward in sn+1 is 0 for every action and the
agents stay in it; in state s2 if the agent performs action 0 then it obtains reward 1 and move
to state sn+1 otherwise it obtains reward 0 and moves to state s7 . In the next timestep the
reward in sn+1 is 0 for every action and the agents moves to x4 , the reward in s7 is 1 for
action 1 and zero for action 0 and moves to s4 for both actions. The rest of the construction
is done identically. Note that time interval [3(? ? 1) + 1, 3?] corresponds to C? and that the
reward obtained in this interval is at most 1. We note that ? has an assignment y1 , . . . , yn
where yi = {0, 1} that satisfies p fraction of it, if and only if ? which takes action yi in si
has average reward p/3. We prove it by looking on each interval separately and noting that
if a reward 1 is obtained then there is an action a that we take in one of the states which has
reward 1 but this action corresponds to a satisfying assignment for this clause.

We are now ready to prove Theorem 11.
Proof: In this proof we make few changes from the construction given in Lemma 12. We
allow the same clause to repeat few times, and its dynamics are described in n steps and
not in 3 steps, where in the k step we move from sk to sk+1 and obtains 0 reward, unless
the action ?satisfies? the chosen clause, if it satisfies then we obtain an immediate reward
1, move to sn+1 and stay there for n ? k ? 1 steps. After n steps the adversary chooses
uniformly at random the next clause. In the analysis we define the n steps related to a clause
as an iteration. The strategy defined by the algorithm at the k iteration is the probability
assigned to action 0/1 at state s? just before arriving to s? . Note that the strategy at each
iteration is actually a stationary policy for M . Thus the strategy in each iteration defines
an assignment for the formula. We also note that before an iteration the expected reward
of the optimal stationary policy in the iteration is k/(nm), where k is the maximal number
of satisfiable clauses and there are m clauses, and we have E[R? (M )] = k/(nm). If we
choose at random an iteration, then the strategy defined in that iteration has an expected
reward which is larger than (0.875 + ?)R? (M ), which implies that we can satisfy more
than 0.875 fraction of satisfiable clauses, but this is impossible unless P = N P .


References
Y. Freund and R. Schapire. Adaptive game playing using multiplicative weights. Games and Economic Behavior, 29:79?103, 1999.
J. Hastad. Some optimal inapproximability results. J. ACM, 48(4):798?859, 2001.
S. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, University College
London, 2003.
A. Kalai and S. Vempala. Efficient algorithms for on-line optimization. Proceedings of COLT, 2003.
M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. Proceedings of
ICML, 1998.
H. McMahan, G. Gordon, and A. Blum. Planning in the presence of cost functions controlled by an
adversary. In In the 20th ICML, 2003.

"
3905,"Fast Bayesian Inference for Non-Conjugate
Gaussian Process Regression

Mohammad Emtiyaz Khan, Shakir Mohamed, and Kevin P. Murphy
Department of Computer Science, University of British Columbia

Abstract
We present a new variational inference algorithm for Gaussian process regression with non-conjugate likelihood functions, with application to a wide array of
problems including binary and multi-class classification, and ordinal regression.
Our method constructs a concave lower bound that is optimized using an efficient
fixed-point updating algorithm. We show that the new algorithm has highly competitive computational complexity, matching that of alternative approximate inference methods. We also prove that the use of concave variational bounds provides
stable and guaranteed convergence ? a property not available to other approaches.
We show empirically for both binary and multi-class classification that our new
algorithm converges much faster than existing variational methods, and without
any degradation in performance.

1

Introduction

Gaussian processes (GP) are a popular non-parametric prior for function estimation. For real-valued
outputs, we can combine the GP prior with a Gaussian likelihood and perform exact posterior inference in closed form. However, in other cases, such as classification, the likelihood is no longer
conjugate to the GP prior, and exact inference is no longer tractable.
Various approaches are available to deal with this intractability. One approach is Markov Chain
Monte Carlo (MCMC) techniques [1, 11, 22, 9]. Although this can be accurate, it is often quite
slow, and assessing convergence is challenging. There is therefore great interest in deterministic approximate inference methods. One recent approach is the Integrated Nested Laplace Approximation
(INLA) [21], which uses numerical integration to approximate the marginal likelihood. Unfortunately, this method is limited to six or fewer hyperparameters, and is thus not suitable for models
with a large number of hyperparameters. Expectation propagation (EP) [17] is a popular alternative, and is a method that approximates the posterior distribution by maintaining expectations and
iterating until these expectations are consistent for all variables. Although this is fast and accurate
for the case of binary classification [15, 18], there are difficulties extending EP to many other cases,
such as multi-class classification and parameter learning [24, 13]. In addition, EP is known to have
convergence issues and can be numerically unstable.
In this paper, we use a variational approach, where we compute a lower bound to the log marginal
likelihood using Jensen?s inequality. Unlike EP, this approach does not suffer from numerical issues
and convergence problems, and can easily handle multi-class and other likelihoods. This is an active
area of research and many solutions have been proposed, see for example, [23, 6, 5, 19, 14]. Unfortunately, most of these methods are slow, since they attempt to solve for the posterior covariance
matrix, which has size O(N 2 ), where N is the number of data points. In [19], a reparameterization was proposed that only requires computing O(N ) variational parameters. Unfortunately, this
method relies on a non-concave lower bound. In this paper, we propose a new lower bound that is
concave, and derive an efficient iterative algorithm for its maximization. Since the original objective
is unimodal, we reach the same global optimum as the other methods, but we do so much faster.
1

p(z|X, ?) = N (z|?, ?)
p(y|z) =

N
Y

(1)

p(yn |zn )

(2)

n=1

Type
Binary
Categorical
Ordinal

Distribution
Bernoulli logit
Multinomial logit
Cumulative logit

p(y|z)
p(y = 1|z) = ?(z)
p(y = k|z) = ezk ?lse(z)
p(y ? k|z) = ?(?k ? z)

Count

Poisson

p(y = k|z) =

?

X

?

?

z1

z2

zN

y1

y2

yN

z

e?e ekz
k!

Table 1: Gaussian process regression (top left) and its graphical model (right), along with the example likelihoods for outputs (bottom left). Here, ?(z) = 1/(1 + e?z ), lse(?) is the log-sum-exp function, k indexes over discrete output values, and ?k are real numbers such that ?1 < ?2 < . . . < ?K
for K ordered categories.

2

Gaussian Process Regression

Gaussian process (GP) regression is a powerful method for non-parametric regression that has gained
a great deal of attention as a flexible and accurate modeling approach. Consider N data points with
the n?th observation denoted by yn , with corresponding features xn . A Gaussian process model uses
a non-linear latent function z(x) to obtain the distribution of the observation y using an appropriate
likelihood [15, 18]. For example, when y is binary, a Bernoulli logit/probit likelihood is appropriate.
Similarly, for count observations, a Poisson distribution can be used.
A Gaussian process [20] specifies a distribution over z(x), and is a stochastic process that is characterized by a mean function ?(x) and a covariance function ?(x, x0 ), which are specified using a
kernel function that depends on the observed features x. Assuming a GP prior over z(x) implies that
a random vector is associated with every input x, such that given all inputs X = [x1 , x2 , . . . , xN ],
the joint distribution over z = [z(x1 ), z(x2 ), . . . , z(xN )] is Gaussian.
The GP prior is shown in Eq. 1. Here, ? is a vector with ?(xi ) as its i?th element, ? is a matrix with
?(xi , xj ) as the (i, j)?th entry, and ? are the hyperparameters of the mean and covariance functions.
We assume throughout a zero mean-function and a squared-exponential covariance function (also
known as radial-basis function or Gaussian) defined as: ?(xi , xj ) = ? 2 exp[?(xi ? xj )T (xi ?
xj )/(2s)]. The set of hyperparameters is ? = (s, ?). We also define ? = ??1 .
Given the GP prior, the observations are modeled using the likelihood shown in Eq. 2. The exact
form of the distribution p(yn |zn ) depends on the type of observations and different choices instantiates many existing models for GP regression [15, 18, 10, 14]. We consider frequently encountered
data such as binary, ordinal, categorical and count observations, and describe their likelihoods in Table 1. For the case of categorical observations, the latent function z is a vector whose k?th element
is the latent function for k?th category. A graphical model for Gaussian process regression is also
shown.
Given these models, there are three tasks that are to be performed: posterior inference, prediction
at test inputs, and model selection. In all cases, the likelihoods we consider are not conjugate to
the Gaussian prior distribution and as a result, the posterior distribution is intractable. Similarly,
the integrations required in computing the predictive distribution and the marginal likelihood are
intractable. To deal with this intractability we make use of variational methods.

3

Variational Lower Bound to the Log Marginal Likelihood

Inference and model selection are always problematic in any Gaussian process regression using nonconjugate likelihoods due to the fact that the marginal likelihood contains an intractable integral. In
this section, we derive a tractable variational lower bound to the marginal likelihood. We show
2

that the lower bound takes a well known form and can be maximized using concave optimization.
Throughout the section, we assume scalar zn , with extension to the vector case being straightforward.
We begin with the intractable log marginal likelihood L(?) in Eq. 3 and introduce a variational
posterior distribution q(z|?). We use a Gaussian posterior with mean m and covariance V. The
full set of variational parameters is thus ? = {m, V}. As log is a concave function, we obtain a
lower bound LJ (?, ?) using Jensen?s inequality, given in Eq. 4. The first integral is simply the
Kullback?Leibler (KL) divergence from the variational Gaussian posterior q(z|m, V) to the GP
prior p(z|?, ?) as shown in Eq. 5, and has a closed-form expression that we substitute to get the
first term in Eq. 6 (inside square brackets), with ? = ??1 .
The second integral can be expressed in terms of the expectation with respect to the marginal
q(zn |mn , Vnn ) as shown in the second term of Eq. 5. Here mn is the n?th element of m and
Vnn is the n?th diagonal element of V, the two variables collectively denoted by ?n . The lower
bound LJ is still intractable since the expectation of log p(yn |zn ) is not available in closed form for
the distributions listed in Table 1. To derive a tractable lower bound, we make use of local variational
bounds (LVB) fb , defined such that E[log p(yn |zn )] ? fb (yn , mn , Vnn ), giving us Eq. 6.
Z
Z
p(z|?)p(y|z)
L(?) = log p(z|?)p(y|z)dz = log q(z|?)
dz
(3)
q(z|?)
Zz
Zz
q(z|?)
? LJ (?, ?) := ? q(z|?) log
dz +
q(z|?) log p(y|z)dz
(4)
p(z|?)
z
z
N
X
Eq(zn |?n ) [log p(yn |zn )]
(5)
= ?DKL [q(z|?)||p(z|?)]+
n=1
N

 X
? LJ (?, ?) := 12 log |V?|?tr(V?) ?(m??)T ?(m??)+N +
fb (yn , mn ,Vnn ).

(6)

n=1

We discuss the choice of LVBs in the next section, but first discuss the well-known form that the
lower bound of Eq. 6 takes. Given V, the optimization function with respect to m is a nonlinear
least-squares function. Similarly, the function with respect to V is similar to the graphical lasso
[8] or covariance selection problem [7], but is different in that the argument is a covariance matrix
instead of a precision matrix [8]. These two objective functions are coupled through the non-linear
term fb (?). Usually this term arises due to the prior distribution and may be non-smooth, for example, in graphical lasso. In our case, this term arises from the likelihood, and is smooth and concave
as we discuss in next section.
It is straightforward to show that the variational lower bound is strictly concave with respect to
? if fb is jointly concave with respect to mn and Vnn . Strict concavity of terms other than fb is
well-known since both the least squares and covariance selection problems are concave. Similar
concavity results have been discussed by Braun and McAuliffe [5] for the discrete choice model,
and more recently by Challis and Barber [6] for the Bayesian linear model, who consider concavity
with respect to the Cholesky factor of V. We consider concavity with respect to V instead of its
Cholesky factor, which allows us to exploit the special structure of V, as explained in Section 5.

4

Concave Local Variational Bounds

In this section, we describe concave LVBs for various likelihoods. For simplicity, we suppress
the dependence on n and consider the log-likelihood of a scalar observation y given a predictor z
distributed according to q(z|?) = N (z|m, v) with ? = {m, v}. We describe the LVBs for the
likelihoods given in Table 1 with z being a scalar for count, binary, and ordinal data, but a vector of
length K for categorical data, K being the number of classes. When V is a matrix, we denote its
diagonal by v.
For the Poison distribution, the expectation is available in closed form and we do not need any
bounding: E[log p(y|?)] = ym ? exp(m + v/2) ? log y!. This function is jointly concave with
respect to m and v since the exponential is a convex function.
3

For binary data, we use the piecewise linear/quadratic bounds proposed by [16], which is a bound
on the logistic-log-partition (LLP) function log(1 + exp(x)) and can be used to obtain a bound over
the sigmoid function ?(x). The final bound can be expressed as sum of R pieces: E(log p(y|?)) =
PR
fb (y, m, v) = ym ? r=1 fbr (m, v) where fbr is the expectation of r?th quadratic piece. The
function fbr is jointly concave with respect to m, v and their gradients are available in closed-form.
An important property of the piecewise bound is that its maximum error is bounded and can be
driven to zero by increasing the number of pieces. This means that the lower bound in Eq. 6 can
be made arbitrarily tight by increasing the number of pieces. For this reason, this bound always
performs better than other existing bounds, such as Jaakola?s bound [12], given that the number
of pieces is chosen appropriately. Finally, the cumulative logit likeilhood for ordinal observations
depends on ?(x) and its expectation can be bounded using piecewise bounds in a similar way.
For the multinomial logit distribution, we can use the bounds proposed by [3] and [4], both leading
to concave LVBs. The first bound takes the form fb (y, m, V) = yT m ? lse(m + v/2) with y
represented using a 1-of-K encoding. This function is jointly concave with respect to m and v,
which can be shown by noting the fact that the log-sum-exp function is convex. The second bound
is the product of sigmoids bound proposed by [4] which bounds the likelihood with product of
sigmoids (see Eq. 3 in [4]), with each sigmoid bounded using Jaakkola?s bound [12]. We can also
use piecewise linear/quadratic bound to bound each sigmoid. Alternatively, we can use the recently
proposed stick-breaking likelihood of [14] which uses piecewise bounds as well.
Finally, note that the original log-likelihood may not be concave itself, but if it is such that LJ has
a unique solution, then designing a concave variational lower bound will allow us to use concave
optimization to efficiently maximize the lower bound.

5

Existing Algorithms for Variational Inference

In this section, we assume that for each output yn there is a corresponding scalar latent function zn .
All our results can be easily extended to the case of multi-class outputs where the latent function is a
vector. In variational inference, we find the approximate Gaussian posterior distribution with mean
m and covariance V that maximizes Eq. 6. The simplest approach is to use gradient-based methods
for optimization, but this can be problematic since the number of variational parameters is quadratic
in N due to the covariance matrix V. The authors of [19] speculate that this may perhaps be the
reason behind limited use of Gaussian variational approximations.
We now show that the problem is simpler than it appears to be, and in fact the number of parameters
can be reduced to O(N ) from O(N 2 ). First, we write the gradients with respect to m and v in Eq.
7 and 8 and equate to zero, using gnm := ?fb (yn , mn , vn )/?mn and gnv := ?fb (yn , mn , vn )/?vn .
Also, gm and gv are the vectors of these gradients, and diag(gv ) is the matrix with gv as its diagonal.
1
2

??(m ? ?) + gm = 0

V?1 ? ? + diag(gv ) = 0

(7)
(8)

v

At the solution, we see that V is completely specified if g is known. This property can be exploited
to reduce the number of variational parameters.
Opper and Archambeau [19] (and [18]) propose a reparameterization to reduce the number of parameters to O(N ). From the fixed-point equation, we note that at the solution m and V will have
the following form,
V = (??1 + diag(?))?1
m = ? + ??,

(9)
(10)

where ? and ? are real vectors with ?d > 0, ?d. At the maximum (but not everywhere), ? and ?
will be equal to gm and gv respectively. Therefore, instead of solving the fixed-point equations to
obtain m and V, we can reparameterize the lower bound with respect to ? and ?. Substituting Eq.
9 and 10 in Eq. 6 and after simplification using the matrix inversion and determinant lemmas, we
get the following new objective function (for a detailed derivation, see [18]),
1
2

N
 X

T
? log(|B? ||diag(?)|) + Tr(B?1
?)
?
?
??
+
fb (yn , mn , Vnn ),
?
n=1

4

(11)

with B? = diag(?)?1 + ?. Since the mapping between {?, ?} and {m, V} is one-to-one, we can
recover the latter given the former. The one-to-one relationship also implies that the new objective
function has a unique maximum. The new lower bound involves vectors of size N , reducing the
number of variational parameters to O(N ).
The problem with this reparameterization is that the new lower bound is no longer concave, even
though it has a unique maximum. To see this, consider the 1-D case. We collect all the terms
involving V from Eq. 6, except the LVB term, to define the function f (V ) = [log(V ??1 ) ?
V ??1 ]/2. We substitute the reparameterization V = (??1 + ?)?1 to get a new function f (?) =
[? log(1 + ??) ? (1 + ??)?1 ]/2. The second derivative of this function is f 00 (?) = 21 [?/(1 +
??)]2 (?? ? 1). Clearly, this derivative is negative for ? < 1/? and non-negative otherwise, making
the function neither concave nor convex.
The objective function is still unimodal and the maximum of (11) is equal to the maximum of
(6). With the reparameterization, we loose concavity and therefore the algorithm may have slow
convergence. Our experimental results (Section 7) confirm the slow convergence.

6

Fast Convergent Variational Inference using Coordinate Ascent

We now derive an algorithm that reduces the number of variational parameters to 2N while maintaining concavity. Our algorithm uses simple scalar fixed-point updates to obtain the diagonal elements
of V. The complete algorithm is shown in Algorithm 1.
To derive the algorithm, we first note that the fixed-point equation Eq. 8 has an attractive property:
at the solution, the off-diagonal elements of V?1 are the same as the off-diagonal elements of ?,
i.e. if we denote K := V?1 , then Kij = ?ij . We need only find the diagonal elements of K to get
the full V. This is difficult, however, since the gradient gv depends on v.
We take the approach of optimizing each diagonal element Kii fixing all others (and fixing m as
well). We partition V as shown on the left side of Eq. 12, indexing the last row by 2 and rest of the
rows by 1. We consider a similar partitioning of K and ?. Our goal is to compute v22 and k22 given
all other elements of K. Matrices K and V are related through the blockwise inversion, as shown
below.
?
?
T
?1
K?1
K?1
?1


11 k12
11 k12 k12 K11
?
K
+
T
T
?1
?1
V11 v12
? 11
k22 ?k12 K11 k12
k22 ?k12 K11 k12 ?
=?
(12)
?
kT12 K?1
vT12 v22
1
11
?
T
T
?1
?1
k22 ?k12 K11 k12
k22 ?k12 K11 k12
From the right bottom corner, we have the first relation below, which we simplify further.
v22 = 1/(k22 ? kT12 K?1
11 k12 )

?

k22 = e
k22 + 1/v22

(13)

where we define e
k22 := kT12 K?1
11 k12 . We also know from the fixed point Eq. 8 that the optimal v22
v
and k22 satisfy Eq. 14 at the solution, where g22
is the gradient of fb with respect to v22 . Substitute
the value of k22 from Eq. 13 in Eq. 14 to get Eq. 15. It is easy to check (by taking derivative) that
the value v22 that satisfies this fixed-point can be found by maximizing the function defined in Eq.
16.
v
0 = k22 ? ?22 + 2g22
0=e
k22 + 1/v22 ? ?22 + 2g v

22

f (v) = log(v) ? (?22 ? e
k22 )v + 2fb (y2 , m22 , v)

(14)
(15)
(16)

The function f (v) is a strictly concave function and can be optimized by iterating the following
v
update: v22 ? 1/(?22 ? e
k22 ? 2g22
). We will refer to this as a ?fixed-point iteration?.
Since all elements of K, except k22 , are fixed, e
k22 can be computed beforehand and need not be
evaluated at every fixed-point iteration. In fact, we do not need to compute it explicitly, since we
can obtain its value using Eq. 13: e
k22 = k22 ? 1/v22 , and we do this before starting a fixed-point
v
iteration. The complexity of these iterations depends on the number of gradient evaluations g22
,
which is usually constant and very low.
5

After convergence of the fixed-point iterations, we update V using Eq. 12. It turns out that this is a
rank-one update, the complexity of which is O(N 2 ). To show these updates, let us denote the new
new
new
values obtained after the fixed-point iterations by k22
and v22
respectively. and denote the old
old
old
values by k22 and v22 . We use the right top corner of Eq. 12 to get first equality in Eq. 17. Using
Eq. 13, we get the second equality. Similarly, we use the top left corner of Eq. 12 to get the first
equality in Eq. 18, and use Eq. 13 and 17 to get the second equality.
old
old
old
old
e
K?1
11 k12 = ?(k22 ? k22 )v12 = ?v12 /v22
old
K?1
11 = V11 ?

T
?1
K?1
11 k12 k12 K11
old ? e
k22
k22

old
old T
old
= Vold
11 ? v12 (v12 ) /v22

(17)
(18)

Note that both K?1
11 and k12 do not change after the fixed point iteration. We use this fact to obtain
Vnew . We use Eq. 12 to write updates for Vnew and use 17, 18, and 13 to simplify.
vnew
12 =

K?1
v new old
11 k12
v
= ? 22
old 12
v22
k new ? e
k22

(19)

22

?1
Vnew
11 = K11 +

T
?1
new
old
K?1
v22
? v22
11 k12 k12 K11
old T
+
= Vold
vold
11
12 (v12 )
old )2
new ? e
(v22
k22
k22

(20)

After updating V, we update m by optimizing the following non-linear least squares problem,
max ? 21 (m ? ?)T ?(m ? ?) +
m

N
X

fb (yn , mn , Vnn )

(21)

n=1

We use Newton?s method, the cost of which is O(N 3 ).
6.1

Computational complexity

The final procedure is shown in Algorithm 1. The main advantage of our algorithm is its fast
convergence
P as we show this in the results section. The overall computational complexity is
O(N 3 + n Inf p ). First term is due to O(N 2 ) update of V for all n and also due to the optimization of m. Second term is for Inf p fixed-point iterations, the total cost of which is linear in N
due to the summation. In all our experiments, Inf p is usually 3 to 5, adding very little cost.
6.2

Proof of convergence

Proposition 2.7.1 in [2] states that the coordinate ascent algorithm converges if the maximization
with respect to each coordinate is uniquely attained. This is indeed the case for us since each fixed
point iteration solves a concave problem of the form given by Eq. 16. Similarly, optimization with
respect to m is also strictly concave. Hence, convergence of our algorithm is assured.
Proof that V will always be positive definite

6.3

Let us assume that we start with a positive definite K, for example, we can initialize it with ?. Now
new
consider the update of v22 and k22 . Note that v22
will be positive since it is the maximum of Eq.
new
16 which involves the log term. Using this and Eq. 13, we get k22
> kT12 K?1
11 k12 . Hence, the
T
?1
new
Schur complement k22 ? k12 K11 k12 > 0. Using this and the fact that K11 is positive definite, it
follows that Knew will also be positive definite, and hence Vnew will be positive definite.

7

Results

We now show that the proposed algorithm leads to a significant gain in the speed of Gaussian process
regression. The software to reproduce the results of this section are available online1 . We evaluate
the performance of our fast variational inference algorithm against existing inference methods for
1

http://www.cs.ubc.ca/emtiyaz/software/codeNIPS2012.html

6

Algorithm 1 Fast convergent coordinate-ascent algorithm
1. Initialize K ? ?, V ? ??1 , m ? ?, where ? := ??1 .
2. Alternate between updating the diagonal of V and then m until convergence, as follows:
(a) Update the i?th diagonal of V for all i = 1, . . . , N :
i. Rearrange V and ? so that the i?th column is the last one.
ii. e
k22 ? k22 ? 1/v22 .
old
iii. Store old value v22
? v22 .
v
iv. Run fixed-point iterations for a few steps: v22 ? 1/(?22 ? e
k22 ? 2g22
).
v. Update V.
old
old 2
A. V11 ? V11 + (v22 ? v22
)v12 vT12 /(v22
) .
old
B. v12 ? ?v22 v12 /v22 .
vi. Update k22 ? e
k22 + 1/v22 .
(b) Update m by maximizing the least-squares problem of Eq. 21.

binary and multi-class classification. For binary classification, we use the UCI ionosphere data (with
351 data examples containing 34 features). For multi-class classification, we use the UCI forensic
glass data set with 214 data examples each with 6 category output and features of length 8. In both
cases, we use 80% of the dataset for training and the rest for testing.
We consider GP classification using the Bernoulli logit likelihood, for which we use the piecewise
bound of [16] with 20 pieces. We compare our algorithm with the approach of Opper and Archambeau [19] (Eq. 11). For the latter, we use L-BFGS method for optimization. We also compared to
the naive method of optimizing with respect to full m and V, e.g. method of [5], but do not present
these results since these algorithms have very slow convergence.
We examine the computational cost for each method in terms of the number of floating point operations (flops) for four hyperparameter settings ? = {log(s), log(?)}. This comparison is shown in
Figure 1(a). The y-axis shows (negative of) the value of the lower bound, and the x-axis shows the
number of flops. We draw markers at iteration 1,2,4,50 and in steps of 50 from then on. In all cases,
due to non-concavity, the optimization of the Opper and Archambeau reparameterization (black
curve with squares) convergence slowly, passing through flat regions of the objective and requiring
a large number of computations to reach convergence. The proposed algorithm (blue curve with
circles) has consistently faster convergence than the existing method. For this dataset, our algorithm
always converged in 5 iterations.
We also compare the total cost to convergence, where we count the total number of flops until
successive increase in the objective function is below 10?3 . Each entry is a different setting of
{log(s), log(?)}. Rows correspond to values of log(s) while columns correspond to log(?), with
units M,G,T denoting Mega-, Giga-, and Terra-flops. We can see that the proposed algorithm takes
a much smaller number of operations compared to the existing algorithm.
Opper and Archambeau
-1
1
3
-1
20G 212G 6T
1
101G
24T
24T
3
38G
1T
24T

Proposed Algorithm
-1
1
3
-1
6M
7M
7M
1
26M 20M 22M
3
47M 81M 75M

We also applied our method to two more datasets of [18], namely ?sonar? and ?usps-3vs5? dataset
and observed similar behavior.
Next, we apply our algorithm to the problem of multi-class classification, following [14], using the
stick-breaking likelihood, and compare to inference using the approach of Opper and Archambeau
[19] (Eq. 11). We show results comparing the lower bound vs the number of flops taken in Figure
1(b), for four hyperparameter settings {log(s), log(?)}. We show markers at iterations 1, 2, 10,
100 and every 100th iteration thereafter. The results follow those discussed for binary classification,
7

(?1.0,?1.0)

(?1.0, ?1.0)

(?1.0,2.5)

(?1.0, 2.5)

320
2000

600

300

300
290
280

1500

1000

500

270
260

134
0

300

600

900

0

1000

2000

0

3000

1000

2000

3000

4000

0

30K

40K

Mega?flops

Mega?flops

(1.0,1.0)

(2.5, 2.5)

(1.0, 1.0)

200

350

Neg?LogLik

110

300
250

100

15K

20K

0

2000

Mega?Flops

4000

6000

0

8000

400
300
200

200

80
10K

proposed
Opper?Arch

500

Neg?LogLik

neg?LogLik

170
140

50K

600
400

Opper?Arch
proposed

5K

20K

Mega?Flops

(3.5,3.5)
300

0

10K

Mega?Flops
200

neg?LogLik

Neg?LogLik

138

Neg?LogLik

neg?LogLik

neg?LogLik

310

900

142

20K

40K

60K

80K

100K

0

10K

Mega?flops

Mega?Flops

(a) Ionosphere data

20K

30K

40K

50K

Mega?flops

(b) Forensic glass data

Figure 1: Convergence results for (a) the binary classification on the ionosphere data set and (b) the
multi-class classification on the glass dataset. We plot the negative of the lower bound vs the number
of flops. Each plot shows the progress of algorithms for a hyperparameter setting {log(s), log(?)}
shown at the top of the plot. The proposed algorithm always converges faster than the other method,
in fact, in less than 5 iterations.
where both methods reach the same lower bound value, but the existing approach converging much
slower, with our algorithm always converged within 20 iterations.

8

Discussion

In this paper we have presented a new variational inference algorithm for non-conjugate GP regression. We derived a concave variational lower bound to the log marginal likelihood, and used
concavity to develop an efficient optimization algorithm. We demonstrated the efficacy of our new
algorithm on both binary and multiclass GP classification, demonstrating significant improvement
in convergence.
Our proposed algorithm is related to many existing methods for GP regression. For example, the
objective function that we consider is exactly the KL minimization method discussed in [18], for
which a gradient based optimization was used. Our algorithm uses an efficient approach where we
update the marginals of the posterior and then do a rank one update of the covariance matrix. Our
results show that this leads to fast convergence.
Our algorithm also takes a similar form to the popular EP algorithm [17], e.g. see Algorithm 3.5 in
[20]. Both EP and our algorithm update posterior marginals, followed by a rank-one update of
the covariance. Therefore, the computational complexity of our approach is similar to that of EP.
The advantage of our approach is that, unlike EP, it does not suffer from any numerical issues (for
example, no negative variances) and is guaranteed to converge.
The derivation of our algorithm is based on the observation that the posterior covariance has a special
structure, and does not directly use the concavity of the lower bound. An alternate derivation based
on the Fenchel duality exists and shows that the fixed-point iterations compute dual variables which
are related to the gradients of fb . We skip this derivation since it is tedious, and present the more
intuitive derivation instead. The alternative derivation will be made available in an online appendix.
Acknowledgements
We thank the reviewers for their valuable suggestions. SM is supported by the Canadian Institute
for Advanced Research (CIFAR).

8

References
[1] J. Albert and S. Chib. Bayesian analysis of binary and polychotomous response data. J. of the
Am. Stat. Assoc., 88(422):669?679, 1993.
[2] Dimitri P. Bertsekas. Nonlinear Programming. Athena Scientific, second edition, 1999.
[3] D. Blei and J. Lafferty. Correlated topic models. In Advances in Neural Information Proceedings Systems, 2006.
[4] G. Bouchard. Efficient bounds for the softmax and applications to approximate inference in
hybrid models. In NIPS 2007 Workshop on Approximate Inference in Hybrid Models, 2007.
[5] M. Braun and J. McAuliffe. Variational inference for large-scale models of discrete choice.
Journal of the American Statistical Association, 105(489):324?335, 2010.
[6] E. Challis and D. Barber. Concave Gaussian variational approximations for inference in largescale Bayesian linear models. In Proceedings of the International Conference on Artificial
Intelligence and Statistics, volume 6, page 7, 2011.
[7] A. Dempster. Covariance selection. Biometrics, 28(1), 1972.
[8] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9(3):432, 2008.
[9] S. Fr?uhwirth-Schnatter and R. Fr?uhwirth. Data augmentation and MCMC for binary and multinomial logit models. Statistical Modelling and Regression Structures, pages 111?132, 2010.
[10] M. Girolami and S. Rogers. Variational Bayesian multinomial probit regression with Gaussian
process priors. Neural Comptuation, 18(8):1790 ? 1817, 2006.
[11] C. Holmes and L. Held. Bayesian auxiliary variable models for binary and multinomial regression. Bayesian Analysis, 1(1):145?168, 2006.
[12] T. Jaakkola and M. Jordan. A variational approach to Bayesian logistic regression problems
and their extensions. In AI + Statistics, 1996.
[13] P. Jyl?anki, J. Vanhatalo, and A. Vehtari. Robust Gaussian process regression with a student-t
likelihood. The Journal of Machine Learning Research, 999888:3227?3257, 2011.
[14] M. Khan, S. Mohamed, B. Marlin, and K. Murphy. A stick-breaking likelihood for categorical
data analysis with latent Gaussian models. In Proceedings of the International Conference on
Artificial Intelligence and Statistics, 2012.
[15] M. Kuss and C. E. Rasmussen. Assessing approximate inference for binary Gaussian process
classification. J. of Machine Learning Research, 6:1679?1704, 2005.
[16] B. Marlin, M. Khan, and K. Murphy. Piecewise bounds for estimating Bernoulli-logistic latent
Gaussian models. In Intl. Conf. on Machine Learning, 2011.
[17] T. Minka. Expectation propagation for approximate Bayesian inference. In UAI, 2001.
[18] H. Nickisch and C.E. Rasmussen. Approximations for binary Gaussian process classification.
Journal of Machine Learning Research, 9(10), 2008.
[19] M. Opper and C. Archambeau. The variational Gaussian approximation revisited. Neural
computation, 21(3):786?792, 2009.
[20] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press,
2006.
[21] H. Rue, S. Martino, and N. Chopin. Approximate Bayesian inference for latent Gaussian
models using integrated nested Laplace approximations. J. of Royal Stat. Soc. Series B, 71:
319?392, 2009.
[22] S. L. Scott. Data augmentation, frequentist estimation, and the Bayesian analysis of multinomial logit models. Statistical Papers, 52(1):87?109, 2011.
[23] M. Seeger. Bayesian Inference and Optimal Design in the Sparse Linear Model. J. of Machine
Learning Research, 9:759?813, 2008.
[24] M. Seeger and H. Nickisch. Fast Convergent Algorithms for Expectation Propagation Approximate Bayesian Inference. In Proceedings of the International Conference on Artificial
Intelligence and Statistics, 2011.

9

"
4298,"Bayesian Inference and Online Experimental Design
for Mapping Neural Microcircuits

Ben Shababo ?
Department of Biological Sciences
Columbia University, New York, NY 10027
bms2156@columbia.edu

Brooks Paige ?
Department of Engineering Science
University of Oxford, Oxford OX1 3PJ, UK
brooks@robots.ox.ac.uk

Ari Pakman
Department of Statistics,
Center for Theoretical Neuroscience,
& Grossman Center for the Statistics of Mind
Columbia University, New York, NY 10027
ap3053@columbia.edu

Liam Paninski
Department of Statistics,
Center for Theoretical Neuroscience,
& Grossman Center for the Statistics of Mind
Columbia University, New York, NY 10027
liam@stat.columbia.edu

Abstract
With the advent of modern stimulation techniques in neuroscience, the opportunity arises to map neuron to neuron connectivity. In this work, we develop
a method for efficiently inferring posterior distributions over synaptic strengths
in neural microcircuits. The input to our algorithm is data from experiments in
which action potentials from putative presynaptic neurons can be evoked while
a subthreshold recording is made from a single postsynaptic neuron. We present
a realistic statistical model which accounts for the main sources of variability in
this experiment and allows for significant prior information about the connectivity
and neuronal cell types to be incorporated if available. Due to the technical challenges and sparsity of these systems, it is important to focus experimental time
stimulating the neurons whose synaptic strength is most ambiguous, therefore we
also develop an online optimal design algorithm for choosing which neurons to
stimulate at each trial.

1

Introduction

A major goal of neuroscience is the mapping of neural microcircuits at the scale of hundreds to
thousands of neurons [1]. By mapping, we specifically mean determining which neurons synapse
onto each other and with what weight. One approach to achieving this goal involves the simultaneous
stimulation and observation of populations of neurons. In this paper, we specifically address the
mapping experiment in which a set of putative presynaptic neurons are optically stimulated while
an electrophysiological trace is recorded from a designated postsynaptic neuron. It should be noted
that the methods we present are general enough that most stimulation and subthreshold monitoring
technology would be well fit by our model with only minor changes. These types of experiments
have been implemented with some success [2, 3, 6], yet there are several issues which prevent
efficient, large scale mapping of neural microcircuitry. For example, while it has been shown that
multiple neurons can be stimulated simultaneously [4, 5], successful mapping experiments have thus
far only stimulated a single neuron per trial which increases experimental time [2, 3, 6]. Stimulating
multiple neurons simultaneously and with high accuracy requires well-tuned hardware, and even
then some level of stimulus uncertainty may remain. In addition, a large portion of connection
?

These authors contributed equally to this work.

1

weights are small which has meant that determining these weights is difficult and that many trials
must be performed. Due to the sparsity of neural connectivity, potentially useful trials are spent
on unconnected pairs instead of refining weight estimates for connected pairs when the stimuli
are chosen non-adaptively. In this paper, we address these issues by developing a procedure for
sparse Bayesian inference and information-based experimental design which can reconstruct neural
microcircuits accurately and quickly despite the issues listed above.

2

A realistic model of neural microcircuits

In this section we propose a novel and thorough statistical model which is specific enough to capture
most of the relevant variability in these types of experiments while being flexible enough to be used
with many different hardware setups and biological preparations.
2.1

Stimulation

In our experimental setup, at each trial, n = 1, . . . , N , the experimenter stimulates R of K possible
presynaptic neurons. We represent the chosen set of neurons for each trial with the binary vector
zn ? {0, 1}K , which has a one in each of the the R entries corresponding to the stimulated neurons
on that trial. One of the difficulties of optical stimulation lies in the experimenter?s inability to
stimulate a specific neuron without possibly failing to stimulate the target neuron or engaging other
nearby neurons. In general, this is a result of the fact that optical excitation does not stimulate a
single point in space but rather has a point spread function that is dependent on the hardware and the
biological tissue. To complicate matters further, each neuron has a different rheobase (a measure of
how much current is needed to generate an action potential) and expression level of the optogenetic
protein. While some work has shown that it may be possible to stimulate exact sets of neurons,
this setup requires very specific hardware and fine tuning [4, 5]. In addition, even if a neuron
fires, there is some probability that synaptic transmission will not occur. Because these events are
difficult or impossible to observe, we model this uncertainty by introducing a second binary vector
xn ? {0, 1}K denoting the neurons that actually release neurotransmitter in trial n. The conditional
distribution of xn given zn can be chosen by the experimenter to match their hardware settings and
understanding of synaptic transmission rates in their preparation.
2.2

Sparse connectivity

Numerous studies have collected data to estimate both connection probabilities and synaptic weight
distributions as a function of distance and cell identity [2, 3, 6, 7, 8, 9, 10, 11, 12]. Generally, the
data show that connectivity is sparse and that most synaptic weights are small with a heavy tail of
strong connections. To capture the sparsity of neural connectivity, we place a ?spike-and-slab? prior
on the synaptic weights wk [13, 14, 15], for each presynaptic neuron k = 1, . . . , K; these priors are
designed to place non-zero probability on the event that a given weight wk is exactly zero. Note that
we do not need to restrict the ?slab? distributions (the conditional distributions of wk given that wk
is nonzero) to the traditional Gaussian choice, and in fact each weight can have its own parameters.
For example, log-normal [12] or exponential [8, 10] distributions may be used in conjunction with
information about cell type and location to assign highly informative priors 1 .
2.3

Postsynaptic response

In our model a subthreshold response is measured from a designated postsynaptic neuron. Here we
assume the measurement is a one-dimensional trace yn ? RT , where T is the number of samples in
the trace. The postsynaptic response for each synaptic event in a given trial can be modeled using an
appropriate template function fk (?) for each presynaptic neuron k. For this paper we use an alpha
function to model the shape of each neuron?s contribution to the postsynaptic current, parameterized
by time constants ?k which define the rise and decay time. As with the synaptic weight priors, the
template functions could be designed based on the cells? identities. The onset of each postsynaptic
1
A cell?s identity can be general such as excitatory or inhibitory, or more specific such as VIP- or PVinterneurons. These identities can be identified by driving the optogenetic channel with a particular promotor
unique to that cell type or by coexpressing markers for various cell types along with the optogenetic channel.

2

Presynaptic weights

Location of presynaptic neurons and stimuli

Weight

1
0
?1
0

20

40

10
Current [pA]

Neuron k

60

80

100

Postsynaptic current trace

0
?10
?20
?30
0

50

100
Time [samples]

150

200

Figure 1: A schematic of the model experiment. The left figure shows the relative location of
100 presynaptic neurons; inhibitory neurons are shown in yellow, and excitatory neurons in purple.
Neurons marked with a black outline have a nonzero connectivity to the postsynaptic neuron (shown
as a blue star, in the center). The blue circles show the diffusion of the stimulus through the tissue.
The true connectivity weights are shown on the upper right, with blue vertical lines marking the five
neurons which were actually fired as a result of this stimulus. The resulting time series postsynaptic
current trace is shown in the bottom right. The connected neurons which fired are circled in red, the
triangle and star marking their weights and corresponding postsynaptic events in the plots at right.
response may be jittered such that each event starts at some time dnk after t = 0, where the delays
could be conditionally distributed on the parameters of the stimulation and cells. Finally, at each time
step the signal is corrupted by zero mean Gaussian noise with variance ? 2 . This noise distribution is
chosen for simplicity; however, the model could easily handle time-correlated noise.
2.4

Full definition of model

The full model can be summarized by the likelihood
 X

T
N Y
Y

2

N ynt 
wk xnk fk (t ? dnk , ?k ), ?
p(Y|w, X, D) =
n=1 t=1

(1)

k

with the general spike-and-slab prior
p(?k ) = Bernoulli(ak ),
N ?T

p(wk |?k ) = ?k p(wk |?k = 1) + (1 ? ?k )?0 (wk )
N ?K

(2a, 2b)

N ?K

where Y ? R
, X ? {0, 1}
, and D ? R
are composed of the responses, latent
neural activity, and delays, respectively; ?k is a binary variable indicating whether or not neuron k
is connected.
We restate that the key to this model is that it captures the main sources of uncertainty in the experiment while providing room for particulars regarding hardware and the anatomy and physiology of
the system to be incorporated. To infer the marginal distribution of the synaptic weights, one can
use standard Bayesian methods such as Gibbs sampling or variational inference, both of which are
discussed below. An example set of neurons and connectivity weights, along with the set of stimuli
and postsynaptic current trace for a single trial, is shown in Figure 1.

3

Inference

Throughout the remainder of the paper, all simulated data is generated from the model presented
above. As mentioned, any free hyperparameters or distribution choices can be chosen intelligently
from empirical evidence. Biological parameters may be specific and chosen on a cell by cell basis
or left general for the whole system. We show in our results that inference and optimal design still
perform well when general priors are used. Details regarding data simulation as well as specific
choices we make in our experiments are presented in Appendix A.
3

3.1

Charge as synaptic strength

To reduce the space P
over which we perform inference, we collapse the variables wk and ?k into a
single variable ck = t wk fk (t ? dnk , ?k ) which quantifies the charge transfer during the synaptic
event and can be used to define the strength of a connection. Integrating over time also eliminates
any dependence on the delays dnk . In this context, we reparameterize the likelihood as a function of
PT
yn = t=0 ynt and ? = ?T 1/2 and the resulting likelihood is
Y
2
p(y|X, c) =
N (yn |x>
(3)
n c, ? ).
n

We found that na??ve MCMC sampling over the posterior of w, ? , ?, X, and D insufficiently explored the support and inference was unsuccessful. In this effort to make the inference procedure
computationally tractable, we discard potentially useful temporal information in the responses. An
important direction for future work is to experiment with samplers that can more efficiently explore
the full posterior (e.g., using Wang-Landau or simulated tempering methods).
3.2

Gibbs sampling

The reparameterized posterior p(c, ?, X|Z, y) can be inferred using a simple Gibbs sampler. We
approximate the prior over c as a spike-and-slab with Gaussian slabs where the slabs could be
truncated if the cells? excitatory or inhibitory identity is known. Each xnk can be sampled by
computing the odds ratio, and following [15] we draw each ck , ?k from the joint distribution
p(ck , ?k |Z, y, X, {cj , ?j |j 6= k}) by sampling first ?k from p(?k |Z, y, X, {cj |j 6= k}), then
p(ck |Z, y, X, {cj , |j 6= k}, ?k ).
3.3

Variational Bayes

As stated earlier we do not only want to recover the parameters of the system, but want to perform
optimal experimental design, which is a closed-loop process. One essential aspect of the design
procedure is that decisions must be returned to the experimenter quickly, on the order of a few
seconds. This means that we must be able to perform inference of the posterior as well as choose
the next stimulus extremely quickly. For realistically sized systems with hundred to thousands of
neurons, Gibbs sampling will be too slow, and we have to explore other options for speeding up
inference.
To achieve this decrease in runtime, we approximate the posterior distribution of c and ? using a
variational approach [16]. The use of variational inference for spike-and-slab regression models has
been explored in [17, 18], and we follow their methods with some minor changes. If we, for now,
assume that X is known and let the spike-and-slab prior on c have untruncated Gaussian slabs, then
this variational approach finds the best fully-factorized approximation to the true posterior
Y
p(c, ?|x1:n , y1:n ) ?
q(ck , ?k )
(4)
k

where the functional form of q(ck , ?k ) is itself restricted to a spike-and-slab distribution

?k N (ck |?k , s2k ) if ?k = 1
q(ck , ?k ) =
(1 ? ?k )?0 (ck ) otherwise.

(5)

The variational parameters ?k , ?k , sk for k = 1, . . . , K are found by minimizing the KL-divergence
KL(q||p) between the left and right hand sides of Eq. 4 with respect to these values. As is the case
with fully-factorized variational distributions, updating the posterior involves an iterative algorithm
which cycles through the parameters for each factor.
The factorized variational approximation is reasonable when the number of simultaneous stimuli,
R, is small. Note that if we examine the posterior distributions of the weights
Y
Y

2
p(c|y, X) ?
N (yn |x>
ak N (ck |?k , ?k2 ) + (1 ? ak )?0 (ck )
(6)
n c, ? )
n

k

we see that if each xn contains only one nonzero value then each factor in the likelihood is dependent on only one of the K weights and can be multiplied into the corresponding k th spike-and-slab.
4

Therefore, since the product of a spike-and-slab and a Gaussian is still a spike-and-slab, if we stimulate only one neuron at each trial then this posterior is also spike-and-slab, and the variational
approximation becomes exact in this limit.
Since we do not directly observe X, we must take the expectation of the variational parameters
?k , ?k , sk with respect to the distribution p(X|Z, y). We Monte Carlo approximate this integral
in a manner similar to the approach used for integrating over the hyperparameters in [17]; however, here we further approximate by sampling over potential stimuli xnk from p(xnk = 1|zn ). In
practice we will see this approximation suffices for experimental design, with the overall variational
approach performing nearly as well for posterior weight reconstruction as Gibbs sampling from the
true posterior.

4

Optimal experimental design

The preparations needed to perform these type of experiments tend to be short-lived, and indeed, the
very act of collecting data ? that is, stimulating and probing cells ? can compromise the health of
the preparation further. Also, one may want to use the connectivity information to perform additional
experiments. Therefore it becomes critical to complete the mapping phase of the experiment as
quickly as possible. We are thus strongly motivated to optimize the experimental design: to choose
the optimal subset of neurons zn to stimulate at each trial to minimize N , the overall number of
trials required for good inference.
The Bayesian approach to the optimization of experimental design has been explored in [19, 20,
21]. In this paper, we maximize the mutual information I(?; D) between the model parameters ?
and the data D; however, other objective functions could be explored. Mutual information can be
decomposed into a difference of entropies, one of which does not depend on the data. Therefore
the optimization reduces to the intuitive objective of minimizing the posterior entropy with respect
to the data. Because the previous data Dn?1 = {(z1 , y1 ), . . . , (zn?1 , yn?1 )} are fixed and yn is
dependent on the stimulus zn , our problem is reduced to choosing the optimal next stimulus, denoted
z?n , in expectation over yn ,
(7)
z?n = arg max Eyn |zn [I(?; D)] = arg min Eyn |zn [H(?|D)] .
zn

zn

5

Experimental design procedure

The optimization described in Section 4 entails performing a combinatorial optimization over zn ,
where for each zn we consider an expectation over all possible yn . In order to be useful to experimenters in an online setting, we must be able to choose the next stimulus in only one or two seconds.
For any realistically sized system, an exact optimization is computationally infeasible; therefore in
the following section we derive a fast method for approximating the objective function.
5.1

Computing the objective function

The variational posterior distribution of ck , ?k can be used to characterize our general objective
function described in Section 4. We define the cost function J to be the right-hand side of Equation 7,
J ? Eyn |zn [H(c, ?|D)]
(8)
?
such that the optimal next stimulus zn can be found by minimizing J. We benefit immediately from
the factorized approximation of the variational posterior, since we can rewrite the joint entropy as
X
H[c, ?|D] ?
H[ck , ?k |D]
(9)
k

allowing us to optimize over the sum of the marginal entropies instead of having to compute the
(intractable) entropy over the full posterior. Using the conditional entropy identity H[ck , ?k |D] =
H[ck |?k , D] + H[?k |D], we see that the entropy of each spike-and-slab is the sum of a weighted
Gaussian entropy and a Bernoulli entropy and we can write out the approximate objective function
as
h?
i
X
k,n
J?
Eyn |zn
(1 + log(2?s2k,n )) ? ?k,n log ?k,n ? (1 ? ?k,n ) log(1 ? ?k,n ) . (10)
2
k

5

Here, we have introduced additional notation, using ?k,n , ?k,n , and sk,n to refer to the parameters of
the variational posterior distribution given the data through trial n. Intuitively, we see that equation
10 represents a balance between minimizing the sparsity pattern entropy H[?k ] of each neuron and
minimizing the weight entropy H[ck |?k = 1] proportional to the probability ?k that the presynaptic
neuron is connected. As p(?k = 1) ? 1, the entropy of the Gaussian slab distribution grows to
dominate. In algorithm behavior, we see when the probability that a neuron is connected increases,
we spend time stimulating it to reduce the uncertainty in the corresponding nonzero slab distribution.
To perform this optimization we must compute the expected joint entropy with respect to p(yn |zn ).
For any particular candidate zn , this can be Monte Carlo approximated by first sampling yn from the
posterior distribution p(yn |zn , c, Dn?1 ), where c is drawn from the variational posterior inferred at
trial n ? 1. Each sampled yn may be used to estimate the variational parameters ?k,n and sk,n with
which we evaluate H[ck , ?k ]; we average over these evaluations of the entropy from each sample to
compute an estimate of J in Eq. 10.
Once we have chosen z?n , we execute the actual trial and run the variational inference procedure on
the full data to obtain the updated variational posterior parameters ?k,n , ?k,n , and sk,n which are
needed for optimization. Once the experiment has concluded, Gibbs sampling can be run, though
we found only a limited gain when comparing Gibbs sampling to variational inference.
5.2

Fast optimization

The major cost to the algorithm is in the stimulus selection phase. It is not feasible to evaluate the
right-hand side of equation 10 for every zn because as K grows there is a combinatorial explosion
of possible stimuli. To avoid an exhaustive search over possible zn , we adopt a greedy approach
for choosing which R of the K locations to stimulate. First we rank the K neurons based on an
?kn , each
approximation of the objective function. To do this, we propose K hypothetical stimuli, z
th
all zeros except the k entry equal to 1 ? that is, we examine only the K stimuli which represent
?
?kn which
stimulating a single location. We then set znk
= 1 for the R neurons corresponding to the z
?
give the smallest values for the objective function and all other entries of zn to zero. We found that
the neurons selected by a brute force approach are most likely to be the neurons that the greedy
selection process chooses (see Figure 1 in the Appendix).
For large systems of neurons, even the above is too slow to perform in an online setting. For each of
?kn , to approximate the expected entropy we must compute the variational
the K proposed stimuli z
>
? n is the random variable
?>
posterior for M samples of [X>
n ] and L samples of yn (where x
1:n?1 x
corresponding to p(?
xn |?
zn )). Therefore we run the variational inference procedure on the full data
on the order of O(M KL) times at each trial. As the system size grows, running the variational
inference procedure this many times becomes intractable because the number of iterations needed
to converge the coordinate ascent algorithm is dependent on the correlations between the rows of
X. This is implicitly dependent on both N , the number of trials, and R, the number of stimulus
locations (see Figure 2 in the Appendix). Note that the stronger dependence here is on R; when
R = 1 the variational parameter updates become exact and independent across the neurons, and
therefore no coordinate ascent is necessary and the runtime becomes linear in K.
We therefore take one last measure to speed up the optimization process by implementing an online
Bayesian approach to updating the variational posterior (in the stimulus selection phase only). Since
the variational posterior of ck and ?k takes the same form as the prior distribution, we can use the
posterior from trial n ? 1 as the prior at trial n, allowing us to effectively summarize the previous
data. In this online setting, when we stimulate only one neuron, only the parameters of that specific
? kn = z
?kn , this results in explicit
neuron change. If during optimization we temporarily assume that x
updates for each variational parameter, with no coordinate ascent iterations required.
In total, the resulting optimization algorithm has a runtime O(KL) with no coordinate ascent algorithms needed. The combined accelerations described in this section result in a speed up of
several orders of magnitude which allows the full inference and optimization procedure to be run
in real time, running at approximately one second per trial in our computing environment for
K = 500, R = 8. It is worth mentioning here that there are several points at which parallelization
could be implemented in the full algorithm. We chose to parallelize over M which distributes the
sampling of X and the running of variational inference for each sample. (Formulae and step-by-step
implementation details are found in Appendix B.)
6

R =2

R =4

R =8

R =16

? =1.0
NRE of E[c]

1.2
1
0.8
0.6
0.4
0.2

1.4

? =2.5
NRE of E[c]

1.2
1
0.8
0.6
0.4
0.2
1.4

? =5.0
NRE of E[c]

1.2
1
0.8
0.6
0.4
0

200

400
trial, n

600

800

0

200

400
trial, n

600

800

0

200

400
trial, n

600

800

0

200

400
trial, n

600

800

Figure 2: A comparison of normalized reconstruction error (NRE) over 800 trials in a system with
500 neurons, between random stimulus selection (red, magenta) and our optimal experimental design approach (blue, cyan). The heavy red and blue lines indicate the results when running the
Gibbs sampler at that point in the experiment, and the thinner magenta and cyan lines indicate the
results from variational inference. Results are shown over three noise levels ? = 1, 2.5, 5, and for
multiple numbers of stimulus locations per trial, R = 2, 4, 8, 16. Each plot shows the median and
quartiles over 50 experiments. The error decreases much faster in the optimal design case, over a
wide parameter range.

6

Experiments and results

We ran our inference and optimal experimental design algorithm on data sets generated from the
model described in Section 2. We benchmarked our optimal design algorithm against a sequence
of randomly chosen stimuli, measuring performance by normalized reconstruction error, defined as
kE[c] ? ck2 /kck2 ; we report the variation in our experiments by plotting the median and quartiles.
Baseline results are shown in Figure 2, over a range of values for stimulations per trial R and
baseline postsynaptic noise levels ?. The results here use an informative prior, where we assume the
excitatory or inhibitory identity is known, and we set individual prior connectivity probabilities for
each neuron based on that neuron?s identity and distance from the postsynaptic cell. We choose to
let X be unobserved and let the stimuli Z produce Gaussian ellipsoids which excite neurons that are
located nearby. All model parameters are given in Appendix A.
We see that inference in general performs well. The optimal procedure was able to achieve equivalent reconstruction quality as a random stimulation paradigm in significantly fewer trials when the
number of stimuli per trial and response noise were in an experimentally realistic range (R = 4
and ? = 2.5 being reasonable values). Interestingly, the approximate variational inference methods
performed about as well as the full Gibbs sampler here (at much less computational cost), although
Gibbs sampling seems to break down when R grows too large and the noise level is small, which
may be a consequence of strong, local peaks in the posterior.
As the the number of stimuli per trial R increases, we start to see improved weight estimates and
faster convergence but a decrease in the relative benefit of optimal design; the random approach
?catches up? to the optimal approach as R becomes large. This is consistent with the results of [22],
who argue that optimal design can provide only modest gains in performing sparse reconstructions,
7

General Prior

X Observed

1.1

1.2

1
1

NRE of E[c]

0.9
0.8

0.8

0.7
0.6

0.6
0.5

0.4

0.4
0

200

400
trial, n

600

800

0

200

400
trial, n

600

800

Figure 3: The results of inference and optimal design
(A) with a single spike-andslab prior for all connections
(prior connection probability
of .1, and each slab Gaussian with mean 0 and standard deviation 31.4); and (B)
with X observed. Both experiments show median and
quartiles range with R = 4
and ? = 2.5.

if the design vectors x are unconstrained. (Note that these results do not apply directly in our setting
if R is small, since in this case x is constrained to be highly sparse ? and this is exactly where we
see major gains from optimal online designs.)
Finally, we see that we are still able to recover the synaptic strengths when we use a more general
prior as in Figure 3A where we placed a single spike-and-slab prior across all the connections. Since
we assumed the cells? identities were unknown, we used a zero-centered Gaussian for the slab and
a prior connection probability of .1. While we allow for stimulus uncertainty, it will likely soon be
possible to stimulate multiple neurons with high accuracy. In Figure 3B we see that - as expected performance improves.
It is helpful to place this observation in the context of [23], which proposed a compressed-sensing
algorithm to infer microcircuitry in experiments like those modeled here. The algorithms proposed
by [23] are based on computing a maximum a posteriori (MAP) estimate of the weights w; note
that to pursue the optimal Bayesian experimental design methods proposed here, it is necessary
to compute (or approximate) the full posterior distribution, not just the MAP estimate. (See, e.g.,
[24] for a related discussion.) In the simulated experiments of [23], stimulating roughly 30 of
500 neurons per trial is found to be optimal; extrapolating from Fig. 2, we would expect a limited
difference between optimal and random designs in this range of R. That said, large values of R
lead to some experimental difficulties: first, stimulating large populations of neurons with high
spatial resolution requires very fined tuned hardware (note that the approach of [23] has not yet
been applied to experimental data, to our knowledge); second, if R is sufficiently large then the
postsynaptic neuron can be easily driven out of a physiologically realistic regime, which in turn
means that the basic linear-Gaussian modeling assumptions used here and in [23] would need to be
modified. We plan to address these issues in more depth in our future work.

7

Future Work

There are several improvements we would like to explore in developing this model and algorithm
further. First, the implementation of an inference algorithm which performs well on the full model
such that we can recover the synaptic weights, the time constants, and the delays would allow us to
avoid compressing the responses to scalar values and recover more information about the system.
Also, it may be necessary to improve the noise model as we currently assume that there are no
spontaneous synaptic events which will confound the determination of each connection?s strength.
Finally, in a recent paper, [25], a simple adaptive compressive sensing algorithm was presented
which challenges the results of [22]. It would be worth exploring whether their algorithm would be
applicable to our problem.
Acknowledgements
This material is based upon work supported by, or in part by, the U. S. Army Research Laboratory
and the U. S. Army Research Office under contract number W911NF-12-1-0594 and an NSF CAREER grant. We would also like to thank Rafael Yuste and Jan Hirtz for helpful discussions, and
our anonymous reviewers.
8

References
[1] R. Reid, ?From Functional Architecture to Functional Connectomics,? Neuron, vol. 75, pp. 209?217, July
2012.
[2] M. Ashby and J. Isaac, ?Maturation of a recurrent excitatory neocortical circuit by experience-dependent
unsilencing of newly formed dendritic spines,? Neuron, vol. 70, no. 3, pp. 510 ? 521, 2011.
[3] E. Fino and R. Yuste, ?Dense Inhibitory Connectivity in Neocortex,? Neuron, vol. 69, pp. 1188?1203,
Mar. 2011.
[4] V. Nikolenko, K. E. Poskanzer, and R. Yuste, ?Two-photon photostimulation and imaging of neural circuits,? Nat Meth, vol. 4, pp. 943?950, Nov. 2007.
[5] A. M. Packer, D. S. Peterka, J. J. Hirtz, R. Prakash, K. Deisseroth, and R. Yuste, ?Two-photon optogenetics of dendritic spines and neural circuits,? Nat Meth, vol. 9, pp. 1202?1205, Dec. 2012.
[6] A. M. Packer and R. Yuste, ?Dense, unspecific connectivity of neocortical parvalbumin-positive interneurons: A canonical microcircuit for inhibition?,? The Journal of Neuroscience, vol. 31, no. 37, pp. 13260?
13271, 2011.
[7] B. Barbour, N. Brunel, V. Hakim, and J.-P. Nadal, ?What can we learn from synaptic weight distributions?,? Trends in neurosciences, vol. 30, pp. 622?629, Dec. 2007.
[8] C. Holmgren, T. Harkany, B. Svennenfors, and Y. Zilberter, ?Pyramidal cell communication within local
networks in layer 2/3 of rat neocortex,? The Journal of Physiology, vol. 551, no. 1, pp. 139?153, 2003.
[9] J. Kozloski, F. Hamzei-Sichani, and R. Yuste, ?Stereotyped position of local synaptic targets in neocortex,? Science, vol. 293, no. 5531, pp. 868?872, 2001.
[10] R. B. Levy and A. D. Reyes, ?Spatial profile of excitatory and inhibitory synaptic connectivity in mouse
primary auditory cortex,? The Journal of Neuroscience, vol. 32, no. 16, pp. 5609?5619, 2012.
[11] R. Perin, T. K. Berger, and H. Markram, ?A synaptic organizing principle for cortical neuronal groups,?
Proceedings of the National Academy of Sciences, vol. 108, no. 13, pp. 5419?5424, 2011.
[12] S. Song, P. J. Sj?ostr?om, M. Reigl, S. Nelson, and D. B. Chklovskii, ?Highly nonrandom features of
synaptic connectivity in local cortical circuits.,? PLoS biology, vol. 3, p. e68, Mar. 2005.
[13] E. I. George and R. E. McCulloch, ?Variable selection via gibbs sampling,? Journal of the American
Statistical Association, vol. 88, no. 423, pp. 881?889, 1993.
[14] T. J. Mitchell and J. J. Beauchamp, ?Bayesian variable selection in linear regression,? Journal of the
American Statistical Association, vol. 83, no. 404, pp. 1023?1032, 1988.
[15] S. Mohamed, K. A. Heller, and Z. Ghahramani, ?Bayesian and l1 approaches to sparse unsupervised
learning,? CoRR, vol. abs/1106.1157, 2011.
[16] C. M. Bishop, Pattern Recognition and Machine Learning. Springer, 2007.
[17] P. Carbonetto and M. Stephens, ?Scalable variational inference for bayesian variable selection in regression, and its accuracy in genetic association studies,? Bayesian Analysis, vol. 7, no. 1, pp. 73?108, 2012.
[18] M. Titsias and M. Lzaro-Gredilla, ?Spike and Slab Variational Inference for Multi-Task and Multiple
Kernel Learning,? in Advances in Neural Information Processing Systems 24, pp. 2339?2347, 2011.
[19] Y. Dodge, V. Fedorov, and H. Wynn, eds., Optimal Design and Analysis of Experiments. North Holland,
1988.
[20] D. J. C. MacKay, ?Information-based objective functions for active data selection,? Neural Comput.,
vol. 4, pp. 590?604, July 1992.
[21] L. Paninski, ?Asymptotic Theory of Information-Theoretic Experimental Design,? Neural Comput.,
vol. 17, pp. 1480?1507, July 2005.
[22] E. Arias-Castro, E. J. Cand`es, and M. A. Davenport, ?On the fundamental limits of adaptive sensing,?
IEEE Transactions on Information Theory, vol. 59, no. 1, pp. 472?481, 2013.
[23] T. Hu, A. Leonardo, and D. Chklovskii, ?Reconstruction of Sparse Circuits Using Multi-neuronal Excitation (RESCUME),? in Advances in Neural Information Processing Systems 22, pp. 790?798, 2009.
[24] S. Ji and L. Carin, ?Bayesian compressive sensing and projection optimization,? in Proceedings of the
24th international conference on Machine learning, ICML ?07, (New York, NY, USA), pp. 377?384,
ACM, 2007.
[25] M. Malloy and R. D. Nowak, ?Near-optimal adaptive compressed sensing,? CoRR, vol. abs/1306.6239,
2013.

9

"
537,"Kernel peA and De-Noising in Feature Spaces

Sebastian Mika, Bernhard Scholkopf, Alex Smola
Klaus-Robert Muller, Matthias Scholz, Gunnar Riitsch
GMD FIRST, Rudower Chaussee 5, 12489 Berlin, Germany
{mika, bs, smola, klaus, scholz, raetsch} @first.gmd.de

Abstract
Kernel PCA as a nonlinear feature extractor has proven powerful as a
preprocessing step for classification algorithms. But it can also be considered as a natural generalization of linear principal component analysis. This gives rise to the question how to use nonlinear features for
data compression, reconstruction, and de-noising, applications common
in linear PCA. This is a nontrivial task, as the results provided by kernel PCA live in some high dimensional feature space and need not have
pre-images in input space. This work presents ideas for finding approximate pre-images, focusing on Gaussian kernels, and shows experimental
results using these pre-images in data reconstruction and de-noising on
toy examples as well as on real world data.

1 peA and Feature Spaces
Principal Component Analysis (PC A) (e.g. [3]) is an orthogonal basis transformation.
The new basis is found by diagonalizing the centered covariance matrix of a data set
{Xk E RNlk = 1, ... ,f}, defined by C = ((Xi - (Xk))(Xi - (Xk))T). The coordinates in the Eigenvector basis are called principal components. The size of an Eigenvalue
>. corresponding to an Eigenvector v of C equals the amount of variance in the direction
of v. Furthermore, the directions of the first n Eigenvectors corresponding to the biggest
n Eigenvalues cover as much variance as possible by n orthogonal directions. In many applications they contain the most interesting information: for instance, in data compression,
where we project onto the directions with biggest variance to retain as much information
as possible, or in de-noising, where we deliberately drop directions with small variance.
Clearly, one cannot assert that linear PCA will always detect all structure in a given data set.
By the use of suitable nonlinear features, one can extract more information. Kernel PCA
is very well suited to extract interesting nonlinear structures in the data [9]. The purpose of
this work is therefore (i) to consider nonlinear de-noising based on Kernel PCA and (ii) to
clarify the connection between feature space expansions and meaningful patterns in input
space. Kernel PCA first maps the data into some feature space F via a (usually nonlinear)
function <II and then performs linear PCA on the mapped data. As the feature space F
might be very high dimensional (e.g. when mapping into the space of all possible d-th
order monomials of input space), kernel PCA employs Mercer kernels instead of carrying

537

Kernel peA and De-Noising in Feature Spaces

out the mapping <I> explicitly. A Mercer kernel is a function k(x, y) which for all data
sets {Xi} gives rise to a positive matrix Kij = k(Xi' Xj) [6]. One can show that using
k instead of a dot product in input space corresponds to mapping the data with some <I>
to a feature space F [1], i.e. k(x,y) = (<I>(x) . <I>(y)). Kernels that have proven useful
include Gaussian kernels k(x, y) = exp( -llx - Yll2 Ie) and polynomial kernels k(x, y) =
(x?y)d. Clearly, all algorithms that can be formulated in terms of dot products, e.g. Support
Vector Machines [1], can be carried out in some feature space F without mapping the data
explicitly. All these algorithms construct their solutions as expansions in the potentially
infinite-dimensional feature space.
The paper is organized as follows: in the next section, we briefly describe the kernel PCA
algorithm. In section 3, we present an algorithm for finding approximate pre-images of
expansions in feature space. Experimental results on toy and real world data are given in
section 4, followed by a discussion of our findings (section 5).

2

Kernel peA and Reconstruction

To perform PCA in feature space, we need to find Eigenvalues A > 0 and Eigenvectors
V E F\{O} satisfying AV = GV with G = (<I>(Xk)<I>(Xk)T).1 Substituting G into the
Eigenvector equation, we note that all solutions V must lie in the span of <I>-images of the
training data. This implies that we can consider the equivalent system

A( <I>(Xk) . V)

= (<I>(Xk) . GV) for all k = 1, ... ,f

and that there exist coefficients Q1 , ...

V =

,Ql

L

(1)

such that
l
i =l

Qi<l>(Xi)

(2)

Substituting C and (2) into (1), and defining an f x f matrix K by Kij := (<I>(Xi)? <I>(Xj)) =
k( Xi, Xj), we arrive at a problem which is cast in terms of dot products: solve

fAa = Ko.

(3)

where 0. = (Q1, ... ,Ql)T (for details see [7]). Normalizing the solutions V k , i.e. (V k .
Vk) = 1, translates into Ak(o.k .o.k) = 1. To extract nonlinear principal components
for the <I>-image of a test point X we compute the projection onto the k-th component by
13k := (V k . <I> (X)) = 2:f=l Q~ k(x, Xi). Forfeature extraction, we thus have to evaluate f
kernel functions instead of a dot product in F, which is expensive if F is high-dimensional
(or, as for Gaussian kernels, infinite-dimensional). To reconstruct the <I>-image of a vector
X from its projections 13k onto the first n principal components in F (assuming that the
Eigenvectors are ordered by decreasing Eigenvalue size), we define a projection operator
P n by

(4)
k=l

If n is large enough to take into account all directions belonging to Eigenvectors with nonzero Eigenvalue, we have Pn<l>(Xi) = <I>(Xi). Otherwise (kernel) PCA still satisfies (i) that
the overall squared reconstruction error 2: i II Pn<l>(Xi) - <I>(xdll 2 is minimal and (ii) the
retained variance is maximal among all projections onto orthogonal directions in F. In
common applications, however, we are interested in a reconstruction in input space rather
than in F. The present work attempts to achieve this by computing a vector z satisfying
<I>(z) = Pn<l>(x). The hope is that for the kernel used, such a z will be a good approximation of X in input space. However. (i) such a z will not always exist and (ii) if it exists,
I For simplicity, we assume that the mapped data are centered in F. Otherwise, we have to go
through the same algebra using ~(x) := <I>(x) - (<I>(x;?).

S. Mika et al.

538

it need be not unique. 2 As an example for (i), we consider a possible representation of F.
One can show [7] that <I> can be thought of as a map <I> (x) = k( x, .) into a Hilbert space 1{k
of functions Li ai k( Xi, .) with a dot product satisfying (k( x, .) . k(y, .)) = k( x, y). Then
1{k is caHed reproducing kernel Hilbert space (e.g. [6]). Now, for a Gaussian kernel, 1{k
contains aHlinear superpositions of Gaussian bumps on RN (plus limit points), whereas
by definition of <I> only single bumps k(x,.) have pre-images under <1>. When the vector
P n<l>( x) has no pre-image z we try to approximate it by minimizing

p(z) = 1I<I>(z) - Pn<l>(x) 112

(5)

This is a special case of the reduced set method [2]. Replacing terms independent of z by
0, we obtain

p(z) = 1I<I>(z)112 - 2(<I>(z) . Pn<l>(x))

+0

(6)

Substituting (4) and (2) into (6), we arrive at an expression which is written in terms of
dot products. Consequently, we can introduce a kernel to obtain a formula for p (and thus
V' % p) which does not rely on carrying out <I> explicitly

p(z) = k(z, z) - 2

3

n

l

k=l

i=l

L f3k L a~ k(z, Xi) + 0

(7)

Pre-Images for Gaussian Kernels

To optimize (7) we employed standard gradient descent methods. If we restrict our attention
to kernels of the form k(x, y) = k(llx - Y1l2) (and thus satisfying k(x, x) == const. for all
x), an optimal z can be determined as foHows (cf. [8]): we deduce from (6) that we have
to maximize
l

p(z)

= (<I>(z) . Pn<l>(x)) + 0' = L

Ii k(z, Xi)

+ 0'

(8)

i=l
where we set Ii = L~=l f3ka: (for some 0' independent of z). For an extremum, the
gradient with respect to z has to vanish: V' %p(z) = L~=l lik'(llz - xi112)(Z - Xi) = O.
This leads to a necessary condition for the extremum: z = Li tJixd Lj tJj, with tJi
,ik'(llz - xiIl 2). For a Gaussian kernel k(x, y) = exp( -lix - Yll2 jc) we get

z=

L~=l Ii exp( -liz - xil1 2 jC)Xi
l

Li=l liexp(-llz - xil1 2jc)

~

We note that the denominator equals (<I>(z) . P n<l>(X)) (cf. (8?. Making the assumption that
Pn<l>(x) i= 0, we have (<I>(x) . Pn<l>(x)) = (Pn<l>(x) . Pn<l>(x)) > O. As k is smooth, we
conclude that there exists a neighborhood of the extremum of (8) in which the denominator
of (9) is i= O. Thus we can devise an iteration scheme for z by

Zt+l =

L~=l Ii exp( -llzt - xill 2 jC)Xi
l

Li=l li exp(-lI z t

-

xil1 2jc)

(10)

Numerical instabilities related to (<I>(z) . Pn<l>(x)) being smaH can be dealt with by restarting the iteration with a different starting value. Furthermore we note that any fixed-point
of (10) will be a linear combination of the kernel PCA training data Xi. If we regard (10)
in the context of clustering we see that it resembles an iteration step for the estimation of
2If the kernel allows reconstruction of the dot-product in input space, and under the assumption
that a pre-image exists, it is possible to construct it explicitly (cf. [7]). But clearly, these conditions
do not hold true in general.

Kernel peA and De-Noising in Feature Spaces

539

the center of a single Gaussian cluster. The weights or 'probabilities' T'i reflect the (anti-)
correlation between the amount of cP (x) in Eigenvector direction Vk and the contribution
of CP(Xi) to this Eigenvector. So the 'cluster center' z is drawn towards training patterns
with positive T'i and pushed away from those with negative T'i, i.e. for a fixed-point Zoo the
influence of training patterns with smaner distance to x wi11 tend to be bigger.

4 Experiments
To test the feasibility of the proposed algorithm, we run several toy and real world experiments. They were performed using (10) and Gaussian kernels of the form k(x, y) =
exp( -(llx - YI12)/(nc)) where n equals the dimension of input space. We mainly focused
on the application of de-noising, which differs from reconstruction by the fact that we are
allowed to make use of the original test data as starting points in the iteration.
Toy examples: In the first experiment (table 1), we generated a data set from eleven Gaussians in RIO with zero mean and variance u 2 in each component, by selecting from each
source 100 points as a training set and 33 points for a test set (centers of the Gaussians randomly chosen in [-1, 1]10). Then we applied kernel peA to the training set and computed
the projections 13k of the points in the test set. With these, we carried out de-noising, yielding an approximate pre-image in RIO for each test point. This procedure was repeated for
different numbers of components in reconstruction, and for different values of u. For the
kernel, we used c = 2u 2 ? We compared the results provided by our algorithm to those of
linear peA via the mean squared distance of an de-noised test points to their corresponding center. Table 1 shows the ratio of these values; here and below, ratios larger than one
indicate that kernel peA performed better than linear peA. For almost every choice of
nand u, kernel PeA did better. Note that using alllO components, linear peA is just a
basis transformation and hence cannot de-noise. The extreme superiority of kernel peA
for small u is due to the fact that all test points are in this case located close to the eleven
spots in input space, and linear PeA has to cover them with less than ten directions. Kernel PeA moves each point to the correct source even when using only a sman number of
components.
n=1
2
3
4
7
8
5
6
9
0.05 2058.42 1238.36 846.14 565.41 309.64 170.36 125.97 104.40 92.23
0.1
10.22
31.32 21.51
29.24 27.66 2:i.5:i 29.64 40.07 63.41
1.12
1.18
0.2
0.99
1.50
2.11
2.73
3.72
5.09 6.32
1.07
1.44
0.4
1.26
1.64
1.91
2.08
2.34 2.47
2.22
0.8
1.39
1.54
2.25 2.39
1.23
1.7U
1.8U
1.96
2.10

Table 1: De-noising Gaussians in RIO (see text). Performance ratios larger than one indicate how much better kernel PeA did, compared to linear PeA, for different choices of the
Gaussians' std. dev. u, and different numbers of components used in reconstruction.
To get some intuitive understanding in a low-dimensional case, figure 1 depicts the results
of de-noising a half circle and a square in the plane, using kernel peA, a nonlinear autoencoder, principal curves, and linear PeA. The principal curves algorithm [4] iteratively
estimates a curve capturing the structure of the data. The data are projected to the closest
point on a curve which the algorithm tries to construct such that each point is the average
of all data points projecting onto it. It can be shown that the only straight lines satisfying
the latter are principal components, so principal curves are a generalization of the latter.
The algorithm uses a smoothing parameter whic:h is annealed during the iteration. In the
nonlinear autoencoder algorithm, a 'bottleneck' 5-layer network is trained to reproduce the
input values as outputs (i.e. it is used in autoassociative mode). The hidden unit activations
in the third layer form a lower-dimensional representation of the data, closely related to

540

S. Mika et al.

PCA (see for instance [3]). Training is done by conjugate gradient descent. In all algorithms, parameter values were selected such that the best possible de-noising result was
obtained. The figure shows that on the closed square problem, kernel PeA does (subjectively) best, followed by principal curves and the nonlinear autoencoder; linear PeA fails
completely. However, note that all algorithms except for kernel PCA actually provide an
explicit one-dimensional parameterization of the data, whereas kernel PCA only provides
us with a means of mapping points to their de-noised versions (in this case, we used four
kernel PCA features, and hence obtain a four-dimensional parameterization).
kernel PCA

nonlinear autoencoder

Principal Curves

linear PCA

~It%i 1J;:qf~~

I::f;~.;~,: '~ll
Figure 1: De-noising in 2-d (see text). Depicted are the data set (small points) and its
de-noised version (big points, joining up to solid lines). For linear PCA, we used one
component for reconstruction, as using two components, reconstruction is perfect and thus
does not de-noise. Note that all algorithms except for our approach have problems in
capturing the circular structure in the bottom example.
USPS example: To test our approach on real-world data, we also applied the algorithm
to the USPS database of 256-dimensional handwritten digits. For each of the ten digits,
we randomly chose 300 examples from the training set and 50 examples from the test
set. We used (10) and Gaussian kernels with c = 0.50, equaling twice the average of
the data's variance in each dimensions. In figure 4, we give two possible depictions of

1111 II iI 111111 fill

0""(1' . ___ ? ?
11m 111111111111

Figure 2: Visualization of Eigenvectors (see
text). Depicted are the 2?, ... , 28 -th Eigenvector (from left to right). First row: linear
PeA, second and third row: different visualizations for kernel PCA.

the Eigenvectors found by kernel PCA, compared to those found by linear PCA for the
USPS set. The second row shows the approximate pre-images of the Eigenvectors V k ,
k = 2?, ... ,2 8 , found by our algorithm. In the third row each image is computed as
follows: Pixel i is the projection of the <II-image of the i-th canonical basis vector in input
space onto the corresponding Eigenvector in features space (upper left <II(el) . V k , lower
right <II (e256) . Vk). In the linear case, both methods would simply yield the Eigenvectors
oflinear PCA depicted in the first row; in this sense, they may be considered as generalized
Eigenvectors in input space. We see that the first Eigenvectors are almost identical (except
for signs). But we also see, that Eigenvectors in linear PeA start to concentrate on highfrequency structures already at smaller Eigenvalue size. To understand this, note that in
linear PCA we only have a maximum number of 256 Eigenvectors, contrary to kernel PCA
which gives us the number of training examples (here 3000) possible Eigenvectors. This

541

Kernel peA and De-Noising in Feature Spaces

? ? ? & ? ? ? ? ? ? ? ? 3.55
? ? ? &3
o.n
1.02

1.02

1.01

0.113

I.CI'

0.111

0.118

0.118

1.01

0.60

0.78

0.76

0.52

0.73

0.7(

0.80

0.7(

0.7(

0.72

? ? ? ? ? ? ? ? ? ? ? ? ? 3S ? ? ? ? S!
Figure 3: Reconstruction of USPS data. Depicted are the reconstructions of the first digit
in the test set (original in last column) from the first n = 1, ... ,20 components for linear
peA (first row) and kernel peA (second row) case. The numbers in between denote the
fraction of squared distance measured towards the original example. For a small number
of components both algorithms do nearly the same. For more components, we see that
linear peA yields a result resembling the original digit, whereas kernel peA gives a result
resembling a more prototypical 'three'

also explains some of the results we found when working with the USPS set. In these
experiments, linear and kernel peA were trained with the original data. Then we added (i)
additive Gaussian noise with zero mean and standard deviation u = 0.5 or (ii) 'speckle'
noise with probability p = 0.4 (i.e. each pixel flips to black or white with probability
p/2) to the test set. For the noisy test sets we computed the projections onto the first n
linear and nonlinear components, and carried out reconstruction for each case. The results
were compared by taking the mean squared distance of each reconstructed digit from the
noisy test set to its original counterpart. As a third experiment we did the same for the
original test set (hence doing reconstruction, not de-noising). In the latter case, where
the task is to reconstruct a given example as exactly as possible, linear peA did better,
at least when using more than about 10 components (figure 3). This is due to the fact
that linear peA starts earlier to account for fine structures, but at the same time it starts
to reconstruct noise, as we will see in figure 4. Kernel PCA, on the other hand, yields
recognizable results even for a small number of components, representing a prototype of
the desired example. This is one reason why our approach did better than linear peA for the
de-noising example (figure 4). Taking the mean squared distance measured over the whole
test set for the optimal number of components in linear and kernel PCA, our approach did
better by a factor of 1.6 for the Gaussian noise, and 1.2 times better for the 'speckle' noise
(the optimal number of components were 32 in linear peA, and 512 and 256 in kernel
PCA, respectively). Taking identical numbers of components in both algorithms, kernel
peA becomes up to 8 (!) times better than linear peA. However, note that kernel PCA
comes with a higher computational complexity.

5 Discussion
We have studied the problem of finding approximate pre-images of vectors in feature space,
and proposed an algorithm to solve it. The algorithm can be applied to both reconstruction
and de-noising. In the former case, results were comparable to linear peA, while in the
latter case, we obtained significantly better results. Our interpretation of this finding is as
follows. Linear peA can extract at most N components, where N is the dimensionality of
the data. Being a basis transform, all N components together fully describe the data. If the
data are noisy, this implies that a certain fraction of the components will be devoted to the
extraction of noise. Kernel peA, on the other hand, allows the extraction of up to f features,
where f is the number of training examples. Accordingly, kernel peA can provide a larger
number of features carrying information about the structure in the data (in our experiments,
we had f > N). In addition, if the structure to be extracted is nonlinear, then linear peA
must necessarily fail , as we have illustrated with toy examples.
These methods, along with depictions of pre-images of vectors in feature space, provide
some understanding of kernel methods which have recently attracted increasing attention.
Open questions include (i) what kind of results kernels other than Gaussians will provide,

542

S. Mika et al.

Figure 4: De-Noising of USPS data (see text). The left half shows: top: the first occurrence
of each digit in the test set, second row: the upper digit with additive Gaussian noise (0' =
0.5), following five rows: the reconstruction for linear PCA using n = 1,4,16,64,256
components, and, last five rows: the results of our approach using the same number of
components. In the right half we show the same but for 'speckle' noise with probability
p = 0.4.
(ii) whether there is a more efficient way to solve either (6) or (8), and (iii) the comparison
(and connection) to alternative nonlinear de-noising methods (cf. [5]).

References
[1] B. Boser, I. Guyon, and V.N. Vapnik. A training algorithm for optimal margin classifiers. In D. Haussler, editor, Proc. COLT, pages 144-152, Pittsburgh, 1992. ACM
Press.
[2] C.J.c. Burges. Simplified support vector decision rules. In L. Saitta, editor, Prooceedings, 13th ICML, pages 71-77, San Mateo, CA, 1996.
[3] K.I. Diamantaras and S.Y. Kung. Principal Component Neural Networks. Wiley, New
York,1996.
[4] T. Hastie and W. Stuetzle. Principal curves. JASA, 84:502-516,1989.
[5] S. Mallat and Z. Zhang. Matching Pursuits with time-frequency dictionaries. IEEE
Transactions on Signal Processing, 41(12):3397-3415, December 1993.
[6] S. Saitoh. Theory of Reproducing Kernels and its Applications. Longman Scientific &
Technical, Harlow, England, 1988.
[7] B. Scholkopf. Support vector learning. Oldenbourg Verlag, Munich, 1997.
[8] B. Scholkopf, P. Knirsch, A. Smola, and C. Burges. Fast approximation of support vector kernel expansions, and an interpretation of clustering as approximation in feature
spaces. In P. Levi et. a1., editor, DAGM'98, pages 124 - 132, Berlin, 1998. Springer.
[9] B. Scholkopf, A.J. Smola, and K.-R. Muller. Nonlinear component analysis as a kernel
eigenvalue problem. Neural Computation, 10:1299-1319,1998.

"
4263,"Exact and Stable Recovery of Pairwise Interaction
Tensors
Shouyuan Chen
Michael R. Lyu Irwin King
The Chinese University of Hong Kong
{sychen,lyu,king}@cse.cuhk.edu.hk

Zenglin Xu
Purdue University
xu218@purdue.edu

Abstract
Tensor completion from incomplete observations is a problem of significant practical interest. However, it is unlikely that there exists an efficient algorithm with
provable guarantee to recover a general tensor from a limited number of observations. In this paper, we study the recovery algorithm for pairwise interaction
tensors, which has recently gained considerable attention for modeling multiple
attribute data due to its simplicity and effectiveness. Specifically, in the absence
of noise, we show that one can exactly recover a pairwise interaction tensor by
solving a constrained convex program which minimizes the weighted sum of nuclear norms of matrices from O(nr log2 (n)) observations. For the noisy cases,
we also prove error bounds for a constrained convex program for recovering the
tensors. Our experiments on the synthetic dataset demonstrate that the recovery
performance of our algorithm agrees well with the theory. In addition, we apply
our algorithm on a temporal collaborative filtering task and obtain state-of-the-art
results.

1

Introduction

Many tasks of recommender systems can be formulated as recovering an unknown tensor (multiway array) from a few observations of its entries [17, 26, 25, 21]. Recently, convex optimization
algorithms for recovering a matrix, which is a special case of tensor, have been extensively studied
[7, 22, 6]. Moreover, there are several theoretical developments that guarantee exact recovery of
most low-rank matrices from partial observations using nuclear norm minimization [8, 5]. These
results seem to suggest a promising direction to solve the general problem of tensor recovery.
However, there are inevitable obstacles to generalize the techniques for matrix completion to tensor
recovery, since a number of fundamental computational problems of matrix is NP-hard in their
tensorial analogues [10]. For instance, H?astad showed that it is NP-hard to compute the rank of a
given tensor [9]; Hillar and Lim proved the NP-hardness to decompose a given tensor into sum of
rank-one tensors even if a tensor is fully observed [10]. The existing evidence suggests that it is
very unlikely that there exists an efficient exact recovery algorithm for general tensors with missing
entries. Therefore, it is natural to ask whether it is possible to identify a useful class of tensors for
which we can devise an exact recovery algorithm.
In this paper, we focus on pairwise interaction tensors, which have recently demonstrated strong
performance in several recommendation applications, e.g. tag recommendation [19] and sequential
data analysis [18]. Pairwise interaction tensors are a special class of general tensors, which directly
model the pairwise interactions between different attributes. Take movie recommendation as an example, to model a user?s ratings for movies varying over time, a pairwise interaction tensor assumes
that each rating is determined by three factors: the user?s inherent preference on the movie, the
movie?s trending popularity and the user?s varying mood over time. Formally, pairwise interaction
tensor assumes that each entry Tijk of a tensor T of size n1 ? n2 ? n3 is given by following
E
D
E D
E D
(a)
(a)
(b)
(b)
(c)
(c)
Tijk = ui , vj
+ uj , vk + uk , vi
, for all (i, j, k) ? [n1 ] ? [n2 ] ? [n3 ], (1)
1

(a)

(a)

(b)

(b)

where {ui }i?[n1 ] , {vi }j?[n2 ] are r1 dimensional vectors, {uj }j?[n2 ] , {vk }k?[n3 ] are r2 di(c)

(c)

mensional vectors and {uk }k?[n3 ] , {vi }i?[n1 ] are r3 dimensional vectors, respectively.

1

The existing recovery algorithms for pairwise interaction tensor use local optimization methods,
which do not guarantee the recovery performance [18, 19]. In this paper, we design efficient recovery algorithms for pairwise interaction tensors with rigorous guarantee. More specifically, in the
absence of noise, we show that one can exactly recover a pairwise interaction tensor by solving a
constrained convex program which minimizes the weighted sum of nuclear norms of matrices from
O(nr log2 (n)) observations, where n = max{n1 , n2 , n3 } and r = max{r1 , r2 , r3 }. For noisy
cases, we also prove error bounds for a constrained convex program for recovering the tensors.
In the proof of our main results, we reformulated the recovery problem as a constrained matrix
completion problem with a special observation operator. Previously, Gross et al. [8] have showed
that the nuclear norm heuristic can exactly recover low rank matrix from a sufficient number of
observations of an orthogonal observation operator. We note that the orthogonality is critical to their
argument. However, the observation operator, in our case, turns out to be non-orthogonal, which
becomes a major challenge in our proof. In order to deal with the non-orthogonal operator, we have
substantially extended their technique in our proof. We believe that our technique can be generalized
to handle other matrix completion problem with non-orthogonal observation operators.
Moreover, we extend existing singular value thresholding method to develop a simple and scalable
algorithm for solving the recovery problem in both exact and noisy cases. Our experiments on the
synthetic dataset demonstrate that the recovery performance of our algorithm agrees well with the
theory. Finally, we apply our algorithm on a temporal collaborative filtering task and obtain stateof-the-art results.

2

Recovering pairwise interaction tensors

In this section, we first introduce the matrix formulation of pairwise interaction tensors and specify
the recovery problem. Then we discuss the sufficient conditions on pairwise interaction tensors
for which an exact recovery would be possible. After that we formulate the convex program for
solving the recovery problem and present our theoretical results on the sample bounds for achieving
an exact recovery. In addition, we also show a quadratically constrained convex program is stable
for the recovery from noisy observations.
A matrix formulation of pairwise interaction tensors. The original formulation of pairwise interaction tensors by Rendle et al. [19] is given by Eq. (1), in which each entry of a tensor is the sum of
inner products of feature vectors. We can reformulate Eq. (1) more concisely using matrix notations.
In particular, we can rewrite Eq. (1) as follows
Tijk = Aij + Bjk + Cki , for all (i, j, k) ? [n1 ] ? [n2 ] ? [n3 ],
(2)
D
E
D
E
D
E
(a)
(a)
(b)
(b)
(c)
(c)
where we set Aij = ui , vj , Bjk = uj , vk , and Cki = uk , vi
for all (i, j, k).
Clearly, matrices A, B and C are rank r1 , r2 and r3 matrices, respectively.
We call tensor T ? Rn1 ?n2 ?n3 a pairwise interaction tensor, which is denoted as T =
Pair(A, B, C), if T obeys Eq. (2). We note that this concise definition is equivalent to the original
one. In the rest of this paper, we will exclusively use the matrix formulation of pairwise interaction
tensors.
Recovery problem. Suppose we have partial observations of a pairwise interaction tensor T =
Pair(A, B, C). We write ? ? [n1 ] ? [n2 ] ? [n3 ] to be the set of indices of m observed entries. In
this work, we shall assume ? is sampled uniformly from the collection of all sets of size m. Our goal
is to recover matrices A, B, C and therefore the entire tensor T from exact or noisy observations of
{Tijk }(ijk)?? .
Before we proceed to the recovery algorithm, we first discuss when the recovery is possible.
Recoverability: uniqueness. The original recovery problem for pairwise interaction tensors is illposed due to a uniqueness issue. In fact, for any pairwise interaction tensor T = Pair(A, B, C),
1

For simplicity, we only consider three-way tensors in this paper.

2

we can construct infinitely manly different sets of matrices A0 , B0 , C0 such that Pair(A, B, C) =
Pair(A0 , B0 , C0 ). For example, we have Tijk = Aij + Bjk + Cki = (Aij + ?ai ) + Bjk + (Cki +
(1 ? ?)ai ), where ? 6= 0 can be any non-zero constant and a is an arbitrary non-zero vector of
0
size n1 . Now, we can construct A0 , B0 and C0 by setting A0ij = Aij + ?ai , Bjk
= Bjk and
0
0
0
0
Cki = Cki + (1 ? ?)ai . It is clear that T = Pair(A , B , C ).
This ambiguity prevents us to recover A, B, C even if T is fully observed, since it is entirely
possible to recover A0 , B0 , C0 instead of A, B, C based on the observations. In order to avoid
this obstacle, we construct a set of constraints such that, given any pairwise interaction tensor Pair(A, B, C), there exists unique matrices A0 , B0 , C0 satisfying the constraints and obeys
Pair(A, B, C) = Pair(A0 , B0 , C0 ). Formally, we prove the following proposition.
Proposition 1. For any pairwise interaction tensor T = Pair(A, B, C), there exists unique A0 ?
SA , B0 ? SB , C0 ? SC such that Pair(A, B, C) = Pair(A0 , B0 , C0 ) where we define SB = {M ?
n2 ?n3
T
R
: 1
M = 0T },SC = {M ? Rn3 ?n1 : 1T M = 0T } and SA = {M ? Rn1 ?n2 : 1T M =

1 T
T
n2 1 M1 1 }.
We point out that there is a natural connection between the uniqueness issue and the ?bias? components, which is a quantity of much attention in the field of recommender system [13]. Due to lack
of space, we defer the detailed discussion on this connection and the proof of Proposition 1 to the
supplementary material.
Recoverability: incoherence. It is easy to see that recovering a pairwise tensor T = Pair(A, 0, 0)
is equivalent to recover the matrix A from a subset of its entries. Therefore, the recovery problem of
pairwise interaction tensors subsumes matrix completion problem as a special case. Previous studies
have confirmed that the incoherence condition is an essential requirement on the matrix in order to
guarantee a successful recovery of matrices. This condition can be stated as follows.
Let M = U?VT be the singular value decomposition of a rank r matrix M. We call matrix M is
(?0 , ?1 )-incoherent if M satisfies:
P
P
2
2
A0. For all i ? [n1 ] and j ? [n2 ], we have nr1 k?[r] Uik
? ?0 and nr2 k?[r] Vjk
? ?0 .
p
A1. The maximum entry of UVT is bounded by ?1 r/(n1 n2 ) in absolute value.
It is well known the recovery is possible only if the matrix is (?0 , ?1 )-incoherent for bounded ?0 , ?1
(i.e, ?0 , ?1 is poly-logarithmic with respect to n). Since the matrix completion problem is reducible
to the recovery problem for pairwise interaction tensors, our theoretical result will inherit the incoherence assumptions on matrices A, B, C.
Exact recovery in the absence of noise. We first consider the scenario where the observations are
exact. Specifically, suppose we are given m observations {Tijk }(ijk)?? , where ? is sampled from
uniformly at random from [n1 ] ? [n2 ] ? [n3 ]. We propose to recover matrices A, B, C and therefore
tensor T = Pair(A, B, C) using the following convex program,
?
?
?
n3 kXk? + n1 kYk? + n2 kZk?
(3)
minimize
X?SA ,Y?SB ,Z?SC

subject to Xij + Yjk + Zki = Tijk , (i, j, k) ? ?,
where kMk? denotes the nuclear norm of matrix M, which is the sum of singular values of M, and
SA , SB , SC is defined in Proposition 1.
We show that, under the incoherence conditions, the above nuclear norm minimization method successful recovers a pairwise interaction tensor T when the number of observations m is O(nr log2 n)
with high probability.
Theorem 1. Let T ? Rn1 ?n2 ?n3 be a pairwise interaction tensor T = Pair(A, B, C) and A ?
SA , B ? SB , C ? SC as defined in Proposition 1. Without loss of generality assume that 9 ? n1 ?
n2 ? n3 . Suppose we observed m entries of T with the locations sampled uniformly at random
from [n1 ] ? [n2 ] ? [n3 ] and also suppose that each of A, B, C is (?0 , ?1 )-incoherent. Then, there
exists a universal constant C, such that if
m > C max{?21 , ?0 }n3 r? log2 (6n3 ),
where r = max{rank(A), rank(B), rank(C)} and ? > 2 is a parameter, the minimizing solution
X, Y, Z for program Eq. (3) is unique and satisfies X = A, Y = B, Z = C with probability at
least 1 ? log(6n3 )6n2??
? 3n2??
.
3
3
3

Stable recovery in the presence of noise. Now, we move to the case where the observations are
perturbed by noise with bounded energy. In particular, our noisy model assumes that we observe
T?ijk = Tijk + ?ijk ,

for all (i, j, k) ? ?,

(4)

where ?ijk is a noise term, which maybe deterministic or stochastic. We assume ? has bounded
energy on ? and specifically that kP? (?)kF ? 1 for some 1 > 0, where P? (?) denotes the
restriction on ?. Under this assumption on the observations, we derive the error bound of the
following quadratically-constrained convex program, which recover T from the noisy observations.
?
?
?
minimize
n3 kXk? + n1 kYk? + n2 kZk?
(5)
X?SA ,Y?SB ,Z?SC




subject to 
P? (Pair(X, Y, Z)) ? P? (T? )
 ? 2 .
F

Theorem 2. Let T = Pair(A, B, C) and A ? SA , B ? SB , C ? SC . Let ? be the set of
observations as described in Theorem 1. Suppose we observe T?ijk for (i, j, k) ? ? as defined in
Eq. (4) and also assume that kP? (?)kF ? 1 holds. Denote the reconstruction error of the optimal
solution X, Y, Z of convex program Eq. (5) as E = Pair(X, Y, Z) ? T . Also assume that 1 ? 2 .
Then, we have
s
2rn1 n22
kEk? ? 5
(1 + 2 ),
8? log(n1 )
with probability at least 1 ? log(6n3 )6n2??
? 3n2??
.
3
3
The proof of Theorem 1 and Theorem 2 is available in the supplementary material.
Related work. Rendle et al. [19] proposed pairwise interaction tensors as a model used for tag recommendation. In a subsequent work, Rendle et al. [18] applied pairwise interaction tensors in the
sequential analysis of purchase data. In both applications, their methods using pairwise interaction
tensor demonstrated excellent performance. However, their algorithms are prone to local optimal
issues and the recovered tensor might be very different from its true value. In contrast, our main results, Theorem 1 and Theorem 2, guarantee that a convex program can exactly or accurately recover
the pairwise interaction tensors from O(nr log2 (n)) observations. In this sense, our work can be
considered as a more effective way to recover pairwise interaction tensors from partial observations.
In practice, various tensor factorization methods are used for estimating missing entries of tensors
[12, 20, 1, 26, 16]. In addition, inspired by the success of nuclear norm minimization heuristics in
matrix completion, several work used a generalized nuclear norm for tensor recovery [23, 24, 15].
However, these work do not guarantee exact recovery of tensors from partial observations.

3

Scalable optimization algorithm

There are several possible methods to solving the optimization problems Eq. (3) and Eq. (5). For
small problem sizes, one may reformulate the optimization problems as semi-definite programs and
solve them using interior point method. The state-of-the-art interior point solvers offer excellent
accuracy for finding the optimal solution. However, these solvers become prohibitively slow for
pairwise interaction tensors larger than 100 ? 100 ? 100. In order to apply the recover algorithms
on large scale pairwise interaction tensors, we use singular value thresholding (SVT) algorithm
proposed recently by Cai et al. [3], which is a first-order method with promising performance for
solving nuclear norm minimization problems.
We first discuss the SVT algorithm for solving the exact completion problem Eq. (3). For convenience, we reformulate the original optimization objective Eq. (3) as follows,
minimize

X?SA ,Y?SB ,Z?SC

subject to

kXk? + kYk? + kZk?
Yjk
Zki
Xij
? + ? + ? = Tijk ,
n3
n1
n2

(6)
(i, j, k) ? ?,

where we have incorporated coefficients on the nuclear norm terms into the constraints. It is easy
?1/2
?1/2
?1/2
to see that the recovered tensor is given by Pair(n3 X, n1 Y, n2 Z), where X, Y, Z is the
4

optimal solution of Eq. (6). Our algorithm solves a slightly relaxed version of the reformulated
objective Eq. (6),

1
2
2
2
minimize
? (kXk? + kYk? + kZk? ) +
kXkF + kYkF + kZkF
(7)
X?SA ,Y?SB ,Z?SC
2
Xij
Yjk
Zki
subject to ? + ? + ? = Tijk , (i, j, k) ? ?.
n3
n1
n2
It is easy to see that Eq. (7) is closely related to Eq. (6) and the original problem Eq. (3), as the
relaxed problem converges to the original one as ? ? ?. Therefore by selecting a large value the
parameter ? , a minimizing solution to Eq. (7) nearly minimizes Eq. (3).
Our algorithm iteratively minimizes Eq. (7) and produces a sequence of matrices {Xk , Yk , Zk }
converging to the optimal solution (X, Y, Z) that minimizes Eq. (7). We begin with several definitions. For observations ? = {ai , bi , ci |i ? [m]}, let operators P?A : Rn1 ?n2 ? Rm ,
P?B : Rn2 ?n3 ? Rm and P?C : Rn3 ?n1 ? Rm represents the influence of X, Y, Z on the
m observations. In particular,
m
1 X
P?A (X) = ?
Xai bi ?i ,
n3 i=1

m
m
1 X
1 X
P?B (Y) = ?
Ybi ci ?i , and P?C (Z) = ?
Zc a ?i .
n1 i=1
n2 i=1 i i
?1/2

?1/2

?1/2

It is easy to verify that P?A (X) + P?B (Y) + P?C (Z) = P? (Pair(n3 X, n1 Y, n2 Z)).
We also denote P?? A be the adjoint operator of P?A and similarly define P?? B and P?? C . Finally, for
a matrix X for size n1 ? n2 , we define center(X) = X ? n11 11T X as the column centering operator
that removes the mean of each n2 columns, i.e., 1T center(X) = 0T .
Starting with y0 = 0 and k = 1, our algorithm iteratively computes
Step (1).

Xk = shrinkA (P?? A (yk?1 ), ? ),
Yk = shrinkB (P?? B (yk?1 ), ? ),
Zk = shrinkC (P?? C (yk?1 ), ? ),
?1/2

Step (2e). ek = P? (T ) ? P? (Pair(n3
k

y =y

k?1

?1/2

X, n1

?1/2

Y, n2

Z))

k

+ ?e .

Here shrinkA is a shrinkage operator defined as follows

2

 
1

?


?
? M
 + ? 
M
shrinkA (M, ? ) , arg min 
M

 .
2
F
?
?
M?S
A

(8)

? belongs SB
Shrinkage operators shrinkB and shrinkC are defined similarly except they require M
and SC , respectively. We note that our definition of the shrinkage operators shrinkA , shrinkB and
? is unconstrained.
shrinkC are slightly different from that of the original SVT [3] algorithm, where M
We can show that our constrained version of shrinkage operators can also be calculated using singular value decompositions of column centered matrices.
Let the SVD of the column centered matrix center(M) be center(M) = U?VT ,
diag({?i }). We can prove that the shrinkage operator shrinkB is given by
shrinkB (M, ? ) = U diag({?i ? ? }+ )VT ,

? =
(9)

where s+ is the positive part of s, that is, s+ = max{0, s}. Since subspace SC is structurally
identical to SB , it is easy to see that the calculation of shrinkC is identical to that of shrinkB . The
computation of shrinkA is a little more complicated. We have
shrinkA (M, ? ) = U diag({?i ? ? }+ )VT + ?

1
({? ? ? }+ + {? + ? }? ) 11T ,
n1 n2

(10)

where U?VT is still the SVD of center(M), ? = ?n11 n2 1T M1 is a constant and s? = min{0, s}
is the negative part of s. The algorithm iterates between Step (1) and Step (2e) and produces a series
of (Xk , Yk , Zk ) converging to the optimal solution of Eq. (7). The iterative procedure terminates
5


 
when the training error is small enough, namely, 
ek 
F ? . We refer interested readers to [3] for
a convergence proof of the SVT algorithm.
The optimization problem for noisy completion Eq. (5) can be solved in a similar manner. We only
need to modify Step (2e) to incorporate the quadratical constraint of Eq. (5) as follows
Step (2n).


?1/2
?1/2
?1/2
ek = P? (T? ) ? P? (Pair(n3 X, n1 Y, n2 Z))





yk
yk?1
ek
= PK
+?
,
?
sk
sk?1

where P? (T? ) is the noisy observations and the cone projection operator PK can be explicitly computed by
?
?
if kxk ? t,
?(x, t)
kxk+t
PK : (x, t) ?
2kxk (x, kxk) if ? kxk ? t ? kxk ,
?
?(0, 0)
if t ? ? kxk .
By iterating between Step (1) and Step (2n) and selecting a sufficiently large ? , the algorithm generates a sequence of {Xk , Yk , Zk } that converges to a nearly optimal solution to the noisy completion
program Eq. (5) [3]. We have also included a detailed description of both algorithms in the supplementary material.
At each iteration, we need to compute one singular value decomposition and perform a few elementary matrix additions. We can see that for each iteration k, Xk vanishes outside of ?A = {ai bi } and
is sparse. Similarly Yk ,Zk are also sparse matrices. Previously, we showed that the computation of
shrinkage operators requires a SVD of a column centered matrix center(M) ? n11 11T X, which is
the sum of a sparse matrix M and a rank-one matrix. Clearly the matrix-vector multiplication of the
form center(M)v can be computed with time O(n + m). This enables the use of Lanczos method
based SVD implementations for example PROPACK [14] and SVDPACKC [2], which only needs
subroutine of calculating matrix-vector products. In our implementation, we develop a customized
version of SVDPACKC for computing the shrinkage operators. Further, for an appropriate choice
of ? , {Xk , Yk , Zk } turned out to be low rank matrices, which matches the observations in the original SVT algorithm [3]. Hence, the storage cost Xk , Yk , Zk can be kept low and we only need to
perform a partial SVD to get the first r singular vectors. The estimated rank r is gradually increased
during the iterations using a similar method suggested in [3, Section 5.1.1]. We can see that, in sum,
the overall complexity per iteration of the recovery algorithm is O(r(n + m)).

4

Experiments

Phase transition in exact recovery. We investigate how the number of measurements affects the
success of exact recovery. In this simulation, we fixed n1 = 100, n2 = 150, n3 = 200 and r1 =
r2 = r3 = r. We tested a variety of choices of (r, m) and for each choice of (r, m), we repeat the
procedure for 10 times. At each time, we randomly generated A ? SA , B ? SB , C ? SC of rank
r. We generated A ? SA by sampling two factor matrices UA ? Rn1 ?r , VA ? Rn2 ?r with i.i.d.
T
standard Gaussian entries and setting A = PSA (UA VA
), where PSA is the orthogonal projection
onto subspace SA . Matrices B ? SB and C ? SC are sampled in a similar way. We uniformly
sampled a subset ? of m entries and reveal them to the recovery algorithm. We deemed A, B, C
successfully recovered if (kAkF + kBkF + kCkF )?1 (kX ? AkF + kY ? BkF + kZ ? CkF ) ?
10?3 , where X, Y and Z are the
? recovered matrices. Finally, we set the parameters ?, ? of the exact
recovery algorithm by ? = 10 n1 n2 n3 and ? = 0.9m(n1 n2 n3 )?1 .
Figure 1 shows the results of these experiments. The x-axis is the ratio between the number of
measurements m and the degree of freedom d = r(n1 + n2 ? r) + r(n2 + n3 ? r) + r(n3 + n1 ? r).
Note that a value of x-axis smaller than one corresponds to a case where there is infinite number of
solutions satisfying given entries. The y-axis is the rank r of the synthetic matrices. The color of
each grid indicates the empirical success rate. White denotes exact recovery in all 10 experiments,
and black denotes failure for all experiments. From Figure 1 (Left), we can see that the algorithm
succeeded almost certainly when the number of measurements is 2.5 times or larger than the degree
of freedom for most parameter settings. We also observe that, near the boundary of m/d ? 2.5,
there is a relatively sharp phase transition. To verify this phenomenon, we repeated the experiments,
6

Figure 1: Phase transition with respect to rank and degree of freedom. Left: m/d ? [1, 5]. Right:
m/d ? [1.5, 3.0].

but only vary m/d between 1.5 and 3.0 with finer steps. The results on Figure 1 (Right) shows that
the phase transition continued to be sharp at a higher resolution.
Stability of recovering from noisy data. In this simulation, we show the recovery performance
with respect to noisy data. Again, we fixed n1 = 100, n2 = 150, n3 = 200 and r1 = r2 = r3 = r
and tested against different choices of (r, m). For each choice of (r, m), we sampled the ground
truth A, B, C using the same method as in the previous simulation. We generated ? uniformly at
random. For each entry (i, j, k) ? ?, we simulated the noisy observation T?ijk = Tijk + ijk , where
ijk is a zero-mean Gaussian random variable with variance ?n2 . Then, we revealed {T?ijk }(ijk)?? to
the noisy recovery algorithm and collect the recovered matrix X, Y, Z. The error of recovery result
is measured by (kX ? AkF + kY ? BkF + kZ ? CkF )/(kAkF + kBkF + kCkF ). We tested the
algorithm with a range of noise levels and for each different configuration of (r, m, ?n2 ), we repeated
the experiments for 10 times and recorded the mean and standard deviation of the relative error.
noise level
0.1
0.2
0.3
0.4
0.5

relative error
0.1020 ? 0.0005
0.1972 ? 0.0007
0.2877 ? 0.0011
0.3720 ? 0.0015
0.4524 ? 0.0015

observations m
m = 3d
m = 4d
m = 5d
m = 6d
m = 7d

relative error
0.1445 ? 0.0008
0.1153 ? 0.0006
0.1015 ? 0.0004
0.0940 ? 0.0007
0.0920 ? 0.0011

rank r
10
20
30
40
50

relative error
0.1134 ? 0.0006
0.1018 ? 0.0007
0.0973 ? 0.0037
0.1032 ? 0.0212
0.1520 ? 0.0344

(a) Fix r = 20, m = 5d and (b) Fix r = 20, 0.1 noise level (c) Fix m = 5d, 0.1 noise level
noise level varies.
and m varies.
and r varies.

Table 1: Simulation results of noisy data.
We present the result of the experiments in Table 1. From the results in Table 1(a), we can see that
the error in the solution is proportional to the noise level. Table 1(b) indicates that the recovery is not
reliable when we have too few observations, while the performance of the algorithm is much more
stable for a sufficient number of observations around four times of the degree of freedom. Table 1(c)
shows that the recovery error is not affected much by the rank, as the number of observations scales
with the degree of freedom in our setting.
Temporal collaborative filtering. In order to demonstrate the performance of pairwise interaction
tensor on real world applications, we conducted experiments on the Movielens dataset. The MovieLens dataset contains 1,000,209 ratings from 6,040 users and 3,706 movies from April, 2000 and
February, 2003. Each rating from Movielens dataset is accompanied with time information provided
in seconds. We transformed each timestamp into its corresponding calendar month. We randomly
select 10% ratings as test set and use the rest of the ratings as training set. In the end, we obtained
a tensor T of size 6040 ? 3706 ? 36, in which the axes corresponded to user, movie and timestamp respectively, with 0.104% observed entries as the training set. We applied the noisy recovery
algorithm on the training set. Following previous studies which applies SVT algorithm on movie
recommendation datasets [11], we used a pre-specified truncation level r for computing SVD in
each iteration, i.e., we only kept top r singular vectors. Therefore, the rank of recovered matrices
are at most r.
7

We evaluated the prediction performance in terms of root mean squared error (RMSE). We compared our algorithm with noisy matrix completion method using standard SVT optimization algorithm [3, 4] to the same dataset while ignore the time information. Here we can regard the noisy
matrix completion algorithm as a special case of the recover a pairwise interaction tensor of size
6040 ? 3706 ? 1, i.e., the time information is ignored. We also noted that the training tensor had
more than one million observed entries and 80 millions total entries. This scale made a number of
tensor recovery algorithms, for example Tucker decomposition and PARAFAC [12], impractical to
apply on the dataset. In contrast, our recovery algorithm took 2430 seconds to finish on a standard
workstation for truncation level r = 100.
The experimental result is shown in Figure 2. The empirical result of Figure 2(a) suggests that, by
incorporating the temporal information, pairwise interaction tensor recovery algorithm consistently
outperformed the matrix completion method. Interestingly, we can see that, for most parameter
settings in Figure 2(b), our algorithm recovered a rank 2 matrix Y representing the change of movie
popularity over time and a rank 15 matrix Z that encodes the change of user interests over time. The
reason of the improvement on the prediction performance may be that the recovered matrix Y and
Z provided meaningful signal. Finally, we note that our algorithm achieves a RMSE of 0.858 when
the truncation level is set to 50, which slightly outperforms the RMSE=0.861 (quote from Figure 7
of the paper) result of 30-dimensional Bayesian Probabilistic Tensor Factorization (BPTF) on the
same dataset, where the authors predict the ratings by factorizing a 6040 ? 3706 ? 36 tensor using
BPTF method [26]. We may attribute the performance gain to the modeling flexibility of pairwise
interaction tensor and the learning guarantees of our algorithm.
1
120

RMSE

0.98
0.96

100

0.94

80

MC

0.92

60

0.9

40

0.88

20

RPIT
20

40

60

80

r3
r2

0

0.86
0

r1

100

20

SVD truncation level

40

60

80

100

120

SVD Truncation Level

(a)

(b)

Figure 2: Empirical results on the Movielens dataset. (a) Comparison of RMSE with different truncation levels. MC: Matrix completion algorithm. RPIT: Recovery algorithm for pairwise interaction
tensor. (b) Rank of recovered matrix X, Y, Z. r1 = rank(X), r2 = rank(Y), r3 = rank(Z).

5

Conclusion

In this paper, we proved rigorous guarantees for convex programs for recovery of pairwise interaction tensors with missing entries, both in the absence and in the presence of noise. We designed a
scalable optimization algorithm for solving the convex programs. We supplemented our theoretical
results with simulation experiments and a real-world application to movie recommendation. In the
noiseless case, simulations showed that the exact recovery almost always succeeded if the number of
observations is a constant time of the degree of freedom, which agrees asymptotically with the theoretical result. In the noisy case, the simulation results confirmed that the stable recovery algorithm
is able to reliably recover pairwise interaction tensor from noisy observations. Our results on the
temporal movie recommendation application demonstrated that, by incorporating the temporal information, our algorithm outperforms conventional matrix completion and achieves state-of-the-art
results.

Acknowledgments
This work was fully supported by the Basic Research Program of Shenzhen (Project No.
JCYJ20120619152419087 and JC201104220300A), and the Research Grants Council of the Hong
Kong Special Administrative Region, China (Project No. CUHK 413212 and CUHK 415212).
8

References
[1] Evrim Acar, Daniel M Dunlavy, Tamara G Kolda, and Morten M?rup. Scalable tensor factorizations for
incomplete data. Chemometrics and Intelligent Laboratory Systems, 106(1):41?56, 2011.
[2] M Berry et al. Svdpackc (version 1.0) user?s guide, university of tennessee tech. Report (393-194, 1993
(Revised October 1996)., 1993.
[3] Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. A singular value thresholding algorithm for matrix
completion. SIAM Journal on Optimization, 20(4):1956?1982, 2010.
[4] Emmanuel J Candes and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE, 98(6):925?
936, 2010.
[5] Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foundations
of Computational mathematics, 9(6):717?772, 2009.
[6] A Evgeniou and Massimiliano Pontil. Multi-task feature learning. 2007.
[7] Maryam Fazel, Haitham Hindi, and Stephen P Boyd. A rank minimization heuristic with application to
minimum order system approximation. In American Control Conference, 2001, 2001.
[8] David Gross, Yi-Kai Liu, Steven T Flammia, Stephen Becker, and Jens Eisert. Quantum state tomography
via compressed sensing. Physical review letters, 105(15):150401, 2010.
[9] Johan H?astad. Tensor rank is np-complete. Journal of Algorithms, 11(4):644?654, 1990.
[10] Christopher Hillar and Lek-Heng Lim. Most tensor problems are np hard. JACM, 2013.
[11] Prateek Jain, Raghu Meka, and Inderjit Dhillon. Guaranteed rank minimization via singular value projection. In NIPS, 2010.
[12] Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):455?
500, 2009.
[13] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30?37, 2009.
[14] Rasmus Munk Larsen. Propack-software for large and sparse svd calculations. Available online., 2004.
[15] Ji Liu, Przemyslaw Musialski, Peter Wonka, and Jieping Ye. Tensor completion for estimating missing
values in visual data. In ICCV, 2009.
[16] Ian Porteous, Evgeniy Bart, and Max Welling. Multi-hdp: A non-parametric bayesian model for tensor
factorization. In AAAI, 2008.
[17] Steffen Rendle, Leandro Balby Marinho, Alexandros Nanopoulos, and Lars Schmidt-Thieme. Learning
optimal ranking with tensor factorization for tag recommendation. In SIGKDD, 2009.
[18] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. Factorizing personalized markov
chains for next-basket recommendation. In WWW, 2010.
[19] Steffen Rendle and Lars Schmidt-Thieme. Pairwise interaction tensor factorization for personalized tag
recommendation. In ICDM, 2010.
[20] Amnon Shashua and Tamir Hazan. Non-negative tensor factorization with applications to statistics and
computer vision. In ICML, 2005.
[21] Yue Shi, Alexandros Karatzoglou, Linas Baltrunas, Martha Larson, Alan Hanjalic, and Nuria Oliver.
Tfmap: Optimizing map for top-n context-aware recommendation. In SIGIR, 2012.
[22] Nathan Srebro, Jason DM Rennie, and Tommi Jaakkola. Maximum-margin matrix factorization. NIPS,
2005.
[23] Ryota Tomioka, Kohei Hayashi, and Hisashi Kashima. Estimation of low-rank tensors via convex optimization. arXiv preprint arXiv:1010.0789, 2010.
[24] Ryota Tomioka, Taiji Suzuki, Kohei Hayashi, and Hisashi Kashima. Statistical performance of convex
tensor decomposition. NIPS, 2011.
[25] Jason Weston, Chong Wang, Ron Weiss, and Adam Berenzweig. Latent collaborative retrieval. ICML,
2012.
[26] Liang Xiong, Xi Chen, Tzu-Kuo Huang, Jeff Schneider, and Jaime G Carbonell. Temporal collaborative
filtering with bayesian probabilistic tensor factorization. In SDM, 2010.

9

"
2181,"Conditional Random Sampling: A Sketch-based
Sampling Technique for Sparse Data
Ping Li
Department of Statistics
Stanford University
Stanford, CA 94305
pingli@stat.stanford.edu
Kenneth W. Church
Microsoft Research
One Microsoft Way
Redmond, WA 98052
church@microsoft.com

Trevor J. Hastie
Department. of Statistics
Stanford University
Stanford, CA 94305
hastie@stanford.edu

Abstract
We1 develop Conditional Random Sampling (CRS), a technique particularly suitable for sparse data. In large-scale applications, the data are often highly sparse.
CRS combines sketching and sampling in that it converts sketches of the data into
conditional random samples online in the estimation stage, with the sample size
determined retrospectively. This paper focuses on approximating pairwise l2 and
l1 distances and comparing CRS with random projections. For boolean (0/1) data,
CRS is provably better than random projections. We show using real-world data
that CRS often outperforms random projections. This technique can be applied in
learning, data mining, information retrieval, and database query optimizations.

1 Introduction
Conditional Random Sampling (CRS) is a sketch-based sampling technique that effectively exploits
data sparsity. In modern applications in learning, data mining, and information retrieval, the datasets
are often very large and also highly sparse. For example, the term-document matrix is often more
than 99% sparse [7]. Sampling large-scale sparse data is challenging. The conventional random
sampling (i.e., randomly picking a small fraction) often performs poorly when most of the samples
are zeros. Also, in heavy-tailed data, the estimation errors of random sampling could be very large.
As alternatives to random sampling, various sketching algorithms have become popular, e.g., random
projections [17] and min-wise sketches [6]. Sketching algorithms are designed for approximating
specific summary statistics. For a specific task, a sketching algorithm often outperforms random
sampling. On the other hand, random sampling is much more flexible. For example, we can use the
same set of random samples to estimate any lp pairwise distances and multi-way associations. Conditional Random Sampling (CRS) combines the advantages of both sketching and random sampling.
Many important applications concern only the pairwise distances, e.g., distance-based clustering
and classification, multi-dimensional scaling, kernels. For a large training set (e.g., at Web scale),
computing pairwise distances exactly is often too time-consuming or even infeasible.
Let A be a data matrix of n rows and D columns. For example, A can be the term-document matrix
with n as the total number of word types and D as the total number of documents. In modern search
engines, n ? 106 ? 107 and D ? 1010 ? 1011 . In general, n is the number of data points and D
is the number of features. Computing all pairwise associations AAT , also called the Gram matrix
in machine learning, costs O(n2 D), which could be daunting for large n and D. Various sampling
methods have been proposed for approximating Gram matrix and kernels [2, 8]. For example, using
T
(normal) random projections [17], we approximate AAT by (AR) (AR) , where the entries of
D?k
2
R?R
are i.i.d. N (0, 1). This reduces the cost down to O(nDk+n k), where k ? min(n, D).
1

The full version [13]: www.stanford.edu/?pingli98/publications/CRS tr.pdf

Sampling techniques can be critical in databases and information retrieval. For example, the
database query optimizer seeks highly efficient techniques to estimate the intermediate join sizes
in order to choose an ?optimum? execution path for multi-way joins.
Conditional Random Sampling (CRS) can be applied to estimating pairwise distances (in any norm)
as well as multi-way associations. CRS can also be used for estimating joint histograms (two-way
and multi-way). While this paper focuses on estimating pairwise l2 and l1 distances and inner
products, we refer readers to the technical report [13] for estimating joint histograms. Our early
work, [11, 12] concerned estimating two-way and multi-way associations in boolean (0/1) data.
We will compare CRS with normal random projections for approximating l2 distances and inner
products, and with Cauchy random projections for approximating l1 distances. In boolean data,
CRS bears some similarity to Broder?s sketches [6] with some important distinctions. [12] showed
that in boolean data, CRS improves Broder?s sketches by roughly halving the estimation variances.

2 The Procedures of CRS
Conditional Random Sampling is a two-stage procedure. In the sketching stage, we scan the data
matrix once and store a fraction of the non-zero elements in each data point, as ?sketches.? In the
estimation stage, we generate conditional random samples online pairwise (for two-way) or groupwise (for multi-way); hence we name our algorithm Conditional Random Sampling (CRS).
2.1 The Sampling/Sketching Procedure
1

2

3

4

5

6

7

1
2
3
4
5
n

8

D

1

2

3

4 5 6 7 8 D

1
2
3
4
5
n

1
2
3
4
5
n

1
2
3
4
5
n

(a) Original

(b) Permuted

(c) Postings

(d) Sketches

Figure 1: A global view of the sketching stage.
Figure 1 provides a global view of the sketching stage. The columns of a sparse data matrix (a)
are first randomly permuted (b). Then only the non-zero entries are considered, called postings (c).
Sketches are simply the front of postings (d). Note that in the actual implementation, we only need
to maintain a permutation mapping on the column IDs.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
u1 0 1 0 2 0 1 0 0 1 2 1 0 1 0 2
u2 1 3 0 0 1 2 0 1 0 0 3 0 0 2 1

(a) Data matrix and random samples
P1 : 2 (1) 4 (2) 6 (1) 9 (1) 10 (2) 11 (1) 13 (1) 15 (2)
P 2 : 1 (1) 2 (3) 5 (1) 6 (2) 8 (1) 11 (3) 14 (2) 15 (1)

K1 : 2 (1) 4 (2) 6 (1) 9 (1) 10 (2)
K 2 : 1 (1) 2 (3) 5 (1) 6 (2) 8 (1) 11 (3)

(b) Postings

(c) Sketches

Figure 2: (a): A data matrix with two rows and D = 15. If the column IDs are random, the first
Ds = 10 columns constitute a random sample. ui denotes the ith row. (b): Postings consist of
tuples ?ID (Value).? (c): Sketches are the first ki entries of postings sorted ascending by IDs. In this
example, k1 = 5, k2 = 6, Ds = min(10, 11) = 10. Excluding 11(3) in K2 , we obtain the same
samples as if we directly sampled the first Ds = 10 columns in the data matrix.
Apparently sketches are not uniformly random samples, which may make the estimation task difficult. We show, in Figure 2, that sketches are almost random samples pairwise (or group-wise).
Figure 2(a) constructs conventional random samples from a data matrix; and we show one can generate (retrospectively) the same random samples from sketches in Figure 2(b)(c).
In Figure 2(a), when the column are randomly permuted, we can construct random samples by simply taking the first Ds columns from the data matrix of D columns (Ds ? D in real applications).
For sparse data, we only store the non-zero elements in the form of tuples ?ID (Value),? a structure
called postings. We denote the postings by Pi for each row ui . Figure 2(b) shows the postings for
the same data matrix in Figure 2(a). The tuples are sorted ascending by their IDs. A sketch, Ki , of
postings Pi , is the first ki entries (i.e., the smallest ki IDs) of Pi , as shown in Figure 2(c).

The central observation is that if we exclude all elements of sketches whose IDs are larger than
Ds = min (max(ID(K1 )), max(ID(K2 ))) ,

(1)

we obtain exactly the same samples as if we directly sampled the first Ds columns from the data
matrix in Figure 2(a). This way, we convert sketches into random samples by conditioning on Ds ,
which differs pairwise and we do not know beforehand.
2.2 The Estimation Procedure
The estimation task for CRS can be extremely simple. After we construct the conditional random
samples from sketches K1 and K2 with the effective sample size Ds , we can compute any distances
(l2 , l1 , or inner products) from the samples and multiply them by DDs to estimate the original space.
(Later, we will show how to improve the estimates by taking advantage of the marginal information.)
We use u?1,j and u
?2,j (j = 1 to Ds ) to denote the conditional random samples (of size Ds ) obtained
by CRS. For example, in Figure 2, we have Ds = 10, and the non-zero u
?1,j and u?2,j are
u
?1,2 = 3, u
?1,4 = 2, u
?1,6 = 1, u
?1,9 = 1, u?1,10 = 2
u
?2,1 = 1, u
?2,2 = 3, u
?2,5 = 1, u
?2,6 = 2, u?2,8 = 1.
Denote the inner product, squared l2 distance, and l1 distance, by a, d(2) , and d(1) , respectively,
a=

D
X

d(2) =

u1,i u2,i ,

i=1

D
X

|u1,i ? u2,i |2 ,

D
X

d(1) =

i=1

|u1,i ? u2,i |

(2)

i=1

Once we have the random samples, we can then use the following simple linear estimators:
a
?MF =

Ds
D X
u
?1,j u?2,j ,
Ds j=1

Ds
D X
(2)
2
d?MF =
(?
u1,j ? u
?2,j ) ,
Ds j=1

Ds
D X
(1)
d?MF =
|?
u1,j ? u
?2,j |. (3)
Ds j=1

2.3 The Computational Cost
Th sketching stage requires generating a random permutation mapping of P
length D, and linear scan
n
all the non-zeros. Therefore, generating sketches for A ? Rn?D costs O( i=1 fi ), where fi is the
number of non-zeros in the ith row, i.e., fi = |Pi |. In the estimation stage, we need to linear scan the
sketches. While the conditional sample size Ds might be large, the cost for estimating the distance
between one pair of data points would be only O(k1 + k2 ) instead of O(Ds ).

3 The Theoretical Variance Analysis of CRS
We give some theoretical analysis on the variances of CRS. For simplicity, we ignore the ?finite
s
population correction factor?, D?D
D?1 , due to ?sample-without-replacement.?
P s
We first consider a
?MF = DDs D
?1,j u
?2,j . By assuming ?sample-with-replacement,? the samples,
j=1 u
(?
u1,j u
?2,j ), j = 1 to Ds , are i.i.d, conditional on Ds . Thus,
Var(?
aM F |Ds ) =
E (?
u1,1 u
?2,1 ) =

?

D
Ds

?2

Ds Var (?
u1,1 u
?2,1 ) =

D
1 X
a
(u1,i u2,i ) = ,
D i=1
D

D
Var(?
aM F |Ds ) =
D
Ds

?
D `
D E (?
u1,1 u
?2,1 )2 ? E2 (?
u1,1 u
?2,1 ) ,
Ds
D
1 X
(u1,i u2,i )2 ,
D i=1
!
D
X
a2
2
2
u1,i u2,i ?
.
D
i=1

E (?
u1,1 u
?2,1 )2 =

D
? a ?2
1 X
(u1,i u2,i )2 ?
D i=1
D

!

=

D
Ds

The unconditional variance would be simply
Var(?
aM F ) = E (Var(?
aM F |Ds )) = E

?

D
Ds

? X
D
i=1

u21,i u22,i

a2
?
D

!

,

(4)
(5)

(6)

?

?

?

?

?

?

? is conditionally unbiased.
? = E Var(X|D
? s ) + Var E(X|D
? s ) = E Var(X|D
? s ) , when X
as Var(X)
 
 


No closed-form expression is known for E DDs ; but we know E DDs ? max kf11 , kf22 (similar
to Jensen?s inequality). Asymptotically (as k1 and k2 increase), the inequality becomes an equality






D
f1 + 1 f2 + 1
f1 f2
E
? max
? max
,
(7)
,
,
Ds
k1
k2
k1 k2
where f1 and f2 are the numbers of non-zeros in u1 and u2 , respectively. See [13] for the proof.
Extensive simulations in [13] verify that the errors of (7) are usually within 5% when k1 , k2 > 20.
(2)
(1)
We similarly derive the variances for d?MF and d?MF . In a summary, we obtain (when k1 = k2 = k)

!
!
? X
D
D
X
D
a2
max(f1 , f2 ) 1
2
2
2
2
2
Var (?
aM F ) = E
u1,i u2,i ?
?
D
u1,i u2,i ? a ,
Ds
D
D
k
i=1
i=1
?
?
??
?
?
?
D
[d(2) ]2
max(f1 , f2 ) 1 ? (4)
(2)
Var d?M F = E
d(4) ?
?
Dd ? [d(2) ]2 ,
Ds
D
D
k
?
??
?
?
?
?
(1) 2
[d
]
max(f
,
f
)
D
1 ? (2)
1
2
(1)
Var d?M F = E
d(2) ?
?
Dd ? [d(1) ]2 .
Ds
D
D
k
?

where we denote d(4) =

PD

(8)
(9)
(10)

4

i=1

(u1,i ? u2,i ) .

1 ,f2 )
1 ,f2 )
The sparsity term max(f
reduces the variances significantly. If max(f
= 0.01, the variances
D
D
can be reduced by a factor of 100, compared to conventional random coordinate sampling.

4 A Brief Introduction to Random Projections
We give a brief introduction to random projections, with which we compare CRS. (Normal) Random
projections [17] are widely used in learning and data mining [2?4].
Random projections multiply the data matrix A ? Rn?D with a random matrix R ? RD?k to
generate a compact representation B = AR ? Rn?k . For estimating l2 distances, R typically
consists of i.i.d. entries in N (0, 1); hence we call it normal random projections. For l1 , R consists
of i.i.d. Cauchy C(0, 1) [9]. However, the recent impossibility result [5] has ruled out estimators
that could be metrics for dimension reduction in l1 .
Denote v1 , v2 ? Rk the two rows in B, corresponding to the original data points u1 , u2 ? RD . We
also introduce the notation for the marginal l2 norms: m1 = ku1 k2 , m2 = ku2 k2 .
4.1 Normal Random Projections
In this case, R consists of i.i.d. N (0, 1). It is easy to show that the following linear estimators of
the inner product a and the squared l2 distance d(2) are unbiased
a
?N RP,MF =

1 T
v v2 ,
k 1

1
(2)
d?N RP,MF = kv1 ? v2 k2 ,
k

(11)

with variances [15, 17]
Var (?
aN RP,MF ) =


1
m 1 m 2 + a2 ,
k


 2[d(2) ]2
(2)
Var d?N RP,MF =
.
k

(12)

Assuming that the margins m1 = ku1 k2 and m2 = ku2 k2 are known, [15] provides a maximum
likelihood estimator, denoted by a
?N RP,MLE , whose (asymptotic) variance is
Var (?
aN RP,MLE ) =

1 (m1 m2 ? a2 )2
+ O(k ?2 ).
k m 1 m 2 + a2

(13)

4.2 Cauchy Random Projections for Dimension Reduction in l1
In this case, R consisting of i.i.d. entries in Cauchy C(0, 1). [9] proposed an estimator based on the
absolute sample median. Recently, [14] proposed a variety of nonlinear estimators, including, a biascorrected sample median estimator, a bias-corrected geometric mean estimator, and a bias-corrected

maximum likelihood estimator. An analog of the Johnson-Lindenstrauss (JL) lemma for dimension
reduction in l1 is also proved in [14], based on the bias-corrected geometric mean estimator.
We only list the maximum likelihood estimator derived in [14], because it is the most accurate one.


1 ?(1)
(1)
dCRP,MLE ,
(14)
d?CRP,MLE,c = 1 ?
k
(1)
where d?CRP,MLE solves a nonlinear MLE equation

?

k
(1)
d?CRP,M LE

+

k
X
j=1

2d?CRP,M LE
?
?2 = 0.
(1)
? v2,j )2 + d?CRP,M LE
(1)

(v1,j

[14] shows that

(15)

 

 2[d(1) ]2
3[d(1) ]2
1
(1)
Var d?CRP,MLE,c =
+
+
O
.
k
k2
k3

(16)

4.3 General Stable Random Projections for Dimension Reduction in lp (0 < p ? 2)
[10] generalized the bias-corrected geometric mean estimator to general stable random projections
for dimension reduction in lp (0 < p ? 2), and provided the theoretical variances and exponential
tail bounds. Of course, CRS can also be applied to approximating any lp distances.

5 Improving CRS Using Marginal Information
It is often reasonable to assume that we know the marginal information such as marginal l2 norms,
numbers of non-zeros, or even marginal histograms. This often leads to (much) sharper estimates,
by maximizing the likelihood under marginal constraints. In the boolean data case, we can express
the MLE solution explicitly and derive a closed-form (asymptotic) variance. In general real-valued
data, the joint likelihood is not available; we propose an approximate MLE solution.
5.1 Boolean (0/1) Data
In 0/1 data, estimating the inner product becomes estimating a two-way contingency table, which
has four cells. Because of the margin constraints, there is only one degree of freedom. Therefore, it
is not hard to show that the MLE of a is the solution, denoted by a
?0/1,MLE , to a cubic equation
s11
s10
s01
s00
?
?
+
= 0,
(17)
a
f1 ? a f2 ? a D ? f1 ? f2 + a
where s11 = #{j : u
?1,j = u
?2,j = 1}, s10 = #{j : u
?1,j = 1, u
?2,j = 0}, s01 = #{j : u?1,j =
0, u
?2,j = 1}, s00 = #{j : u
?1,j = 0, u
?2,j = 0}, j = 1, 2, ..., Ds .
The (asymptotic) variance of a
?0/1,MLE is proved [11?13] to be


D
1
Var(?
a0/1,MLE ) = E
1
1
1
Ds a + f1 ?a + f2 ?a +

1
D?f1 ?f2 +a

.

(18)

5.2 Real-valued Data
A practical solution is to assume some parametric form of the (bivariate) data distribution based
on prior knowledge; and then solve an MLE considering various constraints. Suppose the samples
(?
u1,j , u?2,j ) are i.i.d. bivariate normal with moments determined by the population moments, i.e.,
?

v?1,j
v?2,j

?

=

? = 1 Ds
?
Ds D

?

?

u
?1,j ? u
?1
u
?2,j ? u
?2

?

?N

ku1 k2 ? D?
u21
uT1 u2 ? D?
u1 u
?2

??

0
0

?

?
? ,
,?

uT1 u2 ? D?
u1 u
?2
ku2 k2 ? D?
u22

(19)
?

=

1
Ds

?

m
?1
a
?

a
?
m
?2

?

,

(20)

PD
PD
where u
?1 =  i=1 u1,i /D, u?2 =
?1 =
i=1 u2,i /D are the population
 means. m
Ds
Ds
Ds
2
2
2
2
T
ku
k
?
D?
u
,
m
?
=
ku
k
?
D?
u
2
,
a
?
=
u
u
?
D?
u
u
?
.
Suppose
that
u?1 ,
1
2
2
2
1
2
1
1
D
D
D
u
?2 , m1 = ku1 k2 and m2 = ku2 k2 are known, an MLE for a = uT1 u2 , denoted by a
?MLE,N , is
D?
a
?MLE,N =
a
? + D?
u1 u
?2 ,
(21)
Ds

?
where, similar to Lemma 2 of [15], a
? is the solution to a cubic equation:


3
2
T
a
? ?a
? v?1 v?2 + a
? ?m
? 1m
?2 +m
? 1 k?
v2 k2 + m
? 2 k?
v1 k2 ? m
? 1m
? 2 v?1T v?2 = 0.

(22)

a
?MLE,N is fairly robust, although sometimes we observe the biases are quite noticeable. In general,
this is a good bias-variance trade-off (especially when k is not too large). Intuitively, the reason why
this (seemly crude) assumption of bivariate normality works well is because, once we have fixed the
margins, we have removed to a large extent the non-normal component of the data.

6 Theoretical Comparisons of CRS With Random Projections
As reflected by their variances, for general data types, whether CRS is better than random projections depends on two competing factors: data sparsity and data heavy-tailedness. However, in the
following two important scenarios, CRS outperforms random projections.
6.1 Boolean (0/1) data
In this case, the marginal norms are the same as the numbers of non-zeros, i.e., mi = kui k2 = fi .
Figure 3 plots the ratio,

Var(?
aM F )
Var(?
aN RP,M F ) ,

verifying that CRS is (considerably) more accurate:

Var (?
aMF )
max(f1 , f2 )
=
Var (?
aN RP,MF )
f 1 f 2 + a2

1
1
a

+

1
D?a

?

max(f1 , f2 )a
? 1.
f 1 f 2 + a2

Var(a
? 0/1,M LE )
Figure 4 plots Var(?aN RP,M LE ) . In most possible range of the data, this ratio is less than 1. When
u1 and u2 are very close (e.g., a ? f2 ? f1 ), random projections appear more accurate. However,
when this does occur, the absolute variances are so small (even zero) that their ratio does not matter.

f1 = 0.95D

0.2
0
0

0.2

0.4

a/f

0.6

0.8

0.4
0.2
0
0

1

0.8

1
f /f = 0.8
2 1

Variance ratio

f1 = 0.05D

0.4

0.6

1
f2/f1 = 0.5

Variance ratio

0.6

0.8
f2/f1 = 0.2

Variance ratio

Variance ratio

1
0.8

0.6
0.4
0.2

0.2

0.4

2

a/f

0.6

0.8

0
0

1

0.8

f2/f1 = 1

0.6
0.4
0.2

0.2

0.4

2

a/f

0.6

0.8

0
0

1

0.2

2

0.4

a/f

0.6

0.8

1

2

Var(?
aM F )
Figure 3: The variance ratios, Var(?
aN RP,M F ) , show that CRS has smaller variances than random
projections, when no marginal information is used. We let f1 ? f2 and f2 = ?f1 with ? =
0.2, 0.5, 0.8, 1.0. For each ?, we plot from f1 = 0.05D to f1 = 0.95D spaced at 0.05D.

0.4
0.2
0
0

f1 = 0.05 D

1

0.4

0.6
0.4
0.2

f = 0.95 D

0.2

1
f2/f1 = 0.5

Variance ratio

0.6

0.8

a/f

0.6

0.8

1

0
0

0.8

3
f2/f1 = 0.8

2.5
Variance ratio

1
f2/f1 = 0.2

Variance ratio

Variance ratio

1
0.8

0.6
0.4
0.2

0.2

2

0.4

a/f

2

0.6

0.8

1

0
0

f2/f1 = 1

2
1.5
1
0.5

0.2

0.4

a/f

2

0.6

0.8

1

0
0

0.2

0.4
a/f

0.6

0.8

2

Var(a
?0/1,M LE )
Figure 4: The ratios, Var(?aN RP,M LE ) , show that CRS usually has smaller variances than random
projections, except when f1 ? f2 ? a.

6.2 Nearly Independent Data
Suppose two data points u1 and u2 are independent (or less strictly, uncorrelated to the second
order), it is easy to show that the variance of CRS is always smaller:
max(f1 , f2 ) m1 m2
m 1 m 2 + a2
Var (?
aMF ) ?
? Var (?
aN RP,MF ) =
,
(23)
D
k
k
even if we ignore the data sparsity. Therefore, CRS will be much better for estimating inner products
in nearly independent data. Once we have obtained the inner products, we can infer the l2 distances
easily by d(2) = m1 + m2 ? 2a, since the margins, m1 and m2 , are easy to obtain exactly.
In high dimensions, it is often the case that most of the data points are only very weakly correlated.

6.3 Comparing the Computational Efficiency
As previously mentioned,
the cost of constructing sketches for A ? Rn?D would be O(nD) (or
Pn
more precisely, O( i=1 fi )). The cost of (normal) random projections would be O(nDk), which
can be reduced to O(nDk/3) using sparse random projections [1]. Therefore, it is possible that
CRS is considerably more efficient than random projections in the sampling stage.2
In the estimation stage, CRS costs O(2k) to compute the sample distance for each pair. This cost is
only O(k) in random projections. Since k is very small, the difference should not be a concern.

7 Empirical Evaluations
We compare CRS with random projections (RP) using real data, including n = 100 randomly
sampled documents from the NSF data [7] (sparsity ? 1%), n = 100 documents from the NEWSGROUP data [4] (sparsity ? 1%), and one class of the COREL image data (n = 80, sparsity ? 5%).
We estimate all pairwise inner products, l1 and l2 distances, using both CRS and RP. For each pair,
we obtain 50 runs and average the absolute errors. We compare the median errors and the percentage
in which CRS does better than random projections.
The results are presented in Figures 5, 6, 7. In each panel, the dashed curve indicates that we sample
each data point with equal sample size (k). For CRS, we can adjust the sample size according to
the sparsity, reflected by the solid curves. We adjust sample sizes only roughly. The data points are
divided into 3 groups according to sparsity. Data in different groups are assigned different sample
sizes for CRS. For random projections, we use the average sample size.
For both NSF and NEWSGROUP data, CRS overwhelmingly outperforms RP for estimating inner
products and l2 distances (both using the marginal information). CRS also outperforms RP for
approximating l1 and l2 distances (without using the margins).

Ratio of median errors

For the COREL data, CRS still outperforms RP for approximating inner products and l2 distances
(using the margins). However, RP considerably outperforms CRS for approximating l1 distances
and l2 distances (without using the margins). Note that the COREL image data are not too sparse
and are considerably more heavy-tailed than the NSF and NEWSGROUP data [13].
0.06

0.8
Inner product

1

0.6

0.8

0.4

0.03

0.06
0.04

0.6
0.02
10

20
30
40
Sample size k

0.2
50
10

20
30
40
Sample size k

50

Inner product
0.98

0.999

0.96

10

20
30
40
Sample size k

50

0.02
10

20
30
40
Sample size k

50

1

1

0.9995

L2 distance (Margins)

0.1

L2 distance

0.08

0.04

1
Percentage

0.12
L1 distance

0.05

0.8

1

L2 distance

L2 distance (Margins)

0.9998
0.6

L1 distance

0.9996
0.4
0.9994

0.9985
10

20
30
40
Sample size k

50

0.94
10

20
30
40
Sample size k

50

0.2
10

20
30
40
Sample size k

50

10

20
30
40
Sample size k

50

Figure 5: NSF data. Upper four panels: ratios (CRS over RP ( random projections)) of the median
absolute errors; values < 1 indicate that CRS does better. Bottom four panels: percentage of pairs
for which CRS has smaller errors than RP; values > 0.5 indicate that CRS does better. Dashed
curves correspond to fixed sample sizes while solid curves indicate that we (crudely) adjust sketch
sizes in CRS according to data sparsity. In this case, CRS is overwhelmingly better than RP for
approximating inner products and l2 distances (both using margins).

8 Conclusion
There are many applications of l1 and l2 distances on large sparse datasets. We propose a new
sketch-based method, Conditional Random Sampling (CRS), which is provably better than random
projections, at least for the important special cases of boolean data and nearly independent data. In
general non-boolean data, CRS compares favorably, both theoretically and empirically, especially
when we take advantage of the margins (which are easier to compute than distances).
2

?
[16] proposed very sparse random projections to reduce the cost O(nDk) down to O(n Dk).

Ratio of median errors

0.2

0.7
Inner product

0.6

0.15

1.2
L1 distance

0.2
L2 distance

1

0.15

0.8

0.1

0.5
0.1

0.4

L2 distance (Margins)
0.05
10

20
Sample size k

0.3
10

30

20
Sample size k

30 0.610

Inner product

Percentage

Sample size k

30

0.05
10

1

1

1

20

0.8

20
Sample size k

1
L2 distance

0.995

20
Sample size k

0.9
30
10

20
Sample size k

0.995

0.4

L1 distance
0.985
10

L2 distance (Margins)

0.6

0.95
0.99

30

30

0.2
10

20
Sample size k

30

0.99
10

20
Sample size k

30

Ratio of median errors

Figure 6: NEWSGROUP data. The results are quite similar to those in Figure 5 for the NSF data.
In this case, it is more obvious that adjusting sketch sizes helps CRS.
0.3
0.29

2

4.5

0.8
L2 distance

L1 distance

Inner product
1.9

0.7
0.6

0.28

1.8

3.5

0.27

0.5

1.7
0.26
10

20
30
40
Sample size k

Percentage

0.9

50

10

0.4
20
30
40
Sample size k

0.04
0.03

0.85
Inner product
0.8
0.75
10

L2 distance (Margins)

4

20
30
40
Sample size k

50

3
10

20
30
40
Sample size k

50

0.1
L1 distance

0.05

10

0.8

L2 distance

0.02

0

0.7

0.01

?0.05

0.6

50

0
10

20
30
40
Sample size k

?0.1
50
10

20
30
40
Sample size k

50

0.9

20
30
40
Sample size k

50

0.5
10

L2 distance (Margins)

20
30
40
Sample size k

50

Figure 7: COREL image data.

Acknowledgment
We thank Chris Burges, David Heckerman, Chris Meek, Andrew Ng, Art Owen, Robert Tibshirani,
for various helpful conversations, comments, and discussions. We thank Ella Bingham, Inderjit
Dhillon, and Matthias Hein for the datasets.

References
[1] D. Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. Journal of Computer and System
Sciences, 66(4):671?687, 2003.
[2] D. Achlioptas, F. McSherry, and B. Sch?olkopf. Sampling techniques for kernel methods. In NIPS, pages 335?342, 2001.
[3] R. Arriaga and S. Vempala. An algorithmic theory of learning: Robust concepts and random projection. Machine Learning, 63(2):161?
182, 2006.
[4] E. Bingham and H. Mannila. Random projection in dimensionality reduction: Applications to image and text data. In KDD, pages
245?250, 2001.
[5] B. Brinkman and M. Charikar. On the impossibility of dimension reduction in l1 . Journal of ACM, 52(2):766?788, 2005.
[6] A. Broder. On the resemblance and containment of documents. In the Compression and Complexity of Sequences, pages 21?29, 1997.
[7] I. Dhillon and D. Modha. Concept decompositions for large sparse text data using clustering. Machine Learning, 42(1-2):143?175, 2001.
[8] P. Drineas and M. Mahoney. On the nystrom method for approximating a gram matrix for improved kernel-based learning. Journal of
Machine Learning Research, 6(Dec):2153?2175, 2005.
[9] P. Indyk. Stable distributions, pseudorandom generators, embeddings and data stream computation. In FOCS, pages 189?197, 2000.
[10] P. Li. Very sparse stable random projections, estimators and tail bounds for stable random projections. Technical report, http:
//arxiv.org/PS cache/cs/pdf/0611/0611114.pdf, 2006.
[11] P. Li and K. Church. Using sketches to estimate associations. In HLT/EMNLP, pages 708?715, 2005.
[12] P. Li and K. Church. A sketch algorithm for estimating two-way and multi-way associations. Computational Linguistics, To Appear.
[13] P. Li, K. Church, and T. Hastie. Conditional random sampling: A sketched-based sampling technique for sparse data. Technical Report
2006-08, Department of Statistics, Stanford University), 2006.
[14] P. Li, K. Church, and T. Hastie. Nonlinear estimators and tail bounds for dimensional reduction in l1 using Cauchy random projections.
(http://arxiv.org/PS cache/cs/pdf/0610/0610155.pdf), 2006.
[15] P. Li, T. Hastie, and K. Church. Improving random projections using marginal information. In COLT, pages 635?649, 2006.
[16] P. Li, T. Hastie, and K. Church. Very sparse random projections. In KDD, pages 287?296, 2006.
[17] S. Vempala. The Random Projection Method. American Mathematical Society, Providence, RI, 2004.

"
1347,"A Differential Semantics for Jointree
Algorithms

James D. P ark and Adnan Darwiche
Computer Science Department
Univ ersity of California, Los Angeles, CA 90095
{jd,darwiche}@cs.ucla.edu

Abstract
A new approach to inference in belief networks has been recently
proposed, which is based on an algebraic representation of belief
networks using multi?linear functions. According to this approach,
the key computational question is that of representing multi?linear
functions compactly, since inference reduces to a simple process of
ev aluating and differentiating such functions. W e show here that
mainstream inference algorithms based on jointrees are a special
case of this approach in a v ery precise sense. W e use this result to
prov e new properties of jointree algorithms, and then discuss some
of its practical and theoretical implications.

1

Introduction

It was recently shown that the probability distribution of a belief network can be
represented using a multi?linear function, and that most probabilistic queries of
interest can be retriev ed directly from the partial deriv ativ es of this function [2].
Although the multi?linear function has an exponential number of terms, it can
be represented using a small arithmetic circuit in certain situations [3].1 Once
a belief network is represented as an arithmetic circuit, probabilistic inference is
then performed by ev aluating and differentiating the circuit, using a v ery simple
procedure which resembles back?propagation in neural networks.
W e show in this paper that mainstream inference algorithms based on jointrees [14,
8] are a special-case of the arithmetic?circuit approach proposed in [2]. Specifically,
we show that each jointree is an implicit representation of an arithmetic circuit; that
the inward?pass in jointree propagation ev aluates this circuit; and that the outward?
pass differentiates the circuit. Using these results, we prov e new useful properties
of jointree propagation. W e also suggest a new interpretation of the process of
factoring graphical models into jointrees, as a process of factoring exponentially?
sized multi?linear functions into arithmetic circuits of smaller size.
1
For example, it was shown recently that real?world b elief networks with treewidth up
to 60 can b e compiled into arithmetic circuits with few thousand nodes [3]. Such networks
hav e local structure, and are outside the scope of mainstream algorithms for inference in
b elief networks whose complexity is exponential in treewidth.

A
true
true
false
false

B
true
false
true
false

?b|a
??
b|a
?b|?
a
??
b|?
a

=
=
=
=

.2
.8
.7
.3

A
true
false

?a = .6
?a
? = .4

A
true
true
false
false

C
true
false
true
false

?c|a
?c?|a
?c|?
a
?c?|?
a

=
=
=
=

.8
.2
.15
.85

Figure 1: The CPTs of belief network B ? A ? C.
This paper is structured as follows. Sections 2 and 3 are dedicated to a review of
inference approaches based on arithmetic circuits and jointrees. Section 4 shows
that the jointree approach is a special case of the arithmetic?circuit approach, and
discusses some practical implications of this finding. Finally, Section 5 closes with a
new perspective on factoring graphical models. Proofs of all theorems can be found
in the long version of this paper [11].

2

Belief netw orks as m ulti?linear functions

A belief network is a factored representation of a probability distribution. It consists
of two parts: a directed acyclic graph (D A G) and a set of conditional probability
tables (CPTs). For each node X and its parents U, we have a CPT that specifies
the distribution of X given each instantiation u of the parents; see Figure 1.2
A belief network is a representational factorization of a probability distribution,
not a computational one. That is, although the network compactly represents the
distribution, it needs to be processed further if one is to obtain answers to arbitrary
probabilistic queries. Mainstream algorithms for inference in belief networks operate on the network to generate a computational factorization, allowing one to answer
queries in time which is linear in the factorization size. A most influential computational factorization of belief networks is the jointree [14, 8, 6]. Standard jointree
factorizations are structure?based: their size depend only on the network topology and is invariant to local CPT structure. This observation has triggered much
research for alternative, finer?grained factorizations, since real-world networks can
exhibit significant local structure that leads to smaller factorizations if exploited.
W e discuss next one of the latest proposals in this direction, which calls for using
arithmetic circuits as a computational factorization of belief networks [2]. This
proposal is based on viewing each belief network as a multi?linear function, which
can be represented compactly using an arithmetic circuit. The multi?linear function
itself contains two types of variables. First, evidence indicators, where for each
variable X in the network , we have a variable ?x for each value x of X. Second,
network parameters, where for each variable X and its parents U in the network,
we have a variable ?x|u for each value x of X and instantiation u of U.
The multi?linear function has a term for each instantiation of the network variables,
which is constructed by multiplying all evidence indicators and network parameters
that are consistent with that instantiation. For example, the multi?linear function
of the network in Figure 1 has eight terms corresponding to the eight instantiations
of variables A, B, C: f = ?a ?b ?c ?a ?b|a ?c|a +?a ?b ?c??a ?b|a ?c?|a +. . .+?a? ??b ?c??a? ??b|?a ?c?|?a .
W e will often refer to such a multi?linear function as the network polynomial.
2

Variables are denoted by upper?case letters (A) and their v alues by lower?case letters
(a). Sets of v ariables are denoted by bold?face upper?case letters (A) and their instantiations are denoted by bold?face lower?case letters (a). For a v ariable A with v alues true
and false, we use a to denote A= true and a
? to denote A= false. Finally , for a v ariable X
and its parents U, we use ?x|u to denote the CPT entry corresponding to Pr (x | u).

+
?B?D?D|BC

*

*

A
BCD

+
*

+
*

*

B

C

D

E

*

      
     
 

ABC

?A?A?B|A?C|A

CE

?C?E?E|C



Figure 2: On the left: An arithmetic circuit which computes the function
?a ?b ?a ?b|a + ?a ??b ?a ??b|a + ?a? ?b ?a? ?b|?a + ?a? ??b ?a? ??b|?a . The circuit is a D AG, where
leaf nodes represent function variables and internal nodes represent arithmetic operations. On the right: A belief network structure and its corresponding jointree.

Given the network polynomial f , we can answer any query with respect to the
belief network. Specifically, let e be an instantiation of some network variables,
and suppose we want to compute the probability of e. W e can do this by simply
evaluating the polynomial f while setting each evidence indicator ?x to 1 if x
is consistent with e, and to 0 otherwise. For the network in Figure 1, we can
compute the probability of evidence e = b?
c by evaluating its polynomial above under
?a = 1,?a? = 1,?b = 1, ??b = 0 and ?c = 0, ?c? = 1. This leads to ?a ?b|a ?c?|a +?a? ?b|?a ?c?|?a ,
which equals the probability of b, c? in this case. W e use f (e) to denote the result
of evaluating the polynomial f under evidence e as given above.
This algebraic representation of belief networks is attractive as it allows us to obtain
answers to a large number of probabilistic queries directly from the derivatives of
the network polynomial [2]. For example, the posterior marginal Pr (x|e) for a
? f (e)
1 ? f (e)
variable X 6? E equals f (e)
? ?x , where ? ?x is the partial derivative of f wrt ?x
evaluated at e. Second, the probability of evidence e after having retracted the
P
. Third, the posterior
value of some variable X from e, Pr (e ? X), equals x ? ?f?(e)
x
marginal Pr (x, u|e) for a variable X and its parents U equals

?x|u ? f (e)
f (e) ? ?x|u .

The network polynomial has an exponential number of terms, yet one can represent
it compactly in certain cases using an arithmetic circuit; see Figure 2. The (first)
partial derivatives of an arithmetic circuit can all be computed simultaneously in
time linear in the circuit size [2, 12]. The procedure resembles the back?propagation
algorithm for neural networks as it evaluates the circuit in a single upward?pass,
and then differentiates it through a single downward?pass.
The main computational question is then that of generating the smallest arithmetic
circuit that computes the network polynomial. A structure?based approach for
this has been given in [2], which is guaranteed to generate a circuit whose size is
bounded by O(n exp(w)), where n is the number of nodes in the network and w
is its treewidth. A more recent approach, however, which exploits local structure
has been presented in [3] and was shown experimentally to generate small arithmetic circuits for networks whose treewidth is up to 60. As we show in the rest of
this paper, the process of factoring a belief network into a jointree is yet another
method for generating an arithmetic circuit for the network. Specifically, we show
that the jointree structure is an implicit representation of such a circuit, and that
jointree propagation corresponds to circuit evaluation and differentiation. Moreover, the difference between Shenoy?Shafer and Hugin propagation turns out to be
a difference in the numeric scheme used for circuit differentiation [11].

3

Join tree Algorithms

We now review jointree algorithms, which are quite influential in graphical models.
Let B be a belief network. A jointree for B is a pair (T , L), where T is a tree and
L is a function that assigns labels to nodes in T . A jointree must satisfy three
properties: (1) each label L(i) is a set of variables in the belief network; (2) each
network variable X and its parents U (a family) must appear together in some label
L(i); (3) if a variable appears in the labels of i and j, it must also appear in the
label of each node k on the path connecting them. The label of edge ij in T is
defined as L(i) ? L(j). We will refer to the nodes of a jointree (and sometimes their
labels) as clusters. We will also refer to the edges of a jointree (and sometimes their
labels) as separators. Figure 2 depicts a belief network and one of its jointrees.
Jointree algorithms start by constructing a jointree for a given belief network [14, 8,
6]. They also associate tables (also called potentials) with clusters and separators.3
The conditional probability table (CPT or CP Table) of each variable X with parents
U, denoted ?X|U , is assigned to a cluster that contains X and U. In addition, an
evidence table over variable X, denoted ?X , is assigned to a cluster that contains X.
Figure 2 depicts the assignments of evidence and CP tables to clusters. Evidence e
is entered into a jointree by initializing evidence tables as follows: we set ?X (x) to
1 if x is consistent with evidence e, and we set ?X (x) to 0 otherwise.
Given some evidence e, a jointree algorithm propagates messages between clusters.
After passing two message per edge in the jointree, one can compute the marginals
Pr (C, e) for every cluster C. There are two main methods for propagating messages
in a jointree: the Shenoy?Shafer architecture [14] and the Hugin architecture [8].
Shenoy?Shafer propagation proceeds as follows [14]. First, evidence e is then entered
into the jointree. A cluster is then selected as the root and message propagation
proceeds in two phases, inward and outward. In the inward phase, messages are
passed toward the root. In the outward phase, messages are passed away from the
root. Cluster i sends a message to cluster j only when it has received messages
from all its other neighborsPk. A message
from cluster i to cluster j is a table Mij
Q
defined as follows: Mij = C\S ?i k6=j Mki , where C are the variables of cluster
i, S are the variables of separator ij, and ?i is the multiplication of all evidence
and CP tablesQ
assigned to cluster i. Once message propagation is finished, we have
Pr (C, e) = ?i k Mki , where C are the variables of cluster i.
Hugin propagation proceeds similarly to Shenoy?Shafer by entering evidence; selecting a cluster as root; and propagating messages in two phases, inward and outward
[8]. The Hugin method, however, differs in some major ways. It maintains a table
?ij with each separator, whose entries are initialized to 1s. It also maintains a table
?i with each cluster i, initialized to the multiplication of all CPTs and evidence
tables assigned to cluster i. Cluster i passes a message to neighboring cluster j only
when i has received messages from all its other neighbors k. When cluster i is ready
to send a message to cluster j, it does the following. First, it saves thePtable of
separator ?ij into ?old
ij . Second, it computes a new separator table ?ij =
C\S ?i ,
where C are the variables of cluster i and S are the variables of separator ij. Third,
?ij
. Finally, it multiplies the computed
it computes a message to cluster j: Mij = ?old
ij

message into the table of cluster j: ?j = ?j Mij . After the inward and outward?
passes of Hugin propagation are completed, we have: Pr (C, e) = ?i , where C are
the variables of cluster i.
3
A table is an array which is indexed by v ariable instantiations. Sp ecifically , a table ?
ov er v ariables X is indexed by the instantiations x of X. Its entries ?(x) are in [0, 1].

4

Join trees as arithmetic circuits

We now show that every jointree (together with a root cluster and a particular
assignment of evidence and CP tables to clusters) corresponds precisely to an arithmetic circuit that computes the network polynomial. We also show that the inward?
pass of the Shenoy?Shafer architecture evaluates this circuit, while the outward?pass
differentiates it. We show a similar result for the Hugin architecture.
Definition 1 Given a root cluster, a particular assignment of evidence and CP
tables to clusters, the arithmetic circuit embedded in a jointree is defined as follows:4
Nodes: The circuit includes: an output addition node f ; an addition node s for each
instantiation of a separator S; a multiplication node c for each instantiation of a
cluster C; an input node ?x for each instantiation x of variable X; an input node
?x|u for each instantiation xu of family XU.
Edges: The children of the output node f are the multiplication nodes generated by
the root cluster; the children of an addition node s are all compatible nodes generated
by the child cluster; the children of a multiplication node c are all compatible nodes
generated by child separators, and all compatible input nodes assigned to cluster C.
Hence, separators contribute addition nodes and clusters contribute multiplication
nodes. Moreover, the structure of the jointree dictates how these nodes are connected into a circuit. The arithmetic circuit in Figure 2 is embedded in the jointree
A ? AB, with cluster A as the root, and with tables ?A , ?A assigned to cluster A
and tables ?B and ?B|A assigned to cluster B.
Theorem 1 The circuit embedded in a jointree computes the network polynomial.
Therefore, by constructing a jointree one is generating a compact representation of
the network polynomial in terms of an arithmetic circuit.
We are now ready to state our basic results on the differential semantics of jointree
propagation, but we need some notational conventions first. In the following three
theorems: f denotes the circuit embedded in a jointree or its (unique) output node;
s denotes a separator instantiation or the addition node generated by that instantiation; and c denotes a cluster instantiation or the multiplication node generated by
that instantiation. Moreover, the value that a circuit node v tak es under evidence
e is denoted v(e). Recall that a circuit (or network polynomial) is evaluated under
evidence e by setting each input ?x to 1 if x is consistent with e; and to 0 otherwise. Finally, recall that ? f /? v represents the derivative of the circuit output with
respect to node v. Our first result relates to Shenoy?Shafer propagation.
Theorem 2 The messages produced using Shenoy?Shafer propagation on a jointree
under evidence e have the following semantics. F or each inward message Mij , we
have Mij (s) = s(e). F or each outward message Mji , we have Mji (s) = ?f?s(e) .
Hence, if we interpret separator instantiations as addition nodes in a circuit as given
by Definition 1, we get that a message directed towards the jointree root contains
the values of these addition nodes, while a message directed outward from the root
contains the partial derivatives of the circuit output with respect to these nodes.
Shenoy?Shafer propagation does not compute derivatives with respect to input
nodes ?x and ?x|u , but these can be obtained using local computations as follows.
4
Given a root cluster, one can direct the jointree b y having arrows point away from the
root, which also defines a parent/child relationship b etween clusters and separators.

Theorem 3 If evidence table ?X is assigned to cluster i with variables C:
?
?
Y
? f (e) ? X Y
=
Mji
? ? (x),
? ?x
j
C\X

(1)

?6=?X

where ? ranges over all evidence and CP tables assigned to cluster i. Moreover, if
CPT ?X|U is assigned to cluster i with variables C:
?
?
X
Y
Y
? f (e) ?
=
? ? (xu),
Mji
(2)
? ?x|u
j
C\XU

?6=?X|U

where ? ranges over all evidence and CP tables assigned to cluster i.
Therefore, even though Shenoy?Shafer propagation does not fully differentiate the
embedded circuit, the differentiation process can be completed through local computations after propagation has finished.5
W e now discuss some applications of the partial derivatives with respect to evidence
indicators ?x and network parameters ?x|u .
F ast retraction & evidence flipping. Suppose jointree propagation has been
performed using evidence e, which gives us access directly to the probability of e.
Suppose now we are interested in the probability of a different evidence e0 , which
results from changing the value of some variable X in e to a new value x. The
(e)
probability of e0 in this case is equal to ?f
??x [2], which can be obtained as given
by Equation 1. The ability to perform this computation efficiently is crucial for
algorithms that try to approximate maximum ap osteriori hyp othesis (MAP) using
local search [9, 10]. Another application of this derivative is in computing the
probability of evidence e0 , which results from retracting the value of some variable
P ?f (e)
X from e: Pr (e0 ) =
x ??x . This computation is k ey to analyzing evidence
conflict, as it allows us to determine the extent to which one piece of evidence is
contradicted by the remaining pieces.
(e)
Sensitivity analysis & parameter learning. The derivative ?Pr
??x|u is essential for sensitivity analysis?it is the basis for an efficient approach that identifies minimal network parameters changes that are necessary to satisfy constraints
on probabilistic queries [1]. This derivative is also crucial for gradient ascent approaches for learning network parameters as it is required to compute the gradient
5

Hugin propagation also corresponds to circuit ev aluation/differentiation:
Theorem 4 Cluster tables, separator tables and messages produced using Hugin propagation under evidence e have the following semantics: F or table ?i of cluster i with variables
(e)
. F or table ?ij of separator ij with variables S: ?ij (s) = s(e) ?f?s(e) .
C: ?i (c) = c(e) ?f?c
F or each inward message Mij , we have Mij (s) = s(e). F or each outward message Mji , we
have Mji (s) = ?f?s(e) if s(e) 6= 0.
Again, Hugin propagation does not compute deriv ativ es with respect to input nodes ?x
and ?x|u . Ev en for addition and multiplication nodes, it only retains deriv ativ es multiplied
by v alues. Hence, if we want to recov er the deriv ativ e with respect to, say, multiplication
node c, we must know the v alue of this node and it must be different than zero. In such a
case, we hav e ?f (e)/?c = ?i (c)/c(e), where ?i is the table associated with the cluster i
that generates node c. One can also compute the quantity v ?f /?v for input nodes using
equations similar to those in Theorem 3. But such quantities will be useful for obtaining
deriv ativ es only if the v alues of such input nodes are not zero. Hence, Shenoy?Shafer
propagation is more informativ e than Hugin propagation as far as the computation of
deriv ativ es is concerned.

(e)
used for deciding moves in the search space [13]. This derivative equals ?f
??x|u , and
can be obtained as given by Equation 2. The only other method we are aware
of to compute this derivative (beyond the one in [2]) is the one using the identity
?Pr (e)/??x|u = Pr (x, u, e)/?x|u , which requires ?x|u 6= 0 [13]. Hence, our results
seem to suggest the first general approach for computing this derivative using standard jointree propagation.

Bounding rounding errors. Jointree propagation gives exact results only when
infinite precision arithmetic is used. In practice, however, finite precision floating?
point arithmetic is typically used, in which case the differential semantics of jointree
propagation can be used to bound the rounding error in the computed probability
of evidence. See the full paper [11] for details on computing this bound.

5

A new perspectiv e on factoring graphical models

W e have shown in this paper that each jointree can be viewed as an implicit representation of an arithmetic circuit which computes the network polynomial, and that
jointree propagation corresponds to an evaluation and differentiation of the circuit.
These results have been useful in unifying the circuit approach presented in [2] with
jointree approaches, and in uncovering more properties of jointree propagation.
Another outcome of these results relates to the level at which it is useful to phrase
the problem of factoring graphical probabilistic models. Specifically, the perspective
we are promoting here is that probability distributions defined by graphical models
should be viewed as multi?linear functions, and the construction of jointrees should
be viewed as a process of constructing arithmetic circuits that compute these functions. That is, the fundamental object being factored is a multi?linear function,
and the fundamental result of the factorization is an arithmetic circuit. A graphical
model is a useful abstraction of the multi?linear function, and a jointree is a useful
structure for embedding the arithmetic circuit.
This view of factoring is useful since it allows us to cast the factoring problem in
more refined terms, which puts us in a better position to exploit the local structure
of graphical models in the factorization process. Note that the topology of a graphical model defines the form of the multi?linear function, while the model?s local
structure (as exhibited in its CPTs) constrains the values of variables appearing
in the function. One can factor a multi?linear function without knowledge of such
constraints, but the resulting factorizations will not be optimal. For a dramatic
example, consider a fully connected network with variables X1 , . . . , Xn , where all
parameters are equal to 12 . Any jointree for the network will have a cluster of size
n, leading to O(exp(n)) complexity. There is, however, a circuit of
O(n) size here,
n Qn
since the network polynomial can be easily factored as: f = ( 12 )
i=1 (?xi + ?x?i ).
Hence, in the presence of local structure, it appears more promising to factor the
graphical model into the more refined arithmetic circuit since not every arithmetic
circuit can be embedded in a jointree. This promise is made apparent by the results
in [3], which we sketch next. First, the multi?linear function of a belief network
is ?encoded? using a propositional theory, which is expressive enough to capture
the form of the multi?linear function in addition to constraints on its variables.
The theory is then compiled into a special logical form, known as deterministic
decomposable negation normal form. An arithmetic circuit is finally extracted from
that form. The method was able to generate relatively small arithmetic circuits for
a significant suite of real?world belief networks with treewidths up to 60.
It is worth mentioning here that the above perspective is in harmony with recent

approaches that represent probabilistic models using algebraic decision diagrams
(ADDs), citing the promise of ADDs in exploiting local structure [5]. ADDs and
related representations, such as edge?v alued decision diagrams, are known to be
compact representations of multi?linear functions. Moreov er, each of these representations can be expanded in linear time into an arithmetic circuit that satisfies
some strong properties [4]. Hence, such representations are special cases of arithmetic circuits as well.
W e finally note that the relationship between multi?linear functions (polynomials in
general) and arithmetic circuits is a classical subject of algebraic complexity theory
[15]. In this field of complexity, computational problems are expressed as polynomials, and a central question is that of determining the size of the smallest arithmetic
circuit that computes a giv en polynomial, leading to the notion of circuit complexity. Using this notion, it is then meaningful to talk about the circuit complexity
of a graphical model: the size of the smallest arithmetic circuit that computes the
multi?linear function induced by the model.
Ackno wledgment This work has been partially supported by NSF grant IIS9988543 and MURI grant N00014-00-1-0617.

References
[1] H. Chan and A. Darwiche. When do numbers really matter? JAIR, 17: 265?287,
2002.
[2] A. Darwiche. A differential approach to inference in Bay esian networks. In UAI?00,
pages 123?132, 2000. T o appear in JACM.
[3] A. Darwiche. A logical approach to factoring belief networks. In KR?02, pages 409?
420, 2002.
[4] A. Darwiche. On the factorization of multi?linear functions. T echnical Report D?128,
UCLA, Los Angeles, Ca 90095, 2002.
[5] J. Hoey , R. St-Aubin, A. Hu, and G. Boutilier. SPUDD: Stochastic planning using
decision diagrams. In UAI?99, pages 279?288, 1999.
[6] C. Huang and A. Darwiche. Inference in belief networks: A procedural guide. IJAR,
15(3): 225?263, 1996.
[7] M. Iri. Simultaneous computation of functions, partial deriv ativ es and estimates of
rounding error. Japan J. Appl. Math., 1:223?252, 1984.
[8] F. V. Jensen, S.L. Lauritzen, and K.G. Olesen. Bay esian updating in recursiv e graphical models by local computation. Comp. Stat. Quart., 4:269?282, 1990.
[9] J. Park. MAP complexity results and approximation methods. In UAI?02, pages
388?396, 2002.
[10] J. Park and A. Darwiche. Approximating MAP using stochastic local search. In
UAI?01, pages 403?410, 2001.
[11] J. Park and A. Darwiche. A differential semantics for jointree algorithms. T echnical
Report D?118, UCLA, Los Angeles, Ca 90095, 2001.
[12] G. Rote. Path problems in graphs. Computing Suppl., 7:155?189, 1990.
[13] S. Russell, J. Binder, D. Koller, and K. Kanazawa. Local learning in probabilistic
networks with hidden v ariables. In UAI?95, pages 1146?1152, 1995.
[14] P. P. Shenoy and G. Shafer. Propagating belief functions with local computations.
IEEE Expert, 1(3):43?52, 1986.
[15] J. v on zur Gathen. Algebraic complexity theory . Ann. Rev. Comp. Sci., 3:317?347,
1988.

"
5346,"Software for ANN training on a Ring Array Processor

Phil Kohn, Jeff Bilmes, Nelson Morgan, James Beck
International Computer Science Institute,
1947 Center St., Berkeley CA 94704, USA

Abstract
Experimental research on Artificial Neural Network (ANN) algorithms requires
either writing variations on the same program or making one monolithic program
with many parameters and options. By using an object-oriented library, the size
of these experimental programs is reduced while making them easier to read,
write and modify. An efficient and flexible realization of this idea is Connectionist Layered Object-oriented Network Simulator (CLONES). CLONES runs on
UNIX 1 workstations and on the 100-1000 MFLOP Ring Array Processor (RAP)
that we built with ANN algorithms in mind. In this report we describe CLONES
and show how it is implemented on the RAP.

1 Overview
As we continue to experiment with Artificial Neural Networks (ANNs) to generate phoneme
probabilities for speech recognition (Bourlard & Morgan, 1991), two things have become
increasingly clear:
1. Because of the diversity and continuing evolution of ANN algorithms, the programming environment must be both powerful and flexible.
2. These algorithms are very computationally intensive when applied to large databases
of training patterns.
Ideally we would like to implement and test ideas at about the same rate that we come up
with them. We have approached this goal both by developing application specific parallel
lUNIX is a trademark of AT&T

781

782

Kahn, Bilrnes, Morgan, and Beck

System

Languages Supported

Perfonnance
Assem

~
~

SparcStation 2

2MFLOP

SPERTBowd

~~~L7

c++

Sather pSather

-/ -/ -/ -/ -/

S

o

Desktop RAP +

u
r

-I -I -I -I

Nctwmcd RAP
(1-10 Boards)

1 GFLOP

SparcSlation +
SPERT Board

lOOP

CNS-l System

200 GOP

- / Completed

c
c

lOOMFLOP

Sun 4/330 Host

RAP9jstam

~~mID

C

C
o

m

p

?t

~ In Design

if if if

~

~

~

~

~

<,...-__

i
b
I
c

~

L_in_kC_r_c_om-::pa_lI_b_IC_ _ _
o

>

Figure 1: Hardware and software configurations

hardware, the Ring Array Processor (RAP) (Morgan et al., 1990; Beck, 1990; Morgan et al.,
1992), and by building an object-oriented software environment, the Connectionist Layered
Object-oriented Network Simulator (CLONES) (Kohn, 1991). By using an object-oriented
library, the size of experimental ANN programs can be greatly reduced while making them
easier to read, write and modify. CLONES is written in C++ and utilizes libraries previously
written in C and assem bIer.
Our ANN research currently encompasses two hardware platforms and several languages.
shown in Figure 1. Two new hardware platforms, the SPERT board (Asanovic et al., 1991)
and the CNS-l system are in design (unfilled check marks), and will support source code
compatibility with the existing machines. The SPERT design is a custom VLSI parallel
processor installed on an SBUS card plugged into a SPARC workstation. Using variable
precision fixed point arithmetic, a single SPERT board will have performance comparable
to a 10 board RAP system with 40 processors. The CNS-l system is based on multiple
VLSI parallel processors interconnected by high speed communication rings.
Because the investment in software is generally large, we insiston source level compatibility
across hardware platforms at the level of the system libraries. These libraries include matrix
and vector classes that free the user from concern about the hardware configuration. It is
also considered important to allow routines in different languages to be linked together.
This includes support for Sather, an object-oriented language that has been developed at
ICSI for workstations. The parallel version of Sather, called pSather, will be supported on

Sottware for ANN training on a Ring Array Processor

theCNS-l.
CLONES is seen as the ANN researcher's interface to this multiplatform, multi language
environment. Although CLONES is an application written specifically for ANN algorithms,
it's object-orientation gives it the ability to easily include previously developed libraries.
CLONES currently runs on UNIX workstations and the RAP; this paper focuses on the
RAP implementation.

2 RAP hardware
The RAP consists of cards that are added to a UNIX host machine (currently a VME based
Sun SPARe). A RAP card has four 32 MFlop Digital Signal Processor (DSP) chips (TI
TMS32OC30), each with its own local 256KB or 1MB of fast static RAM and 16MB of
DRAM.
Instead of sharing memory, the processors communicate on a high speed ring that shifts
data in a single machine cycle. For each board, the peak transfer rate between 4 nodes
is 64 million words/sec (256 Mbytes/second). This is a good balance to the 64 million
multiply-accumulates per second (128 MFLOPS) peak performance of the computational
elements.
Up to 16 of these boards can be interconnected and used as one Single Program operating
on Multiple Data stream (SPMD) machine. In this style of parallel computation, all the
processors run the same program and are doing the same operations to different pieces of the
same matrix or vector 2. The RAP can run other styles of parallel computation, including
pipelines where each processor is doing a different operation on different data streams.
However, for fully connected back-propagation networks, SPMD parallelism works well
and is also much easier to program since there is only one flow of control to worry about.
A reasonable design for networks in which all processors need all unit outputs is a single
broadcast bus. However, this design is not appropriate for other related algorithms such
as the backward phase of the back-propagation learning algorithm. By using a ring, backpropagation can be efficiently parallelized without the need to have the complete weight
matrix on all processors. The number of ring operations required for each complete matrix
update cycle is of the same order as the number of units, not the square of the number of
units. It should also be noted that we are using a stochastic or on-line learning algorithm.
The training examples are not di viding among the processors then the weights batch updated
after a complete pass. All weights are updated for each training example. This procedure
greatly decreases the training time for large redundant training sets since more steps are
being taken in the weight-space per training example.
We have empirically derived formulae that predict the performance improvement on backpropagation training as a function of the number of boards. Theoretical peak performance is
128 MFlops/board, with sustained performance of 30-90% for back-propagation problems
of interest to us. Systems with up to 40 nodes have been tested, for which throughputs
1'he hardware does not automatically keep the processors in lock step; for example, they may
become out of sync because of branches conditioned on the processor's node number or on the
data. However, when the processors must communicate with each other through the ring, hardware
synchronization automatically occurs. A node that attempts to read before data is ready. or to write
when there is already data waiting. will stop executing until the data can be moved.

783

784

Kahn, Bilrnes, Morgan, and Beck

of up to 574 Million Connections Per Second (MCPS) have been measured, as well as
learning rates of up to 106 Million Connection Updates Per Second (MCUPS) for training.
Practical considerations such as workstation address space and clock skew restrict current
implementations to 64 nodes, but in principle the architecture scales to about 16,000 nodes
for back-propagation.
We now have considerable experience with the RAP as a day-to-day computational tool for
our research. With the aid of the RAP hardware and software, we have done network training
studies that would have over a century on a UNIX workstation such as the SPARCstation-2.
We have also used the RAP to simulate variable precision arithmetic to guide us in the
design of higher performance hardware such as SPERT.
The RAP hardware remains very flexible because of the extensive use of programmable
logic arrays. These parts are automatically downloaded when the host machine boots up.
By changing the download files, the functionality of the communications ring and the host
interface can be modified or extended without any physical changes to the board.

3

RAP software

The RAP DSP software is built in three levels (Kohn & Bilmes, 1990; Bilmes & Kohn,
(990). At the lowest level are hand coded assembler routines for matrix, vector and ring
operations. Many standard matrix and vector operations are currently supported as well
as some operations specialized for efficient back-propagation. These matrix and vector
routines do not use the communications ring or split up data among processing nodes.
There is also a UNIX compatible library including most standard C functions for file, math
and string operations. All UNIX kernel calls (such as file input or output) cause requests
to be made to the host SPARC over the VMEbus. A RAP dremon process running under
UNIX has all of the RAP memory mapped into its virtual address space. It responds to the
RAP system call interrupts (from the RAP device driver) and can access RAP memory with
a direct memory copy function or assignment statement.
An intermediate level consists of matrix and vector object classes coded in C++. A
programmer writing at this level or above can program the RAP as if it were a conventional
serial machine. These object classes divide the data and processing among the available
processing nodes, using the communication ring to redistribute data as needed. For example,
to multiply a matrix by a vector, each processor would have its own subset of the matrix
rows that must be multiplied. This is equivalent to partitioning the output vector elements
among the processors. If the complete output vector is needed by all processors, a ring
broadcast routine is called to redistribute the part of the output vector from each processor
to all the other processors.
The top level of RAP software is the CLONES environment. CLONES is an object-oriented
library for constructing, training and utilizing connectionist networks. It is designed to
run efficiently on data parallel computers as well as uniprocessor workstations. While
efficiency and portability to parallel computers are the primary goals, there are several
secondary design goals:
1. minimize the learning curve for using CLONES;
2. minimize the additional code required for new experiments;
3. maximize the variety of artificial neural network algorithms supported;

Software for ANN training on a Ring Array Processor

4. allow heterogeneous algorithms and training procedures to be interconnected and
trained together;
5. allow the trained network to be easily embedded into other programs.
The size of experimental ANN programs is greatly reduced by using an object-oriented
library; at the same time these programs are easier to read, write and evolve.
Researchers often generate either a proliferation of versions of the same basic program,
or one giant program with a large number of options and many potential interactions and
side-effects. Some simulator programs include (or worse, evolve) their own language
for describing networks. We feel that a modem object-oriented language (such as C++)
has all the functionality needed to build and train ANNs. By using an object-oriented
design, we attempt to make the most frequently changed parts of the program very small
and well localized. The parts that rarely change are in a centralized library. One of the
many advantages of an object-oriented library for experimental work is that any part can
be specialized by making a new class of object that inherits the desired operations from a
library class.

4

CLONES overview

To make CLONES easier to learn, we restrict ourselves to a subset of the many features
of C++. Excluded features include multiple inheritance, operator overloading (however,
function overloading is used) and references. Since the multiple inheritance feature of C++
is not used, CLONES classes can be viewed as a collection of simple inheritance trees.
This means that all classes of objects in CLONES either have no parent class (top of a class
tree) or inherit the functions and variables of a single parent class.
CLONES consists of a library of C++ classes that represent networks (Net), their components (Net-part) and training procedures. There are also utility classes used during
training such as: databases of training data (Database), tables of parameters and arguments
(Param), and perfonnance statistics (Stats). Database and Param do not inherit from any
other class. Their class trees are independent of the rest of CLONES and each other. The
Stats class inherits from Net-behavior.
The top level of the CLONES class tree is a class called NeLbehavior. It defines function
interfaces for many general functions including file save or restore and debugging. It also
contains behavior functions that are called during different phases of running or training a
network. For example, there are functions that are called before or after a complete training
run (pre_training, posLtraining), before or after a pass over the database (pre_epoch,
post-epoch) and before or after a forward or backward run of the network (pre_forw-pass,
post1orw_pass, pre_back_pass, posLback_pass). The Net, NeLpart and Stats classes
inherit from this class.
All network components used to construct ANNs are derived from the two classes Layer
and Connect. Both of these inherit from class NeLpart. A CLONES network can be
viewed as a graph where the nodes are Layer objects and the arcs are Connect objects.
Each Connect connects a single input Layer with a single output Layer. A Layer holds
the data for a set of units (such as an activation vector), while a Connect transforms the
data as it passes between Layers. Data flows along Connects between the pair of Layers
by calling forw_propagate (input to output) or back_propagate (output to input) behavior

785

786

Kahn, Bilrnes, Morgan, and Beck

functions in the Connect object.
CLONES does not have objects that represent single units (or artificial neurons). Instead t
Layer objects are used to represent a set of units. Because arrays of units are passed
down to the lowest level routines t most of the computation time is focused into a few small
assembly coded loops that easily fit into the processor instruction cache. Time spent in all
of the levels of control code that call these loops becomes less significant as the size of the
Layer is increased.
The Layer class does not place any restrictions on the representation of its internal information. For example t the representation for activations may be a floating point number
for each unit (AnalogJayer)t or it may be a set of unit indices t indicating which units
are active (BinaryJayer). AnalogJayer and BinaryJayer are built into the CLONES
library as subclasses of the class Layer. The AnalogJayer class specifies the representation of activations t but it still leaves open the procedures that use and update the
activation array. BP...analogJayer is a subclass of AnalogJayer that specify these procedures for the back-propagation algorithm. Subclasses of AnalogJayer may also add
new data structures to hold extra internal state such as the error vector in the case of
BP...analogJayer. The BP-AnalogJaycr class has subclasses for various transfer functions such as BP...sigmoidJayer and BPJinear Jayer.
Layer classes also have behavior functions that are called in the course of running the
network. For example t one of these functions (pre_forw-propagate) initializes the Layer
for a forward pass t perhaps by clearing its activation vector. After all of the connections
coming into it are runt another Layer behavior function (postJorw_propagate) is called
that computes the activation vector from the partial results left by these connections. For
example t this function may apply a transfer function such as the sigmoid to the accumulated
sum of all the input activations.
These behavior functions can be changed by making a subclass. BP...analogJayer leaves
open the activation transfer function (or squashing function) and its derivative. Subclasses
define new transfer functions to be applied to the activations. A new class of backpropagation layer with a customized transfer function (instead of the default sigmoid) can
be created with the following C++ code:

My_new_BP_layer_class(int number_of_units)
: BP_analog_layer(number_of_units)i
II constructor
void transfer (Fvec *activation) {
1* apply forward transfer function to my activation vector *1

void d_transfer(Fvec *activation, Fvec *err)
1* apply backward error transfer to err (given activation)

*1

}i

A Connect class includes two behavior functions: one that transforms activations from the
incoming Layer into partial results in the outgoing Layer (forw-propagate) and one that
takes outgoing errors and generates partial results in the incoming Layer (back-propagate).

Software for ANN training on a Ring Array Processor

The structure of a partial result is part of the Layer class. The subclasses of Connect include:
Bus_connect (one to one), Full_connect (all to all) and Sparse_connect (some to some).
Each subclass of Connect may contain a set of internal parameters such as the weight
matrix in a BPJulLconnect. Subclasses of Connect also specify which pairs of Layer
subclasses can be connected. When a pair of Layer objects are connected, type checking
by the C++ compiler insures that the input and output Layer subclasses are supported by
the Connect object.
In order to do its job efficiently, a Connect must know something about the internal
representation of the layers that are connected. By using C++ overloading, the Connect
function selected depends not only on the class of Connect, but also on the classes of
the two layers that are connected. Not all Connect classes are defined for all pairs of
Layer classes. However, Connects that convert between Layer classes can be utilized to
compensate for missing functions.
CLONES allows the user to view layers and connections much like tinker-toy wheels and
rods. ANNs are built up by creating Layer objects and passing them to the create functions
of the desired Connect classes. Changing the interconnection pattern does not require any
changes to the Layer classes or objects and vice-versa.
At the highest level, a Net object delineates a subset of a network and controls its training.
Operations can be performed on these subsets by calling functions on their Net objects. The
Layers of a Net are specified by calling one of new_inputJayer, new_hidden.Jayer, or
new_outputJayer on the Net object for each Layer. Given the Layers, the Connects that
belong to the Net are deduced by the Net-order objects (see below). Layer and Connect
objects can belong to any number of Nets.
The Net labels all of its Layers as one of input, output or hidden. These labels are
used by the NeLorder objects to determine the order in which the behavior functions of
the NeLparts are called. For example, a Net object contains NeLorder objects called
forward_pass_order and backward_pass_order that control the execution sequence for a
forward or backward pass. The Net object also has functions that call a function by the
same name on all of its component parts (for example set.Jearning-.rate).
When a Net-order object is built it scans the connectivity of the Net. The rules that relate
topology to order of execution are centralized and encapsulated in subclasses of NeLorder.
Changes to the structure of the Net are localized to just the code that creates the Layers
and Connects; one does not need to update separate code that contains explict knowledge
about the order of evaluation for running a forward or backward pass.
The training procedure is divided into a series of steps, each of which is a call to a function
in the Net object. At the top level, calling run_training on a Net performs a complete
training run. In addition to calling pre_training, posLtraining behavior functions, it calls
run_epoch in a loop until the the nextJearning-.rate function returns zero. The run_epoch
function calls run_forward and run_backward.
At a lower level there are functions that interface the database(s) of the Net object to the
Layers of the Net. For example, seLinput sets the activations of the input Layers for a
given pattern number of the database. Another of these sets the error vector of the output
layer (seLerror). Some of these functions, such as is_correct evaluate the performance of
the Net on the current pattern.

787

788

Kahn, Bilmes, Morgan, and Beck

In addition to database related functions, the Net object also contains useful global variables
for all of its components. A pointer to the Net object is always passed to all behavior
functions of its Layers and Connects when they are called. One of these variables is a
Param object that contains a table of parameter names, each with a list of values. These
parameters usually come from the command line and/or parameter files. Other variables
include: the current pattern, the correct target output, the epoch number, etc.

5 Conclusions
CLONES is a useful tool for training ANNs especially when working with large training
databases and networks. It runs efficiently on a variety of parallel hardware as well as on
UNIX workstations.

Acknowledgements
Special thanks to Steve Renals for daring to be the first CLONES user and making significant
contributions to the design and implementation. Others who provided valuable input to
this work were: Krste Asanovi~, Steve Omohundro, Jerry Feldman, Heinz Schmidt and
Chuck Wooters. Support from the International Computer Science Institute is gratefully
acknowledged.

References
Asanovi~,

K., Beck, J., Kingsbury, B., Kohn, P., Morgan, N., & Wawrzynek, J. (1991).
SPERT: A VLIWISIMD Microprocessor for Artificial Neural Network Computations.
Tech. rep. TR-91-072, International Computer Science Institute.

Beck, J. (1990). The Ring Array Processor (RAP): Hardware. Tech. rep. TR-90-048,
International Computer Science Institute.
Bilmes, J. & Kohn, P. (1990). The Ring Array Processor (RAP): Software Architecture.
Tech. rep. TR-90-050, International Computer Science Institute.
Bourlard, H. & Morgan, N. (1991). Connectionist approaches to the use of Markov models
for continuous speech recognition. In Touretzky, D. S. (Ed.), Advances in Neural
Information Processing Systems, Vol. 3. Morgan Kaufmann, San Mateo CA.
Kohn, P. & Bilmes, J. (1990). The Ring Array Processor (RAP): Software Users Manual
Version 1.0. Tech. rep. TR-90-049, International Computer Science Institute.
Kohn, P. (1991). CLONES: Connectionist Layered Object-oriented NEtwork Simulator.
Tech. rep. TR-91-073, International Computer Science Institute.
Morgan, N., Beck, J., Kohn, P., Bilmes, J., Allman, E., & Beer, J. (1990). The RAP: a ring
array processor for layered network calculations. In Proceedings IEEE International
Conference on Application Specific Array Processors, pp. 296-308 Princeton NI.
Morgan, N., Beck, J., Kohn, P., & Bilmes, J. (1992). Neurocomputing on the RAP. In
Przytula, K. W. & Prasanna, V. K. (Eds.), Digital Parallellmplemencations of Neural
Networks. Prentice-Hall, Englewood Cliffs NJ.

"
1402,"Stochastic Neighbor Embedding

Geoffrey Hinton and Sam Roweis
Department of Computer Science, University of Toronto
10 King?s College Road, Toronto, M5S 3G5 Canada
hinton,roweis @cs.toronto.edu


Abstract
We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a
low-dimensional space in a way that preserves neighbor identities. A
Gaussian is centered on each object in the high-dimensional space and
the densities under this Gaussian (or the given dissimilarities) are used
to define a probability distribution over all the potential neighbors of
the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the
low-dimensional ?images? of the objects. A natural cost function is a
sum of Kullback-Leibler divergences, one per object, which leads to a
simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic
framework makes it easy to represent each object by a mixture of widely
separated low-dimensional images. This allows ambiguous objects, like
the document count vector for the word ?bank?, to have versions close to
the images of both ?river? and ?finance? without forcing the images of
outdoor concepts to be located close to those of corporate concepts.

1 Introduction
Automatic dimensionality reduction is an important ?toolkit? operation in machine learning, both as a preprocessing step for other algorithms (e.g. to reduce classifier input size)
and as a goal in itself for visualization, interpolation, compression, etc. There are many
ways to ?embed? objects, described by high-dimensional vectors or by pairwise dissimilarities, into a lower-dimensional space. Multidimensional scaling methods[1] preserve
dissimilarities between items, as measured either by Euclidean distance, some nonlinear
squashing of distances, or shortest graph paths as with Isomap[2, 3]. Principal components analysis (PCA) finds a linear projection of the original data which captures as much
variance as possible. Other methods attempt to preserve local geometry (e.g. LLE[4]) or
associate high-dimensional points with a fixed grid of points in the low-dimensional space
(e.g. self-organizing maps[5] or their probabilistic extension GTM[6]). All of these methods, however, require each high-dimensional object to be associated with only a single
location in the low-dimensional space. This makes it difficult to unfold ?many-to-one?
mappings in which a single ambiguous object really belongs in several disparate locations
in the low-dimensional space. In this paper we define a new notion of embedding based on
probable neighbors. Our algorithm, Stochastic Neighbor Embedding (SNE) tries to place
the objects in a low-dimensional space so as to optimally preserve neighborhood identity,
and can be naturally extended to allow multiple different low-d images of each object.

2 The basic SNE algorithm
For each object, , and each potential neighbor,  , we start by computing the asymmetric
probability,  , that would pick  as its neighbor:

	


 	

  
 
 

 

(1)

The dissimilarities,   , may be given as part of the problem definition (and need not be
symmetric), or they may be computed using the scaled squared Euclidean distance (?affinity?) between two high-dimensional points, !""$#$! :

%&%

  
)

'% %
!   !  
() 


(2)

where  is either
) set by hand or (as in some of our experiments) found by a binary search
for the value of  that makes the entropy of the distribution over neighbors equal to *'+,.- .
Here, - is the effective number of local neighbors or ?perplexity? and is chosen by hand.
In the low-dimensional space we also use Gaussian neighborhoods but with a fixed variance
(which we set without loss of generality to be / ) so the induced probability 01 that point
picks point  as its neighbor is a function of the low-dimensional images 23 of all the
objects and is given by the expression:

	4

 %'% 2   2  &% % 
  	4

 '% %    &% % 
2  2

 


0 

(3)

The aim of the embedding is to match these two distributions as well as possible. This is
achieved by minimizing a cost function which is a sum of Kullback-Leibler divergences
between the original (5 ) and induced ( 06 ) distributions over neighbors for each object:

7

98

8




  *'+,

:
@ ?  %&% A  
;8
0
9<>=

(4)

The dimensionality of the 2 space is chosen by hand (much less than the number of objects).
Notice that making 0  large when   is small wastes some of the probability mass in the 0
distribution so there is a cost for modeling a big distance in the high-dimensional space with
a small distance in the low-dimensional space, though it is less than the cost of modeling
a small distance with a big one. In this respect, SNE is an improvement over methods
like LLE [4] or SOM [5] in which widely separated data-points can be ?collapsed? as near
neighbors in the low-dimensional space. The intuition is that while SNE emphasizes local
distances, its cost function cleanly enforces both keeping the images of nearby objects
nearby and keeping the images of widely separated objects relatively far apart.
Differentiating C is tedious because 2
the result is simple: B

B

7

2 



(

8




affects 0  via the normalization term in Eq. 3, but

 2  2   
   0DCEFG  0HG 

(5)

which has the nice interpretation of a sum of forces pulling 2"" toward 2 or pushing it away
depending on whether  is observed to be a neighbor more or less often than desired.

7

Given the gradient, there are many possible ways to minimize and we have only just begun the search for the best method. Steepest descent in which all of the points are adjusted
in parallel is inefficient and can get stuck in poor local optima. Adding random jitter that
decreases with time finds much better local optima and is the method we used for the examples in this paper, even though it is still quite slow. We initialize the embedding by putting
all the low-dimensional images in random locations very close to the origin. Several other
minimization methods, including annealing the perplexity, are discussed in sections 5&6.

3 Application of SNE to image and document collections
As a graphic illustration of the ability of SNE to model high-dimensional, near-neighbor
relationships using only two dimensions, we ran the algorithm on a collection of bitmaps of
handwritten digits and on a set of word-author counts taken from the scanned proceedings
of NIPS conference papers. Both of these datasets are likely to have intrinsic structure in
many fewer dimensions than their raw dimensionalities: 256 for the handwritten digits and
13679 for the author-word counts.
To begin, we used a set of  digit bitmaps from the UPS database[7] with  examples
from each
( of the five classes 0,1,2,3,4. The variance of the Gaussian around each point
in the  -dimensional raw pixel image space was set to achieve a perplexity of 15 in the
distribution over high-dimensional neighbors. SNE was initialized by putting all the 2""
in random locations very close to the origin and then was trained using gradient descent
with annealed noise. Although SNE was given no information about class labels, it quite
cleanly separates the digit groups as shown in figure 1. Furthermore, within each region of
the low-dimensional space, SNE has arranged the data so that properties like orientation,
skew and stroke-thickness tend to vary smoothly. For the embedding shown, the SNE
cost function in Eq. 4 has a value of 
	 nats;
( with a uniform
 distribution across lowdimensional neighbors, the cost is  *'+,  			
    	 nats. We also applied
principal component analysis (PCA)[8] to the same data; the projection onto the first two
principal components does not separate classes nearly as cleanly as SNE because PCA is
much more interested in getting the large separations right which causes it to jumble up
some of the boundaries between similar classes. In this experiment, we used digit classes
that do not have very similar pairs like 3 and 5 or 7 and 9. When there are more classes and
only two available dimensions, SNE does not as cleanly separate very similar pairs.
We have also applied SNE to word-document and word-author matrices calculated from
the OCRed text of NIPS volume 0-12 papers[9]. Figure 2 shows a map locating NIPS authors into two dimensions. Each of the 676 authors who published more than one paper
in NIPS vols. 0-12 is shown by a dot at the position 2  found by SNE; larger red dots
and corresponding last names are authors who published six or more papers in that period.
Distances   were computed as the norm of the difference between log aggregate author
word counts, summed across all NIPS papers. Co-authored papers gave fractional counts
evenly to all authors. All words occurring in six or more documents were included, except for stopwords giving a vocabulary size of) 13649. (The bow toolkit[10] was used for
part of
( the pre-processing of the data.) The  were set to achieve a local perplexity of
- 
neighbors. SNE seems to have grouped authors by broad NIPS field: generative
models, support vector machines, neuroscience, reinforcement learning and VLSI all have
distinguishable localized regions.

4 A full mixture version of SNE
The clean probabilistic formulation of SNE makes it easy to modify the cost function so
that instead of a single image, each high-dimensional object can have several different
versions of its low-dimensional image. These alternative versions have mixing proportions
that sum to  . Image-version  of object has location 2  and mixing proportion 5
  . The
low-dimensional neighborhood distribution for is a mixture of the distributions induced
by each of its image-versions across all image-versions of a potential neighbor  :

0 8 : 8

%&%
%'%

  4	 

  $ 2  
2     
 
	4

$ %&% 2    2 '% %  


(6)

In this multiple-image model, the derivatives with respect to the image locations 23 are
straightforward; the derivatives w.r.t the mixing proportions   are most easily expressed

(
Figure 1: The result of running the SNE algorithm on 
 -dimensional grayscale
images of handwritten digits. Pictures of the original data vectors !  (scans of handwritten
digit) are shown at the location corresponding to their low-dimensional images 23 as found
by SNE. The classes are quite well separated even though SNE had no information about
class labels. Furthermore, within each class, properties like orientation, skew and strokethickness tend to vary smoothly across the space. Not all points are shown: to produce this
display, digits are chosen in random order and are only displayed if a   x   region of the
display centered on the 2-D location of the digit in the embedding does not overlap any of
the   x   regions for digits that have already been displayed.
(SNE was initialized by putting all the  in random locations very close to the origin and then was
trained using batch gradient descent (see Eq. 5) with annealed noise. The learning rate was 0.2. For
the first 3500 iterations, each 2-D point was jittered by adding Gaussian noise with a standard deviation of   after each position update. The jitter was then reduced to  for a further 	
 iterations.)

Touretzky

Wiles

Maass
Kailath
Chauvin Munro Shavlik
Sanger
Movellan Baluja Lewicki Schmidhuber
Hertz
Baldi Buhmann Pearlmutter Yang
Tenenbaum
Cottrell
Krogh
Omohundro Abu?Mostafa
Schraudolph
MacKay
Coolen
Lippmann
Robinson Smyth
Cohn
Ahmad Tesauro
Pentland
Goodman
Atkeson
Neuneier
Warmuth
Sollich Moore
Thrun
Pomerleau

Barber

Ruppin
Horn
Meilijson MeadLazzaro
Koch
Obermayer Ruderman
Eeckman HarrisMurray
Bialek Cowan
Baird Andreou
Mel
Cauwenberghs
Brown Li
Jabri
Giles Chen
Spence Principe
Doya Touretzky
Sun
Stork Alspector Mjolsness
Bell
Lee
Maass
Lee
Gold
Pomerleau Kailath Meir
Seung Movellan
Rangarajan
Yang Amari
Tenenbaum
Cottrell Baldi
Abu?Mostafa
MacKay
Nowlan Lippmann
Smyth Cohn Kowalczyk
Waibel
Pouget
Atkeson
Kawato
Viola Bourlard Warmuth
Dayan
Sollich
Morgan Thrun MooreSutton
Barber Barto Singh
Tishby WolpertOpper
Sejnowski
Williamson
Kearns
Singer
Moody
Shawe?Taylor
Saad
Zemel
Saul
Tresp
Bartlett
Platt
Leen
Mozer
Bishop Jaakkola
Solla
Ghahramani
Smola
Williams
Vapnik
Scholkopf
Hinton
Bengio
Jordan
Muller
Graf
LeCun Simard
Denker
Guyon
Bower

Figure 2: Embedding of NIPS authors into two dimensions. Each of the 676 authors
who published more than one paper in NIPS vols. 0-12 is show by a dot at the location 2  found by the SNE algorithm. Larger red dots and corresponding last names
are authors who published six or more papers in that period. The inset in upper left
shows a blowup of the crowded boxed central portion of the space. Dissimilarities between authors were computed based on squared Euclidean distance between vectors of
log aggregate author word counts. Co-authored papers gave fractional counts evenly
to all authors. All words occurring in six or more documents were included, except
for stopwords giving a vocabulary size of 13649. The NIPS text data is available at
http://www.cs.toronto.edu/ roweis/data.html.

in terms of   , the probability that version  of picks version  of  :

@  

&% %
%&%
   4	 

  $ 2    2      
9  
	
    %'% 2  2 '% %  


(7)

The effect on 06 of changing the mixing proportion for version  of object 

B
B 0  	   8

where    

 if 



 $ C 8 

:






  

    8

is given by

@

(8)

and  otherwise. The effect of changing    on the cost, C, is

B 7

B




B
B 0 
  8 8
  
  0   

(9)

Rather than optimizing the mixing proportions directly, it is easier	
to perform unconstrained
	4


   .
optimization on ?softmax weights? defined by     	4


    
As a ?proof-of-concept?, we recently implemented a simplified mixture version in which
every object is represented in the low-dimensional
space by exactly two components that

are constrained to have mixing proportions of  . The two components are pulled together
by a force which increases linearly up to a threshold separation. Beyond this threshold
the force remains constant.1 We ran two experiments with this simplified mixture version
of SNE. We took a dataset containing  pictures of each of the digits 2,3,4 and added

 hybrid digit-pictures that were each constructed by picking new examples of two of
the classes and taking each pixel at random from one of these two ?parents?. After mini  of the hybrids and only 

	  of the non-hybrids had significantly different
mization, 
locations for their two mixture components. Moreover, the mixture components of each
hybrid always lay in the regions of the space devoted to the classes of its two parents and
never in the region devoted to the third class. For this example we used a perplexity of  
in defining the local neighborhoods, a step size of for each position update of 
   times the
gradient, and used a constant jitter of 
   . Our very simple mixture version of SNE also
makes it possible to map a circle onto a line without losing any near neighbor relationships
or introducing any new ones. Points near one ?cut point? on the circle can mapped to a
mixture of two points, one near one end of the line and one near the other end. Obviously,
the location of the cut on the two-dimensional circle gets decided by which pairs of mixture
components split first during the stochastic optimization. For certain optimization parameters that control the ease with which two mixture components can be pulled apart, only
a single cut in the circle is made. For other parameter settings, however, the circle may
fragment into two or more smaller line-segments, each of which is topologically correct
but which may not be linked to each other.
The example with hybrid digits demonstrates that even the most primitive mixture version
of SNE can deal with ambiguous high-dimensional objects that need to be mapped to two
widely separated regions of the low-dimensional space. More work needs to be done before
SNE is efficient enough to cope with large matrices of document-word counts, but it is
the only dimensionality reduction method we know of that promises to treat homonyms
sensibly without going back to the original documents to disambiguate each occurrence of
the homonym.
1
We used a threshold of     . At threshold the force was   
 nats per unit length. The low-d
space has a natural scale because the variance of the Gaussian used to determine    is fixed at 0.5.

5 Practical optimization strategies
Our current method of reducing the SNE cost is to use steepest descent with added jitter
that is slowly reduced. This produces quite good embeddings, which demonstrates that the
SNE cost function is worth minimizing, but it takes several hours to find a good embedding
for just  datapoints so we clearly need a better search algorithm.
The time per iteration could be reduced considerably by ignoring pairs of points for which
all four of   #  G #G0  #G0 G are small. Since the matrix   is fixed during the learning, it is
natural to sparsify it by replacing all entries below a certain threshold with zero and renormalizing. Then pairs H#  for which both 5 and FG are zero can be ignored from gradient
calculations if both 0  and 0 G are small. This can in turn be determined in logarithmic
time in the size of the training set by using sophisticated geometric data structures such
as K-D trees, ball-trees and AD-trees, since the 0 depend only on 42  2  . Computational physics has attacked exactly this same complexity when performing multibody
gravitational or electrostatic simulations using, for example, the fast multipole method.
In the mixture version of SNE there appears to be an interesting way of avoiding local
optima that does not involve annealing the jitter. Consider two components in the mixture
for an object that are far apart in the low-dimensional space. By raising the mixing proportion of one and lowering the mixing proportion of the other, we can move probability mass
from one part of the space to another without it ever appearing at intermediate locations.
This type of ?probability wormhole? seems like a good way to avoid local optima that arise
because a cluster of low-dimensional points must move through a bad region of the space
in order to reach a better one.
Yet another search method, which we have used with some success on toy problems, is
to provide extra dimensions in the low-dimensional space but to penalize non-zero values
on these dimensions. During the search, SNE will use the extra dimensions to go around
lower-dimensional barriers but as the penalty on using these dimensions is increased, they
will cease to be used, effectively constraining the embedding to the original dimensionality.

6 Discussion and Conclusions
Preliminary
experiments show that we can find good optima by first annealing the perplex)
ities   (using high jitter) and only reducing the jitter after the final perplexity
has been
)
reached. This raises the question of what SNE is doing when the variance,   , of the Gaussian centered on each high-dimensional point is very big so that the distribution across
neighbors is almost uniform. It is clear that in the high variance limit, the contribution of
:*&+ ,  : 
 06  to) the SNE cost function is just as important for distant neighbors as for
close ones. When   is very large, it can be shown that SNE is equivalent to minimizing the
mismatch between squared distances in the two spaces, provided all the squared distances
from an object are first normalized by subtracting off their ?antigeometric? mean,    :



@        
           
 
	4

   
 #
   *&+ , 8
   %&% !  ! %&%  
 )  #


  
 
	4

$     

    '% % 2   2  '% %  
 )  #
      *'+, 8 
  
 

	


where  is the number of objects.

;8

(10)
(11)

(12)

This mismatch is very similar to ?stress? functions used in nonmetric versions of MDS,
and enables us to understand the large-variance limit of SNE as a particular variant of such
procedures. We are still investigating the relationship to metric MDS and to PCA.
SNE can also be seen as an interesting special case of Linear Relational Embedding (LRE)
[11]. In LRE the data consists of triples (e.g. Colin has-mother Victoria) and the task
is to predict the third term from the other two. LRE learns an N-dimensional vector for
each object and an NxN-dimensional matrix for each relation. To predict the third term in
a triple, LRE multiplies the vector representing the first term by the matrix representing
the relationship and uses the resulting vector as the mean of a Gaussian. Its predictive
distribution for the third term is then determined by the relative densities of all known
objects under this Gaussian. SNE is just a degenerate version of LRE in which the only
relationship is ?near? and the matrix representing this relationship is the identity.
In summary, we have presented a new criterion, Stochastic Neighbor Embedding, for mapping high-dimensional points into a low-dimensional space based on stochastic selection
of similar neighbors. Unlike self-organizing maps, in which the low-dimensional coordinates are fixed to a grid and the high-dimensional ends are free to move, in SNE the
high-dimensional coordinates are fixed to the data and the low-dimensional points move.
Our method can also be applied to arbitrary pairwise dissimilarities between objects if such
are available instead of (or in addition to) high-dimensional observations. The gradient of
the SNE cost function has an appealing ?push-pull? property in which the forces acting on
2 to bring it closer to points it is under-selecting and further from points it is over-selecting
as its neighbor. We have shown results of applying this algorithm to image and document
collections for which it sensibly placed similar objects nearby in a low-dimensional space
while keeping dissimilar objects well separated.
Most importantly, because of its probabilistic formulation, SNE has the ability to be extended to mixtures in which ambiguous high-dimensional objects (such as the word ?bank?)
can have several widely-separated images in the low-dimensional space.
Acknowledgments We thank the anonymous referees and several visitors to our poster for helpful
suggestions. Yann LeCun provided digit and NIPS text data. This research was funded by NSERC.

References
[1] T. Cox and M. Cox. Multidimensional Scaling. Chapman & Hall, London, 1994.
[2] J. Tenenbaum. Mapping a manifold of perceptual observations. In Advances in Neural Information Processing Systems, volume 10, pages 682?688. MIT Press, 1998.
[3] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear
dimensionality reduction. Science, 290:2319?2323, 2000.
[4] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding.
Science, 290:2323?2326, 2000.
[5] T. Kohonen. Self-organization and Associative Memory. Springer-Verlag, Berlin, 1988.
[6] C. Bishop, M. Svensen, and C. Williams. GTM: The generative topographic mapping. Neural
Computation, 10:215, 1998.
[7] J. J. Hull. A database for handwritten text recognition research. IEEE Transaction on Pattern
Analysis and Machine Intelligence, 16(5):550?554, May 1994.
[8] I. T. Jolliffe. Principal Component Analysis. Springer-Verlag, New York, 1986.
[9] Yann LeCun. Nips online web site. http://nips.djvuzone.org, 2001.
[10] Andrew Kachites McCallum. Bow: A toolkit for statistical language modeling, text retrieval,
classification and clustering. http://www.cs.cmu.edu/ mccallum/bow, 1996.
[11] A. Paccanaro and G.E. Hinton. Learning distributed representations of concepts from relational
data using linear relational embedding. IEEE Transactions on Knowledge and Data Engineering, 13:232?245, 2000.

"
3283,"A primal-dual algorithm for group sparse
regularization with overlapping groups

Silvia Villa
DISI- Universit`a di Genova
villa@dima.unige.it

Sofia Mosci
DISI- Universit`a di Genova
mosci@disi.unige.it

Lorenzo Rosasco
IIT - MIT
lrosasco@MIT.EDU

Alessandro Verri
DISI- Universit`a di Genova
verri@disi.unige.it

Abstract
We deal with the problem of variable selection when variables must be selected
group-wise, with possibly overlapping groups defined a priori. In particular we
propose a new optimization procedure for solving the regularized algorithm presented in [12], where the group lasso penalty is generalized to overlapping groups
of variables. While in [12] the proposed implementation requires explicit replication of the variables belonging to more than one group, our iterative procedure
is based on a combination of proximal methods in the primal space and projected
Newton method in a reduced dual space, corresponding to the active groups. This
procedure provides a scalable alternative with no need for data duplication, and
allows to deal with high dimensional problems without pre-processing for dimensionality reduction. The computational advantages of our scheme with respect
to state-of-the-art algorithms using data duplication are shown empirically with
numerical simulations.

1

Introduction

Sparsity has become a popular way to deal with small samples of high dimensional data and, in a
broad sense, refers to the possibility of writing the solution in terms of a few building blocks. Often,
sparsity based methods are the key towards finding interpretable models in real-world problems. In
particular, regularization based on `1 type penalties is a powerful approach for dealing with the problem of variable selection, since it provides sparse solutions by minimizing a convex functional. The
success of `1 regularization motivated exploring different kinds of sparsity properties for (generalized) linear models, exploiting available a priori information, which restricts the admissible sparsity
patterns of the solution. An example of a sparsity pattern is when the input variables are partitioned
into groups (known a priori), and the goal is to estimate a sparse model where variables belonging to
the same group are either jointly selected or discarded. This problem can be solved by regularizing
with the group-`1 penalty, also known as group lasso penalty, which is the sum, over the groups, of
the euclidean norms of the coefficients restricted to each group.
A possible generalization of group lasso is to consider groups of variables which can be potentially
overlapping, and the goal is to estimate a model which support is the union of groups. This is a
common situation in bioinformatics (especially in the context of high-throughput data such as gene
expression and mass spectrometry data), where problems are characterized by a very low number of
samples with several thousands of variables. In fact, when the number of samples is not sufficient
to guarantee accurate model estimation, an alternative is to take advantage of the huge amount of
prior knowledge encoded in online databases such as the Gene Ontology. Largely motivated by applications in bioinformatics, a new type of penalty is proposed in [12], which is shown to give better
1

performances than simple `1 regularization.
A straightforward solution to the minimization problem underlying the method proposed in [12] is
to apply state-of-the-art techniques for group lasso (we recall interior-points methods [3, 20], block
coordinate descent [16], and proximal methods [9, 21], also known as forward-backward splitting
algorithms, among others) in an expanded space, built by duplicating variables that belong to more
than one group.
As already mentioned in [12], though very simple, such an implementation does not scale to large
datasets, when the groups have significant overlap, and a more scalable algorithm with no data duplication is needed. For this reason we propose an alternative optimization approach to solve the
group lasso problem with overlap. Our method does not require explicit replication of the features
and is thus more appropriate to deal with high dimensional problems with large groups overlap.
Our approach is based on a proximal method (see for example [18, 6, 5]), and two ad hoc results
that allow to efficiently compute the proximity operator in a much lower dimensional space: with
Lemma 1 we identify the subset of active groups, whereas in Theorem 2 we formulate the reduced
dual problem for computing the proximity operator, where the dual space dimensionality coincides
with the number of active groups. The dual problem can then be solved via Bertsekas? projected
Newton method [7]. We recall that a particular overlapping structure is the hierarchical structure,
where the overlap between groups is limited to inclusion of a descendant in its ancestors. In this case
the CAP penalty [24] can be used for model selection, as it has been done in [2, 13], but ancestors
are forced to be selected when any of their descendant are selected. Thanks to the nested structure,
the proximity operator of the penalty term can be computed exactly in a finite number of steps [14].
This is no longer possible in the case of general overlap. Finally it is worth noting that the penalty
analyzed here can be applied also to hierarchical group lasso. Differently from [2, 13] selection of
ancestors is no longer enforced.
The paper is organized as follows. In Section 2 we recall the group lasso functional for overlapping groups and set some notations. In Section 3 we state the main results, present a new iterative
optimization procedure, and discuss computational issues. Finally in Section 4 we present some
numerical experiments comparing running time of our algorithm with state-of-the-art techniques.
The proofs are reported in the Supplementary material.

2

Problem and Notations

We first fix some notations.
Given a vector ? ? Rd , while k?k denotes the `2 -norm, we will use the
P
2 1/2
notation k?kG = ( j?G ?j )
to denote the `2 -norm of the components of ? in G ? {1, . . . , d}.
Then, for any differentiable function f : RB ? R, we denote by ?r f its partial derivative with
respect to variables r, and by ?f = (?r f )B
r=1 its gradient.
We are now ready to cast group `1 regularization with overlapping groups as the following variational problem. Given a training set {(xi , yi )ni=1 } ? (X ? Y )n , a dictionary (?j )dj=1 , and B subsets
of variables G = {Gr }B
Gr ? {1, . . . , d}, we assume the estimator to be described by a
r=1 with P
d
generalized linear model f (x) = j=1 ?j (x)?j and consider the following regularization scheme


1
2
? ? = argmin E? (?) = argmin
k?? ? yk + 2? ?Goverlap (?) ,
(1)
n
??Rd
??Rd
where ? is the n ? d matrix given by the features ?j in the dictionary evaluated in the training set
Pn
2
points, [?]i,j = ?j (xi ). The term n1 k?? ? yk is the empirical error, n1 i=1 ` (f (xi ), yi ), when
the cost function1 ` : R ? Y ? R+ is the square loss, `(f (x), y) = (y ? f (x))2 .
The penalty term ?Goverlap : Rd ? R+ is lower semicontinuous, convex, and one-homogeneous,
(?Goverlap(??) = ??Goverlap(?), ?? ? Rd and ? ? R+ ), and is defined as
B
X
?Goverlap (?) =
inf
kvr k .
P
(v1 ,...,vB ),vr ?Rd ,supp(vr)?Gr ,

B
r=1

vr =?

r=1

?Goverlap

The functional
was introduced in [12] as a generalization of the group lasso penalty to
allow overlapping groups, while maintaining the group lasso property of enforcing sparse solutions
which support is a union of groups. When groups do not overlap, ?Goverlap reduces to the group lasso
1

Note our analysis would immediately apply to other loss functions, e.g. the logistic loss.

2

PB
penalty. Note that, as pointed out in [12], using r=1 k?kGr as generalization of the group lasso
penalty leads to a solution which support is the complement of the union of groups. For an extensive
study of the properties of ?Goverlap, its comparison with the `1 norm, and its extension to graph lasso,
we therefore refer the interested reader to [12].

3

The GLO-pridu Algorithm

If one needs to solve problem (1) for high dimensional data, the use of standard second-order methods such as interior-point methods is precluded (see for instance [6]), since they need to solve large
systems of linear equations to compute the Newton steps. On the other hand, first order methods
inspired to Nesterov?s seminal paper [19] (see also [18]) and based on proximal methods already
proved to be a computationally efficient alternative in many machine learning applications [9, 21].
3.1

A Proximal algorithm
2

Given the convex functional E? in (1), which is sum of a differentiable term, namely n1 k?? ? yk ,
and a non-differentiable one-homogeneous term 2? ?Goverlap, its minimum can be computed with
following acceleration of the iterative forward-backward splitting scheme



1 T
? p = I ? ?? /?K hp ?
? (?hp ? y)
n?
q


cp = (1 ? tp )cp?1 ,
tp+1 = ?cp + c2p + 8cp /4
(2)
tp+1
tp+1
) + ? p?1 (tp ? 1)
tp
tp
for a suitable choice of ?. Due to one-homogeneity of ?Goverlap, the proximity operator associated
to ?? ?Goverlap reduces to the identity minus the projection onto the subdifferential of ?? ?Goverlap at
the origin, which is a closed and convex set. We will denote such a projection as ?? /?K , where
K = ??Goverlap(0). The above scheme is inspired to [10], and is equivalent to the algorithm named
FISTA [5], which convergence is guaranteed, as recalled in the following theorem
hp+1 = ? p (1 ? tp+1 +

Theorem 1 Given ? 0 ? Rd , and ? = ||?T ?||/n, let h1 = ? 0 and t1 = 1, c0 = 1, then there exists
a constant C0 such that the iterative update (10) satisfies
C0
E? (? p ) ? E? (? ? ) ? 2 .
(3)
p
As it happens for other accelerations of the basic forward-backward splitting algorithm such as [19,
6, 4], convergence of the sequence ? p is no longer guaranteed unless strong convexity is assumed.
However, sacrificing theoretical convergence for speed may be mandatory in large scale applications.
Furthermore, there is a strong empirical evidence that ? p is indeed convergent (see Section 4).
3.2

The projection

Note that the proximity operator of the penalty ?Goverlap does not admit a closed form and must be
computed approximatively. In fact the projection on the convex set
K = ??Goverlap(0) = {v ? Rd , kvkGr ? 1 for r = 1, . . . , B}.
cannot be decomposed group-wise, as in standard group `1 regularization, which proximity operator
resolves to a group-wise soft-thresholding operator (see Eq. (9) later). Nonetheless, the following
lemma shows that, when evaluating the projection, ?K , we can restrict ourselves to a subset of
? = |G|
? ? B active groups. This equivalence is crucial for speeding up the algorithm, in fact B
? is
B
the number of selected groups which is small if one is interested in sparse solutions.
Lemma 1 Given ? ? Rd , G = {Gr }B
r=1 with Gr ? {1, . . . , d}, and ? > 0, the projection onto the
convex set ? K with K = {v ? Rd , kvkGr ? 1 for r = 1, . . . , B} is given by
Minimize
subject to

2

kv ? ?k
?
v ? Rd , kvkG ? ? for G ? G.

where G? := {G ? G, k?kG > ? } .
3

(4)

The proof (given in the supplementary material) is based on the fact that the convex set ? K is the
? is typically much
intersection of cylinders that are all centered on a coordinate subspace. Since B
smaller than d, it is convenient to solve the dual problem associated to (4).
Theorem 2 Given ? ? Rd , {Gr }B
r=1 with Gr ? {1, . . . , d}, and ? > 0, the projection onto the
convex set ? K with K = {v ? Rd , kvkGr ? ? for r = 1, . . . , B} is given by
[?? K (?)]j =

(1 +

?j
PB?

where ?? is the solution of
argmax f (?),

r=1

for j = 1, . . . , d

with f (?) :=

?
??RB
+

(5)

??r 1r,j )
d
X
j=1

1+

??j2
PB?
r=1

?
1r,j ?r

?
B
X

?r ? 2 ,

(6)

r=1

?1, . . . , G
? ? }, and 1r,j is 1 if j belongs to group G
? r and 0 otherwise.
G? = {G ? G, k?kG > ? } := {G
B
Equation (6) is the dual problem associated to (4), and, since strong duality holds, the minimum
of (4) is equal to the maximum of the dual problem, which can be efficiently solved via Bertsekas?
projected Newton method described in [7], and here reported as Algorithm 1.
Algorithm 1 Projection
?

Given: ? ? Rd , ?init ? RB , ? ? (0, 1), ? ? (0, 1/2),  > 0
Initialize: q = 0, ?0 = ?init
? do
while (?r f (?q ) > 0 if ?qr = 0, or |?r f (?q )| >  if ?qr > 0, for r = 1, . . . , B)
q := q + 1
q = min{, ||?q ? [?q ? ?f (?q )]+ ||}
q
I+
= {r such that 0 ? ?qr ? q , ?r f (?q ) > 0}

q
q
0
if r 6= s, and r ? I+
or s ? I+
Hr,s =
q
?r ?s f (? ) otherwise

(7)

?(?) = [?q ? ?(H q )?1 ?f (?q )]+
m=0
n P
o
P
q
m
q
q
q ?r f (? ) +
q ?r f (? )[? ? ?r (?
while f (?q ) ? f (?(? m )) ? ? ? m r?I
)] do
r
/ +
r?I+
m := m + 1
end while
?q+1 = ?(? m )
end while
return ?q+1
Bertsekas? iterative scheme combines the basic simplicity of the steepest descent iteration [22] with
the quadratic convergence of the projected Newton?s method [8]. It does not involve the solution of
a quadratic program thereby avoiding the associated computational overhead.
3.3

Computing the regularization path

In Algorithm 2 we report the complete Group Lasso with Overlap primal-dual (GLO-pridu) scheme
for computing the regularization path, i.e. the set of solutions corresponding to different values of
the regularization parameter ?1 > . . . > ?T , for problem (1). Note that we employ the continuation
strategy proposed in [11]. A similar warm starting is applied to the inner iteration, where at the p-th
step ?init is determined by the solution of the (p?1)-th projection. Such an initialization empirically
proved to guarantee convergence, despite the local nature of Bertsekas? scheme.
3.4

The replicates formulation

An alternative way to solve the optimization problem (1) is proposed by [12], where the authors
show that problem (1) is equivalent to the standard group `1 regularization (without overlap) in an
expanded space built by replicating variables belonging to more than one group:
4

Algorithm 2 GLO-pridu regularization path
Given: ?1 > ?2 > ? ? ? > ?T , G, ? ? (0, 1), ? ? (0, 1/2), 0 > 0, ? > 0
Let: ? = ||?T ?||/n
Initialize: ?(?0 ) = 0
for t = 1, . . . , T do
Initialize: ? 0 = ?(?t?1 ), ??0 = 0
while ||? p ? ? p?1 || > ?||? p?1 || do
? w = hp ? (n?)?1 ?T (?hp ? y)
? Find G? = {G ? G, kwkG ? ? }
? initialization ?? and tolerance 0 p?3/2
? Compute ??p via Algorithm 1 with groups G,
p?1
P
?
B
?1
? Compute ? p as ?jp = wj (1 + r=1 ?q+1
1
)
for
j
=
1,
. . . , d, see Equation (5)
r,j
r
? Update cp , tp , and hp as in (10)
end while
?(?t ) = ? p
end for
return ?(?1 ), . . . , ?(?T )

)
B
X
1
? ? ,
? ?? ? y||2 + 2?
(8)
||?||
??? ? argmin
||?
Gr
n
? d?
??R
r=1
? is the matrix built by concatenating copies of ? restricted each to a certain group, i.e.
where ?
?
?1, . . . , G
? B } = {[1, . . . , |G1 |], [1+|G1 |, . . . , |G1 |+|G2 |], . . . , [d??
(?j )j?G? r = (?j )j?Gr , where {G
P
B
?
|GB |, . . . , d|]},
and d? =
r=1 |Gr | is the number of total variables obtained after including the
PB
?
replicates. One can then reconstruct ? ? from ??? as ?j? = r=1 ?Gr (??? ), where ?Gr : Rd ? Rd
maps ?? in v ? Rd , such that supp(v) ? Gr and (vj )j?Gr = (??j )j?G?r , for r = 1, . . . , B. The main
advantage of the above formulation relies on the possibility of using any state-of-the-art optimization
procedure for group lasso. In terms of proximal methods, a possible solution is given by Algorithm
3, where S? /? is the proximity operator of the new penalty, and can be computed exactly as




?
? ? ??
?r,
S? /? (?)
= ||?||
??j ,
for j ? G
for r = 1, . . . , B.
(9)
Gr
? +
j
Algorithm 3 GL-prox
? T ?||/n
?
Given: ??0 ? Rd , ? > 0, ? = ||?
1
0 1
?
?
Initialize: p = 0, h = ? , t = 1
while convergence not reached do


p := p + 1
? p ? (n?)?1 ?
? p ? y)
? T (?
?h
??p = S? /? h
(10)
q
1
cp = (1 ? tp )cp?1 ,
tp+1 = (?cp + c2p + 8cp )
4
tp+1
tp+1
p+1
p
?
?
h
= ? (1 ? tp+1 +
) + ??p?1 (tp ? 1)
tp
tp
end while
return ??p
(

Note that in principle, by applying Lemma 1, the group-soft-thresholding operator in (9) can be computed only on the active groups. In practice this does not yield any advantage, since the identification
of the active groups has the same computational cost of the thresholding itself.
3.5

Computational issues

For both GL-prox and GLO-pridu, the complexity of one iteration is the sum of the complexity of
computing the gradient of the data term and the complexity of computing the proximity operator
? for GLO-pridu and GL-prox,
of the penalty term. The former has complexity O(dn) and O(dn)
5

respectively, for the case n < d. One should then add at each iteration, the cost of performing the
projection onto K. This can be neglected for the case of replicated variables.On the other hand,
?
the time complexity of one iteration for Algorithm 1 is driven by the number of active groups B.
This number is typically small when looking for sparse solutions. The complexity is thus given
??B
? matrix H, O(B
? 3 ), and the
by the sum of the complexity of evaluating the inverse of the B
? 2 ). The worst case complexity would then
complexity of performing the product H ?1 ?g(?), O(B
3
?
be O(B ). Nevertheless, in practice the complexity is much lower because matrix H is highly
sparse. In fact, Equation (7) tells us that the part of matrix H corresponding to the active set I+ is
?=B
?? + B
?+ , where B
?? is the number of non active constraints,
diagonal. As a consequence, if B
?
and B+ is the number of active constraints, then the complexity of inverting matrix H is at most
3
?+ ) + O(B
??
?? ? B
?? non diagonal part of matrix H is highly sparse, since
O(B
). Furthermore the B
?r ? G
? s = ? and the complexity of inverting it is in practice much lower than O(B
? 3 ).
Hr,s = 0 if G
?
3
?
?
The worst case complexity for computing the projection onto K is thus O(q ? B+ ) + O(q ? B?
),
where q is the number of iterations necessary to reach convergence. Note that even if, in order to
guarantee convergence, the tolerance for evaluating convergence of the inner iteration must decrease
with the number of external iterations, in practice, thanks to warm starting, we observed that q is
rarely greater than 10 in the experiments presented here.
Concerning the number of iterations required to reach convergence for GL-prox in the replicates
formulation, we empirically observed that it requires a much higher number of iterations than GLOpridu (see Table 3). We argue that such behavior is due to the combination of two occurences: 1) the
? is 0 even if ? is locally well conditioned, 2) the decomposition
local condition number of matrix ?
?
?
?
of ? as ? is possibly not unique, which is required in order to have a unique solution for (8).
? In fact, since E? is convex but not
The former is due to the presence of replicated columns in ?.
necessarily strictly convex ? as when n < d ?, uniqueness and convergence is not always guaranteed
unless some further assumption is imposed. Most convergence results relative to `1 regularization
link uniqueness of the solution as well as the rate of convergence of the Soft Thresholding Iteration
to some measure of local conditioning of the Hessian of the differentiable part of E? (see for instance
Proposition 4.1 in [11], where the Hessian restricted to the set of relevant variables is required to
? so that, if the relevant
? = 1/n?
? T ?,
be full rank). In our case the Hessian for GL-prox is simply H
?
groups have non null intersection, then H restricted to the set of relevant variables is by no means
full rank. Concerning the latter argument, we must say that in many real world problems, such as
bioinformatics, one cannot easily verify that the solution indeed has a unique decomposition. In
fact, we can think of trivial examples where the replicates formulation has not a unique solution.

4

Numerical Experiments

In this section we present numerical experiments aimed at comparing the running time performance
of GLO-pridu with state-of-the-art algorithms. To ensure a fair comparison, we first run some preliminary experiments to identify the fastest codes for group `1 regularization with no overlap. We
refer to [6] for an extensive empirical and theoretical comparison of different optimization procedures for solving `1 regularization. Further empirical comparisons can be found in [15].
4.1

Comparison of different implementations for standard group lasso

We considered three algorithms which are representative of the optimization techniques used to
solve group lasso: interior-point methods, (group) coordinate descent and its variations, and proximal methods. As an instance of the first set of techniques we employed the publicly available
Matlab code at http://www.di.ens.fr/?fbach/grouplasso/index.htm described
in [1]. For coordinate descent methods, we employed the R-package grlplasso, which implements block coordinate gradient descent minimization for a set of possible loss functions. In the
following we will refer to these two algorithms as ??GL-IP? and ?GL-BCGD?. Finally we use our
Matlab implementation of Algorithm GL-prox as an instance of proximal methods.
We first observe that the solutions of the three algorithms coincide up to an error which depends
on each algorithm tolerance. We thus need to tune each tolerance in order to guarantee that all
iterative algorithms are stopped when the level of approximation to the true solution is the same.
6

Table 1: Running time (mean and standard deviation) in seconds for computing the entire regularization path of GL-IP, GL-BCGD, and GL-prox for different values of B, and n. For B = 1000,
GL-IP could not be computed due to memory reasons.
n = 100

n = 500

n = 1000

GL-IP
GL-BCGD
GL-prox
GL-IP
GL-BCGD
GL-prox

B = 10
5.6 ? 0.6
2.1 ? 0.6
0.21 ? 0.04

B = 10
2.30 ? 0.27
2.15 ? 0.16
0.1514 ? 0.0025

GL-IP
GL-BCGD
GL-prox

B = 10
1.92 ? 0.25
2.06 ? 0.26
0.182 ? 0.006

B = 100
60 ? 90
2.8 ? 0.6
2.9 ? 0.4

B = 1000
?
14.4 ? 1.5
183 ? 19

B = 100
370 ? 30
4.7 ? 0.5
2.54 ? 0.16
B = 100
328 ? 22
18 ? 3
4.7 ? 0.5

B = 1000
?
16.5 ? 1.2
109 ? 6
B = 1000
?
20.6 ? 2.2
112 ? 6

Toward this end, we run Algorithm GL-prox with machine precision, ? = 10?16 , in order to have
a good approximation of the asymptotic solution. We observe that for many values of n and d, and
over a large range of values of ? , the approximation of GL-prox when ? = 10?6 is of the same
order of the approximation of GL-IP with optparam.tol = 10?9 , and of GL-BCGD with tol =
10?12 . Note also that with these tolerances the three solutions coincide also in terms of selection,
i.e. their supports are identical for each value of ? . Therefore the following results correspond to
optparam.tol = 10?9 for GL-IP, tol = 10?12 for GL-BCGD, and ? = 10?6 for GL-prox.
For the other parameters of GL-IP we used the values used in the demos supplied with the code.
Concerning the data generation protocol, the input variables x = (x1 , . . . , xd ) are uniformly drawn
from [?1, 1]d . The labels y are computed using a noise-corrupted linear regression function, i.e. y =
? ? x + w, where ? depends on the first 30 variables, ?j = 1 if j = 1, . . . , 30, and 0 otherwise, w is an
additive gaussian white noise, and the signal to noise ratio is 5:1. In this case the dictionary coincides
with the variables, ?j (x) = xj for j = 1, . . . , d. We then evaluate the entire regularization path for
the three algorithms with B sequential groups of 10 variables, (G1 =[1, . . . , 10], G2 =[11, . . . , 20],
and so on), for different values of n and B. In order to make sure that we are working on the correct
range of values for the parameter ? , we first evaluate the set of solutions of GL-prox corresponding
to a large range of 500 values for ? , with ? = 10?4 . We then determine the smallest value of ?
which corresponds to selecting less than n variables, ?min , and the smallest one returning the null
solution, ?max . Finally we build the geometric series of 50 values between ?min and ?max , and use
it to evaluate the regularization path on the three algorithms. In order to obtain robust estimates of
the running times, we repeat 20 times for each pair n, B.
In Table 1 we report the computational times required to evaluate the entire regularization path for
the three algorithms. Algorithms GL-BCGD and GL-prox are always faster than GL-IP which, due
to memory reasons, cannot by applied to problems with more than 5000 variables, since it requires
to store the d ? d matrix ?T ? ?. It must be said that the code for GP-IL was made available
mainly in order to allow reproducibility of the results presented in [1], and is not optimized in terms
of time and memory occupation. However it is well known that standard second-order methods are
typically precluded on large data sets, since they need to solve large systems of linear equations
to compute the Newton steps. GL-BCGD is the fastest for B = 1000, whereas GL-prox is the
fastest for B = 10, 100. The candidates as benchmark algorithms for comparison with GLO-pridu
are GL-prox and GL-BCGD. Nevertheless we observed that, when the input data matrix contains
a significant fraction of replicated columns, this algorithm does not provide sparse solutions. We
therefore compare GLO-pridu with GL-prox only.
4.1.1

Projection vs duplication

The data generation protocol is equal to the one described in the previous experiments, but ? depends
on the first 12/5b variables (which correspond to the first three groups)
? = ( c, . . . , c , 0, 0, . . . , 0).
{z
}
| {z } |
b?12/5 times d?b?12/5 times

7

We then define B groups of size b, so that d? = B ? b > d. The first three groups correspond to the
subset of relevant variables, and are defined as G1 = [1, . . . , b], G2 = [4/5b + 1, . . . , 9/5b], and
G3 = [1, . . . , b/5, 8/5b + 1, . . . , 12/5b], so that they have a 20% pair-wise overlap. The remaining
B ? 3 groups are built by randomly drawing sets of b indexes from [1, d]. In the following we
will let n = 10|G1 ? G2 ? G3 |, i.e. n is ten times the number of relevant variables, and vary d, b.
We also vary the number of groups B, so that the dimension of the expanded space is ? times the
input dimension, d? = ?d, with ? = 1.2, 2, 5. Clearly this amounts to taking B = ? ? d/b. The
parameter ? can be thought of as the average number of groups a single variable belongs to. We
identify the correct range of values for ? as in the previous experiments, using GLO-pridu with loose
tolerance, and then evaluate the running time and the number of iterations necessary to compute the
entire regularization path for GL-prox on the expanded space and GLO-pridu, both with ? = 10?6 .
Finally we repeat 20 times for each combination of the three parameters d, b, and ?.
Table 2: Running time (mean ? standard deviation) in seconds for b = 10 (top), and b = 100 (below).
For each d and ?, the left and right side correspond to GLO-pridu, and GL-prox, respectively.
d =1000
d =5000
d =10000
d =1000
d =5000
d =10000

? = 1.2
0.15 ? 0.04 0.20 ? 0.09
1.1 ? 0.4
1.0 ? 0.6
2.1 ? 0.7
2.1 ? 1.4

?=2
1.6 ? 0.9
5.1 ? 2.0
1.55 ? 0.29 2.4 ? 0.7
3.0 ? 0.6
4.5 ? 1.4

?=5
12.4 ? 1.3
68 ? 8
103 ? 12
790 ? 57
460 ? 110 2900 ? 400

? = 1.2
11.7 ? 0.4 24.1 ? 2.5
31 ? 13
38 ? 15
16.6 ? 2.1
13 ? 3

?=2
11.6 ? 0.4
42 ? 4
90 ? 5
335 ? 21
90 ? 30
270 ? 120

?=5
13.5 ? 0.7 1467 ? 13
85 ? 3
1110 ? 80
296 ? 16
?

Table 3: Number of iterations (mean ? standard deviation) for b = 10 (top) and b = 100 (below).
For each d and ?, the left and right side correspond to GLO-pridu, and GL-prox, respectively.

d =1000
d =5000
d =10000

? = 1.2
100 ? 30 80 ? 30
100 ? 40 70 ? 30
100 ? 30 70 ? 40

?=2
1200 ? 500 1900 ? 800
148 ? 25
139 ? 24
160 ? 30
137 ? 26

d =1000
d =5000
d =10000

? = 1.2
913 ? 12 2160 ? 210
600 ? 400 600 ? 300
81 ? 11
63 ? 11

?=5
2150 ? 160
11000 ? 1300
6600 ? 500
27000 ? 2000
13300 ? 1900 49000 ? 6000

?=2
894 ? 11
2700 ? 300
1860 ? 110 4590 ? 290
1000 ? 500 1800 ? 900

?=5
895 ? 10 4200 ? 400
1320 ? 30 6800 ? 500
2100 ? 60
?

Running times and number of iterations are reported in Table 2 and 3, respectively. When the degree
of overlap ? is low the computational times of GL-prox and GLO-pridu are comparable. As ?
increases, there is a clear advantage in using GLO-pridu instead of GL-prox. The same behavior
occurs for the number of iterations.

5

Discussion

We have presented an efficient optimization procedure for computing the solution of group lasso
with overlapping groups of variables, which allows dealing with high dimensional problems with
large groups overlap. We have empirically shown that our procedure has a great computational
advantage with respect to state-of-the-art algorithms for group lasso applied on the expanded space
built by replicating variables belonging to more than one group. We also mention that computational
performance may improve if our scheme is used as core for the optimization step of active set
methods, such as [23]. Finally, as shown in [17], the improved computational performance enables
to use group `1 regularization with overlap for pathway analysis of high-throughput biomedical data,
since it can be applied to the entire data set and using all the information present in online databases,
without pre-processing for dimensionality reduction.
8

References
[1] F. Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine
Learning Research, 9:1179?1225, 2008.
[2] F. Bach. High-dimensional non-linear variable selection through hierarchical kernel learning.
Technical Report HAL 00413473, INRIA, 2009.
[3] F. R. Bach, G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the smo
algorithm. In ICML, volume 69 of ACM International Conference Proceeding Series, 2004.
[4] A. Beck and Teboulle. M. Fast gradient-based algorithms for constrained total variation image
denoising and deblurring problems. IEEE Transactions on Image Processing, 18(11):2419?
2434, 2009.
[5] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM J. Imaging Sci., 2(1):183?202, 2009.
[6] S. Becker, J. Bobin, and E. Candes. Nesta: A fast and accurate first-order method for sparse
recovery, 2009.
[7] D. Bertsekas. Projected newton methods for optimization problems with simple constraints.
SIAM Journal on Control and Optimization, 20(2):221?246, 1982.
[8] R. Brayton and J. Cullum. An algorithm for minimizing a differentiable function subject to. J.
Opt. Th. Appl., 29:521?558, 1979.
[9] J. Duchi and Y. Singer. Efficient online and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10:28992934, December 2009.
[10] O. Guler. New proximal point algorithm for convex minimization. SIAM J. on Optimization,
2(4):649?664, 1992.
[11] E. T. Hale, W. Yin, and Y. Zhang. Fixed-point continuation for l1-minimization: Methodology
and convergence. SIOPT, 19(3):1107?1130, 2008.
[12] L. Jacob, G. Obozinski, and J.-P. Vert. Group lasso with overlap and graph lasso. In ICML,
page 55, 2009.
[13] R. Jenatton, J.-Y . Audibert, and F. Bach. Structured variable selection with sparsity-inducing
norms. Technical report, INRIA, 2009.
[14] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for sparse hierarchical
dictionary learning. In Proceeding of ICML 2010, 2010.
[15] I. Loris. On the performance of algorithms for the minimization of l1 -penalized functionals.
Inverse Problems, 25(3):035008, 16, 2009.
[16] L. Meier, S. van de Geer, and P. Buhlmann. The group lasso for logistic regression. J. R.
Statist. Soc, B(70):53?71, 2008.
[17] S. Mosci, S. Villa, Verri A., and L. Rosasco. A fast algorithm for structured gene selection.
presented at MLSB 2010, Edinburgh.
[18] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o(1/k 2 ). Doklady AN SSSR, 269(3):543?547, 1983.
[19] Y. Nesterov. Smooth minimization of non-smooth functions.
103(1):127?152, 2005.

Math. Prog. Series A,

[20] M. Y. Park and T. Hastie. L1-regularization path algorithm for generalized linear models. J. R.
Statist. Soc. B, 69:659?677, 2007.
[21] L. Rosasco, M. Mosci, S. Santoro, A. Verri, and S. Villa. Iterative projection methods for
structured sparsity regularization. Technical Report MIT-CSAIL-TR-2009-050, MIT, 2009.
[22] J. Rosen. The gradient projection method for nonlinear programming, part i: linear constraints.
J. Soc. Ind. Appl. Math., 8:181?217, 1960.
[23] V. Roth and B. Fischer. The group-lasso for generalized linear models: uniqueness of solutions
and efficient algorithms. In Proceedings of 25th ICML, 2008.
[24] P. Zhao, G. Rocha, and B. Yu. The composite absolute penalties family for grouped and
hierarchical variable selection. Annals of Statistics, 37(6A):3468?3497, 2009.
9

"
6371,"COMBINED NEURAL NETWORKS
FOR TIME SERIES ANALYSIS
Iris Ginzburg and David Horn
School of Physics and Astronomy
Raymond and Beverly Sackler Faculty of Exact Science
Tel-Aviv University
Tel-A viv 96678, Israel

Abstract
We propose a method for improving the performance of any network designed to predict the next value of a time series. Vve advocate analyzing the deviations of the network's predictions from the
data in the training set . This can be carried out by a secondary network trained on the time series of these residuals. The combined
system of the two networks is viewed as the new predictor. We
demonstrate the simplicity and success of this method, by applying it to the sunspots data. The small corrections of the secondary
network can be regarded as resulting from a Taylor expansion of
a complex network which includes the combined system. \\Te find
that the complex network is more difficult to train and performs
worse than the two-step procedure of the combined system.

1

INTRODUCTION

The use of neural networks for computational tasks is based on the idea that the
efficient way in which the nervous system handles memory and cognition is worth
immitating. Artificial implementations are often based on a single network of mathematical neurons. We note, however, that in biological systems one can find collections of consecutive networks, performing a complicated task in several stages, with
later stages refining the performance of earlier ones. Here we propose to follow this
strategy in artificial applications.

224

Combined Neural Networks for Time Series Analysis

We study the analysis of time series, where the problem is to predict the next element on the basis of previous elements of the series. One looks then for a functional
relation
Yn = f (Yn -1 , Yn - 2, ... , Yn - m) .
(1 )
This type of representation is particularly useful for the study of dynamical systems. These are characterized by a common continuous variable, time, and many
correlated degrees of freedom which combine into a set of differential equations.
Nonetheless, each variable can in principle be described by a lag-space representation of the type 1 . This is valid even if the Y = y(t) solution is unpredictable as in
chaotic phenomena.
Weigend Huberman and Rumelhart (1990) have studied the experimental series
of yearly averages of sunspots activity using this approach. They have realized
the lag-space representation on an (m, d, 1) network, where the notation implies a
hidden layer of d sigmoidal neurons and one linear output. Using m
12 and a
weight-elimination method which led to d = 3, they obtained results which compare
favorably with the leading statistical model (Tong and Lim, 1980). Both models do
well in predicting the next element of the sunspots series. Recently, Nowlan and
Hinton (1992) have shown that a significantly better network can be obtained if the
training procedure includes a complexity penalty term in which the distribution of
weights is modelled as a mixture of multiple gaussians whose parameters vary in an
adaptive manner as the system is being trained.

=

We propose an alternative method which is capable of improving the performance
of neural networks: train another network to predict the errors of the first one, to
uncover and remove systematic correlations that may be found in the solution given
by the trained network, thus correcting the original predictions. This is in agreement
with the general philosophy mentioned at the beginning, where we take from Nature
the idea that the task does not have to be performed by one complicated network; it
is advantageous to break it into stages of consecutive analysis steps. Starting with
a network which is trained on the sunspots data with back-propagation, we show
that the processed results improve considerably and we find solutions which match
the performance of Weigend et. al.

2

CONSTRUCTION OF THE PRIMARY NETWORK

Let us start with a simple application of back-propagation to the construction of
a neural network describing the sunspots data which are normalized to lie between
o and 1. The network is assumed to have one hidden layer of sigmoidal neurons,
hi i
1"" . "" d, which receives the input of the nth vector:

=

m

hi

= 0'(2: WijYn-j -

Oi)

(2)

j=l

The output of the network, Pn, is constructed linearly,
d

Pn =

2: Wi hi i=l

O.

(3)

225

226

Ginzburg and Hom

The error-function which we minimize is defined by

1

E

N

=2 L

(4)

(Pn - Yn)2

n=m+l

where we try to equate Pn, the prediction or output of the network, with Yn, the
nth value of the series. This is the appropriate formulation for a training set of N
data points which are viewed as N - m strings of length m used to predict the point
following each string.
We will work with two sets of data points. One will be labelled T and be used for
training the network, and the other P will be used for testing its predictive power.
Let us define the average error by

1

{s

= jjSfj 2:(Pn -

(5)

Yn)2

nES

where the set S is either Tor P. An alternative parameter was used by Weigend et.
al. ,in which the error is normalized by the standard deviation of the data. This
leads to an average relative variance (arv) which is related to the average error
through

(6)

=

Following Weigend et. al. we choose m
12 neurons in the first layer and
IITII 220 data points for the training set. The following IIPII 35 years are
used for testing the predictions of our network. We use three sigmoidal units in the
hidden layer and run with a slow convergence rate for 7000 periods. This is roughly
where cross-validation would indicate that a minimum is reached. The starting
parameters of our networks are chosen randomly. Five examples of such networks
are presented in Table 1.

=

3

=

THE SECONDARY NETWORK

Given the networks constructed above, we investigate their deviations from the
desired values
qn = Yn - Pn?
(7)
A standard statistical test for the quality of any predictor is the analysis of the
correlations between consecutive errors. If such correlations are found, the predictor
must be improved. The correlations reflect a systematic deviation of the primary
network from the true solution. We propose not to improve the primary network
by modifying its architecture but to add to it a secondary network which uses the
residuals qn as its new data. The latter is being trained only after the training
session of the primary network has been completed.
Clearly one may expect some general relation of the type
(8)
to exist. Looking for a structure of this kind enlarges considerably the original
space in which we searched for a solution to 1 . We wish the secondary network

Combined Neural Networks for Time Series Analysis

to do a modest task, therefore we assume that much can be gained by looking at
the interdependence of the residuals qn on themselves. This reduces the problem to
finding the best values of
Tn

= b(qn-l, qn-2,""', qn-I)

(9)

which would minimize the new error function
1
E2='2

N

L

(Tn-qn)2.

(10)

n=I+1

Alternatively, one may try to express the residual in terms of the functional values
Tn

=!2(Yn-1, Yn-2,""', Yn-I)

(11)

minimizing again the expression 10 .
When the secondary network completes its training, we propose to view
tn = Pn

+ Tn

(12)

as the new prediction of the combined system. We will demonstrate that a major
improvement can be obtained already with a linear perceptron. This means that
the linear regression
1

Tn =

L aIqn-i + /3

1

(13)

2

(14)

i=l

or

1

Tn =

L a;Yn-i + /3
i=l

is sufficient to account for a large fraction of the systematic deviations of the primary
networks from the true function that they were trained to represent.

4

NUMERICAL RESULTS

We present in Table 1 five examples of results of (12,5,1) networks, i.e. m = 12
inputs, a hidden layer of three sigmoidal neurons and a linear output neuron. These
five examples were chosen from 100 runs of simple back-propagation networks with
random initial conditions by selecting the networks with the smallest R values
(Ginzburg and Horn, 1992). This is a weak constraint which is based on letting
the network generate a large sequence of data by iterating its own predictions, and
selecting the networks whose distribution of function values is the closest to the
corresponding distribution of the training set.
The errors of the primary networks, in particular those of the prediction set ?p, are
quite higher than those quoted by Weigend et. al. who started out from a (12,8,1)
network and brought it down through a weight elimination technique to a (12,5,1)
structure. They have obtained the values ?T = 0.059 ?p = 0.06. We can reduce our
errors and reach the same range by activating a secondary network with I = 11 to
perform the linear regression (3.6) on the residuals of the predictions of the primary
network. The results are the primed errors quoted in the table. Characteristically
we observe a reduction of ?T by 3 - 4% and a reduction of ?p by more than 10%.

227

228

Ginzburg and Hom

#

fT

1
2
3
4
5

0.0614
0.0600
0.0611
0.0621
0.0616

{p

f'

T

0.0587
0.0585
0.0580
0.0594
0.0589

0.0716
0.0721
0.0715
0.0698
0.0681

{'

P

0.0620
0.0663
0.0621
0.0614
0.0604

Table 1
Error parameters of five networks. The unprimed errors are those of the primary
networks. The primed errors correspond to the combined system which includes
correction of the residuals by a linear perceptron with I 11 , which is an autoregressions of the residuals. Slightly better results for the short term predictions are
achieved by corrections based on regression of the residuals on the original input
vectors, when the regression length is 13 (Table 2).

=

#

{T

fT

fp

f'p

1
2
3
4
5

0.061
0.060
0.061
0.062
0.062

0.059
0.059
0.058
0.060
0.059

0.072
0.072
0.072
0.070
0.068

0.062
0.065
0.062
0.061
0.059

Table 2
Error parameters for the same five networks. The primed errors correspond to the
combined system which includes correction of the residuals by a linear perceptron
based on original input vectors with I 13.

=

5

LONG TERM PREDICTIONS

When short term prediction is performed, the output of the original network is
corrected by the error predicted by the secondary network. This can be easily generalized to perform long term predictions by feeding the corrected output produced
by the combined system of both networks back as input to the primary network. The
corrected residuals predicted by the secondary network are viewed as the residuals
needed as further inputs if the secondary network is the one performing autoregression of residuals. We run both systems based on regression on residuals and
regression on functional values to produce long term predictions.
In table 3 we present the results of this procedure for the case of a secondary
network performing regression on residuals. The errors of the long term predictions
are averaged over the test set P of the next 35 years. We see that the errors of
the primary networks are reduced by about 20%. The quality of these long term
predictions is within the range of results presented by Weigend et. al. Using the
regression on (predicted) functional values, as in Eq. 14 , the results are improved
by up to 15% as shown in Table 4.

Combined Neural Networks for Time Series Analysis

,

#

f2

fj

f5

f~

fll

f11

1
2
3
4
5

0.118
0.118
0.117
0.116
0.113

0.098
0.106
0.099
0.099
0.097

0.162
0.164
0.164
0.152
0.159

0.109
0.125
0.112
0.107
0.112

0.150
0.131
0.136
0.146
0.147

0.116
0.101
0.099
0.120
0.123

Table 3
Long term predictions into the future. fn denotes the average error of n time steps
predictions over the P set. The unprimed errors are those of the primary networks.
The primed errors correspond to the combined system which includes correction of
the residuals by a linear perceptron.

,

#

f2

f'2

f5

f'5

f11

f11

1
2
3
4
5

0.118
0.118
0.117
0.117
0.113

0.098
0.104
0.098
0.098
0.096

0.162
0.164
0.164
0.152
0.159

0.107
0.117
0.108
0.105
0.110

0.150
0.131
0.136
0.146
0.147

0.101
0.089
0.086
0.105
0.109

Table 4
Long term predictions into the future. The primed errors correspond to the combined system which includes correction of the residuals by a linear perceptron based
on the original inputs.

6

THE COMPLEX NETWORK

Since the corrections of the secondary network are much smaller than the characteristic weights of the primary network, the corrections can be regarded as resulting
from a Taylor expansion of a complex network which include's the combined system.
This can be simply implemented in the case of Eq. 14 which can be incorporated in
the complex network as direct linear connections from the input layer to the output
neuron, in addition to the non-linear hidden layer, i.e.,

tn

d

m

i=l

i=l

= L:: Wihi + L

viYn-i - () .

(15)

We train such a complex network on the same problem to see how it compares with
the two-step approach of the combined networks described in the previous chapters.
The results depend strongly on the training rates of the direct connections, as
compared with the training rates of the primary connections (i.e. those of the
primary network). When the direct connections are trained faster than the primary
ones, the result is a network that resembles a linear perceptron, with non-linear

229

230

Ginzburg and Hom

corrections. In this case, the assumption of the direct connections being small
corrections to the primary ones no longer holds. The training error and prediction
capability of such a network are worse than those of the primary network. On the
other hand, when the primary connections are trained using a faster training rate,
we expect the final network to be similar in nature to the combined system. Still,
the quality of training and prediction of these solutions is not as good as the quality
of the combined system, unless a big effort is made to find the correct rates. Typical
results of the various systems are presented in Table 5.

type of network
primary network
learning rate of linear weights = 0.1
learning rate of linear weights = 0.02
combined system

0.061
0.062
0.061
0.058

0.072
0.095
0.068
0.062

Table 5
Short term predictions of various networks. The learning rate of primary weights
is 0.04.
The performance of the complex network can be better than that of the primary
network by itself, but it is surpassed by the achievements of the combined system.

7

DISCUSSION

It is well known that increasing the complexity of a network is not the guaranteed
solution to better performance (Geman et. al. 1992). In this paper we propose an
alternative which increases very little the number of free parameters, and focuses on
the residual errors one wants to eliminate. Still one may raise the question whether
this cannot be achieved in one complex network. It can, provided we are allowed to
use different updating rates for different connections. In the extreme limit in which
one rate supersedes by far the other one, this is equivalent to a disjoint architecture
of a combined two-step system. This emphasizes the point that a solution of a
feedforward network to any given task depends on the architecture of the network
as well as on its training procedure.

The secondary network which we have used was linear, hence it defined a simple
regression of the residual on a series of residuals or a series of function values. In
both cases the minimum which the network looks for is unique. In the case in
which the residual is expressed as a regression on function values, the problem can
be recast in a complex architecture. However, the combined procedure guarantees
that the linear weights will be small, i.e. we look for a small linear correction to the
prediction of the primary network. If one trains all weights of the complex network
at the same rate this condition is not met, hence the worse results.
We advocate therefore the use of the two-step procedure of the combined set of
networks. We note that combined set of networks. We note that the secondary
networks perform well on all possible tests: they reduce the training errors, they

Combined Neural Networks for Time Series Analysis

improve short term predictions and they do better on long term predictions as well.
Since this approach is quite general and can be applied to any time-series forecasting
problem, we believe it should be always tried as a correction procedure.

REFERENCES
Geman, S., Bienenstock, E., & Doursat, R., 1992.
bias/variance dilemma. Neural Compo 4, 1-58.

Neural networks and the

Ginzburg, I. & Horn, D. 1992. Learning the rule of a time series. Int. Journal of
Neural Systems 3, 167-177.
Nowlan, S. J. & Hinton, G. E. 1992. Simplifying neural networks by soft weightsharing. Neural Compo 4, 473-493.
Tong, H., & Lim, K. S., 1980. Threshold autoregression, limit cycles and cyclical
data. J. R. Stat. Soc. B 42, 245.
Weigend, A. S., Huberman, B. A. & Rumelhart, D. E., 1990. Predicting the Future:
A Connectionist Approach, Int. Journal of Neural Systems 1, 193-209.

231

"
53,"Analog VLSI Processor Implementing the
Continuous Wavelet Transform

R. Timothy Edwards and Gert Cauwenberghs
Department of Electrical and Computer Engineering
Johns Hopkins University
3400 North Charles Street
Baltimore, MD 21218-2686
{tim,gert}@bach.ece.jhu.edu

Abstract
We present an integrated analog processor for real-time wavelet decomposition and reconstruction of continuous temporal signals covering the
audio frequency range. The processor performs complex harmonic modulation and Gaussian lowpass filtering in 16 parallel channels, each clocked
at a different rate, producing a multiresolution mapping on a logarithmic
frequency scale. Our implementation uses mixed-mode analog and digital circuits, oversampling techniques, and switched-capacitor filters to
achieve a wide linear dynamic range while maintaining compact circuit
size and low power consumption. We include experimental results on the
processor and characterize its components separately from measurements
on a single-channel test chip.

1 Introduction
An effective mathematical tool for multiresolution analysis [Kais94], the wavelet transform
has found widespread use in various signal processing applications involving characteristic
patterns that cover multiple scales of resolution, such as representations of speech and vision.
Wavelets offer suitable representations for temporal data that contain pertinent features both
in the time and frequency domains; consequently, wavelet decompositions appear to be
effective in representing wide-bandwidth signals interfacing with neural systems [Szu92].
The present system performs a continuous wavelet transform on temporal one-dimensional
analog signals such as speech, and is in that regard somewhat related to silicon models
of the cochlea implementing cochlear transforms [Lyon88], [Liu92] , [Watt92], [Lin94].
The multiresolution processor we implemented expands on the architecture developed
in [Edwa93], which differs from the other analog auditory processors in the way signal
components in each frequency band are encoded. The signal is modulated with the center

Analog VLSI Processor Implementing the Continuous Wavelet Transform

1m

_: '\/'V

-I

~
x

Multiplier

s'(t)

LPF

set)

x(t)

693

x(t)

~ ~ yet)
Lff

LPF

~

~

get)

yet)

h(t)

h(t)

Prefilter

(a)

Multiplexer
(b)

Figure 1: Demodulation systems, (a) using multiplication, and (b) multiplexing.
frequency of each channel and subsequently lowpass filtered, translating signal components
taken around the center frequency towards zero frequency. In particular. we consider wavelet
decomposition and reconstruction of analog continuous-time temporal data with a complex
Gaussian kernel according to the following formulae:

Yk(t)

{teo x(e) exp (jWke- Q(Wk(t - e))2) de
(decomposition)

x'(t)

(1)

C 2::y\(t) exp(-jwkt)
k

(reconstruction)
where the center frequencies Wk are spaced on a logarithmic scale. The constant Q sets the
relative width of the frequency bins in the decomposition , and can be adjusted (together
with C) alter the shape of the wavelet kernel. Successive decomposition and reconstruction
transforms yield an approximate identity operation; it cannot be exact as no continuous
orthonormal basis function exists for the CWT [Kais94].

2

Architecture

The above operations are implemented in [Edwa93] using two demodulator systems per
channel, one for the real component of (1), and another for the imaginary component, 90?
out of phase with the first. Each takes the form of a sinusoidal modulator oscillating at
the channel center frequency, followed by a Gaussian-shaped lowpass filter, as shown in
Figure 1 (a). This arrangement requires a precise analog sine wave generator and an accurate
linear analog multiplier. In the present implementation, we circumvent both requirements
by using an oversampled binary representation of the modulation reference signal.

2.1

Multiplexing vs. MUltiplying

Multiplication of an analog signal x(t) with a binary (? 1) sequence is naturally implemented
with high precision using a mUltiplexer, which alternates between presenting either the
input or its inverse -x(t) to the output. This principle is applied to simplify harmonic
modulation. and is illustrated in Figure 1 (b). The multiplier has been replaced by an analog
inverter followed by a multiplexer, where the multiplexer is controlled by an oversampled
binary periodic sequence representing the sine wave reference. The oversampled binary
sequence is chosen to approximate the analog sine wave as closely as possible. disregarding
components at high frequency which are removed by the subsequent lowpass filter. The
assumption made is that no high frequency components are present in the input signal

R. T. EDWARDS, G. CAUWENBERGHS

694
SiglUll

,---- ? ________ ----I In

,,:

In Seltrt

CLK2

eLK]

f ---<.-iK4----1:CLKS-- -: ;----- -- ---- ---- --1

eLK/:

,,

II

E

?

I

II

""

I'

'I

.---f-..., ::

::

Ret'onJlructed :

: cmLy,

0.,.

.--...L..--,::
I

,

I

:

~~--+,~,

'-==:.J

,

: ~ ____ __________ __J

'

I. ________________ :

Reconstruction Input

i

:,

""

Wav elet
Reconstruction

Mult;pl;er

Gaussian Filter

Output Mux ing

Figure 2: Block diagram of a single channel in the wavelet processor, showing test points
A through E.
under modulation, which otherwise would convolve with corresponding high frequency
components in the binary sequence to produce low frequency distortion components at the
output. To that purpose, an additionallowpass filter is added in front of the multiplexer.
Residual low-frequency distortion at the output is minimized by maximizing roll-off of the
filters, placing proper constraints on their cutofffrequencies, and optimally choosing the bit
sequence in the oversampled reference [Edwa95]. Clearly, the signal accuracy that can be
achieved improves as the length N of the sequence is extended. Constraints on the length
N are given by the implied overhead in required signal bandwidth, power dissipation, and
complexity of implementation.

2.2

Wavelet Gaussian Function

The reason for choosing a Gaussian kernel in (l) is to ensure optimal support in both
time and frequency [Gros89]. A key requirement in implementing the Gaussian filter
is linear phase, to avoid spectral distortion due to non-uniform group delays. A worryfree architecture would be an analog FIR filter; however the number of taps required to
accommodate the narrow bandwidth required would be prohibitively large for our purpose.
Instead, we approximate a Gaussian filter by cascading several first-order lowpass filters .
From probabilistic arguments, the obtained lowpass filter approximates a Gaussian filter
increasingly well as the number of stages increases [Edwa93] .

3

Implementation

Two sections of a wavelet processor, each containing 8 parallel channels, were integrated
onto a single 4 mm x 6 mm die in 2 /lm CMOS technology. Both sections can be configured
to perform wavelet decomposition as well as reconstruction. The block diagram for one
of the channels is shown in Figure 2. In addition, a separate test chip was designed which
performs one channel of the wavelet function . Test points were made available at various
points for either input or output, as indicated in boldface capitals, A through E, in Figure 2.
Each channel performs complex harmonic modulation and Gaussian lowpass filtering, as
defined above. At the front end of the chip is a sample-and-hold section to sample timemultiplexed wavelet signals for reconstruction . In cases of both signal decomposition
and reconstruction, each channel removes the input DC component removed, filters the
result through the premultiplication lowpass (PML) filter, inverts the result, and passes
both non-inverted and inverted signals onto the multiplexer. The multiplexer output is
passed through a postmultiplication lowpass filter (PML, same architecture) to remove high
frequency components of the oversampled sequence, and then passed through the Gaussianshaped lowpass filter. The cutoff frequencies of all filters are controlled by the clock rates

695

Analog VLSI Processor Implementing the Continuous Wavelet Transform

(CLKI to CLK4 in Figure 2). The remainder of the system is for reconstruction and for
time-multiplexing the output.

3.1

MUltiplier

The multiplier is implemented by use of the above multiplexing scheme, driven by an
oversampled binary sequence representing a sine wave. The sequence we used was 256
samples in length, created from a 64-sample base sequence by reversal and inversion . The
sequence length of256 generates a modulator wave of 4 kHz (useful for speech applications)
from a clock of about 1 MHz.
We derived a sequence which, after postfiltering through a 3rd-order lowpass filter of the
fonn of the PML prefilter (see below), produces a sine wave in which all hannonics are
more than 60 dB down from the primary [Edwa95]. The optimized 64-bit base sequence
consists of 11 zeros and 53 ones, allowing a very simple implementation in which an address
decoder decodes the ""zero"" bits. The binary sequence is shown in Figure 4. The magnitude
of the prime hannonic of the sequence is approximately 1.02, within 2% of unity.
The process of reversing and inverting the sequence is simplified by using a gray code
counter to produce the addresses for the sequence, with only a small amount of combinatorial
logic needed to achieve the desired result [Edwa95]. It is also straightforward to generate
the addresses for the cosine channel, which is 90? out of phase with the original.

3.2 Linear Filtering
All filters used are implemented as linear cascades of first-order, single-pole filter sections.
The number of first-order sections for the PML filters is 3. The number of sections for the
""Gaussian"" filter is 8, producing a suitable approximation to a Gaussian filter response for
all frequencies of interest (Figure 5).
Figure 3 shows one first-order lowpass section of the filters as implemented. This standard

>-.+--o

v,,,

va""'

+

Figure 3: Single discrete-time lowpass filter section.
switched-capacitor circuit implements a transfer function containing a single pole, approximately located in the Laplace domain at s = Is / a for large values of the parameter a, with
Is being the sampling frequency. The value for this parameter a is fixed at the design stage
as the ratio of two capacitors in Figure 3, and was set to be 15 for the The PML filters and
12 for the Gaussian filters.

4 Measured Results
4.1

Sine wave modulator

We tested the accuracy of the sine wave modulation signal by applying two constant voltages
at test points A and B, such that the sine wave modulation signal is effectively multiplied

R. T. EDWARDS, G. CAUWENBERGHS

696

Sine sequence and filtered sine wave output

Binary sine sequence
Simulated filtered output
x
Measured output
-1.5 L -_ _ _- - '_ _ _ _- ' -_ _ _ _........_ _ _ _- ' -_ _ _ _.J...J

o

50

100

150

200

250

Time (us)

Figure 4: Filtered sine wave output.

by a constant. The output of the mUltiplier is filtered and the output taken at test point D,
before the Gaussian filter. Figure 4 shows the (idealized) multiplexer output at test point
C, which accurately creates the desired binary sequence. Figure 4 also shows the measured
sine wave after filtering with the PML filter and the expected output from the simulation
model, using a deviating value of 8.0 for the capacitor ratio a, as justified below. FFT
analysis of Figure 4 has shown that the resulting sine wave has all harmonics below about
-49 dB . This is in good agreement with the simulation model, provided a correction is made
for the value of the capacitor ratio a to account for fringe and (large) parasitic capacitances.
The best fit for the measured data from the postmultiplication filter is a = 8.0, compared to
the desired value of a = 15.0. The transform of the simulated output shown in the figure
takes into account the smaller value of a. Because the postmultiplication filter is followed
by the Gaussian filter, the bandwidth of the output can be directly controlled by proper
clocking ofthe Gaussian filter, so the distortion in the sine wave is ultimately much smaller
than that measured at the output of the postmultiplication filter.

4.2

Gaussian filter

The Gaussian filter was tested by applying a signal at test point D and measuring the
response at test point E. Figure 5 shows the response of the Gaussian filter as compared to
expected responses. There are two sets of curves, one for a filter clocked at 64 kHz, and the
other clocked at 128 kHz; these curves are normalized by plotting time relative to the clock
frequency is . The solid line indicates the best match for an 8th-order lowpass filter, using
the capacitor ratio, a, as a fitting parameter. The best-fit value of a is approximately 6.8.
This is again much lower than the capacitor area ratio of 12 on the chip. The dotted line is
the response of the ideal Gaussian characteristic exp ( _w 2 / (2aw~)) approximated by the
cascade of first-order sections with capacitor ratio a.
Figure 5 (b) shows the measured phase response of the Gaussian filter for the 128 kHz
clock. The phase response is approximately linear throughout the passband region.

Analog VLSI Processor Implementing the Continuous Wavelet Transform

697

Gaussian filter response

o~~~~--~-=~~~~~~~--~

x
o

x
0

iii'-10
~

Chip data at 64kHz clock
Chip data at 128kHz clock
8th-order filter ideal response
Gaussian filter ideal response

500

<lJ

]1

-20

~

?E

-30

""0
<lJ

o

N

~ -40

Theoretical 8-stage phase
Measured response

?

o
Z -50
0.01

0.07

Frequency (units fs)

0.08

0.0 I

0.02

0.03

0.04

0.05

0.06

0.07

Frequency (units fs)

Figure 5: Gaussianfilter transfer functions: theoretical and actual. (a) Relative amplitude;
(b) Phase.

4.3

Wavelet decomposition

Figure 6 shows the test chip performing a wavelet transform on a simple sinusoidal input,
illustrating the effects of (oversampled) sinusoidal modulation followed by lowpass filtering
through the Gaussian filter. The chip multiplier system is clocked at 500 kHz. The input
wave is approximately 3.1 kHz, close to the center frequency of the modulator signal,
which is the clock rate divided by 128, or about 3.9 kHz (a typical value for the highestfrequency channel in an auditory application). The top trace in the figure shows the filtered
and inverted input, taken from test point B. The middle trace shows the output of the
multiplexer (test point C), wherein the output is multiplexed between the signal and its
inverse. The bottom trace is taken from the system output (labeled Cosine Out in Figure 2)
and shows the demodulated signal of frequency 800 Hz (= 3.9 kHz - 3.1 kHz). Not shown
is the cosine output, which is 90? out of phase with the one shown. This demonstrates
the proper operation of complex demodulation in a single channel configured for wavelet
decomposition. In addition, we have tested the full16-channel chip decomposition, and all
individual parts function properly. The total power consumption of the 16-channel wavelet
chip was measured to be less than 50mW, of which a large fraction can be attributed to
external interfacing and buffering circuitry at the periphery of the chip.

5

Conclusions

We have demonstrated the full functionality of an analog chip performing the continuous
wavelet transform (decomposition). The chip is based on mixed analog/digital signal
processing principles, and uses a demodulation scheme which is accurately implemented
using oversampling methods. Advantages of the architecture used in the chip are an
increased dynamic range and a precise control over lateral synchronization of wavelet
components. An additional advantage inherent to the modulation scheme used is the
potential to tune the channel bandwidths over a wide range, down to unusually narrow
bands, since the cutoff frequency of the Gaussian filter and the center frequency of the
modulator are independently adjustable and precisely controllable parameters.

References
G. Kaiser, A Friendly Guide to Wavelets, Boston, MA: Birkhauser, 1994.

T. Edwards and M. Godfrey, ""An Analog Wavelet Transform Chip,"" IEEE Int'l Can! on

0.08

698

R. T. EDWARDS, G. CAUWENBERGHS

Figure 6: Scope trace of the wavelet transform: filtered input (top), multiplexed signal
(middle), and wavelet output (bottom).
Neural Networks, vol. III, 1993, pp. 1247-1251.
T. Edwards and G. Cauwenberghs, ""Oversampling Architecture for Analog Harmonic

Modulation,"" to appear in Electronics Letters, 1996.
A Grossmann, R Kronland-Martinet, and J. MorIet, ""Reading and understanding continuous wavelet transforms,"" Wavelets: Time-Frequency Methods and Phase Space. SpringerVerlag, 1989, pp. 2-20.
W. Liu, AG. Andreou, and M.G. Goldstein, ""Voiced-Speech Representation by an Analog
Silicon Model ofthe Auditory Periphery,"" IEEE T. Neural Networks, vol. 3 (3), pp 477-487,
1992.
J. Lin, W.-H. Ki, T. Edwards, and S. Shamma, ""Analog VLSI Implementations of Auditory Wavelet Transforms Using Switched-Capacitor Circuits,"" IEEE Trans. Circuits and
Systems-I, vol.41 (9), pp. 572-583, September 1994.
A Lu and W. Roberts, ''A High-Quality Analog Oscillator Using Oversampling D/A Conversion Techniques,"" IEEE Trans. Circuits and Systems-II, vol.41 (7), pp. 437-444, July
1994.

RF. Lyon and C.A Mead, ""An Analog Electronic Cochlea,"" IEEE Trans. Acoustics, Speech
and Signal Proc., vol. 36, pp 1119-1134, 1988.
H.H. Szu, B. Tefter, and S. Kadembe, ""Neural Network Adaptive Wavelets for Signal Representation and Classification,"" Optical Engineering, vol. 31 (9), pp. 1907-1916, September
1992.
L. Watts, D.A Kerns, and RF. Lyon, ""Improved Implementation of the Silicon Cochlea,""
IEEE Journal of Solid-State Circuits, vol. 27 (5), pp 692-700,1992.

"
3795,"Continuous-Time Regression Models for
Longitudinal Networks

Duy Q. Vu
Department of Statistics
Pennsylvania State University
University Park, PA 16802
dqv100@stat.psu.edu

Arthur U. Asuncion?
Department of Computer Science
University of California, Irvine
Irvine, CA 92697
asuncion@ics.uci.edu

David R. Hunter
Department of Statistics
Pennsylvania State University
University Park, PA 16802
dhunter@stat.psu.edu

Padhraic Smyth
Department of Computer Science
University of California, Irvine
Irvine, CA 92697
smyth@ics.uci.edu

Abstract
The development of statistical models for continuous-time longitudinal network
data is of increasing interest in machine learning and social science. Leveraging
ideas from survival and event history analysis, we introduce a continuous-time
regression modeling framework for network event data that can incorporate both
time-dependent network statistics and time-varying regression coefficients. We
also develop an efficient inference scheme that allows our approach to scale to
large networks. On synthetic and real-world data, empirical results demonstrate
that the proposed inference approach can accurately estimate the coefficients of
the regression model, which is useful for interpreting the evolution of the network;
furthermore, the learned model has systematically better predictive performance
compared to standard baseline methods.

1 Introduction
The analysis of the structure and evolution of network data is an increasingly important task in
a variety of disciplines, including biology and engineering. The emergence and growth of largescale online social networks also provides motivation for the development of longitudinal models
for networks over time. While in many cases the data for an evolving network are recorded on a
continuous time scale, a common approach is to analyze ?snapshot? data (also known as collapsed
panel data), where multiple cross-sectional snapshots of the network are recorded at discrete time
points. Various statistical frameworks have been previously proposed for discrete snapshot data,
including dynamic versions of exponential random graph models [1, 2, 3] as well as dynamic block
models and matrix factorization methods [4, 5]. In contrast, there is relatively little work to date on
continuous-time models for large-scale longitudinal networks.
In this paper, we propose a general regression-based modeling framework for continuous-time network event data. Our methods are inspired by survival and event history analysis [6, 7]; specifically,
we employ multivariate counting processes to model the edge dynamics of the network. Building
on recent work in this context [8, 9], we use both multiplicative and additive intensity functions that
allow for the incorporation of arbitrary time-dependent network statistics; furthermore, we consider
?

current affiliation: Google Inc.

1

time-varying regression coefficients for the additive approach. The additive form in particular enables us to develop an efficient online inference scheme for estimating the time-varying coefficients
of the model, allowing the approach to scale to large networks. On synthetic and real-world data, we
show that the proposed scheme accurately estimates these coefficients and that the learned model is
useful for both interpreting the evolution of the network and predicting future network events.
The specific contributions of this paper are: (1) We formulate a continuous-time regression model
for longitudinal network data with time-dependent statistics (and time-varying coefficients for the
additive form); (2) we develop an accurate and efficient inference scheme for estimating the regression coefficients; and (3) we perform an experimental analysis on real-world longitudinal networks
and demonstrate that the proposed framework is useful in terms of prediction and interpretability.
The next section introduces the general regression framework and the associated inference scheme
is described in detail in Section 3. Section 4 describes the experimental results on synthetic and
real-world networks. Finally, we discuss related work and conclude with future research directions.

2 Regression models for continuous-time network data
Below we introduce multiplicative and additive regression models for the edge formation process
in a longitudinal network. We also describe non-recurrent event models and give examples of timedependent statistics in this context.
2.1 General framework
Assume in our network that nodes arrive according to some stochastic process and directed edges
among these nodes are created over time. Given the ordered pair (i, j) of nodes in the network at
time t, let Nij (t) be a counting process denoting the number of edges from i to j up to time t.
In this paper, each Nij (t) will equal zero or one, though this can be generalized. Combining the
individual counting processes of all potential edges gives a multivariate counting process N(t) =
(Nij (t) : i, j ? {1, . . . n}, i 6= j); we make no assumption about the independence of individual
edge counting processes. (See [7] for an overview of counting processes.) We do not consider
an edge dissolution process in this paper, although in theory it is possible to do so by placing a
second counting process on each edge for dissolution events. (See [10, 3] for different examples
of formation?dissolution process models.) As proposed in [9], we model the multivariate counting
process via the Doob-Meyer decomposition [7],
Z t
N(t) =
?(s) ds + M(t),
(1)
0

where essentially ?(t) and M(t) may be viewed as the (deterministic) signal and (martingale) noise,
respectively. To model the so-called intensity process ?(t), we denote the entire past of the network,
up to but not including time t, by Ht? and consider for each potential directed edge (i, j) two
possible intensity forms, the multiplicative Cox and the additive Aalen functions [7], respectively:


?ij (t|Ht? ) = Yij (t)?0 (t) exp ? ? s(i, j, t) ;
(2)


?
?ij (t|Ht? ) = Yij (t) ?0 (t) + ?(t) s(i, j, t) ,
(3)
where the ?at risk? indicator function Yij (t) equals one if and only if (i, j) could form an edge
at time t, a concept whose interpretation is determined by the context (e.g., see Section 2.2). In
equations (2) and (3), s(i, j, t) is a vector of p statistics for directed edge (i, j) constructed based on
Ht? ; examples of these statistics are given in Section 2.2. In each of the two models, the intensity
process depends on a linear combination of the coefficients ?, which can be time-varying in the
additive Aalen formulation. When all elements of sk (i, j, t) equal zero, we obtain the baseline
hazards ?0 (t) and ?0 (t).
The two intensity forms above, the Cox and Aalen, each have their respective strengths (e.g., see [7,
chapter 4]). In particular, the coefficients of the Aalen model are quite easy to estimate via linear
regression, unlike the Cox model. We leverage this computational advantage to develop an efficient
inference algorithm for the Aalen model later in this paper. On the other hand, the Cox model
forces the hazard function to be non-negative, while the Aalen model does not?however, in our
experiments on both simulated and real-world data we did not encounter any issues with negative
hazard functions when using the Aalen model.
2

2.2 Non-recurrent event models for network formation processes
If tarr
and tarr
are the arrival times of nodes i and j, then the risk indicator of equations (2) and (3)
i
j

arr
is Yij (t) = I max(tarr
i , tj ) < t ? teij . The time teij of directed edge (i, j) is taken to be +?
if the edge is never formed during the observation time. The reason for the upper bound teij is that
the counting process is non-recurrent; i.e., formation of an edge means that it can never occur again.
The network statistics s(i, j, t) of equations (2) and (3), corresponding to the ordered pair (i, j), can
be time-invariant (such as gender match) or time-dependent (such as the number of two-paths from
i to j just before time t). Since it has been found empirically that most new edges in social networks
are created between nodes separated by two hops [11], we limit our statistics to the following:
P
1. Out-degree of sender i: s1 (i, j, t) = h?V,h6=i Nih (t? )
P
2. In-degree of sender i: s2 (i, j, t) = h?V,h6=i Nhi (t? )
P
3. Out-degree of receiver j: s3 (i, j, t) = h?V,h6=j Njh (t? )
P
4. In-degree of receiver j: s4 (i, j, t) = h?V,h6=j Nhj (t? )
5. Reciprocity: s5 (i, j, t) = Nji (t? )
P
6. Transitivity: s6 (i, j, t) = h?V,h6=i,j Nih (t? )Nhj (t? )
P
7. Shared contactees: s7 (i, j, t) = h?V,h6=i,j Nih (t? )Njh (t? )
P
8. Triangle closure: s8 (i, j, t) = h?V,h6=i,j Nhi (t? )Njh (t? )
P
9. Shared contacters: s9 (i, j, t) = h?V,h6=i,j Nhi (t? )Nhj (t? )
Here Nji (t? ) denotes the value of the counting process (i, j) right before time t. While this paper
focuses on the non-recurrent setting for simplicity, one can also develop recurrent models using this
framework, by capturing an alternative set of statistics specialized for the recurrent case [8, 12, 9].
Such models are useful for data where interaction edges occur multiple times (e.g., email data).

3 Inference techniques
In this section, we describe algorithms for estimating the coefficients of the multiplicative Cox and
additive Aalen models. We also discuss an efficient online inference technique for the Aalen model.
3.1 Estimation for the Cox model
Recent work has posited Cox models similar to (2) with the goal of estimating general network
effects [8, 12] or citation network effects [9]. Typically, ?0 (t) is considered a nuisance parameter,
and estimation for ? proceeds by maximization of the so-called partial likelihood of Cox [13]:

m
Y
exp ? ? s(ie , je , te )
,
L(?) =
Pn P
(4)
?
i=1
j6=i Yij (te ) exp ? s(i, j, te )
e=1
where m is the number of edge formation events, and te , ie , and je are the time, sender, and receiver
of the eth event. In this paper, maximization is performed via the Newton-Raphson algorithm. The
covariance matrix of ?? is estimated as the inverse of the negative Hessian matrix of the last iteration.

We use the caching method of [9] to compute the likelihood, the score vector, and the Hessian matrix
more efficiently. We will illustrate this method through the computation of the likelihood, where the
most expensive computation is for the denominator
?(te ) =

n X
X
i=1 j6=i


Yij (te ) exp ? ? s(i, j, te ) .

(5)

For models such as the one in Section 2.2, a na??ve update for ?(te ) needs O(pn2 ) operations, where
n is the the current number of nodes. A na??ve calculation of log L(?) needs O(mpn2 ) operations
(where m is the number of edge events), which is costly since m and n may be large. Calculations
of the score vector and Hessian matrix are similar, though they involve higher exponents of p.
3

Alternatively, as in [9], we may simply write ?(te ) = ?(te?1 ) + ??(te ), where ??(te ) entails all
of the possible changes that occur during the time interval [te?1 , te ). Since we assume in this paper
that edges do not dissolve, it is necessary to keep track only of the group of edges whose covariates
change during this interval, which we call Ue?1 , and those that first become at risk during this interval, which we call Ce?1 . These groups of edges may be cached in memory during an initialization
step; then, subsequent calculations of ??(te ) are simple functions of the values of s(i, j, te?1 ) and
s(i, j, te ) for (i, j) in these two groups (for Ce?1 , only the time te statistic is relevant).
The number of edges cached at each time step tends to be small, generally O(n) because our network
statistics s are limited to those based on node degrees and two-paths. This leads to substantial
computational savings; since we must still initialize ?(t1 ), the total computational complexity of
each Newton-Raphson iteration is O(p2 n2 + m(p2 n + p3 )).
3.2 Estimation for the Aalen model
Inference in model (3) proceeds not for the ?k parameters directly but rather for their time-integrals
Z t
Bk (t) =
?k (s)ds.
(6)
0

The reason for this is that B(t) = [B1 (t), . . . , Bp (t)] may be estimated straightforwardly using a
procedure akin to simple least squares [7]: First, let us impose some ordering on the n(n ? 1)
possible ordered pairs (i, j) of nodes. Take W(t) to be the n(n ? 1) ? p matrix whose (i, j)th row
equals Yij (t)s(i, j, t)? . Then
Z t
X
?
B(t)
=
J(s)W? (s)dN(s) =
J(te )W? (te )?N(te )
(7)
0

te ?t

is the estimator of B(t), where the multivariate counting process N(te ) uses the same ordering of its
n(n ? 1) entries as the W(t) matrix,

?1
W? (t) = W(t)? W(t)
W(t)? ,

and J(t) is the indicator that W(t) has full column rank, where we take J(t)W? (t) = 0 whenever
W(t) does not have full column rank. As with typical least squares, a covariance matrix for these
?
B(t)
may also be estimated [7]; we give a formula for this matrix in equation (11). If estimates of
?k (t) are desired for the sake of interpretability, a kernel smoothing method may be used:
1 X  t ? te  ?
??k (t) =
?Bk (te ),
(8)
K
b t
b
e

?k (te ) = B
?k (te ) ? B
?k (te?1 ), and K is a bounded kernel
where b is the bandwidth parameter, ?B
function with compact support [?1, 1] such as the Epanechnikov kernel.
3.3 Online inference for the Aalen model
Similar to the caching method for the Cox model in Section 3.1, it is possible to streamline the
computations for estimating the integrated Aalen model coefficients B(t). First, we rewrite (7) as
X
X

?1
? =
B(t)
J(te ) W(te )? W(te ) W(te )? ?N(te ) =
A?1 (te )W(te )? ?N(te ),
(9)
te ?t

te ?t

?

where A(te ) = W(te ) W(te ) and J(te ) is omitted because for large network data sets and for
reasonable choices of starting observation times, the covariate matrix is always of full rank. The
computation of W(te )? ?N(te ) is simple because ?N(te ) consists of all zeros except for a single
entry equal to one. The most expensive computation is to update the (p + 1) ? (p + 1) matrix A(te )
at every event time te ; inverting A(te ) is not expensive since p is relatively small.
Using Ue?1 and Ce?1 as in Section 3.1, the component (k, l) of the matrix A(te ) corresponding to
covariates k and l can be written as Akl (te ) = Akl (te?1 ) + ?Akl (te?1 ), where
X
X
Wijk (te )Wijl (te ).
(10)
Wijk (te?1 )Wijl (te?1 ) +
?Akl (te?1 ) = ?
(i,j)?Ue?1 ?Ce?1

(i,j)?Ue?1

4

For models such as the one presented in Section 2.2, if n is the current number of nodes, the cost of
na??vely calculating Akl (te ) by iterating through all ?at-risk? edges is nearly n2 . As in Section 3.1,
the cost will be O(n) if we instead use caching together with equation (10). In other cases, there
may be restrictions on the set of edges at risk at a particular time. Here the computational burden for
the na??ve calculation can be substantially smaller than O(n2 ); yet it is generally the case that using
(10) will still provide a substantial reduction in computing effort.
Our online inference algorithm during the time interval [te?1 , te ) may be summarized as follows:
1. Update A(te?1 ) using equation (10).
? e?1 ) = B(t
? e?2 ) + A?1 (te?1 )W(te?1 )? ?N(te?1 ).
2. Compute B(t
3. Compute and cache the network statistics changed by the event e ? 1, then initialize Ue?1
with a list of those at-risk edges whose network statistics are changed by this event.
4. Compute and cache all values of network statistics changed during the time interval
[te?1 , te ). Define Ce?1 as the set of edges that switch to at-risk during this interval.
5. Before considering the event e:
(a) Compute look-ahead summations at time te?1 indexed by Ue?1 .
(b) Update the covariate matrix W(te?1 ) based on the cache.
(c) Compute forward summations at time te indexed by Ue?1 and Ce?1 .
For the first event, A(t1 ) must be initialized by na??ve summation over all current at-risk edges, which
requires O(p2 n2 ) calculations. Assuming that the number n of nodes stays roughly the same over
each of the m edge events, the overall computational complexity of this online inference algorithm
? is desired, it can also be
is thus O(p2 n2 + m(p2 n + p3 )). If a covariance matrix estimate for B(t)
derived online using the ideas above, since we may write it as
X
X


?
?(t)
=
W? (te )diag{?N(te )}W? (te )? =
A?1 (te ) Wij e (te ) ? Wij e (te ) A?1 (te ), (11)
te ?t

te ?t

where Wij e (te ) denotes the vector W(te )? ?N(te ) and ? is the outer product.

4 Experimental analysis
In this section, we empirically analyze the ability of our inference methods to estimate the regression
coefficients as well as the predictive power of the learned models. Before discussing the experimental results, we briefly describe the synthetic and real-world data sets that we use for evaluation.
We simulate two data sets, SIM-1 and SIM-2, from ground-truth regression coefficients. In particular, we simulate a network formation process starting from time unit 0 until time 1200, where
nodes arrive in the network at a constant rate ?0 = 10 (i.e., on average, 10 nodes join the network
at each time unit); the resulting simulated networks have 11,997 nodes. The edge formation process is simulated via Otaga?s modified thinning algorithm [14] with an additive conditional intensity
function. From time 0 to 1000, the baseline coefficient is set to ?0 = 10?6 ; the coefficients for
sender out-degree and receiver in-degree are set to ?1 = ?4 = 10?7 ; the coefficients for reciprocity,
transitivity, and shared contacters are set to ?5 = ?6 = ?9 = 10?5 ; and the coefficients for sender
in-degree, receiver out-degree, shared contactees, and triangle closure are set to 0. For SIM-1, these
coefficients are kept constant and 118,672 edges are created. For SIM-2, between times 1000 and
1150, we increase the coefficients for transitivity and shared contacters to ?6 = ?9 = 4 ? 10?5, and
after 1150, the coefficients return to their original values; in this case, 127,590 edges are created.
We also evaluate our approach on two real-world data sets, IRVINE and METAFILTER. IRVINE
is a longitudinal data set derived from an online social network of students at UC Irvine [15]. This
dataset has 1,899 users and 20,296 directed contact edges between users, with timestamps for each
node arrival and edge creation event. This longitudinal network spans April to October of 2004.
The METAFILTER data set is from a community weblog where users can share links and discuss
Web content1 . This dataset has 51,362 users and 76,791 directed contact edges between users. The
continuous-time observation spans 8/31/2007 to 2/5/2011. Note that both data sets are non-recurrent
in that the creation of an edge between two nodes only occurs at most once.
1

The METAFILTER data are available at http://mssv.net/wiki/index.php/Infodump

5

1150

(a)

1000

Constant
Transitivity

(b)

4e?5

Coefficient

1150

Time

1e?5

4e?5

Coefficient
1000

Time

1e?5

4e?5

Coefficient

1e?5

4e?5

Coefficient

1e?5
1000

1150

1000

Time

Constant
Shared Contacters

(c)

1150

Time

Piecewise
Transitivity

(d)

Piecewise
Shared Contacters

6/1/04

7/21/04

9/9/04

Coefficient

?5e?5

.0003

Coefficient

0

.02
0

Coefficient

3e?5
0

Coefficient

0

Figure 1: (a,b) Estimated time-varying coefficients on SIM-1; (c,d) Estimated time-varying coefficients on SIM-2. Ground-truth coefficients are also shown in red dashed lines.

6/1/04

Time

7/21/04

9/9/04

6/1/04

Time

(a) Sender Out-Degree

7/21/04

9/9/04

6/1/04

Time

(b) Reciprocity

7/21/04

9/9/04

Time

(c) Transitivity

(d) Shared Contacters

1/21/10

7/10/10

12/2/10

Time

(a) Sender Out-Degree

0

Coefficient

?2e?6

1e?5
0

Coefficient

4e?4
0

Coefficient

1e?8
0

Coefficient

Figure 2: Estimated time-varying coefficients on IRVINE data. These plots suggest that there are
two distinct phases of network evolution, consistent with an independent analysis of these data [15].

1/21/10

7/10/10

12/2/10

Time

1/21/10

7/10/10

12/2/10

Time

(b) Reciprocity

(c) Transitivity

1/21/10

7/10/10

12/2/10

Time

(d) Shared Contacters

Figure 3: Estimated time-varying coefficients on METAFILTER. Here, the network effects continuously change during the observation time.
4.1 Recovering the time-varying regression coefficients
This section focuses on the ability of our additive Aalen modeling approach to estimate the timevarying coefficients, given an observed longitudinal network.
The first set of experiments attempts to recover the ground-truth coefficients on SIM-1 and SIM-2.
We run the inference algorithm described in Section 3.3 and use an Epanechnikov smoothing kernel
(with a bandwidth of 10 time units) to obtain smoothed coefficients. On SIM-1, Figures 1(a,b)
show the estimated coefficients associated with the transitivity and shared contacters statistics, as
well as the ground-truth coefficients. Likewise, Figures 1(c,d) show the same estimated and groundtruth coefficients for SIM-2. These results demonstrate that our inference algorithm can accurately
recover the ground-truth coefficients in cases where the coefficients are fixed (SIM-1) and modulated
(SIM-2). We also tried other settings for the ground-truth coefficients (e.g., multiple sinusoidal-like
bumps) and found that our approach can accurately recover the coefficients in those cases as well.
On the IRVINE and METAFILTER data, we also learn time-varying coefficients which are useful for interpreting network evolution. Figure 2 shows several of the estimated coefficients for the
IRVINE data, using an Epanechnikov kernel (with a bandwidth of 30 days). These coefficients
suggest the existence of two distinct phases in the evolution of the network. In the first phase of
network formation, the network grows at an accelerated rate. Positive coefficients for sender outdegree, reciprocity, and transitivity in these plots imply that users with a high numbers of friends
tend to make more friends, tend to reciprocate their relations, and tend to make friends with their
friends? friends, respectively. However, these coefficients decrease towards zero (the blue line) and
enter a second phase where the network is structurally stable. Both of these phases have also been
observed in an independent study of the data [15]. Figure 3 shows the estimated coefficients for
METAFILTER, using an Epanechnikov kernel (with a bandwidth of 30). Interestingly, the coefficients suggest that there is a marked change in the edge formation process around 7/10/10. Unlike
the IRVINE coefficients, the estimated METAFILTER coefficients continue to vary over time.
6

Table 1: Lengths of building, training, and test periods. The number of events are in parentheses.
IRVINE
METAFILTER

Building
4/15/04 ? 5/11/04 (7073)
6/15/04 ? 12/21/09 (60376)

Training
5/12/04 ? 5/31/04 (7646)
12/22/09 ? 7/9/10 (8763)

Test
6/1/04 ? 10/19/04 (5507)
7/10/10 ? 2/5/11 (7620)

4.2 Predicting future links
We perform rolling prediction experiments over the real-world data sets to evaluate the predictive
power of the learned regression models. Following the evaluation methodology of [9], we split
each longitudinal data set into three periods: a statistics-building period, a training period, and a test
period (Table 1). The statistics-building period is used solely to build up the network statistics, while
the training period is used to learn the coefficients and the test period is used to make predictions.
Throughout the training and test periods, the time-dependent statistics are continuously updated.
Furthermore, for the additive Aalen model, we use the online inference technique from Section 3.3.
When we predict an event in the test period, all the previous events from the test period are used
as training data as well. Meanwhile, for the multiplicative Cox model, we adaptively learn the
model in batch-online fashion; during the test period, for every 10 days, we retrain the model (using
the Newton-Raphson technique described in Section 3.1) with additional training examples coming
from the test set. Our Newton-Raphson implementation uses a step-halving procedure, halving the
length of each step if necessary until log L(?) increases. The iterations continue until every element
in ? log L(?) is smaller that 10?3 in absolute value, or until the relative increase in log L(?) is less
than 10?100 , or until 100 Newton-Raphson iterations are reached, whichever occurs first.
The baseline that we consider is logistic regression (LR) with the same time-dependent statistics
used in the Aalen and Cox models. Note that logistic regression is a competitive baseline that
has been used in previous link prediction studies (e.g., [11]). We learn the LR model in the same
adaptive batch-online fashion as the Cox model. We also use case control sampling to address the
imbalance between positive and negative cases (since at each ?positive? edge event there are order
of n2 ?negative? training cases). At each event, we sample K negative training examples for that
same time point. We use two settings for K in the experiments: K = 10 and K = 50.
To make predictions using the additive Aalen model, one would need to extrapolate the time-varying
coefficients to future time points. For simplicity, we use a uniform smoothing kernel (weighting all
observations equally), with a window size of 1 or 10 days. A more advanced extrapolation technique
could yield even better predictive performance for the Aalen model.
Each model can provide us with the probability of an edge formation event between two nodes at a
given point in time, and so we can calculate an accumulative recall metric across all test events:
P
(i?j,t)?TestSet I[j ? Top(i, t, K)]
Recall =
,
(12)
|TestSet|
where Top(i, t, K) is the top-K list of i?s potential ?friends? ranked based on intensity ?ij (t).
We evaluate the predictive performance of the Aalen model (with smoothing windows of 1 and 10),
the Cox model, and the LR baseline (with case control ratios 1:10 and 1:50). Figure 4(a) shows the
recall results on IRVINE. In this case, both the Aalen and Cox models outperform the LR baseline;
furthermore, it is interesting to note that the Aalen model with time-varying coefficients does not
outperform the Cox model. One explanation for this result is that the IRVINE coefficients are pretty
stable (apart from the initial phase as shown in Figure 2), and thus time-varying coefficients do not
provide additional predictive power in this case. Also note that LR with ratio 1:10 outperforms 1:50.
We also tried an LR ratio of 1:3 (not shown) but found that it performed nearly identically to LR
1:10; thus, both the Aalen and Cox models outperform the baseline substantially on these data.
Figure 4(b) shows the recall results on METAFILTER. As in the previous case, both the Aalen and
Cox models significantly outperform the LR baseline. However, the Aalen model with time-varying
coefficients also substantially outperforms the Cox model with time-fixed coefficients. In this case,
estimating time-varying coefficients improves predictive performance, which makes sense because
we have seen in Figure 3 that METAFILTER?s coefficients tend to vary more over time. We also
calculated precision results (not shown) on these data sets which confirm these conclusions.
7

0.3

0.4
0.3

Recall
0.2

Recall
1

5

10
Cut?Point K

15

Adaptive LR (1:10)
Adaptive LR (1:50)
Adaptive Cox
Aalen (Uniform?1)
Aalen (Uniform?10)

0.1

0.2

Adaptive LR (1:10)
Adaptive LR (1:50)
Adaptive Cox
Aalen (Uniform?1)
Aalen (Uniform?10)
20

1

(a) IRVINE

5

10
Cut?Point K

15

20

(b) METAFILTER

Figure 4: Predictive performance of the additive Aalen model, multiplicative Cox model, and logistic
regression baseline on the IRVINE and METAFILTER data sets, using recall as the metric.

5 Related Work and Conclusions
Evolving networks have been descriptively analyzed in exploratory fashion in a variety of domains,
including email data [16], citation graphs [17], and online social networks [18]. On the modeling side, temporal versions of exponential random graph models [1, 2, 3] and latent space models [19, 4, 5, 20] have been developed. Such methods operate on cross-sectional snapshot data, while
our framework models continuous-time network event data. It is worth noting that continuous-time
Markov process models for longitudinal networks have been proposed previously [21]; however,
these approaches have only been applied to very small networks, while our regression-based approach can scale to large networks. Recently, there has also been work on inferring unobserved
time-varying networks from evolving nodal attributes which are observed [22, 23, 24]. In this paper,
the main focus is the statistical modeling of observed continuous-time networks.
More recently, survival and event history models based on the Cox model have been applied to
network data [8, 12, 9]. A significant difference between our previous work [9] and this paper is
that scalability is achieved in our earlier work by restricting the approach to ?egocentric? modeling,
in which counting processes are placed only on nodes. In contrast, here we formulate scalable
inference techniques for the general ?relational? setting where counting processes are placed on
edges. Prior work also assumed static regression coefficients, while here we develop a framework for
time-varying coefficients for the additive Aalen model. Regression models with varying coefficients
have been previously proposed in other contexts [25], including a time-varying version of the Cox
model [26], although to the best of our knowledge such models have not been developed or fitted on
longitudinal networks.
A variety of link prediction techniques have also been investigated by the machine learning community over the past decade (e.g., [27, 28, 29]). Many of these methods use standard classifiers (such as
logistic regression) and take advantage of key features (such as similarity measures among nodes)
to make accurate predictions. While our focus is not on feature engineering, we note that arbitrary
network and nodal features such as those developed for link prediction can be incorporated into our
continuous-time regression framework. Other link prediction techniques based on matrix factorization [30] and random walks [11] have also been studied. While these link prediction techniques
mainly focus on making accurate predictions, our proposed approach here not only gives accurate
predictions but also provides a statistical model (with time-varying coefficient estimates) which can
be useful in evaluating scientific hypotheses.
In summary, we have developed multiplicative and additive regression models for large-scale
continuous-time longitudinal networks. On simulated and real-world data, we have shown that
the proposed inference approach can accurately estimate regression coefficients and that the learned
model can be used for interpreting network evolution and predicting future network events. An interesting direction for future work would be to incorporate time-dependent nodal attributes (such as
textual content) into this framework and to investigate regularization methods for these models.
Acknowledgments
This work is supported by ONR under the MURI program, Award Number N00014-08-1-1015.
8

References
[1] S. Hanneke and E. P. Xing. Discrete temporal models of social networks. In Proc. 2006 Conf. on Statistical
Network Analysis, pages 115?125. Springer-Verlag, 2006.
[2] D. Wyatt, T. Choudhury, and J. Bilmes. Discovering long range properties of social networks with multivalued time-inhomogeneous models. In Proc. 24th AAAI Conf. on AI, 2010.
[3] P. N. Krivitsky and M. S. Handcock. A separable model for dynamic networks. Under review, November
2010. http://arxiv.org/abs/1011.1937.
[4] W. Fu, L. Song, and E. P. Xing. Dynamic mixed membership blockmodel for evolving networks. In Proc.
26th Intl. Conf. on Machine Learning, pages 329?336. ACM, 2009.
[5] J. Foulds, C. DuBois, A. Asuncion, C. Butts, and P. Smyth. A dynamic relational infinite feature model
for longitudinal social networks. In AI and Statistics, volume 15 of JMLR W&C Proceedings, pages
287?295, 2011.
[6] P. K. Andersen, O. Borgan, R. D. Gill, and N. Keiding. Statistical Models Based on Counting Processes.
Springer, 1993.
[7] O. O. Aalen, O. Borgan, and H. K. Gjessing. Survival and Event History Analysis: A Process Point of
View. Springer, 2008.
[8] C. T. Butts. A relational event framework for social action. Soc. Meth., 38(1):155?200, 2008.
[9] D. Q. Vu, A. U. Asuncion, D. R. Hunter, and P. Smyth. Dynamic egocentric models for citation networks.
In Proc. 28th Intl. Conf. on Machine Learning, pages 857?864, 2011.
[10] P. Holland and S. Leinhardt. A dynamic model for social networks. J. Math. Soc., 5:5?20, 1977.
[11] L. Backstrom and J. Leskovec. Supervised random walks: Predicting and recommending links in social
networks. In Proceedings of the 4th ACM International Conference on Web Search and Data Mining,
pages 635?644. ACM, 2011.
[12] P. O. Perry and P. J. Wolfe. Point process modeling for directed interaction networks. Under review,
October 2011. http://arxiv.org/abs/1011.1703.
[13] D. R. Cox. Regression models and life-tables. J. Roy. Stat. Soc., Series B, 34:187?220, 1972.
[14] D. J. Daley and D. Vere-Jones. An Introduction to the Theory of Point Processes, Volume 1. Probability
and its Applications (New York). Springer, New York, 2nd edition, 2008.
[15] P. Panzarasa, T. Opsahl, and K. M. Carley. Patterns and dynamics of users? behavior and interaction:
Network analysis of an online community. J. Amer. Soc. for Inf. Sci. and Tech., 60(5):911?932, 2009.
[16] G. Kossinets and D. J. Watts. Empirical analysis of an evolving social network. Science, 311(5757):88?
90, 2006.
[17] J. Leskovec, J. Kleinberg, and C. Faloutsos. Graphs over time: densification laws, shrinking diameters
and possible explanations. In Proc. 11th ACM SIGKDD Intl. Conf. on Knowledge Discovery in Data
Mining, pages 177?187. ACM, 2005.
[18] B. Viswanath, A. Mislove, M. Cha, and K. P. Gummadi. On the evolution of user interaction in Facebook.
In Proc. 2nd ACM SIGCOMM Wkshp. on Social Networks, pages 37?42. ACM, 2009.
[19] P. Sarkar and A. Moore. Dynamic social network analysis using latent space models. SIGKDD Explorations, 7(2):31?40, 2005.
[20] Q. Ho, L. Song, and E. Xing. Evolving cluster mixed-membership blockmodel for time-varying networks.
In AI and Statistics, volume 15 of JMLR W&C Proceedings, pages 342?350, 2011.
[21] T. A. B. Snijders. Models for longitudinal network data. Mod. Meth. in Soc. Ntwk. Anal., pages 215?247,
2005.
[22] S. Zhou, J. Lafferty, and L. Wasserman. Time varying undirected graphs. Machine Learning, 80:295?319,
2010.
[23] A. Ahmed and E. P. Xing. Recovering time-varying networks of dependencies in social and biological
studies. Proc. Natl. Acad. Scien., 106(29):11878?11883, 2009.
[24] M. Kolar, L. Song, A. Ahmed, and E. P. Xing. Estimating time-varying networks. Ann. Appl. Stat.,
4(1):94?123, 2010.
[25] Z. Cai, J. Fan, and R. Li. Efficient estimation and inferences for varying-coefficient models. J. Amer. Stat.
Assn., 95(451):888?902, 2000.
[26] T. Martinussen and T.H. Scheike. Dynamic Regression Models for Survival Data. Springer, 2006.
[27] D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. J. Amer. Soc. for Inf.
Sci. and Tech., 58(7):1019?1031, 2007.
[28] M. Al Hasan, V. Chaoji, S. Salem, and M. Zaki. Link prediction using supervised learning. In SDM ?06:
Workshop on Link Analysis, Counter-terrorism and Security, 2006.
[29] J. Leskovec, D. Huttenlocher, and J. Kleinberg. Predicting positive and negative links in online social
networks. In Proc. 19th Intl. World Wide Web Conference, pages 641?650. ACM, 2010.
[30] D. M. Dunlavy, T. G. Kolda, and E. Acar. Temporal link prediction using matrix and tensor factorizations.
ACM Transactions on Knowledge Discovery from Data, 5(2):10, February 2011.

9

"
4160,"Fast Variational Inference in the
Conjugate Exponential Family

James Hensman?
Department of Computer Science
The University of Sheffield
james.hensman@sheffield.ac.uk

Magnus Rattray
Faculty of Life Science
The University of Manchester
magnus.rattray@manchester.ac.uk

Neil D. Lawrence?
Department of Computer Science
The University of Sheffield
n.lawrence@sheffield.ac.uk

Abstract
We present a general method for deriving collapsed variational inference algorithms for probabilistic models in the conjugate exponential family. Our method
unifies many existing approaches to collapsed variational inference. Our collapsed
variational inference leads to a new lower bound on the marginal likelihood. We
exploit the information geometry of the bound to derive much faster optimization
methods based on conjugate gradients for these models. Our approach is very
general and is easily applied to any model where the mean field update equations
have been derived. Empirically we show significant speed-ups for probabilistic
inference using our bound.

1

Introduction

Variational bounds provide a convenient approach to approximate inference in a range of intractable
models [Ghahramani and Beal, 2001]. Classical variational optimization is achieved through coordinate ascent which can be slow to converge. A popular solution [King and Lawrence, 2006, Teh et al.,
2007, Kurihara et al., 2007, Sung et al., 2008, L?azaro-Gredilla and Titsias, 2011, L?azaro-Gredilla
et al., 2011] is to marginalize analytically a portion of the variational approximating distribution,
removing this from the optimization. In this paper we provide a unifying framework for collapsed
inference in the general class of models composed of conjugate-exponential graphs (CEGs).
First we review the body of earlier work with a succinct and unifying derivation of the collapsed
bounds. We describe how the applicability of the collapsed bound to any particular CEG can be
determined with a simple d-separation test. Standard variational inference via coordinate ascent
turns out to be steepest ascent with a unit step length on our unifying bound. This motivates us
to consider natural gradients and conjugate gradients for fast optimization of these models. We
apply our unifying approach to a range of models from the literature obtaining, often, an order of
magnitude or more increase in convergence speed. Our unifying view allows collapsed variational
methods to be integrated into general inference tools like infer.net [Minka et al., 2010].

?

also at Sheffield Institute for Translational Neuroscience, SITraN

1

2

The Marginalised Variational Bound

The advantages to marginalising analytically a subset of variables in variational bounds seem to
be well understood: several different approaches have been suggested in the context of specific
models. In Dirichlet process mixture models Kurihara et al. [2007] proposed a collapsed approach
using both truncated stick-breaking and symmetric priors. Sung et al. [2008] proposed ?latent space
variational Bayes? where both the cluster-parameters and mixing weights were marginalised, again
with some approximations. Teh et al. [2007] proposed a collapsed inference procedure for latent
Dirichlet allocation (LDA). In this paper we unify all these results from the perspective of the ?KL
corrected bound? [King and Lawrence, 2006]. This lower bound on the model evidence is also an
upper bound on the original variational bound, the difference between the two bounds is given by a
Kullback Leibler divergence. The approach has also been referred to as the marginalised variational
bound by L?azaro-Gredilla et al. [2011], L?azaro-Gredilla and Titsias [2011]. The connection between
the KL corrected bound and the collapsed bounds is not immediately obvious. The key difference
between the frameworks is the order in which the marginalisation and variational approximation are
applied. However, for CEGs this order turns out to be irrelevant. Our framework leads to a more
succinct derivation of the collapsed approximations. The resulting bound can then be optimised
without recourse to approximations in either the bound?s evaluation or its optimization.
2.1

Variational Inference

Assume we have a probabilistic model for data, D, given parameters (and/or latent variables), X, Z,
of the form p(D, X, Z) = p(D | Z, X)p(Z | X)p(X). In variational Bayes (see e.g. Bishop [2006])
we approximate the posterior p(Z, X|D) by a distribution q(Z, X). We use Jensen?s inequality
to derive a lower bound on the model evidence L, which serves as an objective function in the
variational optimisation:
Z
p(D, Z, X)
p(D) ? L = q(Z, X) ln
dZ dX.
(1)
q(Z, X)
For tractability the mean field (MF) approach assumes q factorises across its variables, q(Z, X) =
q(Z)q(X). It is then possible to implement an optimisation scheme which analytically optimises
each factor alternately, with the optimal distribution given by
Z

q ? (X) ? exp
q(Z) ln p(D, X|Z) dZ ,
(2)
and similarly for Z: these are often referred to as VBE and VBM steps. King and Lawrence [2006]
substituted the expression for the optimal distribution (for example q ? (X)) back into the bound (1),
eliminating one set of parameters from the optimisation, an approach that has been reused by L?azaroGredilla et al. [2011], L?azaro-Gredilla and Titsias [2011]. The resulting bound is not dependent on
q(X). King and Lawrence [2006] referred to this new bound as ?the KL corrected bound?. The
difference between the bound, which we denote LKL , and a standard mean field approximation LMF ,
is the Kullback Leibler divergence between the optimal form of q ? (X) and the current q(X).
We re-derive their bound by first using Jensen?s inequality to construct the variational lower bound
on the conditional distribution,
Z
p(D, Z|X)
dZ , L1 .
(3)
ln p(D|X) ? q(Z) ln
q(Z)
This object turns out to be of central importance in computing the final KL-corrected bound and
also in computing gradients, curvatures and the distribution of the collapsed variables q ? (X). It is
easy to see that it is a function of X which lower-bounds the log likelihood p(D | X), and indeed
our derivation treats it as such. We now marginalize the conditioned variable from this expression,
Z
ln p(D) ? ln p(X) exp{L1 } dX , LKL ,
(4)
giving us the bound of King and Lawrence [2006] & L?azaro-Gredilla et al. [2011]. Note that one set
of parameters was marginalised after the variational approximation was made.
Using (2), this expression also provides the approximate posterior for the marginalised variables X:
q ? (X) = p(X)eL1 ?LKL
(5)
LKL
and e
appears as the constant of proportionality in the mean-field update equation (2).
2

3

Partial Equivalence of the Bounds

We can recover LMF from LKL by again applying Jensen?s inequality,


Z
Z
p(X)
p(X)
LKL = ln q(X)
exp{L1 } dX ? q(X) ln
exp{L1 } dX,
q(X)
q(X)
which can be re-arranged to give the mean-field bound,


Z
p(D|Z, X)p(Z)p(X)
LKL ? q(X)q(Z) ln
dX dZ,
q(Z)q(X)

(6)

(7)

and it follows that LKL = LMF + KL(q ? (X)||q(X)) and1 LKL ? LMF . For a given q(Z), the bounds
are equal after q(X) is updated via the mean field method: the approximations are ultimately the
same. The advantage of the new bound is to reduce the number of parameters in the optimisation. It
is particularly useful when variational parameters are optimised by gradient methods. Since VBEM
is equivalent to a steepest descent gradient method with a fixed step size, there appears to be a lot to
gain by combining the KLC bound with more sophisticated optimization techniques.
3.1

Gradients

Consider the gradient of the KL corrected bound with respect to the parameters of q(Z):
Z
h ?L i
?LKL
?
1
= exp{?LKL }
exp{L1 }p(X) dX = Eq? (X)
,
??z
??z
??z

(8)

where we have used the relation (5). To find the gradient of the mean-field
bound we note that
h
i it can
be written in terms of our conditional bound (3) as LMF = Eq(X) L1 + ln p(X) ? ln q(X) giving
h ?L i
?LMF
1
= Eq(X)
??z
??z

(9)

thus setting q(X) = q ? (X) not only makes the bounds equal, LMF = LKL , but also their gradients
with respect to ?Z .
Sato [2001] has shown that the variational update equation can be interpreted as a gradient method,
where each update is also a step in the steepest direction in the canonical parameters of q(Z). We
can combine this important insight with the above result to realize that we have a simple method for
computing the gradients of the KL corrected bound: we only need to look at the update expressions
for the mean-field method. This result also reveals the weakness of standard variational Bayesian
expectation maximization (VBEM): it is a steepest ascent algorithm. Honkela et al. [2010] looked to
rectify this weakness by applying a conjugate gradient algorithm to the mean field bound. However,
they didn?t obtain a significant improvement in convergence speed. Our suggestion is to apply
conjugate gradients to the KLC bound. Whilst the value and gradient of the MF bound matches
that of the KLC bound after an update of the collapsed variables, the curvature is always greater. In
practise this means that much larger steps (which we compute using conjugate gradient methods) can
be taken when optimizing the KLC bound than for the MF bound leading to more rapid convergence.
3.2

Curvature of the Bounds

King and Lawrence [2006] showed empirically that the KLC bound could lead to faster convergence
because the bounds differ in their curvature: the curvature of the KLC bound enables larger steps to
be taken by an optimizer. We now derive analytical expressions for the curvature of both bounds.
For the mean field bound we have
h ?2L i
? 2 LMF
1
=
E
,
(10)
q(X)
??z2
??z2
1

We use KL(?||?) to denote the Kullback Leibler divergence between two distributions.

3

and for the KLC bound, with some manipulation of (4) and using (5):
n LKL on ?eLKL o
2 LKL
? 2 LKL
?LKL ? e
?2LKL ?e
=
e
?
e
[i]
[i]
[i]
[j]
[j]
[j]
??z ??z
??z ??z
??z
??z
h ?2L i
h ?L ion
h ?L io
h ?L ?L i n
1
1
1
1
1
?
?
?
= Eq? (X)
?
E
E
.
+
E
q (X)
q (X)
q (X)
[i]
[i]
[j]
[i]
[j]
[j]
??z ??z
??z ??z
??z
??z
(11)
In this result the first term is equal to (10), and the second two terms combine to be always positive
semi-definite, proving King and Lawrence [2006]?s intuition about the curvature of the bound. When
curvature is negative definite (e.g. near a maximum), the KLC bound?s curvature is less negative
definite, enabling larger steps to be taken in optimization. Figure 1(b) illustrates the effect of this as
well as the bound?s similarities.
3.3

Relationship to Collapsed VB

In collapsed inference some parameters are marginalized before applying the variational bound. For
example, Sung et al. [2008] proposed a latent variable model where the model parameters were
marginalised, and Teh et al. [2007] proposed a non-parametric topic model where the document
proportions were collapsed. These procedures lead to improved inference, or faster convergence.
The KLC bound derivation we have provided also marginalises parameters, but after a variational
approximation is made. The difference between the two approaches is distilled in these expressions:



 





ln Ep(X) exp Eq(Z) ln p(D|X, Z)
Eq(Z) ln Ep(X) p(D|X, Z)
(12)
where the left expression appears in the KLC bound, and the right expression appears in the bound
for collapsed variational Bayes, with the remainder of the bounds being equal. Whilst appropriately conjugate formulation of the model will always ensure that the KLC expression is analytically
tractable, the expectation in the collapsed VB expression is
h not. iSung et al. [2008]
h i propose a first
order approximation to the expectation of the form Eq(Z) f (Z) ? f (Eq(Z) Z ), which reduces
the right expression to the that on the left. Under this approximation2 the KL corrected approach is
equivalent to the collapsed variational approach.
3.4

Applicability

To apply the KLC bound we need to specify a subset, X, of variables to marginalize. We select
the variables that break the dependency structure of the graph to enable the analytic computation
of the integral in (4). Assuming the appropriate conjugate exponential structure for the model we
are left with the requirement to select a sub-set that induces the appropriate factorisation. These
induced factorisations are discussed in some detail in Bishop [2006]. They are factorisations in
the approximate posterior which arise from the form of the variational approximation and from the
structure of the model. These factorisations allow application of KLC bound, and can be identified
using a simple d-separation test as Bishop discusses.
The d-separation test involves checking for independence amongst the marginalised variables (X in
the above) conditioned on the observed data D and the approximated variables (Z in the above). The
requirement is to select a sufficient set of variables, Z, such that the effective likelihood for X, given
by (3) becomes conjugate to the prior. Figure 1(a) illustrates the d-separation test with application
to the KLC bound.
For latent variable models, it is often sufficient to select the latent variables for X whilst collapsing
the model variables. For example, in the specific case of mixture models and topic models, approximating the component labels allows for the marginalisation of the cluster parameters (topics
2
Kurihara et al. [2007] and Teh et al. [2007] suggest a further second order correction and assume that that
q(Z) is Gaussian to obtain tractability. This leads to additional correction terms that augment KLC bound. The
form of these corrections would need to be determined on a case by case basis, and has in fact been shown to
be less effective than those methods unified here [Asuncion et al., 2012].

4

A

B

E

C

F

D

(a)

(b)

Figure 1: (a) An example directed graphical model on which we could use the KLC bound. Given
the observed node C, the nodes A,F d-separate given nodes B,D,E. Thus we could make an explicit
variational approximation for A,F, whilst marginalising B,D,E. Alternatively, we could select B,D,E
for a parameterised approximate distribution, whilst marginalising A,F. (b) A sketch of the KLC
and MF bounds. At the point where the mean field method has q(X) = q ? (X), the bounds are
equal in value as well as in gradient. Away from the this point, the different between the bounds
is the Kullback Leibler divergence between the current MF approximation for X and the implicit
distribution q ? (X) of the KLC bound.
allocations) and mixing proportions. This allowed Sung et al. [2008] to derive a general form for
latent variable models, though our formulation is general to any conjugate exponential graph.

4

Riemannian Gradient Based Optimisation

Sato [2001] and Hoffman et al. [2012] showed that the VBEM procedure performs gradient ascent in
the space of the natural parameters. Using the KLC bound to collapse the problem, gradient methods
seem a natural choice for optimisation, since there are fewer parameters to deal with, and we have
shown that computation of the gradients is straightforward (the variational update equations contain
the model gradients). It turns out that the KLC bound is particularly amenable to Riemannian or
natural gradient methods, because the information geometry of the exponential family distrubution(s), over which we are optimising, leads to a simple expression for the natural gradient. Previous
investigations of natural gradients for variational Bayes [Honkela et al., 2010, Kuusela et al., 2009]
required the inversion of the Fisher information at every step (ours does not), and also used VBEM
steps for some parameters and Riemannian optimisation for other variables. The collapsed nature
of the KLC bound means that these VBEM steps are unnecessary: the bound can be computed by
parameterizing the distribution of only one set of variables (q(Z)) whilst the implicit distribution of
the other variables is given in terms of the first distribution and the data by equation (5).
We optimize the lower bound LKL with respect to the parameters of the approximating distribution
of the non-collapsed variables. We showed in section 2 that the gradient of the KLC bound is given
by the gradient of the standard MF variational bound, after an update of the collapsed variables. It
is clear from their definition that the same is true of the natural gradients.
4.1

Variable Transformations

We can compute the natural gradient of our collapsed bound by considering the update equations of
the non-collapsed problem as described above. However, if we wish to make use of more powerful
optimisation methods like conjugate gradient ascent, it is helpful to re-parameterize the natural parameters in an unconstrained fashion. The natural gradient is given by [Amari and Nagaoka, 2007]:
e(?) = G(?)?1
g

?LKL
??

(13)

where G(?) is the Fisher information matrix whose i,j th element is given by
G(?)[i,j] = ?Eq(X | ?)
5

h ? 2 ln q(X | ?) i
?? [i] ?? [j]

.

(14)

For exponential family distributions, this reduces to ?2? ?(?), where ? is the log-normaliser. Further,
for exponential family distributions, the Fisher information in the canonical parameters (?) and that
in the expectation parameters (?) are reciprocal, and we also have G(?) = ??/??. This means that
the natural gradient in ? is given by
e(?) = G(?)?1
g

?? ?LKL
?LKL
?LKL
e(?) =
=
and g
.
?? ??
??
??

(15)

The gradient in one set of parameters provides the natural gradient in the other. Thus when our
approximating distribution q is exponential family, we can compute the natural gradient without the
expensive matrix inverse.
4.2

Steepest Ascent is Coordinate Ascent

Sato [2001] showed that the VBEM algorithm was a gradient based algorithm. In fact, VBEM
consists of taking unit steps in the direction of the natural gradient of the canonical parameters.
From equation (9) and the work of Sato [2001], we see that the gradient of the KLC bound can
be obtained by considering the standard mean-field update for the non-collapsed parameter Z. We
confirm these relationships for the models studied in the next section in the supplementary material.
Having confirmed that the VB-E step is equivalent to steepest-gradient ascent we now explore
whether the procedure could be improved by the use of conjugate gradients.
4.3

Conjugate Gradient Optimization

One idea for solving some of the problems associated with steepest ascent is to ensure each gradient
step is conjugate (geometrically) to the previous. Honkela et al. [2010] applied conjugate gradients
to the standard mean field bound, we expect much faster convergence for the KLC bound due to
its differing curvature. Since VBEM uses a step length of 1 to optimize,3 we also used this step
length in conjugate gradients. In the natural conjugate gradient method, the search direction at the
ith iteration is given by si = ?e
gi + ?si?1 . Empirically the Fletcher-Reeves method for estimating
? worked well for us:
e i ii
he
gi , g
?F R =
(16)
ei?1 ii?1
he
gi?1 , g
e> G(?)e
where h?, ?ii denotes the inner product in Riemannian geometry, which is given by g
g. We
e> Ge
e> GG?1 g = g
e> g, and
note from Kuusela et al. [2009] that this can be simplified since g
g=g
other conjugate methods, defined in the supplementary material, can be applied similarly.

5

Experiments

For empirical investigation of the potential speed ups we selected a range of probabilistic models.
We provide derivations of the bound and fuller explanations of the models in the supplementary
material. In each experiment, the algorithm was considered to have converged when the change
in the bound or the Riemannian gradient reached below 10?6 . Comparisons between optimisation
procedures always used the same initial conditions (or set of initial conditions) for each method.
First we recreate the mixture of Gaussians example described by Honkela et al. [2010].
5.1

Mixtures of Gaussians

For a mixture of Gaussians, using the d-separation rule, we select for X the cluster allocation (latent)
variables. These are parameterised through the softmax function for unconstrained optimisation.
Our model includes a fully Bayesian treatment of the cluster parameters and the mixing proportions,
whose approximate posterior distributions appear as (5). Full details of the algorithm derivation are
given in the supplementary material. A neat feature is that we can make use of the discussion above
to derive an expression for the natural gradient without a matrix inverse.
3
We empirically evaluated a line-search procedure, but found that in most cases that Wolfe-Powell conditions were met after a single step of unit length.

6

Table 1: Iterations to convergence for the mixture of Gaussians problem, with varying overlap (R). This table
reports the average number of iterations taken to reach (within 10 nats of) the best known solution. For the more
difficult scenarios (with more overlap in the clusters) the VBEM method failed to reach the optimum solution
within 500 restarts

CG. method
Polack-Ribi?ere
Hestenes-Stiefel
Fletcher-Reeves
VBEM

R=1
3, 100.37
1, 371.55
416.18
?

R=2
15, 698.57
5, 501.25
1, 161.35
?

R=3
5, 767.12
5, 922.4
5, 091.0
?

R=4
1, 613.09
358.03
792.10
992.07

R=5
3, 046.25
172.39
494.24
429.57

Table 2: Time and iterations taken to run LDA on the NIPS 2011 corpus, ? one standard deviation, for two
conjugate methods and VBEM. The Fletcher-Reeves conjugate algorithm is almost ten times as fast as VBEM.
The value of the bound at the optimum was largely the same: deviations are likely just due to the choice of
initialisations, of which we used 12.
Method
Hestenes-Steifel
Fletcher-Reeves
VBEM

Time (minutes)
56.4 ? 18.5
38.5 ? 8.7
370 ? 105

Iterations
644.3 ? 214.5
447.8 ? 100.5
4, 459 ? 1, 296

Bound
?1, 998, 780 ? 201
?1, 998, 743 ? 194
?1, 998, 732 ? 241

In Honkela et al. [2010] data are drawn from a mixture of five two-dimensional Gaussians with
equal weights, each with unit spherical covariance. The centers of the components are at (0, 0) and
(?R, ?R). R is varied from 1 (almost completely overlapping) to 5 (completely separate). The
model is initialised with eight components with an uninformative prior over the mixing proportions:
the optimisation procedure is left to select an appropriate number of components.
Sung et al. [2008] reported that their collapsed method led to improved convergence over VBEM.
Since our objective is identical, though our optimisation procedure different, we devised a metric for
measuring the efficacy of our algorithms which also accounts for their propensity to fall into local
minima. Using many randomised restarts, we measured the average number of iterations taken to
reach the best-known optimum. If the algorithm converged at a lesser optimum, those iterations were
included in the denomiator, but we didn?t increment the numerator when computing the average. We
compared three different conjugate gradient approaches and standard VBEM (which is also steepest
ascent on the KLC bound) using 500 restarts.
Table 1 shows the number of iterations required (on average) to come within 10 nats of the best
known solution for three different conjugate-gradient methods and VBEM. VBEM sometimes failed
to find the optimum in any of the 500 restarts. Even relaxing the stringency of our selection to 100
nats, the VBEM method was always at least twice as slow as the best conjugate method.
5.2

Topic Models

Latent Dirichlet allocation (LDA) [Blei et al., 2003] is a popular approach for extracting topics
from documents. To demonstrate the KLC bound we applied it to 200 papers from the 2011 NIPS
conference. The PDFs were preprocessed with pdftotext, removing non-alphabetical characters
and coarsely filtering words by popularity to form a vocabulary size of 2000.4 We selected the latent
topic-assignment variables for parameterisation, collapsing the topics and the document proportions.
Conjugate gradient optimization was compared to the standard VBEM approach.
We used twelve random initializations, starting each algorithm from each initial condition. Topic and
document distributions where treated with fixed, uninformative priors. On average, the HestenesSteifel algorithm was almost ten times as fast as standard VB, as shown in Table 2, whilst the final
bound varied little between approaches.
4

Some extracted topics are presented in the supplementary material.

7

5.3

RNA-seq alignment

An emerging problem in computational biology is inference of transcript structure and expression
levels using next-generation sequencing technology (RNA-Seq). Several models have been proposed. The BitSeq method [Glaus et al., 2012] is based on a probabilistic model and uses Gibbs
sampling for approximate inference. The sampler can suffer from particularly slow convergence
due to the large size of the problem, which has six million latent variables for the data considered
here. We implemented a variational version of their model and optimised it using VBEM and our
collapsed Riemannian method. We applied the model to data described in Xu et al. [2010], a study
of human microRNA. The model was initialised using four random initial conditions, and optimised
using standard VBEM and the conjugate gradient versions of the algorithm. The Polack-Ribi?ere
conjugate method performed very poorly for this problem, often giving negative conjugation: we
omit it here. The solutions found for the other algorithms were all fairly close, with bounds coming within 60 nats. The VBEM method was dramatically outperformed by the Fletcher-Reeves and
Hestenes-Steifel methods: it took 4600 ? 20 iterations to converge, whilst the conjugate methods
took only 268 ? 4 and 265 ? 1 iterations to converge. At about 8 seconds per iteration, our collapsed
Riemannian method requires around forty minutes, whilst VBEM takes almost eleven hours. All the
variational approaches represent an improvement over a Gibbs sampler, which takes approximately
one week to run for this data [Glaus et al., 2012].

6

Discussion

Under very general conditions (conjugate exponential family) we have shown the equivalence of
collapsed variational bounds and marginalized variational bounds using the KL corrected perspective of King and Lawrence [2006]. We have provided a succinct derivation of these bounds, unifying
several strands of work and laying the foundations for much wider application of this approach.
When the collapsed variables are updated in the standard MF bound the KLC bound is identical to
the MF bound in value and gradient. Sato [2001] has shown that coordinate ascent of the MF bound
(as proscribed by VBEM updates) is equivalent to steepest ascent of the MF bound using natural
gradients. This implies that standard variational inference is also performing steepest ascent on the
KLC bound. This equivalence between natural gradients and the VBEM update equations means
our method is quickly implementable for any model where the mean field update equations have
been computed. It is only necessary to determine which variables to collapse using a d-separation
test. Importantly this implies our approach can readily be incorporated in automated inference engines such as that provided by infer.net [Minka et al., 2010]. We?d like to emphasise the ease with
which the method can be applied: we have provided derivations of equivalencies of the bounds and
gradients which should enable collapsed conjugate optimisation of any existing mean field algorithm, with minimal changes to the software. Indeed our own implementations (see supplementary
material) use just a few lines of code to switch between the VBEM and conjugate methods.
The improved performance arises from the curvature of the KLC bound. We have shown that it is
always less negative than that of the original variational bound allowing much larger steps in the
variational parameters as King and Lawrence [2006] suggested. This also provides a gateway to
second-order optimisation, which could prove even faster.
We provided empirical evidence of the performance increases that are possible using our method in
three models. In a thorough exploration of the convergence properties of a mixture of Gaussians
model, we concluded that a conjugate Riemannian algorithm can find solutions that are not found
with standard VBEM. In a large LDA model, we found that performance can be improved many
times over that of the VBEM method. In the BitSeq model for differential expression of genes
transcripts we showed that very large improvements in performance are possible for models with
huge numbers of latent variables.

Acknowledgements
The authors would like to thank Michalis Titsias for helpful commentary on a previous draft and
Peter Glaus for help with a C++ implementation of the RNAseq alignment algorithm. This work
was funded by EU FP7-KBBE Project Ref 289434 and BBSRC grant number BB/1004769/1.
8

References
S. Amari and H. Nagaoka. Methods of information geometry. AMS, 2007.
A. Asuncion, M. Welling, P. Smyth, and Y. Teh. On smoothing and inference for topic models.
arXiv preprint arXiv:1205.2662, 2012.
C. M. Bishop. Pattern Recognition and Machine Learning. Springer New York, 2006.
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. The Journal of Machine Learning
Research, 3:993?1022, 2003.
Z. Ghahramani and M. Beal. Propagation algorithms for variational Bayesian learning. Advances in
neural information processing systems, pages 507?513, 2001.
P. Glaus, A. Honkela, and M. Rattray. Identifying differentially expressed transcripts from RNAseq data with biological variation. Bioinformatics, 2012. doi: 10.1093/bioinformatics/bts260.
Advance Access.
M. Hoffman, D. Blei, C. Wang, and J. Paisley. Stochastic variational inference. arXiv preprint
arXiv:1206.7051, 2012.
A. Honkela, T. Raiko, M. Kuusela, M. Tornio, and J. Karhunen. Approximate Riemannian conjugate
gradient learning for fixed-form variational Bayes. The Journal of Machine Learning Research,
9999:3235?3268, 2010.
N. King and N. D. Lawrence. Fast variational inference for Gaussian process models through KLcorrection. Machine Learning: ECML 2006, pages 270?281, 2006.
K. Kurihara, M. Welling, and Y. W. Teh. Collapsed variational Dirichlet process mixture models. In
Proceedings of the International Joint Conference on Artificial Intelligence, volume 20, page 19,
2007.
M. Kuusela, T. Raiko, A. Honkela, and J. Karhunen. A gradient-based algorithm competitive with
variational Bayesian EM for mixture of Gaussians. In Neural Networks, 2009. IJCNN 2009.
International Joint Conference on, pages 1688?1695. IEEE, 2009.
M. L?azaro-Gredilla and M. K. Titsias. Variational heteroscedastic Gaussian process regression. In
Proceedings of the International Conference on Machine Learning (ICML), 2011, 2011.
M. L?azaro-Gredilla, S. Van Vaerenbergh, and N. Lawrence. Overlapping mixtures of Gaussian
processes for the data association problem. Pattern Recognition, 2011.
T. P. Minka, J. M. Winn, J. P. Guiver, and D. A. Knowles. Infer .NET 2.4. Microsoft Research
Cambridge, 2010.
M. A. Sato. Online model selection based on the variational Bayes. Neural Computation, 13(7):
1649?1681, 2001.
J. Sung, Z. Ghahramani, and S. Bang. Latent-space variational Bayes. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 30(12):2236?2242, 2008.
Y. W. Teh, D. Newman, and M. Welling. A collapsed variational Bayesian inference algorithm for
latent Dirichlet allocation. Advances in neural information processing systems, 19:1353, 2007.
G. Xu et al. Transcriptome and targetome analysis in MIR155 expressing cells using RNAseq. RNA, pages 1610?1622, June 2010. ISSN 1355-8382. doi: 10.1261/rna.2194910. URL
http://rnajournal.cshlp.org/cgi/doi/10.1261/rna.2194910.

9

"
4929,"Efficient learning by implicit exploration in bandit
problems with side observations

Tom?as? Koc?ak
Gergely Neu
Michal Valko
R?emi Munos?
SequeL team, INRIA Lille ? Nord Europe, France
{tomas.kocak,gergely.neu,michal.valko,remi.munos}@inria.fr

Abstract
We consider online learning problems under a a partial observability model capturing situations where the information conveyed to the learner is between full
information and bandit feedback. In the simplest variant, we assume that in addition to its own loss, the learner also gets to observe losses of some other actions.
The revealed losses depend on the learner?s action and a directed observation system chosen by the environment. For this setting, we propose the first algorithm
that enjoys near-optimal regret guarantees without having to know the observation system before selecting its actions. Along similar lines, we also define a new
partial information setting that models online combinatorial optimization problems where the feedback received by the learner is between semi-bandit and full
feedback. As the predictions of our first algorithm cannot be always computed
efficiently in this setting, we propose another algorithm with similar properties
and with the benefit of always being computationally efficient, at the price of a
slightly more complicated tuning mechanism. Both algorithms rely on a novel
exploration strategy called implicit exploration, which is shown to be more efficient both computationally and information-theoretically than previously studied
exploration strategies for the problem.

1

Introduction

Consider the problem of sequentially recommending content for a set of users. In each period of
this online decision problem, we have to assign content from a news feed to each of our subscribers
so as to maximize clickthrough. We assume that this assignment needs to be done well in advance,
so that we only observe the actual content after the assignment was made and the user had the
opportunity to click. While we can easily formalize the above problem in the classical multi-armed
bandit framework [3], notice that we will be throwing out important information if we do so! The
additional information in this problem comes from the fact that several news feeds can refer to the
same content, giving us the opportunity to infer clickthroughs for a number of assignments that
we did not actually make. For example, consider the situation shown on Figure 1a. In this simple
example, we want to suggest one out of three news feeds to each user, that is, we want to choose a
matching on the graph shown on Figure 1a which covers the users. Assume that news feeds 2 and 3
refer to the same content, so whenever we assign news feed 2 or 3 to any of the users, we learn
the value of both of these assignments. The relations between these assignments can be described
by a graph structure (shown on Figure 1b), where nodes represent user-news feed assignments, and
edges mean that the corresponding assignments reveal the clickthroughs of each other. For a more
compact representation, we can group the nodes by the users, and rephrase our task as having to
choose one node from each group. Besides its own reward, each selected node reveals the rewards
assigned to all their neighbors.
?

Current affiliation: Google DeepMind

1

user1

user2

,2

e1,

3

e1,1

e 2,1

2

e 2,

content2
e1,2
e1,3

e2,3

e 1,1

e1

user1

user2

news f eed1
content1

news f eed2

news f eed3

e2,1

content2
e2,2
e2,3

content2

Figure 1a: Users and news feeds. The thick edges represent one
potential matching of users to feeds, grouped news feeds show the
same content.

Figure 1b: Users and news
feeds. Connected feeds mutually
reveal each others clickthroughs.

The problem described above fits into the framework of online combinatorial optimization where in
each round, a learner selects one of a very large number of available actions so as to minimize the
losses associated with its sequence of decisions. Various instances of this problem have been widely
studied in recent years under different feedback assumptions [7, 2, 8], notably including the so-called
full-information [13] and semi-bandit [2, 16] settings. Using the example in Figure 1a, assuming full
information means that clickthroughs are observable for all assignments, whereas assuming semibandit feedback, clickthroughs are only observable on the actually realized assignments. While
it is unrealistic to assume full feedback in this setting, assuming semi-bandit feedback is far too
restrictive in our example. Similar situations arise in other practical problems such as packet routing
in computer networks where we may have additional information on the delays in the network
besides the delays of our own packets.
In this paper, we generalize the partial observability model first proposed by Mannor and Shamir
[15] and later revisited by Alon et al. [1] to accommodate the feedback settings situated between the
full-information and the semi-bandit schemes. Formally, we consider a sequential decision making
problem where in each time step t the (potentially adversarial) environment assigns a loss value to
each out of d components, and generates an observation system whose role will be clarified soon.
Obliviously of the environment?s choices, the learner chooses an action Vt from a fixed action
set S ? {0, 1}d represented by a binary vector with at most m nonzero components, and incurs
the sum of losses associated with the nonzero components of Vt . At the end of the round, the
learner observes the individual losses along the chosen components and some additional feedback
based on its action and the observation system. We represent this observation system by a directed
observability graph with d nodes, with an edge connecting i ? j if and only if the loss associated
with j is revealed to the learner whenever Vt,i = 1. The goal of the learner is to minimize its total
loss obtained over T repetitions of the above procedure. The two most well-studied variants of this
general framework are the multi-armed bandit problem [3] where each action consists of a single
component and the observability graph is a graph without edges, and the problem of prediction with
expert advice [17, 14, 5] where each action consists of exactly one component and the observability
graph is complete. In the true combinatorial setting where m > 1, the empty and complete graphs
correspond to the semi-bandit and full-information settings respectively.
Our model directly extends the model of Alon et al. [1], whose setup coincides with m = 1 in our
framework. Alon et al. themselves were motivated by the work of Mannor and Shamir [15], who
considered undirected observability systems where actions mutually uncover each other?s losses.
Mannor
? and Shamir proposed an algorithm based on linear programming that achieves a regret of
?
O( cT ), where c is the number of cliques into which the graph can be ?
split. Later, Alon et al. [1]
proposed an algorithm called E XP 3-SET that guarantees a regret of O( ?T log d), where ? is an
upper bound on the independence numbers of the observability graphs assigned by the environment.
In particular, this bound is tighter than the bound of Mannor and Shamir since ? ? c for any graph.
Furthermore, E XP 3-SET is much more efficient than the algorithm of Mannor and Shamir as it only
requires running the E XP 3 algorithm of Auer et al. [3] on the decision set, which runs in time linear
in d. Alon et al. [1] also extend the model of Mannor and Shamir in allowing the observability
graph to be directed. For this setting, they offer another algorithm called E XP 3-DOM with similar
guarantees, although with the serious drawback that it requires access to the observation system
before choosing its actions. This assumption poses severe limitations to the practical applicability
of E XP 3-DOM, which also needs to solve a sequence of set cover problems as a subroutine.
2

In the present paper, we offer two computationally and information-theoretically efficient algorithms
for bandit problems with directed observation systems. Both of our algorithms circumvent the costly
exploration phase required by E XP 3-DOM by a trick that we will refer to IX as in Implicit eXploration. Accordingly, we name our algorithms E XP 3-IX and FPL-IX, which are variants of the
well-known E XP 3 [3] and FPL [12] algorithms enhanced with implicit exploration. Our first algorithm E XP 3-IX is specifically designed1 to work in the setting of Alon et al. [1] with m = 1 and
does not need to solve any set cover problems or have any sort of prior knowledge concerning the
observation systems chosen by the adversary.2 FPL-IX, on the other hand, does need either to solve
set cover problems or have a prior upper bound on the independence numbers of the observability
graphs, but can be computed efficiently for a wide range of true combinatorial problems with m > 1.
We note that our algorithms do not even need to know the number of rounds T and our regret bounds
scale with the average independence number ?
? of the graphs played by the adversary rather than the
largest of these numbers. They both employ adaptive learning rates and unlike E XP 3-DOM, they
do not need to use a doubling trick to be anytime or to aggregate outputs of multiple algorithms
to
?
? 3/2 ?
optimally set their learning rates. Both algorithms
achieve
regret
guarantees
of
O(m
?
T
)
in
the
?
? ?
? T ) in the simple setting.
combinatorial setting, which becomes O(
Before diving into the main content, we give an important graph-theoretic statement that we will
rely on when analyzing both of our algorithms. The lemma is a generalized version of Lemma 13 of
Alon et al. [1] and its proof is given in Appendix A.
Lemma 1. Let G be a directed graph with vertex set V = {1, . . . , d}. Let Ni? be the inneighborhood of node i, i.e., the set of nodes j such that (j ? i) ? G. Let ? be the independence
Pd
number of G and p1 ,. . . ,pd are numbers from [0, 1] such that i=1 pi ? m. Then
d
X
i=1

where Pi =

2

P

j?Ni?



mdd2 /ce + d
?
2m?
log
1
+
+ 2m,
1
1
?
m pi + m Pi + c
pi

pj and c is a positive constant.

Multi-armed bandit problems with side information

In this section, we start by the simplest setting fitting into our framework, namely the multi-armed
bandit problem with side observations. We provide intuition about the implicit exploration procedure
behind our algorithms and describe E XP 3-IX, the most natural algorithm based on the IX trick.
The problem we consider is defined as follows. In each round t = 1, 2, . . . , T , the environment assigns a loss vector `t ? [0, 1]d for d actions and also selects an observation system described by the
directed graph Gt . Then, based on its previous observations (and likely some external source of randomness) the learner selects action It and subsequently incurs and observes loss `t,It . Furthermore,
the learner also observes the losses `t,j for all j such that (It ? j) ? Gt , denoted by the indicator
Ot,i . Let Ft?1 = ?(It?1 , . . . , I1 ) capture the interaction history up to time t. As usual in online
settings [6], the performance is measured in terms of (total expected) regret, which is the difference
between a total loss received and the total loss of the best single action chosen in hindsight,
"" T
#
X
RT = max E
(`t,It ? `t,i ) ,
i?[d]

t=1

where the expectation integrates over the random choices made by the learning algorithm. Alon
et al. [1] adapted the well-known E XP 3 algorithm of Auer et al. [3] for this precise problem. Their
algorithm, E XP 3-DOM, works by maintaining a weight wt,i for each individual arm i ? [d] in each
round, and selecting It according to the distribution
wt,i
+ ??t,i ,
P [It = i |Ft?1 ] = (1 ? ?)pt,i + ??t,i = (1 ? ?) Pd
j=1 wt,j
1

E XP 3-IX can also be efficiently implemented for some specific combinatorial decision sets even with
m > 1, see, e.g., Cesa-Bianchi and Lugosi [7] for some examples.
2
However, it is still necessary to have access to the observability graph to construct low bias estimates of
losses, but only after the action is selected.

3

where ? ? (0, 1) is parameter of the algorithm and ?t is an exploration distribution whose role we
will shortly clarify. After each round, E XP 3-DOM defines the loss estimates
`t,i
`?t,i =
1{(It ?i)?Gt } where ot,i = E [Ot,i |Ft?1 ] = P [(It ? i) ? Gt |Ft?1 ]
ot,i
for each i ? [d]. These loss estimates are then used to update the weights for all i as
?

wt+1,i = wt,i e?? `t,i .
It is easy to see that the these loss estimates `?t,i are unbiased estimates of the true losses whenever
pt,i > 0 holds for all i. This requirement along with another important technical issue justify
the presence of the exploration distribution ?t . The key idea behind E XP 3-DOM is to compute a
dominating set Dt ? [d] of the observability graph Gt in each round, and define ?t as the uniform
distribution over Dt . This choice ensures that ot,i ? pt,i + ?/|Dt |, a crucial requirement for the
analysis of [1]. In what follows, we propose an exploration scheme that does not need any fancy
computations but, more importantly, works without any prior knowledge of the observability graphs.
2.1

Efficient learning by implicit exploration

In this section, we propose the simplest exploration scheme imaginable, which consists of merely
pretending to explore. Precisely, we simply sample our action It from the distribution defined as
wt,i
P [It = i |Ft?1 ] = pt,i = Pd
,
(1)
j=1 wt,j
without explicitly mixing with any exploration distribution. Our key trick is to define the loss estimates for all arms i as
`t,i
`?t,i =
1{(It ?i)?Gt } ,
ot,i + ?t
where ?t > 0 is a parameter of our algorithm. It is easy to check that `?t,i is a biased estimate of `t,i .
?
The nature of this bias,hhowever, is
i very special. First, observe that `t,i is an optimistic estimate of
?
`t,i in the sense that E `t,i |Ft?1 ? `t,i . That is, our bias always ensures that, on expectation, we
underestimate the loss of any fixed arm i. Even more importantly, our loss estimates also satisfy

"" d
#


d
d

X
X
X
ot,i

?
?1
E
pt,i `t,i  Ft?1 =
pt,i `t,i +
pt,i `t,i

ot,i + ?t
i=1
i=1
i=1
(2)
d
d
X
X
pt,i `t,i
,
=
pt,i `t,i ? ?t
o + ?t
i=1
i=1 t,i
that is, the bias of the estimated losses suffered by our algorithm is directly controlled by ?t . As we
will see in the analysis, it is sufficient to control the bias of our own estimated performance as long
as we can guarantee that the loss estimates associated with any fixed arm are optimistic?which is
precisely what we have. Note that this slight modification ensures that the denominator of `?t,i is
lower bounded by pt,i + ?t , which is a very similar property as the one achieved by the exploration
scheme used by E XP 3-DOM. We call the above loss estimation method implicit exploration or IX,
as it gives rise to the same effect as explicit exploration without actually having to implement any
exploration policy. In fact, explicit and implicit explorations can both be regarded as two different
approaches for bias-variance tradeoff: while explicit exploration biases the sampling distribution
of It to reduce the variance of the loss estimates, implicit exploration achieves the same result by
biasing the loss estimates themselves.
From this point on, we take a somewhat more predictable course and define our algorithm E XP 3-IX
as a variant of E XP 3 using the IX loss estimates. One of the twists is that E XP 3-IX is actually based
on the adaptive learning-rate variant of E XP 3 proposed by Auer et al. [4], which avoids the necessity
of prior knowledge of the observability graphs in order to set a proper learning rate. This algorithm
b t?1,i = Pt?1 `?s,i and for all i ? [d] computing the weights as
is defined by setting L
s=1
wt,i = (1/d)e??t Lt?1,i .
b

These weights are then used to construct the sampling distribution of It as defined in (1). The
resulting E XP 3-IX algorithm is shown as Algorithm 1.
4

2.2

Performance guarantees for E XP 3-IX

Our analysis follows the footsteps of Auer et al.
[3] and Gy?orfi and Ottucs?ak [9], who provide
an improved analysis of the adaptive learningrate rule proposed by Auer et al. [4]. However,
a technical subtlety will force us to proceed a
little differently than these standard proofs: for
achieving the tightest possible bounds and the
most efficient algorithm, we need to tune our
learning rates according to some random quantities that depend on the performance of E XP 3IX. In fact, the key quantities in our analysis are
the terms
Qt =

d
X
i=1

Algorithm 1 E XP 3-IX
1: Input: Set of actions S = [d],
2:
parameters ?t ? (0, 1), ?t > 0 for t ? [T ].
3: for t = 1 to T do
b t?1,i ) for i ? [d]
4:
wt,i ? (1/d) exp (??t L
5:
An adversary privately chooses losses `t,i
6:
7:
8:
9:
10:
11:

for i ? [d] and generates a graph Gt
Pd
Wt ? i=1 wt,i
pt,i ? wt,i /Wt
Choose It ? pt = (pt,1 , . . . , pt,d )
Observe graph Gt
ObservePpairs {i, `t,i } for (It ? i) ? Gt
ot,i ? (j?i)?Gt pt,j for i ? [d]
`
`?t,i ? t,i 1{(I ?i)?G } for i ? [d]

12:
13: end for

pt,i
,
ot,i + ?t

ot,i +?t

t

t

which depend on the interaction history Ft?1 for all t. Our theorem below gives the performance
guarantee for E XP 3-IX using a parameter setting adaptive to the values of Qt . A full proof of the
theorem is given in the supplementary material.
q
Pt?1
Theorem 1. Setting ?t = ?t = (log d)/(d + s=1 Qs ) , the regret of E XP 3-IX satisfies
""r
#


PT
d + t=1 Qt log d .
RT ? 4E
(3)
Proof sketch. Following the proof of Lemma 1 in Gy?orfi and Ottucs?ak [9], we can prove that

d
d
 2  log W
X
log Wt+1
?t X
t
?
?
pt,i `t,i +
?
.
pt,i `t,i ?
2 i=1
?t
?t+1
i=1

(4)

Taking conditional expectations, using Equation (2) and summing up both sides, we get



T X
d
T
T 

X
X
X
log Wt
log Wt+1 
?t
+ ?t Qt +
E
?
pt,i `t,i ?
 Ft?1 .
2
?t
?t+1
t=1 i=1
t=1
t=1
Using Lemma 3.5 of Auer et al. [4] and plugging in ?t and ?t , this becomes
r



d
T X
T

X
X
PT
log Wt+1 
log Wt
pt,i `t,i ? 3
F
d + t=1 Qt log d +
?
E
 t?1 .
?t
?t+1
t=1 i=1
t=1
Taking expectations on both sides, the second term on the right hand side telescopes into






h
i
log W1
log WT +1
log wT +1,j
log d
? T,j
E
?
?E ?
=E
+E L
?1
?T +1
?T +1
?T +1
for any j ? [d], giving the desired result as
T X
d
X
t=1 i=1

pt,i `t,i ?

T
X

`t,j + 4E

""r


d+

PT

t=1



#

Qt log d ,

t=1

where we used the definition of ?T and the optimistic property of the loss estimates.
Setting m = 1 and c = ?t in Lemma 1, gives the following deterministic upper bound on each Qt .
Lemma 2. For all t ? [T ],


d
X
pt,i
dd2 /?t e + d
Qt =
? 2?t log 1 +
+ 2.
o + ?t
?t
i=1 t,i
5

Combining Lemma 2 with Theorem 1 we prove our main result concerning the regret of E XP 3-IX.
Corollary 1. The regret of E XP 3-IX satisfies
r

PT
d + 2 t=1 (Ht ?t + 1) log d,
RT ? 4
where
Ht = log 1 +

3

dd2

p

td/ log de + d
?t

!
= O(log(dT )).

Combinatorial semi-bandit problems with side observations

We now turn our attention to the setting of online combinatorial optimization (see [13, 7, 2]). In
this variant of the online learning problem, the learner has access to a possibly huge action set
d
S ? {0, 1} where each action is represented by a binary vector v of dimensionality d. In what
follows, we assume that kvk1 ? m holds for all v ? S and some 1 ? m  d, with the case m = 1
corresponding to the multi-armed bandit setting considered in the previous section. In each round
t = 1, 2, . . . , T of the decision process, the learner picks an action Vt ? S and incurs a loss of VtT `t .
At the end of the round, the learner receives some feedback based on its decision Vt and the loss
vector `t . The regret of the learner is defined as
"" T
#
X
T
RT = max E
(Vt ? v) `t .
v?S

t=1

Previous work has considered the following feedback schemes in the combinatorial setting:
? The full information scheme where the learner?gets to observe `t regardless of the chosen
action. The minimax optimal regret of order m T log d here is achieved by C OMPONENT H EDGE algorithm of [13], while the?Follow-the-Perturbed-Leader (FPL) [12, 10] was
shown to enjoy a regret of order m3/2 T log d by [16].
? The semi-bandit scheme where the learner gets to observe the components `t,i of the loss
vector where Vt,i = 1, that is, the losses along the components chosen by ?
the learner at
mdT log d)
time t. As shown by [2], C OMPONENT H EDGE achieves a near-optimal
O(
?
regret guarantee, while [16] show that FPL enjoys a bound of O(m dT log d).
? The bandit scheme where the learner only observes its own loss VtT `t . There are currently
no known efficient algorithms that get close to the minimax regret in this setting?the
reader is referred to Audibert et al. [2] for an overview of recent results.
In this section, we define a new feedback scheme situated between the semi-bandit and the fullinformation schemes. In particular, we assume that the learner gets to observe the losses of some
other components not included in its own decision vector Vt . Similarly to the model of Alon et al.
[1], the relation between the chosen action and the side observations are given by a directed observability Gt (see example in Figure 1). We refer to this feedback scheme as semi-bandit with side
observations. While our theoretical results stated in the previous section continue to hold in this setting, combinatorial E XP 3-IX could rarely be implemented efficiently?we refer to [7, 13] for some
positive examples. As one of the main concerns in this paper is computational efficiency, we take
a different approach: we propose a variant of FPL that efficiently implements the idea of implicit
exploration in combinatorial semi-bandit problems with side observations.
3.1

Implicit exploration by geometric resampling

b t?1 =
In each round t, FPL bases its decision on some estimate L
Pt?1
Lt?1 = s=1 `s as follows:


b t?1 ? Zt .
Vt = arg min v T ?t L

Pt?1 ?
s=1 `s of the total losses
(5)

v?S

Here, ?t > 0 is a parameter of the algorithm and Zt is a perturbation vector with components drawn
independently from an exponential distribution with unit expectation. The power of FPL lies in
that it only requires an oracle that solves the (offline) optimization problem minv?S v T ` and thus
6

can be used to turn any efficient offline solver into an online optimization algorithm with strong
guarantees. To define our algorithm precisely, we need to some further notation. We redefine Ft?1
to be ?(Vt?1 , . . . , V1 ), Ot,i to be the indicator of the observed component and let
qt,i = E [Vt,i |Ft?1 ]

and

ot,i = E [Ot,i |Ft?1 ] .

The most crucial point of our algorithm is the construction of our loss estimates. To implement
the idea of implicit exploration by optimistic biasing, we apply a modified version of the geometric
resampling method of Neu and Bart?ok [16] constructed as follows: Let Ot0 (1), Ot0 (2), . . . be independent copies3 of Ot and let Ut,i be geometrically distributed random variables for all i = [d] with
parameter ?t . We let

	

0
Kt,i = min k : Ot,i
(k) = 1 ? {Ut,i }
(6)
and define our loss-estimate vector `?t ? Rd with its i-th element as
`?t,i = Kt,i Ot,i `t,i .

(7)

By definition, we have E [Kt,i |Ft?1 ] = 1/(ot,i + (1 ? ot,i )?t ), implying that our loss estimates are
optimistic in the sense that they lower bound the losses in expectation:
i
h 
ot,i

E `?t,i  Ft?1 =
`t,i ? `t,i .
ot,i + (1 ? ot,i )?t
Here we used the fact that Ot,i is independent of Kt,i and has expectation ot,i given Ft?1 . We call
this algorithm Follow-the-Perturbed-Leader with Implicit eXploration (FPL-IX, Algorithm 2).
Note that the geometric resampling procedure can be terminated as soon as Kt,i becomes welldefined for all i with Ot,i = 1. As noted by Neu and Bart?ok [16], this requires generating at most d
copies of Ot on expectation. As each of these copies requires one access to the linear optimization
oracle over S, we conclude that the expected running time of FPL-IX is at most d times that of
the expected running time of the oracle. A high-probability guarantee of the running time can be
obtained by observing that Ut,i ? log 1? /?t holds with probability at least 1 ? ? and thus we can

stop sampling after at most d log d? /?t steps with probability at least 1 ? ?.
3.2

Performance guarantees for FPL-IX

The analysis presented in this section com- Algorithm 2 FPL-IX
bines some techniques used by Kalai and Vem1: Input: Set of actions S,
pala [12], Hutter and Poland [11], and Neu
2:
parameters ?t ? (0, 1), ?t > 0 for t ? [T ].
and Bart?ok [16] for analyzing FPL-style learn- 3: for t = 1 to T do
ers. Our proofs also heavily rely on some spe4:
An adversary privately chooses losses `t,i
cific properties of the IX loss estimate defined
for all i ? [d] and generates a graph Gt
in Equation 7. The most important difference
5:
Draw Zt,i ? Exp(1) for
 all i ? [d] 
from the analysis presented in Section 2.2 is
T
b t?1 ? Zt
6:
Vt ? arg minv?S v ?t L
that now we are not able to use random learn7:
Receive loss VtT `t
ing rates as we cannot compute the values cor8:
Observe graph Gt
responding to Qt efficiently. In fact, these val9:
Observe pairs {i, `t,i } for all i, such that
ues are observable in the information-theoretic
(j ? i) ? Gt and v(It )j = 1
sense, so we could prove bounds similar to TheCompute Kt,i for all i ? [d] using Eq. (6)
orem 1 had we had access to infinite compu- 10:
tational resources. As our focus in this paper 11:
`?t,i ? Kt,i Ot,i `t,i
is on computationally efficient algorithms, we 12: end for
choose to pursue a different path. In particular,
our learning rates will be tuned according to efficiently computable approximations ?
et of the respective independence numbers ?t that satisfy ?t /C ? ?
et ? ?t ? d for some C ? 1. For the sake
of simplicity, we analyze the algorithm in the oblivious adversary model. The following theorem
states the performance guarantee for FPL-IX in terms of the learning rates and random variables of
the form
d
X
qt,i
e t (c) =
.
Q
o +c
i=1 t,i
3
Such independent copies can be simply generated by sampling independent copies of Vt using the FPL rule
(5) and then computing Ot0 (k) using the observability Gt . Notice that this procedure requires no interaction
between the learner and the environment, although each sample requires an oracle access.

7

Theorem 2. Assume ?t ? 1/2 for all t and ?1 ? ?2 ? ? ? ? ? ?T . The regret of FPL-IX satisfies
 X
 
T
T
h
i
X
m (log d + 1)
?t
et
e t (?t ) .
RT ?
+ 4m
+
?t E Q
?t E Q
?T
1 ? ?t
t=1
t=1
Proof sketch. As usual for analyzing FPL methods [12, 11, 16], we first define a hypothetical learner
e ? Z1 and has access to `?t on top of L
b t?1
that uses a time-independent perturbation vector Z


bt ? Z
e .
Vet = arg min v T ?t L
v?S

Clearly, this learner is infeasible as it uses observations from the future. Also, observe that this
learner does not actually interact with the environment and depends on the predictions made by the
actual learner only through the loss estimates. By standard arguments, we can prove
"" T
#
T
X
m (log d + 1)
.
E
Vet ? v `?t ?
?T
t=1
Using the techniques of Neu and Bart?ok [16], we can relate the performance of Vt to that of Vet ,
which we can further upper bounded after a long and tedious calculation as




 

h
i
2 

?

T
et
 Ft?1 .
E (Vt ? Vet )T `?t  Ft?1 ? ?t E Vet?1
`?t  Ft?1 ? 4m?t E Q
1?? 

h
i

The result follows by observing that E v T `?t  Ft?1 ? v T `t for any fixed v ? S by the optimistic
property of the IX estimate and also from the fact that by the definition of the estimates we infer that

h
i
i
h

T
e t (?t ) .
E Vet?1
`?t  Ft?1 ? E [ VtT `t | Ft?1 ] ? ?t E Q
The next lemma shows a suitable upper bound
P for the last two terms in the bound of Theorem 2. It
follows from observing that ot,i ? (1/m) j?{N ? ?{i}} qt,j and applying Lemma 1.
t,i

Lemma 3. For all t ? [T ] and any c ? (0, 1),
e t (c) =
Q

d
X
i=1



qt,i
mdd2 /ce + d
? 2m?t log 1 +
+ 2m.
ot,i + c
?t

We are now ready to state the main result of this section, which is obtained by combining Theorem 2,
Lemma 3, and Lemma 3.5 of Auer et al. [4] applied to the following upper bound
q
q
T
T
X
X
PT
PT
?t
?t
q
q
?
2
?
C
?
?
2
d + C t=1 ?t .
t
t=1
Pt
Pt?1
t=1
t=1
d + s=1 ?
es
s=1 ?s /C
Corollary 2. Assume that for
? ?
et ? ?t ? d for some C > 1, and assume
r all t ? [T ], ?t /C

Pt?1 
md > 4. Setting ?t = ?t = (log d + 1) / m d + s=1 ?
es , the regret of FPL-IX satisfies
RT ? Hm

3/2

r

d+C


?
t=1 t (log d + 1),

PT

where H = O(log(mdT )).

Conclusion We presented an efficient algorithm for learning with side observations based on implicit exploration. This technique gave rise to multitude of improvements. Remarkably, our algorithms no longer need to know the observation system before choosing the action unlike the method
of [1]. Moreover, we extended the partial observability model of [15, 1] to accommodate problems
with large and structured action sets and also gave an efficient algorithm for this setting.
Acknowledgements The research presented in this paper was supported by French Ministry
of Higher Education and Research, by European Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no 270327 (CompLACS), and by FUI project Herm`es.
8

References
[1] Alon, N., Cesa-Bianchi, N., Gentile, C., and Mansour, Y. (2013). From Bandits to Experts: A
Tale of Domination and Independence. In Neural Information Processing Systems.
[2] Audibert, J. Y., Bubeck, S., and Lugosi, G. (2014). Regret in Online Combinatorial Optimization. Mathematics of Operations Research, 39:31?45.
[3] Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. (2002a). The nonstochastic multiarmed bandit problem. SIAM J. Comput., 32(1):48?77.
[4] Auer, P., Cesa-Bianchi, N., and Gentile, C. (2002b). Adaptive and self-confident on-line learning
algorithms. Journal of Computer and System Sciences, 64:48?75.
[5] Cesa-Bianchi, N., Freund, Y., Haussler, D., Helmbold, D., Schapire, R., and Warmuth, M.
(1997). How to use expert advice. Journal of the ACM, 44:427?485.
[6] Cesa-Bianchi, N. and Lugosi, G. (2006). Prediction, Learning, and Games. Cambridge University Press, New York, NY, USA.
[7] Cesa-Bianchi, N. and Lugosi, G. (2012). Combinatorial bandits. Journal of Computer and
System Sciences, 78:1404?1422.
[8] Chen, W., Wang, Y., and Yuan, Y. (2013). Combinatorial Multi-Armed Bandit: General Framework and Applications. In International Conference on Machine Learning, pages 151?159.
[9] Gy?orfi, L. and Ottucs?ak, b. (2007). Sequential prediction of unbounded stationary time series.
IEEE Transactions on Information Theory, 53(5):866?1872.
[10] Hannan, J. (1957). Approximation to Bayes Risk in Repeated Play. Contributions to the theory
of games, 3:97?139.
[11] Hutter, M. and Poland, J. (2004). Prediction with Expert Advice by Following the Perturbed
Leader for General Weights. In Algorithmic Learning Theory, pages 279?293.
[12] Kalai, A. and Vempala, S. (2005). Efficient algorithms for online decision problems. Journal
of Computer and System Sciences, 71:291?307.
[13] Koolen, W. M., Warmuth, M. K., and Kivinen, J. (2010). Hedging structured concepts. In
Proceedings of the 23rd Annual Conference on Learning Theory (COLT), pages 93?105.
[14] Littlestone, N. and Warmuth, M. (1994). The weighted majority algorithm. Information and
Computation, 108:212?261.
[15] Mannor, S. and Shamir, O. (2011). From Bandits to Experts: On the Value of SideObservations. In Neural Information Processing Systems.
[16] Neu, G. and Bart?ok, G. (2013). An Efficient Algorithm for Learning with Semi-bandit Feedback. In Jain, S., Munos, R., Stephan, F., and Zeugmann, T., editors, Algorithmic Learning
Theory, volume 8139 of Lecture Notes in Computer Science, pages 234?248. Springer Berlin
Heidelberg.
[17] Vovk, V. (1990). Aggregating strategies. In Proceedings of the third annual workshop on
Computational learning theory (COLT), pages 371?386.

9

"
5480,"A Gaussian Process Model of Quasar
Spectral Energy Distributions

Andrew Miller? , Albert Wu
School of Engineering and Applied Sciences
Harvard University
acm@seas.harvard.edu, awu@college.harvard.edu
Jeffrey Regier, Jon McAuliffe
Department of Statistics
University of California, Berkeley
{jeff, jon}@stat.berkeley.edu

Dustin Lang
McWilliams Center for Cosmology
Carnegie Mellon University
dstn@cmu.edu
Ryan Adams ?
School of Engineering and Applied Sciences
Harvard University
rpa@seas.harvard.edu

Prabhat, David Schlegel
Lawrence Berkeley National Laboratory
{prabhat, djschlegel}@lbl.gov

Abstract
We propose a method for combining two sources of astronomical data, spectroscopy and photometry, that carry information about sources of light (e.g., stars,
galaxies, and quasars) at extremely different spectral resolutions. Our model treats
the spectral energy distribution (SED) of the radiation from a source as a latent
variable that jointly explains both photometric and spectroscopic observations.
We place a flexible, nonparametric prior over the SED of a light source that admits a physically interpretable decomposition, and allows us to tractably perform
inference. We use our model to predict the distribution of the redshift of a quasar
from five-band (low spectral resolution) photometric data, the so called ?photoz? problem. Our method shows that tools from machine learning and Bayesian
statistics allow us to leverage multiple resolutions of information to make accurate predictions with well-characterized uncertainties.

1

Introduction

Enormous amounts of astronomical data are collected by a range of instruments at multiple spectral
resolutions, providing information about billions of sources of light in the observable universe [1,
10]. Among these data are measurements of the spectral energy distributions (SEDs) of sources of
light (e.g. stars, galaxies, and quasars). The SED describes the distribution of energy radiated by a
source over the spectrum of wavelengths or photon energy levels. SEDs are of interesting because
they convey information about a source?s physical properties, including type, chemical composition,
and redshift, which will be an estimand of interest in this work.
The SED can be thought of as a latent function of which we can only obtain noisy measurements.
Measurements of SEDs, however, are produced by instruments at widely varying spectral resolutions ? some instruments measure many wavelengths simultaneously (spectroscopy), while others
?
?

http://people.seas.harvard.edu/~acm/
http://people.seas.harvard.edu/~rpa/

1

8

PSFFLUX

flux (nanomaggies)

7
6
5
4
3
2
1
0

u

g

r
band

i

z

Figure 1: Left: example of a BOSS-measured quasar SED with SDSS band filters, Sb (?), b ?
{u, g, r, i, z}, overlaid. Right: the same quasar?s photometrically measured band fluxes. Spectroscopic measurements include noisy samples at thousands of wavelengths, whereas SDSS photometric fluxes reflect the (weighted) response over a large range of wavelengths.

average over large swaths of the energy spectrum and report a low dimensional summary (photometry). Spectroscopic data describe a source?s SED in finer detail than broadband photometric
data. For example, the Baryonic Oscillation Spectroscopic Survey [5] measures SED samples at
over four thousand wavelengths between 3,500 and 10,500 ?. In contrast, the Sloan Digital Sky
Survey (SDSS) [1] collects spectral information in only 5 broad spectral bins by using broadband
filters (called u, g, r, i, and z), but at a much higher spatial resolution. Photometric preprocessing
models can then aggregate pixel information into five band-specific fluxes and their uncertainties
[17], reflecting the weighted average response over a large range of the wavelength spectrum. The
two methods of spectral information collection are graphically compared in Figure 1.
Despite carrying less spectral information, broadband photometry is more widely available and exists for a larger number of sources than spectroscopic measurements. This work develops a method
for inferring physical properties sources by jointly modeling spectroscopic and photometric data.
One use of our model is to measure the redshift of quasars for which we only have photometric observations. Redshift is a phenomenon in which the observed SED of a source of light is stretched toward longer (redder) wavelengths. This effect is due to a combination of radial velocity with respect
to the observer and the expansion of the universe (termed cosmological redshift) [8, 7]. Quasars, or
quasi-stellar radio sources, are extremely distant and energetic sources of electromagnetic radiation
that can exhibit high redshift [16]. Accurate estimates and uncertainties of redshift measurements
from photometry have the potential to guide the use of higher spectral resolution instruments to study
sources of interest. Furthermore, accurate photometric models can aid the automation of identifying
source types and estimating physical characteristics of faintly observed sources in large photometric
surveys [14].
To jointly describe both resolutions of data, we directly model a quasar?s latent SED and the process
by which it generates spectroscopic and photometric observations. Representing a quasar?s SED as
a latent random measure, we describe a Bayesian inference procedure to compute the marginal probability distribution of a quasar?s redshift given observed photometric fluxes and their uncertainties.
The following section provides relevant application and statistical background. Section 3 describes
our probabilistic model of SEDs and broadband photometric measurements. Section 4 outlines
our MCMC-based inference method for efficiently computing statistics of the posterior distribution. Section 5 presents redshift and SED predictions from photometric measurements, among other
model summaries, and a quantitative comparison between our method and two existing ?photo-z?.
We conclude with a discussion of directions for future work.

2

Background

The SEDs of most stars are roughly approximated by Planck?s law for black body radiators and
stellar atmosphere models [6]. Quasars, on the other hand, have complicated SEDs characterized by
some salient features, such as the Lyman-? forest, which is the absorption of light at many wavelengths from neutral hydrogen gas between the earth and the quasar [19]. One of the most interesting
properties of quasars (and galaxies) conveyed by the SED is redshift, which gives us insight into an
object?s distance and age. Redshift affects our observation of SEDs by ?stretching? the wavelengths,
? ? ?, of the quasar?s rest frame SED, skewing toward longer (redder) wavelengths. Denoting the
(rest)
rest frame SED of a quasar n as a function, fn
: ? ? R+ , the effect of redshift with value zn
2

Figure 2: Spectroscopic measurements of multiple quasars at different redshifts, z. The upper graph
depicts the sample spectrograph in the observation frame, intuitively thought of as ?stretched? by a
factor (1 + z). The lower figure depicts the ?de-redshifted? (rest frame) version of the same quasar
spectra, The two lines show the corresponding locations of the characteristic peak in each reference
frame. Note that the x-axis has been changed to ease the visualization - the transformation is much
more dramatic. The appearance of translation is due to missing data; we don?t observe SED samples
outside the range 3,500-10,500 ?.
(typically between 0 and 7) on the observation-frame SED is described by the relationship


?
fn(obs) (?) = fn(rest)
.
1 + zn

(1)

Some observed quasar spectra and their ?de-redshifted? rest frame spectra are depicted in Figure 2.

3

Model

This section describes our probabilistic model of spectroscopic and photometric observations.
Spectroscopic flux model The SED of a quasar is a non-negative function f : ? ? R+ , where ?
denotes the range of wavelengths and R+ are non-negative real numbers representing flux density.
Our model specifies a quasar?s rest frame SED as a latent random function. Quasar SEDs are highly
structured, and we model this structure by imposing the assumption that each SED is a convex
mixture of K latent, positive basis functions. The model assumes there are a small number (K) of
latent features or characteristics and that each quasar can be described by a short vector of mixing
weights over these features.
We place a normalized log-Gaussian process prior on each of these basis functions (described in
supplementary material). The generative procedure for quasar spectra begins with a shared basis
iid

?k (?) ? GP(0, K? ), k = 1, . . . , K,

Bk (?) = R

exp(?k (?))
,
exp(?k (?)) d?
?

(2)

where K? is the kernel and Bk is the exponentiated and normalized version of ?k . For each quasar n,
X
wn ? p(w) , s.t.
wk = 1,
mn ? p(m) , s.t. mn > 0,
zn ? p(z),
(3)
wk

where wn mixes over the latent types, mn is the apparent brightness, zn is the quasar?s redshift,
and distributions p(w), p(m), and p(z) are priors to be specified later. As each positive SED basis
function, Bk , is normalized to integrate to one, and each quasar?s weight vector wn also sums to
one, the latent normalized SED is then constructed as
X
fn(rest) (?) =
wn,k Bk (?)
(4)
k
(rest)
f?n (?)

(rest)

and we define the unnormalized SED
? mn ? fn (?). This parameterization admits the
(rest)
interpretation of fn (?) as a probability density scaled by mn . This interpretation allows us to
3

`, ?

Figure 3: Graphical model representation
of the joint photometry and spectroscopy
model. The left shaded variables represent
spectroscopically measured samples and
their variances. The right shaded variables
represent photometrically measured fluxes
and their variances. The upper box represents the latent basis, with GP prior parameters ` and ?. Note that Nspec + Nphoto
replicates of wn , mn and zn are instantiated.

Bk
K

xn,?

wn

yn,b

mn
2
?n,b

2
?n,?

zn

???
Nspec

b ? {u, g, r, i, z}
Nphoto

separate out the apparent brightness, which is a function of distance and overall luminosity, from the
SED itself, which carries information pertinent to the estimand of interest, redshift.
For each quasar with spectroscopic data, we observe noisy samples of the redshifted and scaled spectral energy distribution at a grid of P wavelengths ? ? {?1 , . . . , ?P }. For quasar n, our observation
frame samples are conditionally distributed as




?
ind
2
xn,? |zn , wn , {Bk } ? N f?n(rest)
, ?n,?
(5)
1 + zn
2
where ?n,?
is known measurement variance from the instruments used to make the observations.

The BOSS spectra (and our rest frame basis) are stored in units 10?17 ? erg ? cm?2 ? s?1 ? ?

?1

.

Photometric flux model Photometric data summarize the amount of energy observed over a
large swath of the wavelength spectrum. Roughly, a photometric flux measures (proportionally) the
number of photons recorded by the instrument over the duration of an exposure, filtered by a bandspecific sensitivity curve. We express flux in nanomaggies [15]. Photometric fluxes and measurement error derived from broadband imagery have been computed directly from pixels [17]. For each
quasar n, SDSS photometric data are measured in five bands, b ? {u, g, r, i, z}, yielding a vector of
2
five flux values and their variances, yn and ?n,b
. Each band, b, measures photon observations at each
wavelength in proportion to a known filter sensitivity, Sb (?). The filter sensitivities for the SDSS
ugriz bands are depicted in Figure 1, with an example observation frame quasar SED overlaid. The
(obs)
actual measured fluxes can be computed by integrating the full object?s spectrum, mn ? fn (?)
against the filters. For a band b ? {u, g, r, i, z}
Z
?b (fn(rest) , zn ) = fn(obs) (?) Sb (?) C(?) d? ,
(6)
where C(?) is a conversion factor to go from the units of fn (?) to nanomaggies (details of this
conversion are available in the supplementary material). The function ?b takes in a rest frame SED,
a redshift (z) and maps it to the observed b-band specific flux. The results of this projection onto
SDSS bands are modeled as independent Gaussian random variables with known variance
ind

2
yn,b | fn(rest) , zn ? N (?b (fn(rest) , zn ), ?n,b
).

(7)

(rest)

Conditioned on the basis, B = {Bk }, we can represent fn
with a low-dimensional vector. Note
(rest)
that fn
is a function of wn , zn , mn , and B (see Equation 4), so we can think of ?b as a function
of wn , zn , mn , and B. We overload notation, and re-write the conditional likelihood of photometric
observations as
2
yn,b | wn , zn , mn , B ? N (?b (wn , zn , mn , B), ?n,b
).

(8)

Intuitively, what gives us statistical traction in inferring the posterior distribution over zn is the structure learned in the latent basis, B, and weights w, i.e., the features that correspond to distinguishing
bumps and dips in the SED.
Note on priors For photometric weight and redshift inference, we use a flat prior on zn ? [0, 8],
and empirically derived priors for mn and wn , from the sample of spectroscopically measured
sources. Choice of priors is described in the supplementary material.
4

4

Inference

Basis estimation For computational tractability, we first compute a maximum a posteriori (MAP)
2
estimate of the basis, Bmap to condition on. Using the spectroscopic data, {xn,? , ?n,?
, zn }, we compute a discretized MAP estimate of {Bk } by directly optimizing the unnormalized (log) posterior
implied by the likelihood in Equation 5, the GP prior over B, and diffuse priors over wn and mn ,
N
 Y
2
p {wn , mn }, {Bk }|{xn,? , ?n,?
, zn } ?
p(xn,? |zn , wn , mn , {Bk })p({Bk })p(wn )p(mn ) .
n=1

(9)
We use gradient descent with momentum and LBFGS [12] directly on the parameters ?k , ?n,k , and
log(mn ) for the Nspec spectroscopically measured quasars. Gradients were automatically computed
using autograd [9]. Following [18], we first resample the observed spectra into a common rest
frame grid, ?0 = (?0,1 , . . . , ?0,V ), easing computation of the likelihood. We note that although our
model places a full distribution over Bk , efficiently integrating out those parameters is left for future
work.
Sampling wn , mn , and zn The Bayesian ?photo-z? task requires that we compute posterior
marginal distributions of z, integrating out w, and m. To compute these distributions, we construct a Markov chain over the state space including z, w, and m that leaves the target posterior
distribution invariant. We treat the inference problem for each photometrically measured quasar,
yn , independently. Conditioned on a basis Bk , k = 1, . . . , K, our goal is to draw posterior samples
of wn , mn and zn for each n. The unnormalized posterior can be expressed
p(wn , mn , zn |yn , B) ? p(yn |wn , mn , zn , B)p(wn , mn , zn )

(10)

where the left likelihood term is defined in Equation 8. Note that due to analytic intractability, we
R (obs)
numerically integrate expressions involving ? fn (?)d? and Sb (?). Because the observation yn
can often be well explained by various redshifts and weight settings, the resulting marginal posterior, p(zn |X, yn , B), is often multi-modal, with regions of near zero probability between modes.
Intuitively, this is due to the information loss in the SED-to-photometric flux integration step.
This multi-modal property is problematic for many standard MCMC techniques. Single chain
MCMC methods have to jump between modes or travel through a region of near-zero probability, resulting in slow mixing. To combat this effect, we use parallel tempering [4], a method that is
well-suited to constructing Markov chains on multi-modal distributions. Parallel tempering instantiates C independent chains, each sampling from the target distribution raised to an inverse temperature. Given a target distribution, ?(x), the constructed chains sample ?c (x) ? ?(x)1/Tc , where Tc
controls how ?hot? (i.e., how close to uniform) each chain is. At each iteration, swaps between
chains are proposed and accepted with a standard Metropolis-Hastings acceptance probability
Pr(accept swap c, c0 ) =

?c (xc0 )?c0 (xc )
.
?c (xc )?c0 (xc0 )

(11)

Within each chain, we use component-wise slice sampling [11] to generate samples that leave each
chain?s distribution invariant. Slice-sampling is a (relatively) tuning-free MCMC method, a convenient property when sampling from thousands of independent posteriors. We found parallel tempering to be essential for convincing posterior simulations. MCMC diagnostics and comparisons to
single-chain samplers are available in the supplemental material.

5

Experiments and Results

We conduct three experiments to test our model, where each experiment measures redshift predictive
accuracy for a different train/test split of spectroscopically measured quasars from the DR10QSO
dataset [13] with confirmed redshifts in the range z ? (.01, 5.85). Our experiments split train/test
in the following ways: (i) randomly, (ii) by r-band fluxes, (iii) by redshift values. In split (ii), we
train on the brightest 90% of quasars, and test on a subset of the remaining. Split (iii) takes the
lowest 85% of quasars as training data, and a subset of the brightest 15% as test cases. Splits (ii)
5

Figure 4: Top: MAP estimate of the
latent bases B = {Bk }K
k=1 . Note the
different ranges of the x-axis (wavelength). Each basis function distributes
its mass across different regions of the
spectrum to explain different salient
features of quasar spectra in the rest
frame. Bottom: model reconstruction
of a training-sample SED.

and (iii) are intended to test the method?s robustness to different training and testing distributions,
mimicking the discovery of fainter and farther sources. For each split, we find a MAP estimate of the
basis, B1 , . . . , BK , and weights, wn to use as a prior for photometric inference. For computational
purposes, we limit our training sample to a random subsample of 2,000 quasars. The following
sections outline the resulting model fit and inferred SEDs and redshifts.
Basis validation We examined multiple choices of K using out of sample likelihood on a validation set. In the following experiments we set K = 4, which balances generalizability and computational tradeoffs. Discussion of this validation is provided in the supplementary material.
SED Basis We depict a MAP estimate of B1 , . . . , BK in Figure 4. Our basis decomposition
enjoys the benefit of physical interpretability due to our density-estimate formulation of the problem.
Basis B4 places mass on the Lyman-? peak around 1,216 ?, allowing the model to capture the cooccurrence of more peaked SEDs with a bump around 1,550 ?. Basis B1 captures the H-? emission
line at around 6,500 ?. Because of the flexible nonparametric priors on Bk our model is able to
automatically learn these features from data. The positivity of the basis and weights distinguishes
our model from PCA-based methods, which sacrifice physical interpretability.
Photometric measurements For each test quasar, we construct an 8-chain parallel tempering sampler and run for 8,000 iterations, and discard the first 4,000 samples as burn-in. Given posterior samples of zn , we take the posterior mean as a point estimate. Figure 5 compares the posterior mean to
spectroscopic measurements (for three different data-split experiments), where the gray lines denote
posterior sample quantiles. In general there is a strong correspondence between spectroscopically
measured redshift and our posterior estimate. In cases where the posterior mean is off, our distribution often covers the spectroscopically confirmed value with probability mass. This is clear upon
inspection of posterior marginal distributions that exhibit extreme multi-modal behavior. To combat
this multi-modality, it is necessary to inject the model with more information to eliminate plausible
hypotheses; this information could come from another measurement (e.g., a new photometric band),
or from structured prior knowledge over the relationship between zn , wn , and mn . Our method
simply fits a mixture of Gaussians to the spectroscopically measured wn , mn sample to formulate
a prior distribution. However, incorporating dependencies between zn , wn and mn , similar to the
XDQSOz technique, will be incorporated in future work.
5.1

Comparisons

We compare the performance of our redshift estimator with two recent photometric redshift estimators, XDQSOz [2] and a neural network [3]. The method in [2] is a conditional density estimator
that discretizes the range of one flux band (the i-band) and fits a mixture of Gaussians to the joint
distribution over the remaining fluxes and redshifts. One disadvantage to this approach is there there
6

Figure 5: Comparison of spectroscopically (x-axis) and photometrically (y-axis) measured redshifts
from the SED model for three different data splits. The left reflects a random selection of 4,000
quasars from the DR10QSO dataset. The right graph reflects a selection of 4,000 test quasars from
the upper 15% (zcutof f ? 2.7), where all training was done on lower redshifts. The red estimates
are posterior means.

Figure 6: Left: inferred SEDs from photometric data. The black line is a smoothed approximation to
the ?true? SED using information from the full spectral data. The red line is a sample from the pos(obs)
terior, fn (?)|X, yn , B, which imputes the entire SED from only five flux measurements. Note
that the bottom sample is from the left mode, which under-predicts redshift. Right: corresponding posterior predictive distributions, p(zn |X, yn , B). The black line marks the spectroscopically
confirmed redshift; the red line marks the posterior mean. Note the difference in scale of the x-axis.
is no physical significance to the mixture of Gaussians, and no model of the latent SED. Furthermore, the original method trains and tests the model on a pre-specified range of i-magnitudes, which
is problematic when predicting redshifts on much brighter or dimmer stars. The regression approach
from [3] employs a neural network with two hidden layers, and the SDSS fluxes as inputs. More
features (e.g., more photometric bands) can be incorporated into all models, but we limit our experiments to the five SDSS bands for the sake of comparison. Further detail on these two methods and
a broader review of ?photo-z? approaches are available in the supplementary material.
Average error and test distribution We compute mean absolute error (MAE), mean absolute
percentage error (MAPE), and root mean square error (RMSE) to measure predictive performance.
Table 1 compares prediction errors for the three different approaches (XD, NN, Spec). Our experiments show that accurate redshift measurements are attainable even when the distribution of
training set is different from test set by directly modeling the SED itself. Our method dramatically
outperforms [2] and [3] in split (iii), particularly for very high redshift fluxes. We also note that
our training set is derived from only 2,000 examples, whereas the training set for XDQSOz and the
neural network were ? 80,000 quasars and 50,000 quasars, respectively. This shortcoming can be
overcome with more sophisticated inference techniques for the non-negative basis. Despite this, the
7

split
random (all)
flux (all)
redshift (all)
random (z > 2.35)
flux (z > 2.33)
redshift (z > 3.20)
random (z > 3.11)
flux (z > 2.86)
redshift (z > 3.80)

XD
0.359
0.308
0.841
0.247
0.292
1.327
0.171
0.373
2.389

MAE
NN
0.773
0.483
0.736
0.530
0.399
1.149
0.418
0.493
2.348

Spec
0.485
0.497
0.619
0.255
0.326
0.806
0.289
0.334
0.829

XD
0.293
0.188
0.237
0.091
0.108
0.357
0.050
0.112
0.582

MAPE
NN
0.533
0.283
0.214
0.183
0.143
0.317
0.117
0.144
0.569

Spec
0.430
0.339
0.183
0.092
0.124
0.226
0.082
0.103
0.198

XD
0.519
0.461
1.189
0.347
0.421
1.623
0.278
0.606
2.504

RMSE
NN
0.974
0.660
0.923
0.673
0.550
1.306
0.540
0.693
2.405

Spec
0.808
0.886
0.831
0.421
0.531
0.997
0.529
0.643
1.108

Table 1: Prediction error for three train-test splits, (i) random, (ii) flux-based, (iii) redshift-based,
corresponding to XDQSOz [2] (XD), the neural network approach [3] (NN), our SED-based model
(Spec). The middle and lowest sections correspond to test redshifts in the upper 50% and 10%,
respectively. The XDQSOz and NN models were trained on (roughly) 80,000 and 50,000 example
quasars, respectively, while the Spec models were trained on 2,000.
SED-based predictions are comparable. Additionally, because we are directly modeling the latent
SED, our method admits a posterior estimate of the entire SED. Figure 6 displays posterior SED
samples and their corresponding redshift marginals for test-set quasars inferred from only SDSS
photometric measurements.

6

Discussion

We have presented a generative model of two sources of information at very different spectral resolutions to form an estimate of the latent spectral energy distribution of quasars. We also described
an efficient MCMC-based inference algorithm for computing posterior statistics given photometric
observations. Our model accurately predicts and characterizes uncertainty about redshifts from only
photometric observations and a small number of separate spectroscopic examples. Moreover, we
showed that we can make reasonable estimates of the unobserved SED itself, from which we can
make inferences about other physical properties informed by the full SED.
We see multiple avenues of future work. Firstly, we can extend the model of SEDs to incorporate
more expert knowledge. One such augmentation would include a fixed collection of features, curated by an expert, corresponding to physical properties already known about a class of sources.
Furthermore, we can also extend our model to directly incorporate photometric pixel observations,
as opposed to preprocessed flux measurements. Secondly, we note that our method is more more
computationally burdensome than XDQSOz and the neural network approach. Another avenue of
future work is to find accurate approximations of these posterior distributions that are cheaper to
compute. Lastly, we can extend our methodology to galaxies, whose SEDs can be quite complicated. Galaxy observations have spatial extent, complicating their SEDs. The combination of SED
and spatial appearance modeling and computationally efficient inference procedures is a promising
route toward the automatic characterization of millions of sources from the enormous amounts of
data available in massive photometric surveys.

Acknowledgments
The authors would like to thank Matthew Hoffman and members of the HIPS lab for helpful discussions. This work is supported by the Applied Mathematics Program within the Office of Science
Advanced Scientific Computing Research of the U.S. Department of Energy under contract No.
DE-AC02-05CH11231. This work used resources of the National Energy Research Scientific Computing Center (NERSC). We would like to thank Tina Butler, Tina Declerck and Yushu Yao for their
assistance.

References
[1] Shadab Alam, Franco D Albareti, Carlos Allende Prieto, F Anders, Scott F Anderson, Brett H
Andrews, Eric Armengaud, ?ric Aubourg, Stephen Bailey, Julian E Bautista, et al. The
8

eleventh and twelfth data releases of the Sloan digital sky survey: Final data from SDSS-III.
arXiv preprint arXiv:1501.00963, 2015.
[2] Jo Bovy, Adam D Myers, Joseph F Hennawi, David W Hogg, Richard G McMahon, David
Schiminovich, Erin S Sheldon, Jon Brinkmann, Donald P Schneider, and Benjamin A Weaver.
Photometric redshifts and quasar probabilities from a single, data-driven generative model. The
Astrophysical Journal, 749(1):41, 2012.
[3] M Brescia, S Cavuoti, R D?Abrusco, G Longo, and A Mercurio. Photometric redshifts for
quasars in multi-band surveys. The Astrophysical Journal, 772(2):140, 2013.
[4] Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of Markov Chain
Monte Carlo. CRC press, 2011.
[5] Kyle S Dawson, David J Schlegel, Christopher P Ahn, Scott F Anderson, ?ric Aubourg,
Stephen Bailey, Robert H Barkhouser, Julian E Bautista, Alessandra Beifiori, Andreas A
Berlind, et al. The baryon oscillation spectroscopic survey of SDSS-III. The Astronomical
Journal, 145(1):10, 2013.
[6] RO Gray, PW Graham, and SR Hoyt. The physical basis of luminosity classification in the late
a-, f-, and early g-type stars. ii. basic parameters of program stars and the role of microturbulence. The Astronomical Journal, 121(4):2159, 2001.
[7] Edward Harrison. The redshift-distance and velocity-distance laws. The Astrophysical Journal,
403:28?31, 1993.
[8] David W Hogg. Distance measures in cosmology. arXiv preprint astro-ph/9905116, 1999.
[9] Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Autograd: Reverse-mode differentiation of native python. ICML workshop on Automatic Machine Learning, 2015.
[10] D Christopher Martin, James Fanson, David Schiminovich, Patrick Morrissey, Peter G Friedman, Tom A Barlow, Tim Conrow, Robert Grange, Patrick N Jelinksy, Bruno Millard, et al.
The galaxy evolution explorer: A space ultraviolet survey mission. The Astrophysical Journal
Letters, 619(1), 2005.
[11] Radford M Neal. Slice sampling. Annals of statistics, pages 705?741, 2003.
[12] Jorge Nocedal. Updating quasi-newton matrices with limited storage. Mathematics of computation, 35(151):773?782, 1980.
[13] Isabelle P?ris, Patrick Petitjean, ?ric Aubourg, Nicholas P Ross, Adam D Myers, Alina
Streblyanska, Stephen Bailey, Patrick B Hall, Michael A Strauss, Scott F Anderson, et al.
The Sloan digital sky survey quasar catalog: tenth data release. Astronomy & Astrophysics,
563:A54, 2014.
[14] Jeffrey Regier, Andrew Miller, Jon McAuliffe, Ryan Adams, Matt Hoffman, Dustin Lang,
David Schlegel, and Prabhat. Celeste: Variational inference for a generative model of astronomical images. In Proceedings of The 32nd International Conference on Machine Learning,
2015.
[15] SDSSIII. Measures of flux and magnitude. 2013. https://www.sdss3.org/dr8/
algorithms/magnitudes.php.
[16] Joseph Silk and Martin J Rees. Quasars and galaxy formation. Astronomy and Astrophysics,
1998.
[17] Chris Stoughton, Robert H Lupton, Mariangela Bernardi, Michael R Blanton, Scott Burles,
Francisco J Castander, AJ Connolly, Daniel J Eisenstein, Joshua A Frieman, GS Hennessy,
et al. Sloan digital sky survey: early data release. The Astronomical Journal, 123(1):485,
2002.
[18] Jakob Walcher, Brent Groves, Tam?s Budav?ri, and Daniel Dale. Fitting the integrated spectral
energy distributions of galaxies. Astrophysics and Space Science, 331(1):1?51, 2011.
[19] David H Weinberg, Romeel Dav?e, Neal Katz, and Juna A Kollmeier. The Lyman-alpha forest
as a cosmological tool. Proceedings of the 13th Annual Astrophysica Conference in Maryland,
666, 2003.

9

"
5305,"Planar Ultrametrics for Image Segmentation

Charless C. Fowlkes
Department of Computer Science
University of California Irvine
fowlkes@ics.uci.edu

Julian Yarkony
Experian Data Lab
San Diego, CA 92130
julian.yarkony@experian.com

Abstract
We study the problem of hierarchical clustering on planar graphs. We formulate
this in terms of finding the closest ultrametric to a specified set of distances and
solve it using an LP relaxation that leverages minimum cost perfect matching as
a subroutine to efficiently explore the space of planar partitions. We apply our
algorithm to the problem of hierarchical image segmentation.

1

Introduction

We formulate hierarchical image segmentation from the perspective of estimating an ultrametric
distance over the set of image pixels that agrees closely with an input set of noisy pairwise distances.
An ultrametric space replaces the usual triangle inequality with the ultrametric inequality d(u, v) ?
max{d(u, w), d(v, w)} which captures the transitive property of clustering (if u and w are in the
same cluster and v and w are in the same cluster, then u and v must also be in the same cluster).
Thresholding an ultrametric immediately yields a partition into sets whose diameter is less than
the given threshold. Varying this distance threshold naturally produces a hierarchical clustering in
which clusters at high thresholds are composed of clusters at lower thresholds.
Inspired by the approach of [1], our method represents an ultrametric explicitly as a hierarchical
collection of segmentations. Determining the appropriate segmentation at a single distance threshold
is equivalent to finding a minimum-weight multicut in a graph with both positive and negative edge
weights [3, 14, 2, 11, 20, 21, 4, 19, 7]. Finding an ultrametric imposes the additional constraint that
these multicuts are hierarchically consistent across different thresholds. We focus on the case where
the input distances are specified by a planar graph. This arises naturally in the domain of image
segmentation where elements are pixels or superpixels and distances are defined between neighbors
and allows us to exploit fast combinatorial algorithms for partitioning planar graphs that yield tighter
LP relaxations than the local polytope relaxation often used in graphical inference [20].
The paper is organized as follows. We first introduce the closest ultrametric problem and the relation between multicuts and ultrametrics. We then describe an LP relaxation that uses a delayed
column generation approach and exploits planarity to efficiently find cuts via the classic reduction
to minimum-weight perfect matching [13, 8, 9, 10]. We apply our algorithm to the task of natural
image segmentation and demonstrate that our algorithm converges rapidly and produces optimal or
near-optimal solutions in practice.

2

Closest Ultrametric and Multicuts

Let G = (V, E) be a weighted graph with non-negative edge weights ? indexed by edges e =
(u, v) ? E. Our goal is to find an ultrametric distance d(u,v) over vertices of the graph that is
P
close to ? in the sense that the distortion (u,v)?E k?(u,v) ? d(u,v) k22 is minimized. We begin by
reformulating this closest ultrametric problem in terms of finding a set of nested multicuts in a family
of weighted graphs.
1

We specify a partitioning or multicut of the vertices of the graph G into components using a binary
? ? {0, 1}|E| where X
? e = 1 indicates that the edge e = (u, v) is ?cut? and that the vertices
vector X
u and v associated with the edge are in separate components of the partition. We use MCUT(G)
? that represent valid multicuts of the graph G. For
to denote the set of binary indicator vectors X
notational simplicity, in the remainder of the paper we frequently omit the dependence on G which
is given as a fixed input.
? to define a valid multicut in G is that
A necessary and sufficient condition for an indicator vector X
for every cycle of edges, if one edge on the cycle is cut then at least one other edge in the cycle must
also be cut. Let C denote the set of all cycles in G where each cycle c ? C is a set of edges and
c ? e? is the set of edges in cycle c excluding edge e?. We can express MCUT in terms of these cycle
inequalities as:
(
)
X
|E|
?
?
?
Xe ? Xe?, ?c ? C, e? ? c
(1)
MCUT = X ? {0, 1} :
e?c??
e

A hierarchical clustering of a graph can be described by a nested collection of multicuts. We denote
? L which we represent by a set of L
the space of valid hierarchical partitions with L layers by ?
1 ?2 ?3
L
?
?
edge-indicator vectors X = (X , X , X , . . . , X ) in which any cut edge remains cut at all finer
layers of the hierarchy.
? L = {(X
? 1, X
? 2, . . . X
? L) : X
? l ? MCUT, X
?l ? X
? l+1 ?l}
?
(2)
Given a valid hierarchical clustering X , an ultrametric d can be specified over the vertices of the
graph by choosing a sequence of real values 0 = ? 0 < ? 1 < ? 2 < . . . < ? L that indicate a distance
threshold associated with each level l of the hierarchical clustering. The ultrametric distance d
specified by the pair (X , ?) assigns a distance to each pair of vertices d(u,v) based on the coarsest
level of the clustering at which they remain in separate clusters. For pairs corresponding to an edge
in the graph (u, v) = e ? E we can write this explicitly in terms of the multicut indicator vectors
as:
L
X
? el =
? el > X
? el+1 ]
de =
max ? l X
? l [X
(3)
l?{0,1,...,L}

l=0

? eL+1 = 0. Pairs (u, v) that do not correspond to an
? e0 = 1 and X
We assume by convention that X
edge in the original graph can still be assigned a unique distance based on the coarsest level l at
which they lie in different connected components of the cut specified by X l .
To compute the quality of an ultrametric d with respect to an input set of edge weights ?, we measure
the squared L2 difference between the edge weights and the ultrametric distance k? ? dk22 . To write
this compactly in terms of multicut
Pm indicator vectors, we construct a set of weights for each edge
and layer, denoted ?el so that l=0 ?el = k?e ? ? m k2 . These weights are given explicitly by the
telescoping series:
?e0 = k?e k2
l

We use ? ? R

|E|

?el = k?e ? ? l k2 ? k?e ? ? l?1 k2

to denote the vector containing

?el

?l > 1

(4)

for all e ? E.

For a fixed number of levels L and fixed set of thresholds ?, the problem of finding the closest
ultrametric d can then be written as an integer linear program (ILP) over the edge cut indicators.


2
L
L

X
X
XX

l ?l
l+1 
?
? el ? X
? el+1 )
min
? [Xe > Xe ]
 = min
k?e ? ? l k2 (X
(5)

?e ?
?L
?L


X ??
X ??
e?E
e?E l=0
l=0
!
L
X
X

2 ?0
l 2
l?1 2 ? l
L 2 ? L+1
k?e ? ? k ? k?e ? ? k X + k?e ? ? k X
= min
k?e k X +
?L
X ??

= min

?L
X ??

e

e?E
L X
X
l=0 e?E

e

e

l=1

? el = min
?el X

?L
X ??

L
X

?l
?l ? X

(6)

l=0

This optimization corresponds to solving a collection of minimum-weight multicut problems where
the multicuts are constrained to be hierarchically consistent.
2

(a) Linear combination of cut vectors

(b) Hierarchical cuts

Figure 1: (a) Any partitioning X can be represented as a linear superposition of cuts Z where
each cut isolates a connected component of the partition and is assigned a weight ? = 12 [20]. By
introducing an auxiliary slack variables ?, we are able to represent a larger set of valid indicator
vectors X using fewer columns of Z. (b) By introducing additional slack variables at each layer of
the hierarchical segmentation, we can efficiently represent many hierarchical segmentations (here
{X 1 , X 2 , X 3 }) that are consistent from layer to layer while using only a small number of cut indicators as columns of Z.
Computing minimum-weight multicuts (also known as correlation clustering) is NP hard even in the
case of planar graphs [6]. A direct approach to finding an approximate solution to Eq 6 is to relax
? l and instead optimize over the whole polytope defined by the set of
the integrality constraints on X
? L . While the resulting
cycle inequalities. We use ?L to denote the corresponding relaxation of ?
polytope is not the convex hull of MCUT, the integral vertices do correspond exactly to the set of
valid multicuts [12].
In practice, we found that applying a straightforward cutting-plane approach that successively adds
violated cycle inequalities to this relaxation of Eq 6 requires far too many constraints and is too
slow to be useful. Instead, we develop a column generation approach tailored for planar graphs that
allows for efficient and accurate approximate inference.

3

The Cut Cone and Planar Multicuts

Consider a partition of a planar graph into two disjoint sets of nodes. We denote the space of
indicator vectors corresponding to such two-way cuts by CUT. A cut may yield more than two
connected components but it can not produce every possible multicut (e.g., it can not split a triangle
of three nodes into three separate components). Let Z ? {0, 1}|E|?|CUT| be an indicator matrix
where each column specifies a valid two-way cut with Zek = 1 if and only if edge e is cut in twoway cut k. The indicator vector of any multicut in a planar graph can be generated by a suitable
linear combination of of cuts (columns of Z) that isolate the individual components from the rest of
the graph where the weight of each such cut is 12 .
Let ? ? R|CUT| be a vector specifying a positive weighted combination of cuts. The set CUT4 =
{Z? : ? ? 0} is the conic hull of CUT or ?cut cone?. Since any multicut can be expressed as a
superposition of cuts, the cut cone is identical to the conic hull of MCUT. This equivalence suggests
an LP relaxation of the minimum-cost multicut given by
min ? ? Z?

s.t. Z? ? 1

??0

(7)

where the vector ? ? R|E| specifies the edge weights. For the case of planar graphs, any solution to
this LP relaxation satisfies the cycle inequalities (see supplement and [12, 18, 10]).
Expanded Multicut Objective: Since the matrix Z contains an exponential number of cuts, Eq. 7
is still intractable. Instead we consider an approximation using a constraint set Z? which is a subset
3

of columns of Z. In previous work [20], we showed that since the optimal multicut may no longer
? it is useful to allow some values of Z?
? exceed 1 (see
lie in the span of the reduced cut matrix Z,
Figure 1(a) for an example).
We introduce a slack vector ? ? 0 that tracks the presence of any ?overcut? edges and prevents
them from contributing to the objective when the corresponding edge weight is negative. Let ?e? =
min(?e , 0) denote the non-positive component of ?e . The expanded multi-cut objective is given by:
? ? ?? ? ?
min ? ? Z?

? ?? ?1
s.t. Z?

??0
??0

(8)

For any edge e such that ?e < 0, any decrease in the objective from overcutting by an amount ?e is
exactly compensated for in the objective by the term ??e? ?e .
When Z? contains all cuts (i.e., Z? = Z) then Eq 7 and Eq 8 are equivalent [20]. Further, if ? ? is the
minimizer of Eq 8 when Z? only contains a subset of columns, then the edge indicator vector given
? ? ) still satisfies the cycle inequalities (see supplement for details).
by X = min(1, Z?

4

Expanded LP for Finding the Closest Ultrametric

To develop an LP relaxation of the closest ultrametric problem, we replace the multicut problem at
each layer l with the expanded multicut objective described by Eq 8. We let ? = {? 1 , ? 2 , ? 3 . . . ? L }
and ? = {? 1 , ? 2 , ? 3 . . . ? L } denote the collection of weights and slacks for the levels of the hierarchy and let ?e+l = max(0, ?el ) and ?e?l = min(0, ?el ) denote the positive and negative components
of ?l .
To enforce hierarchical consistency between layers, we would like to add the constraint that
Z? l+1 ? Z? l . However, this constraint is too rigid when Z does not include all possible cuts.
It is thus computationally useful to introduce an additional slack vector associated with each level l
and edge e which we denote as ? = {?1 , ?2 , ?3 . . . ?L?1 }. The introduction of ?el allows for cuts
represented by Z? l to violate the hierarchical constraint. We modify the objective so that violations
to the original hierarchy constraint are paid for in proportion to ?e+l . The introduction of ? allows
us to find valid ultrametrics while using a smaller number of columns of Z to be used than would
otherwise be required (illustrated in Figure 1(b)).
We call this relaxed closest ultrametric problem including the slack variable ? the expanded closest
ultrametric objective, written as:
min

L
L
L?1
X
X
X
?l ? Z? l +
???l ? ? l +
?+l ? ?l

??0
??0 l=1
??0

l=1

s.t. Z? l+1 + ?l+1 ? Z? l + ?l
l

l

Z? ? ? ? 1

(9)

l=1

?l < L

?l

where by convention we define ?L = 0 and we have dropped the constant l = 0 term from Eq 6.
Given a solution (?, ?, ?) we can recover a relaxed solution to the closest ultrametric problem (Eq.
6) over ?L by setting Xel = min(1, maxm?l (Z? m )e ). In the supplement, we demonstrate that for
any (?, ?, ?) that obeys the constraints in Eq 9, this thresholding operation yields a solution X that
lies in ?L and achieves the same or lower objective value.

5

The Dual Objective

We optimize the dual of the objective in Eq 9 using an efficient column generation approach based
on perfect matching. We introduce two sets of Lagrange multipliers ? = {? 1 , ? 2 , ? 3 . . . ? L?1 } and
? = {?1 , ?2 , ?3 . . . ?L } corresponding to the between and within layer constraints respectively. For
4

Algorithm 1 Dual Closest Ultrametric via Cutting Planes
Z? l ? {} ?l, residual ? ??
while residual < 0 do
{?}, {?} ? Solve Eq 10 given Z?
residual = 0
for l = 1 : L do
z l ? arg minz?CUT (?l + ?l + ? l?1 ? ? l ) ? z
residual ? residual + 32 (?l + ?l + ? l?1 ? ? l ) ? z l
{z(1), z(2), . . . , z(M )} ? isocuts(z l )
Z? l ? Z? l ? {z(1), z(2), . . . , z(M )}
end for
end while
notational convenience, let ? 0 = 0. The dual objective can then be written as
max

L
X

??0,??0

l=1
?l

?

??l ? 1
? ??l

? (?

l?1

l

(10)
?l

? ? l ) ? ?+l

l

(? + ? + ?

l?1

l

?l

?? )?Z ?0

?l

The dual LP can be interpreted as finding a small modification of the original edge weights ?l so
that every possible two-way cut of each resulting graph at level l has non-negative weight. Observe
that the introduction of the two slack terms ? and ? in the primal problem (Eq 9) results in bounds
on the Lagrange multipliers ? and ? in the dual problem in Eq 10. In practice these dual constraints
turn out to be essential for efficient optimization and constitute the core contribution of this paper.

6

Solving the Dual via Cutting Planes

The chief complexity of the dual LP is contained in the constraints including Z which encodes
non-negativity of an exponential number of cuts of the graph represented by the columns of Z. To
circumvent the difficulty of explicitly enumerating the columns of Z, we employ a cutting plane
method that efficiently searches for additional violated constraints (columns of Z) which are then
successively added.
Let Z? denote the current working set of columns. Our dual optimization algorithm iterates over
? (2) find the most violated constraint of the
the following three steps: (1) Solve the dual LP with Z,
l
l
l?1
l
form (? + ? + ?
? ? ) ? Z ? 0 for layer l, (3) Append a column to the matrix Z? for each
such cut found. We terminate when no violated constraints exist or a computational budget has been
exceeded.
Finding Violated Constraints: Identifying columns to add to Z? is carried out for each layer l
separately. Finding the most violated constraint of the full problem corresponds to computing the
minimum-weight cut of a graph with edge weights ?l + ?l + ? l?1 ? ? l . If this cut has non-negative
weight then all the constraints are satisfied, otherwise we add the corresponding cut indicator vector
as an additional column of Z.
To generate a new constraint for layer l based on the current Lagrange multipliers, we solve
X
z l = arg min
(?el + ?le + ?el?1 ? ?el )ze
z?CUT

(11)

e?E

? z 1 , z 2 , . . . z L ].
and subsequently add the new constraints from all layers to our LP, Z? ? [Z,
Unlike the multicut problem, finding a (two-way) cut in a planar graph can be solved exactly by a
reduction to minimum-weight perfect matching. This is a classic result that, e.g. provides an exact
solution for the ground state of a 2D lattice Ising model without a ferromagnetic field [13, 8, 9, 10]
3
in O(N 2 log N ) time [15].
5

80

UB
LB

60

?2

Bound

Counts

10

40

?4

10

20

0

10

1

10

2

10

0
0.2

3

10

Time (sec)

0.4
0.6
0.8
Objective ratio (UCM / UM)

1

Figure 2: (a): The average convergence of the upper (blue) and lower-bounds (red) as a function
of running time. Values plotted are the gap between the bound and the best lower-bound computed
(at termination) for a given problem instance. This relative gap is averaged over problem instances
which have not yet converged at a given time point. We indicate the percentage of problem instances
that have yet to terminate using black bars marking [95, 85, 75, 65, .....5] percent. (b) Histogram of
the ratio of closest ultrametric objective values for our algorithm (UM) and the baseline clustering
produced by UCM. All ratios were less than 1 showing that in no instances did UM produce a worse
solution than UCM

Computing a lower bound: At a given iteration, prior to adding a newly generated set of constraints
P
we can compute the total residual constraint violation over all layers of hierarchy by ? = l (?l +
?l + ? l?1 ? ? l ) ? z l . In the supplement we demonstrate that the value of the dual objective plus
3
2 ? is a lower-bound on the relaxed closest ultrametric problem in Eq 9. Thus, as the costs of the
minimum-weight matchings approach zero from below, the objective of the reduced problem over
?L
Z? approaches an accurate lower-bound on optimization over ?
Expanding generated cut constraints: When a given cut z l produces more than two connected
components, we found it useful to add a constraint corresponding to each component, following the
approach of [20]. Let the number of connected components of z l be denoted M . For each of the
M components then we add one column to Z corresponding to the cut that isolates that connected
component from the rest. This allows more flexibility in representing the final optimum multicut as
superpositions of these components. In addition, we also found it useful in practice to maintain a
separate set of constraints Z? l for each layer l. Maintaining independent constraints Z? 1 , Z? 2 , . . . , Z? L
can result in a smaller overall LP.
Speeding convergence of ?: We found that adding an explicit penalty term to the objective that
encourages small values of ? speeds up convergence dramatically with no loss in solution quality.
In our experiments, this penalty is scaled by a parameter  = 10?4 which is chosen to be extremely
small in magnitude relative to the values of ? so that it only has an influence when no other ?forces?
are acting on a given term in ?.
Primal Decoding: Algorithm 1 gives a summary of the dual solver which produces a lower-bound
as well as a set of cuts described by the constraint matrices Z? l . The subroutine isocuts(z l ) computes
the set of cuts that isolate each connected component of z l . To generate a hierarchical clustering,
we solve the primal, Eq 9, using the reduced set Z? in order to recover a fractional solution Xel =
min(1, maxm?l (Z? m ? m )e ). We use an LP solver (IBM CPLEX) which provides this primal solution
?for free? when solving the dual in Alg. 1.
We round the fractional primal solution X to a discrete hierarchical clustering by thresholding:
? el ? [Xel > t]. We then repair (uncut) any cut edges that lie inside a connected component. In our
X
implementation we test a few discrete thresholds t ? {0, 0.2, 0.4, 0.6, 0.8} and take that threshold
? with the lowest cost. After each pass through the loop of Alg. 1 we compute these
that yields X
upper-bounds and retain the optimum solution observed thus far.
6

1

Precision

0.8

Maximum F?measure

UCM
UCM?L
UM

0.9

0.7
0.6
0.5
0.4
0

0.2

0.4

0.6

0.8

0.7
0.6
0.5

0.3 0
10

1

Recall

UM
UCM?L
UCM

0.4

1

10

2

10
Time (sec)

3

10

Figure 3: (a) Boundary detection performance of our closest ultrametric algorithm (UM) and the
baseline ultrametric contour maps algorithm with (UCM) and without (UCM-L) length weighting
[5] on BSDS. Black circles indicate thresholds used in the closest UM optimization. (b) Anytime
performance: F-measure on the BSDS benchmark as a function of run-time. UM, UCM with and
without length weighting achieve a maximum F-measure of 0.728, 0.726, and 0.718 respectively.

7

Experiments

We applied our algorithm to segmenting images from the Berkeley Segmentation Data set (BSDS)
[16]. We use superpixels generated by performing an oriented watershed transform on the output
of the global probability of boundary (gPb) edge detector [17] and construct a planar graph whose
vertices are superpixels with edges connecting neighbors in the image plane whose base distance ?
is derived from gP b.
Let gP be be the local estimate of boundary contrast given by averaging the gP b classifier output
over the boundary between a pair of neighboring superpixels.
We 
truncate extreme values to enforce


gP be
that gP be ? [, 1 ? ] with  = 0.001 and set ?e = log 1?gP be + log 1?
The additive offset

assures that ?e ? 0. In our experiments we use a fixed set of eleven distance threshold levels {?l }
chosen to uniformly span the useful range of threshold values [9.6, 12.6]. Finally, we weighted edges
proportionally to the length of the corresponding boundary in the image.
We performed dual cutting plane iterations until convergence or 2000 seconds had passed. Lowerbounds for the BSDS segmentations were on the order of ?103 or ?104 . We terminate when the
total residual is greater than ?2 ? 10?4 . All codes were written in MATLAB using the Blossom
V implementation of minimum-weight perfect matching [15] and the IBM ILOG CPLEX LP solver
with default options.
Baseline: We compare our results with the hierarchical clusterings produced by the Ultrametric
Contour Map (UCM) [5]. UCM performs agglomerative clustering of superpixels and assigns the
length-weighted averaged gP b value as the distance between each pair of merged regions. While
UCM was not explicitly designed to find the closest ultrametric, it provides a strong baseline for
hierarchical clustering. To compute the closest l-level ultrametric corresponding to the UCM clustering result, we solve the minimization in Eq. 6 while restricting each multicut to be the partition
at some level of the UCM hierarchy.
Convergence and Timing: Figure 2 shows the average behavior of convergence as a function of
runtime. We found the upper-bound given by the cost of the decoded integer solution and the lowerbound estimated by the dual LP are very close. The integrality gap is typically within 0.1% of the
lower-bound and never more than 1 %. Convergence of the dual is achieved quite rapidly; most
instances require less than 100 iterations to converge with roughly linear growth in the size of the
LP at each iteration as cutting planes are added. In Fig 2 we display a histogram, computed over test
image problem instances, of the cost of UCM solutions relative to those produced by closest ultrametric (UM) estimated by our method. A ratio of less than 1 indicates that our approach generated
a solution with a lower distortion ultrametric. In no problem instance did UCM outperform our UM
algorithm.
7

UM

MC

UM

MC

Figure 4: The proposed closest ultrametric (UM) enforces consistency across levels while performing independent multi-cut clustering (MC) at each threshold does not guarantee a hierarchical
segmentation (c.f. first image, columns 3 and 4). In the second image, hierarchical segmentation
(UM) better preserves semantic parts of the two birds while correctly merging the background regions.
Segmentation Quality: Figure 3 shows the segmentation benchmark accuracy of our closest ultrametric algorithm (denoted UM) along with the baseline ultrametric contour maps algorithm (UCM)
with and without length weighting [5]. In terms of segmentation accuracy, UM performs nearly identically to the state of the art UCM algorithm with some small gains in the high-precision regime. It
is worth noting that the BSDS benchmark does not provide strong penalties for small leaks between
two segments when the total number of boundary pixels involved is small. Our algorithm may find
strong application in domains where the local boundary signal is noisier (e.g., biological imaging)
or when under-segmentation is more heavily penalized.
While our cutting-plane approach is slower than agglomerative clustering, it is not necessary to wait
for convergence in order to produce high quality results. We found that while the upper and lower
bounds decrease as a function of time, the clustering performance as measured by precision-recall
is often nearly optimal after only ten seconds and remains stable. Figure 3 shows a plot of the
F-measure achieved by UM as a function of time.
Importance of enforcing hierarchical constraints: Although independently finding multicuts at
different thresholds often produces hierarchical clusterings, this is by no means guaranteed. We ran
Algorithm 1 while setting ?el = 0, allowing each layer to be solved independently. Fig 4 shows
examples where hierarchical constraints between layers improves segmentation quality relative to
independent clustering at each threshold.

8

Conclusion

We have introduced a new method for approximating the closest ultrametric on planar graphs that
is applicable to hierarchical image segmentation. Our contribution is a dual cutting plane approach
that exploits the introduction of novel slack terms that allow for representing a much larger space of
solutions with relatively few cutting planes. This yields an efficient algorithm that provides rigorous
bounds on the quality the resulting solution. We empirically observe that our algorithm rapidly
produces compelling image segmentations along with lower- and upper-bounds that are nearly tight
on the benchmark BSDS test data set.
Acknowledgements: JY acknowledges the support of Experian, CF acknowledges support of NSF
grants IIS-1253538 and DBI-1262547
8

References
[1] Nir Ailon and Moses Charikar. Fitting tree metrics: Hierarchical clustering and phylogeny. In
Foundations of Computer Science, 2005., pages 73?82, 2005.
[2] Bjoern Andres, Joerg H. Kappes, Thorsten Beier, Ullrich Kothe, and Fred A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proc. of ICCV, pages 2611?2618,
2011.
[3] Bjoern Andres, Thorben Kroger, Kevin L. Briggman, Winfried Denk, Natalya Korogod, Graham Knott, Ullrich Kothe, and Fred. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proc. of ECCV, 2012.
[4] Bjoern Andres, Julian Yarkony, B. S. Manjunath, Stephen Kirchhoff, Engin Turetken, Charless
Fowlkes, and Hanspeter Pfister. Segmenting planar superpixel adjacency graphs w.r.t. nonplanar superpixel affinity graphs. In Proc. of EMMCVPR, 2013.
[5] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and
hierarchical image segmentation. IEEE Trans. Pattern Anal. Mach. Intell., 33(5):898?916,
May 2011.
[6] Yoram Bachrach, Pushmeet Kohli, Vladimir Kolmogorov, and Morteza Zadimoghaddam. Optimal coalition structure generation in cooperative graph games. In Proc. of AAAI, 2013.
[7] Shai Bagon and Meirav Galun. Large scale correlation clustering. In CoRR, abs/1112.2903,
2011.
[8] F Barahona. On the computational complexity of ising spin glass models. Journal of Physics
A: Mathematical, Nuclear and General, 15(10):3241?3253, april 1982.
[9] F Barahona. On cuts and matchings in planar graphs. Mathematical Programming, 36(2):53?
68, november 1991.
[10] F Barahona and A Mahjoub. On the cut polytope. Mathematical Programming, 60(1-3):157?
173, September 1986.
[11] Thorsten Beier, Thorben Kroeger, Jorg H Kappes, Ullrich Kothe, and Fred A Hamprecht. Cut,
glue, and cut: A fast, approximate solver for multicut partitioning. In Computer Vision and
Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 73?80, 2014.
[12] Michel Deza and Monique Laurent. Geometry of cuts and metrics, volume 15. Springer
Science & Business Media, 1997.
[13] Michael Fisher. On the dimer solution of planar ising models. Journal of Mathematical
Physics, 7(10):1776?1781, 1966.
[14] Sungwoong Kim, Sebastian Nowozin, Pushmeet Kohli, and Chang Dong Yoo. Higher-order
correlation clustering for image segmentation. In Advances in Neural Information Processing
Systems,25, pages 1530?1538, 2011.
[15] Vladimir Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching
algorithm. Mathematical Programming Computation, 1(1):43?67, 2009.
[16] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring
ecological statistics. In Proc. of ICCV, pages 416?423, 2001.
[17] David Martin, Charless C. Fowlkes, and Jitendra Malik. Learning to detect natural image
boundaries using local brightness, color, and texture cues. IEEE Trans. Pattern Anal. Mach.
Intell., 26(5):530?549, May 2004.
[18] Julian Yarkony. Analyzing PlanarCC. NIPS 2014 workshop, 2014.
[19] Julian Yarkony, Thorsten Beier, Pierre Baldi, and Fred A Hamprecht. Parallel multicut segmentation via dual decomposition. In New Frontiers in Mining Complex Patterns, 2014.
[20] Julian Yarkony, Alexander Ihler, and Charless Fowlkes. Fast planar correlation clustering for
image segmentation. In Proc. of ECCV, 2012.
[21] Chong Zhang, Julian Yarkony, and Fred A. Hamprecht. Cell detection and segmentation using
correlation clustering. In MICCAI, volume 8673, pages 9?16, 2014.

9

"
4279,"Bayesian entropy estimation for binary spike train
data using parametric prior knowledge

Evan Archer13 , Il Memming Park123 , Jonathan W. Pillow123
1. Center for Perceptual Systems, 2. Dept. of Psychology,
3. Division of Statistics & Scientific Computation
The University of Texas at Austin
{memming@austin., earcher@, pillow@mail.} utexas.edu

Abstract
Shannon?s entropy is a basic quantity in information theory, and a fundamental
building block for the analysis of neural codes. Estimating the entropy of a discrete distribution from samples is an important and difficult problem that has received considerable attention in statistics and theoretical neuroscience. However,
neural responses have characteristic statistical structure that generic entropy estimators fail to exploit. For example, existing Bayesian entropy estimators make
the naive assumption that all spike words are equally likely a priori, which makes
for an inefficient allocation of prior probability mass in cases where spikes are
sparse. Here we develop Bayesian estimators for the entropy of binary spike trains
using priors designed to flexibly exploit the statistical structure of simultaneouslyrecorded spike responses. We define two prior distributions over spike words using mixtures of Dirichlet distributions centered on simple parametric models. The
parametric model captures high-level statistical features of the data, such as the
average spike count in a spike word, which allows the posterior over entropy to
concentrate more rapidly than with standard estimators (e.g., in cases where the
probability of spiking differs strongly from 0.5). Conversely, the Dirichlet distributions assign prior mass to distributions far from the parametric model, ensuring
consistent estimates for arbitrary distributions. We devise a compact representation of the data and prior that allow for computationally efficient implementations
of Bayesian least squares and empirical Bayes entropy estimators with large numbers of neurons. We apply these estimators to simulated and real neural data and
show that they substantially outperform traditional methods.

Introduction
Information theoretic quantities are popular tools in neuroscience, where they are used to study
neural codes whose representation or function is unknown. Neuronal signals take the form of fast
(? 1 ms) spikes which are frequently modeled as discrete, binary events. While the spiking response
of even a single neuron has been the focus of intense research, modern experimental techniques make
it possible to study the simultaneous activity of hundreds of neurons. At a given time, the response
of a population of n neurons may be represented by a binary vector of length n, where each entry
represents the presence (1) or absence (0) of a spike. We refer to such a vector as a spike word.
For n much greater than 30, the space of 2n spike words becomes so large that effective modeling
and analysis of neural data, with their high dimensionality and relatively low sample size, presents
a significant computational and theoretical challenge.
We study the problem of estimating the discrete entropy of spike word distributions. This is a difficult problem when the sample size is much less than 2n , the number of spike words. Entropy
estimation in general is a well-studied problem with a literature spanning statistics, physics, neuro1

A

B

neurons

frequecny

time

words

0 1 0 0 1 0 1 1
0 0 1 0 1 1 0 1
0 0 0 1 0 1 1 1
word distribution

0 1 0
0 1 1
1 0 1

Figure 1: Illustrated example of binarized spike responses for n = 3 neurons and corresponding
word distribution. (A) The spike responses of n = 3 simultaneously-recorded neurons (green,
orange, and purple). Time is discretized into bins of size ?t. A single spike word is a 3 ? 1 binary
vector whose entries are 1 or 0 corresponding to whether the neuron spiked or not within the time
bin. (B) We model spike words as drawn iid from the word distribution ?, a probability distribution
supported on the A = 2n unique binary words. Here we show a schematic ? for the data of panel
(A). The spike words (x-axis) occur with varying probability (blue)
science, ecology, and engineering, among others [1?7]. We introduce a novel Bayesian estimator
which, by incorporating simple a priori information about spike trains via a carefully-chosen prior,
can estimate entropy with remarkable accuracy from few samples. Moreover, we exploit the structure of spike trains to compute efficiently on the full space of 2n spike words.
We begin by briefly reviewing entropy estimation in general. In Section 2 we discuss the statistics
of spike trains and emphasize a statistic, called the synchrony distribution, which we employ in
our model. In Section 3 we introduce two novel estimators, the Dirichlet?Bernoulli (DBer) and
Dirichlet?Synchrony (DSyn) entropy estimators, and discuss their properties and computation. We
? DBer and H
? DSyn to other entropy estimation techniques in simulation and on neural data,
compare H
? DBer drastically outperforms other popular techniques when applied to real neural
and show that H
data. Finally, we apply our estimators to study synergy across time of a single neuron.

1

Entropy Estimation
A

N

Let x := {xk }k=1 be spike words drawn iid from an unknown word distribution ? := {?i }i=1 .
There are A = 2n unique words for a population of n neurons, which we index by {1, 2, . . . , A}.
Each sampled word xk is a binary vector of length n, where xki records the presence or absence of
a spike from the ith neuron. We wish to estimate the entropy of ?,
H(?) = ?

A
X

?k log ?k ,

(1)

k=1

where ?k > 0 denotes the probability of observing the kth word.
A naive method for estimating H is to first estimate ? using the count of observed words nk =
PN
? where ?
?k = nk /N . Evali=1 1{xi =k} for each word k. This yields the empirical distribution ?,
uating eq. 1 on this estimate yields the ?plugin? estimator,
? plugin = ?
H

A
X

?
?i log ?
?i ,

(2)

i=1

which is also the maximum-likelihood estimator under the multinomial likelihood. Although con? plugin is in general severely biased when N  A.
sistent and straightforward to compute, H
Indeed, all entropy estimators are biased when N  A [8]. As a result, many techniques for biascorrection have been proposed in the literature [6, 9?18]. Here, we extend the Bayesian approach
of [19], focusing in particular on the problem of entropy estimation for simultaneously-recorded
neurons.
In a Bayesian paradigm, rather than attempting to directly compute and remove the bias for a given
estimator, we instead choose a prior distribution over the space of discrete distributions. Nemenman
2

RGC Empirical Synchrony Distribution, 2ms bins
0.7

B

0.6
0.5
RGC data

0.4

NSB prior

0.3
0.2
0.1
0

0

1

2
3
4
5
6
7
number of spikes in a word

Empirical Synchrony Distribution of Simulated Ising Model
and ML Binomial Fit
0.7
proportion of words (of 600000 words)

proportion of words (out of 600000 words)

A

0.6
0.5

Binomial fit

0.3
0.2
0.1
0

8

Ising

0.4

0

1

2
3
4
5
6
7
number of spikes in a word

8

Figure 2: Sparsity structure of spike word distribution illustrated using the synchrony distribution.
(A) The empirical synchrony distribution of 8 simultaneously-recorded retinal ganglion cells (blue).
The cells were recorded for 20 minutes and binned with ?t = 2 ms bins. Spike words are overwhelmingly sparse, with w0 by far the most common word. In contrast, we compare the prior empirical synchrony distribution sampled using 106 samples from the NSB prior (? ? Dir(?, . . . , ?) ,
with p(?) ? A?1 (A? + 1) ? ?1 (? + 1), and ?1 the digamma function) (red). The empirical synchrony distribution shown is averaged across samples. (B) The synchrony distribution of an Ising
model (blue) compared to its best binomial fit (red). The Ising model parameters were set randomly
by drawing the entries of the matrix J and vector h iid from N(0, 1). A binomial distribution cannot
accurately capture the observed synchrony distribution.

et al. showed Dirichlet to be priors highly informative about the entropy, and thus a poor prior for
Bayesian entropy estimation [19]. To rectify this problem, they introduced the Nemenman?Shafee?
Bialek (NSB) estimator, which uses a mixture of Dirichlet distributions to obtain an approximately
flat prior over H. As a prior on ?, however, the NSB prior is agnostic about application: all symbols
have the same marginal probability under the prior, an assumption that may not hold when the
symbols correspond to binary spike words.

2

Spike Statistics and the Synchrony Distribution

We discretize neural signals by binning multi-neuron spike trains in time, as illustrated in Fig. 1. At a
time t, then, the spike response of a population of n neurons is a binary vector w
~ = (b1 , b2 , . . . , bn ),
where bi ? {0, 1} corresponds to the event that the ith neuron spikes within the time window
Pn?1
(t, t + ?t). We let w
~ k be that word such that k = i=0 bi 2i . There are a total of A = 2n possible
words.
For a sufficiently small bin size ?t, spike words are likely to be sparse, and so our strategy will be
to choose priors that place high prior probability on sparse words. To quantify sparsity we use the
synchrony distribution: the distribution of population spike counts across all words. In Fig. 2 we
compare the empirical synchrony distribution for a population of 8 simultaneously-recorded retinal
ganglion cells (RGCs) with the prior synchrony distribution under the NSB model. For real data, the
synchrony distribution is asymmetric and sparse, concentrating around words with few simultaneous
spikes. No more than 4 synchronous spikes are observed in the data. In contrast, under the NSB
model all words are equally likely, and the prior synchrony distribution is symmetric and centered
around 4.
These deviations in the synchrony distribution are noteworthy: beyond quantifying sparseness, the
synchrony distribution provides a surprisingly rich characterization of a neural population. Despite
its simplicity, the synchrony distribution carries information about the higher-order correlation structure of a population [20,21]. It uniquely specifies distributions
? for which the probability of a word
P
wk depends only on its spike count [k] = [w
~ k ] := i bi . Equivalently: all words with spike count
k, Ek = {w : [w] = k}, have identical probability ?k of occurring. For such a ?, the synchrony
3

distribution ? is given by,
?k =

X
wi ?Ek

 
n
?i =
?k .
k

(3)

Different neural models correspond to different synchrony distributions. Consider an independentlyBernoulli spiking model. Under this model, the number of spikes in a word w is distributed binomially, [w]
~ ? Bin(p, n), where p is a uniform spike probability across neurons. The probability of a
word wk is given by,
P (w
~ k |p) = ?[k] = p[k] (1 ? p)n?[k] ,

(4)

while the probability of observing a word with i spikes is,
 
n
P (Ei |p) =
?i .
i

3

(5)

Entropy Estimation with Parametric Prior Knowledge

Although a synchrony distribution may capture our prior knowledge about the structure of spike
patterns, our goal is not to estimate the synchrony distribution itself. Rather, we use it to inform a
prior on the space of discrete distributions, the (2n ?1)-dimensional simplex. Our strategy is to use a
synchrony distribution G as the base measure of a Dirichlet distribution. We construct a hierarchical
model where ? is a mixture of Dir(?G), and counts n of spike train observations are multinomial
given ? (See Fig. 3(A). Exploiting the conjugacy of Dirichlet and multinomial, and the convenient
symmetries of both the Dirichlet distribution and G, we obtain a computationally efficient Bayes
least squares estimator for entropy. Finally, we discuss using empirical estimates of the synchrony
distribution ? as a base measure.
3.1

Dirichlet?Bernoulli entropy estimator

We model spike word counts n as drawn iid multinomial given the spike word distribution ?. We
place a mixture-of-Dirichlets prior on ?, which in general takes the form,
n ? Mult(?)
? ? Dir(?1 , ?2 , . . . , ?A ),
|
{z
}

(6)
(7)

?
~ := (?1 , ?2 , . . . , ?A ) ? P (~
?),

(8)

2n

where ?i > 0 are concentration parameters, and P (~
?) is a prior distribution of our choosing. Due
to the conjugacy of Dirichlet and multinomial, the posterior distribution given observations and ?
~ is
?|n, ?
~ ? Dir(?1 + n1 , . . . , ?A + nA ), where ni is the number of observations for the i-th spiking
pattern. The posterior expected entropy given ?
~ is given by [22],
E[H(?)|~
?] = ?0 (? + 1) ?

A
X
?i
i=1

where ?0 is the digamma function, and ? =

PA

i=1

?

?0 (?i + 1)

(9)

?i .

For large A, ?
~ is too large to select arbitrarily, and so in practice we center the Dirichlet around
a simple, parametric base measure G [23]. We rewrite the vector of concentration parameters as
?
~ ? ?G, where G = Bernoulli(p) is a Bernoulli distribution with spike rate p and ? > 0 is a scalar.
The general prior of eq. 7 then takes the form,
? ? Dir(?G) ? Dir(?g1 , ?g2 . . . , ?gA ),
where each gk is the probability of the kth word under the base measure, satisfying

(10)
P

gk = 1.

We illustrate the dependency structure of this model schematically in Fig. 3. Intuitively, the base
measure incorporates the structure of G into the prior by shifting the Dirichlet?s mean. With a
base measure G the prior mean satisfies E[?|p] = G|p. Under the NSB model, G is the uniform
distribution; thus, when p = 0.5 the Binomial G corresponds exactly to the NSB model. Since
4

in practice choosing a base measure is equivalent to selecting distinct values of the concentration
parameter ?i , the posterior mean of entropy under this model has the same form as eq. 9, simply
replacing ?k = ?gk . Given hyper-prior distributions P (?) and P (p), we obtain the Bayes least
squares estimate, the posterior mean of entropy under our model,
ZZ
? DBer = E[H|x] =
H
E [H|?, p] P (?, p|x) d? dp.
(11)
? DBer . Thanks to the closedWe refer to eq. 11 as the Dirichlet?Bernoulli (DBer) entropy estimator, H
form expression for the conditional mean eq. 9 and the convenient symmetries of the Bernoulli
distribution, the estimator is fast to compute using a 2D numerical integral over the hyperparameters
? and p.
3.1.1

Hyper-priors on ? and p

Previous work on Bayesian entropy estimation has focused on Dirichlet priors with scalar, constant
concentration parameters ?i = ?. Nemenman et al. [19] noted that these fixed-? priors yield poor
estimators for entropy, because p(H|?) is highly concentrated around its mean. To address this
problem, [19] proposed a Dirichlet mixture prior on ?,
Z
P (?) = PDir (?|?)P (?)d?,
(12)
d
where the hyper-prior, P (?) ? d?
E[H(?)|?] assures an approximately flat prior distribution over
H. We adopt the same strategy here, choosing the prior,
n  
X
n 2
d
? ?1 (??i + 1).
(13)
E[H(?)|?, p] = ?1 (? + 1) ?
P (?) ?
i i
d?
i=0

Entropy estimates are less sensitive to the choice of prior on p. Although we experimented with
several priors
P on p, in all examples we found that the evidence for p was highly concentrated around
p? = N1n ij xij , the maximum (Bernoulli) likelihood estimate for p. In practice, we found that an
empirical Bayes procedure, fitting p? from data first and then using the fixed p? to perform the integral
eq. 11, performed indistinguishably from a P (p) uniform on [0, 1].
3.1.2

Computation

For large n, the 2n distinct values of ?i render the sum of eq. 9 potentially intractable to compute.
We sidestep this exponential scaling of terms by exploiting the redundancy of Bernoulli and binomial
distributions. Doing so, we are able to compute eq. 9 without explicitly representing the 2N values
of ?i .
Under the Bernoulli
model, each element gk of the base measure takes the value ?[k]

 (eq. 4). Further,
Pn
there are ni words for which the value of ?i is identical, so that A = i=0 ? ni ?i = ?. Applied
to eq. 9, we have,
n  
X
n
E[H(?)|?, p] = ?0 (? + 1) ?
?i ?0 (??i + 1).
i
i=0
For the posterior, the sum takes the same form, except that A = n + ?, and the mean is given by,
E[H(?)|?, p, x] = ?0 (n + ? + 1) ?

A
X
ni + ??[i]
i=1

= ?0 (n + ? + 1) ?

X ni + ??[i]
i?I
n
X

??

i=0

5

n+?

?0 (ni + ??[i] + 1)

?0 (ni + ??[i] + 1)
n+?


n
? i ?i
i ?n
?0 (??i + 1),
n+?

(14)

words

B
N neurons

A
prior

distribution

0
0
0
0

most likely

1
0
0
0

0
1
0
0

0
0
1
0

less likely

0
0
0
1

1
1
0
0

1
0
1
0

1
0
0
1

0
1
1
0

0
1
0
1

even less likely

0
0
1
1

0
1
1
1

1
0
1
1

1
1
0
1

1
1
1
0

less likely still

1
1
1
1
least likely

C

entropy
data

,

,

,

,

Figure 3: Model schematic and intuition for Dirichlet?Bernoulli entropy estimation. (A) Graphical
model for Dirichlet?Bernoulli entropy estimation. The Bernoulli base measure G depends on the
spike rate parameter p. In turn, G acts as the mean of a Dirichlet prior over ?. The scalar Dirichlet
concentration parameter ? determines the variability of the prior around the base measure. (B) The
set of possible spike words for n = 4 neurons. Although easy to enumerate for this small special
case, the number of words increases exponentially with n. In order to compute with this large set,
we assume a prior distribution with a simple equivalence class structure: a priori, all words with the
same number of synchronous spikes (outlined in blue) occur with equal probability. We then need
only n parameters, the synchrony distribution of eq. 3, to specify the distribution. (C) We center a
Dirichlet distribution on a model of the synchrony distribution. The symmetries of the count and
Dirichlet distributions allow us to compute without explicitly representing all A words.
where I = {k : nk > 0}, the set of observed characters, and n
? i is the count of observed words with
i spikes (i.e., observations of the equivalence class Ei ). Note that eq. 14 is much more computationally tractable than the mathematically equivalent form given immediately above it. Thus, careful
bookkeeping allows us to efficiently evaluate eq. 9 and, in turn, eq. 11.1
3.2

Empirical Synchrony Distribution as a Base Measure

While the Bernoulli base measure captures the sparsity structure of multi-neuron recordings, it also
imposes unrealistic independence assumptions. In general, the synchrony distribution can capture
correlation structure that cannot be represented by a Bernoulli model. For example, in Fig. 2B, a
maximum likelihood Bernoulli fit fails to capture the sparsity structure of a simulated Ising model.
We might address this by choosing a more flexible parametric base measure. However, since the
dimensionality of ? scales only linearly with the number of neurons, the empirical synchrony distribution (ESD),
N
1 X
?
?i =
1{[xj ]=i} ,
(15)
N j=1
converges quickly even when the sample size is inadequate for estimating the full ?.
?
This suggests an empirical Bayes procedure where we use the ESD as a base measure (take G = ?)
for entropy estimation. Computation proceeds exactly as in Section 3.1.2
with the Bernoulli base

measure replaced by the ESD by setting gk = ?k and ?i = ?i / mi . The resulting Dirichlet?
Synchrony (DSyn) estimator may incorporate more varied sparsity and correlation structures into its
? DBer (see Fig. 4), although it depends on an estimate of the synchrony distribution.
prior than H

4

Simulations and Comparisons

? DBer and H
? DSyn to the Nemenman?Shafee?Bialek (NSB) [19] and Best Upper Bound
We compared H
? DSyn , we regular(BUB) entropy estimators [8] for several simulated and real neural datasets. For H
1
For large n, the binomial coefficient of eq. 14 may be difficult to compute. By writing it in terms of the
Bernoulli probability eq. 5, it may be computed using the Normal approximation to the Binomial.

6

B

2
1.8

8

1.6

7

1.4

Entropy (nats)

9

6
5

0.3

4
3
2

0.2

10

10

2

3

N

10

1.2
1
0.8

0.1
0

Power Law Synchrony Distribution (30 Neurons)

plugin
DBer
DSyn
BUB
NSB

0.6
0

10

20

number of spikes per word

10

4

30

10

5

0.4

6

frequency

Bimodal Synchrony Distribution (30 Neurons)

frequency

Entropy (nats)

A 10

10

2

0.8
0.6
0.4
0.2
0

10

3

N

10

0

10

20

number of spikes per word

10

4

10

5

30

6

? DBer , H
? DSyn , H
? NSB , H
? BUB , and H
? plugin as a function of sample size for
Figure 4: Convergence of H
two simulated examples of 30 neurons. Binary word data are drawn from two specified synchrony
distributions (insets). Error bars indicate variability of the estimator over independent samples (?1
standard deviation).
(A) Data drawn from a bimodal
synchrony distribution with peaks at 0 spikes


1 ?4(i?2n/3)2
and 10 spikes ?i = e?2i + 10
e
. (B) Data generated from a power-law synchrony
distribution (?i ? i?3 ).

A

B

RGC Spike Data (27 Neurons)
1 ms bins

3.5

Entropy (nats)

102

frequency

plugin
DBer
DSyn
BUB
NSB

0.3

103

10

0.2

8

0.1
0

N

0.2

12

0.4

frequency

Entropy (nats)

14

2.5

1.5

10 ms bins

16

3

2

RGC Spike Data (27 neurons)
18

0

10
20
number of spikes per word

104

0

6

0
10
20
30
number of spikes per word

102

105

0.1

103

N

104

105

? DBer , H
? DSyn , H
? NSB , H
? BUB , and H
? plugin as a function of sample size for
Figure 5: Convergence of H
27 simultaneously-recorded retinal ganglion cells (RGC). The two figures show the same RGC data
binned and binarized at ?t = 1 ms (A) and 10 ms (B). The error bars, axes, and color scheme are as
? plugin , H
? DSyn and H
? DBer both show
in Fig. 4. While all estimators improve upon the performance of H
excellent performance for very low sample sizes (10?s of samples). (inset) The empirical synchrony
distribution estimated from 120 minutes of data.
1
ized the estimated ESD by adding a pseudo-count of K
, where K is the number of unique words
observed in the sample. In Fig. 4 we simulated data from two distinct synchrony distribution mod? DSyn converges the fastest with increasing sample size
els. As is expected, among all estimators, H
?
N . The HDBer estimator converges more slowly, as the Bernoulli base measure is not capable of
capturing the correlation structure of the simulated synchrony distributions. In Fig. 5, we show
convergence performance on increasing subsamples of 27 simultaneously-recorded retinal ganglion
? DBer and H
? DSyn show excellent performance. Although the true word distribution is
cells. Again, H
not described by a synchrony distribution, the ESD proves an excellent regularizer for the space of
distributions, even for very small sample sizes.

5

Application: Quantification of Temporal Dependence

We can gain insight into the coding of a single neural time-series by quantifying the amount of information a single time bin contains about another. The correlation function (Fig. 6A) is the statistic
most widely used for this purpose. However, correlation cannot capture higher-order dependencies.
In neuroscience, mutual information is used to quantify higher-order temporal structure [24]. A re7

B

delayed mutual information

C

growing block mutual information
6

bits

auto-correlation function

4
2

spike rate

A

growing block mutual information

0

20 spk/s

D

1

5

10

15

information gain

bits

10 ms
0.5

0

dMI

5

10

lags (ms)

15

? DBer . (A) The auto-correlation
Figure 6: Quantifying temporal dependence of RGC coding using H
function of a single retinal ganglion neuron. Correlation does not capture the full temporal dependence. We bin with ?t = 1 ms bins. (B) Schematic definition of time delayed mutual information (dMI), and block mutual information. The information gain of the sth bin is ?(s) =
I(Xt ; Xt+1:t+s ) ? I(Xt ; Xt+1:t+s?1 ). (C) Block mutual information estimate as a function of
growing block size. Note that the estimate is monotonically increasing, as expected, since adding
new bins can only increase the mutual information. (D) Information gain per bin assuming temporal
independence (dMI), and with difference between block mutual informations (?(s)). We observe
synergy for the time bins in the 5 to 10 ms range.

lated quantity, the delayed mutual information (dMI) provides an indication of instantaneous dependence: dM I(s) = I(Xt ; Xt+s ), where Xt is a binned spike train, and I(X; Y ) = H(X)?H(X|Y )
denotes the mutual information. However, this quantity ignores any temporal dependences in
the intervening times: Xt+1 , . . . , Xt+s?1 . An alternative approach allows us to consider such
dependences: the ?block mutual information? ?(s) = I(Xt ; Xt+1:t+s ) ? I(Xt ; Xt+1:t+s?1 )
(Fig. 6B,C,D)
The relationship between ?(s) and dM I(s) provides insight about the information contained in the
recent history of the signal. If each time bin is conditionally independent given Xt , then ?(s) =
dM I(s). In contrast, if ?(s) < dM I(s), instantaneous dependence is partially explained by history.
Finally, ?(s) > dM I(s) implies that the joint distribution of Xt , Xt+1 , . . . , Xt+s contains more
? DBer entropy
information about Xt than the joint distribution of Xt and Xt+s alone. We use the H
estimator to compute mutual information (by computing H(X) and H(X|Y )) accurately for ? 15
bins of history. Surprisingly, individual retinal ganglion cells code synergistically in time (Fig. 6D).

6

Conclusions

? DBer and H
? DSyn . These estimators use a
We introduced two novel Bayesian entropy estimators, H
hierarchical mixture-of-Dirichlets prior with a base measure designed to integrate a priori knowledge about spike trains into the model. By choosing base measures with convenient symmetries,
we simultaneously sidestepped potentially intractable computations in the high-dimensional space
of spike words. It remains to be seen whether these symmetries, as exemplified in the structure of
the synchrony distribution, are applicable across a wide range of neural data. Finally, however, we
? DSyn , perform exceptionally well in
showed several examples in which these estimators, especially H
application to neural data. A MATLAB implementation of the estimators will be made available at
https://github.com/pillowlab/CDMentropy.

Acknowledgments
We thank E. J. Chichilnisky, A. M. Litke, A. Sher and J. Shlens for retinal data. This work was
supported by a Sloan Research Fellowship, McKnight Scholar?s Award, and NSF CAREER Award
IIS-1150186 (JP).
8

References
[1] K. H. Schindler, M. Palus, M. Vejmelka, and J. Bhattacharya. Causality detection based on informationtheoretic approaches in time series analysis. Physics Reports, 441:1?46, 2007.
[2] A. R?enyi. On measures of dependence. Acta Mathematica Hungarica, 10(3-4):441?451, 9 1959.
[3] C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees. Information
Theory, IEEE Transactions on, 14(3):462?467, 1968.
[4] A. Chao and T. Shen. Nonparametric estimation of Shannon?s index of diversity when there are unseen
species in sample. Environmental and Ecological Statistics, 10(4):429?443, 2003.
[5] P. Grassberger. Estimating the information content of symbol sequences and efficient codes. Information
Theory, IEEE Transactions on, 35(3):669?675, 1989.
[6] S. Ma. Calculation of entropy from data of motion. Journal of Statistical Physics, 26(2):221?240, 1981.
[7] S. Panzeri, R. Senatore, M. A. Montemurro, and R. S. Petersen. Correcting for the sampling bias problem
in spike train information measures. J Neurophysiol, 98(3):1064?1072, Sep 2007.
[8] L. Paninski. Estimation of entropy and mutual information. Neural Computation, 15:1191?1253, 2003.
[9] W. Bialek, F. Rieke, R. R. de Ruyter van Steveninck, R., and D. Warland. Reading a neural code. Science,
252:1854?1857, 1991.
[10] R. Strong, S. Koberle, de Ruyter van Steveninck R., and W. Bialek. Entropy and information in neural
spike trains. Physical Review Letters, 80:197?202, 1998.
[11] L. Paninski. Estimation of entropy and mutual information. Neural Computation, 15:1191?1253, 2003.
[12] R. Barbieri, L. Frank, D. Nguyen, M. Quirk, V. Solo, M. Wilson, and E. Brown. Dynamic analyses of
information encoding in neural ensembles. Neural Computation, 16:277?307, 2004.
[13] M. Kennel, J. Shlens, H. Abarbanel, and E. Chichilnisky. Estimating entropy rates with Bayesian confidence intervals. Neural Computation, 17:1531?1576, 2005.
[14] J. Victor. Approaches to information-theoretic analysis of neural activity. Biological theory, 1(3):302?
316, 2006.
[15] J. Shlens, M. B. Kennel, H. D. I. Abarbanel, and E. J. Chichilnisky. Estimating information rates with
confidence intervals in neural spike trains. Neural Computation, 19(7):1683?1719, Jul 2007.
[16] V. Q. Vu, B. Yu, and R. E. Kass. Coverage-adjusted entropy estimation. Statistics in medicine,
26(21):4039?4060, 2007.
[17] V. Q. Vu, B. Yu, and R. E. Kass. Information in the nonstationary case. Neural Computation, 21(3):688?
703, 2009, http://www.mitpressjournals.org/doi/pdf/10.1162/neco.2008.01-08-700. PMID: 18928371.
[18] E. Archer, I. M. Park, and J. Pillow. Bayesian estimation of discrete entropy with mixtures of stickbreaking priors. In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in
Neural Information Processing Systems 25, pages 2024?2032. MIT Press, Cambridge, MA, 2012.
[19] I. Nemenman, F. Shafee, and W. Bialek. Entropy and inference, revisited. In Advances in Neural Information Processing Systems 14, pages 471?478. MIT Press, Cambridge, MA, 2002.
[20] M. Okun, P. Yger, S. L. Marguet, F. Gerard-Mercier, A. Benucci, S. Katzner, L. Busse, M. Carandini, and
K. D. Harris. Population rate dynamics and multineuron firing patterns in sensory cortex. The Journal of
Neuroscience, 32(48):17108?17119, 2012, http://www.jneurosci.org/content/32/48/17108.full.pdf+html.
[21] G. Tka?cik, O. Marre, T. Mora, D. Amodei, M. J. Berry II, and W. Bialek. The simplest maximum
entropy model for collective behavior in a neural network. Journal of Statistical Mechanics: Theory and
Experiment, 2013(03):P03011, 2013.
[22] D. Wolpert and D. Wolf. Estimating functions of probability distributions from a finite set of samples.
Physical Review E, 52(6):6841?6854, 1995.
[23] I. M. Park, E. Archer, K. Latimer, and J. W. Pillow. Universal models for binary spike patterns using
centered Dirichlet processes. In Advances in Neural Information Processing Systems (NIPS), 2013.
[24] A. Panzeri, S. Treves, S. Schultz, and E. Rolls. On decoding the responses of a population of neurons
from short time windows. Neural Computation, 11:1553?1577, 1999.

9

"
1111,"Grouping with Bias

Stella X. Yu
Robotics Institute
Carnegie Mellon University
Center for the Neural Basis of Cognition
Pittsburgh, PA 15213-3890

Jianbo Shi
Robotics Institute
Carnegie Mellon University
5000 Forbes Ave
Pittsburgh, PA 15213-3890

stella. yu@es. emu. edu

jshi@es.emu.edu

Abstract
With the optimization of pattern discrimination as a goal, graph
partitioning approaches often lack the capability to integrate prior
knowledge to guide grouping. In this paper, we consider priors
from unitary generative models, partially labeled data and spatial
attention. These priors are modelled as constraints in the solution
space. By imposing uniformity condition on the constraints, we
restrict the feasible space to one of smooth solutions. A subspace
projection method is developed to solve this constrained eigenproblema We demonstrate that simple priors can greatly improve image
segmentation results.

1 "" Introduction
Grouping is often thought of as the process of finding intrinsic clusters or group
structures within a data set. In image segmentation, it means finding objects or
object segments by clustering pixels and segregating them from background. It
is often considered a bottom-up process. Although never explicitly stated, higher
level of knowledge, such as familiar object shapes, is to be used only in a separate
post-processing step.
The need for the integration of prior knowledge arises in a number of applications. In
computer vision, we would like image segmentation to correspond directly to object
segmentation. In data clustering, if users provide a few examples of clusters, we
would like a system to adapt the grouping process to achieve the desired properties.
In this case, there is an intimate connection to learning classification with partially
labeled data.
We show in this paper that it is possible to integrate both bottom-up and top-down
information in a single grouping process. In the proposed method, the bottom-up
grouping process is modelled as a graph partitioning [1, 4, 12, 11, 14, 15] problem, and the top-down knowledge is encoded as constraints on the solution space.
Though we consider normalized cuts criteria in particular, similar derivation can be
developed for other graph partitioning criteria as well. We show that it leads to a
constrained eigenvalue problem, where the global optimal solution can be obtained
by eigendecomposition. Our model is expanded in detail in Section 2. Results and
conclusions are given in Section 3.

2

Model

In graph theoretic methods for grouping, a relational graph GA == (V, E, W) is first
constructed based on pairwise similarity between two elements. Associated with
the graph edge between vertex i and j is weight Wij , characterizing their likelihood
of belonging in the same group.

For image segmentation, pixels are taken as graph nodes, and pairwise pixel similarity can be evaluated based on a number of low level grouping cues. Fig. Ic shows
one possible defini~ion, where the weight b.etween two pixels is inversely proportional
to the magnitude of the strongest intervening edge [9].

a)Image.

d)NCuts.

e)Segmentation.

Figure 1: Segmentation by graph partitioning. a)200 x 129 image with a few pixels
marked( +). b)Edge map extracted using quadrature filters.c)Local affinity fields of
marked pixels superimposed together. For every marked pixel, we compute its affinity
with its neighbours within a radius of 10. The value is determined by a Gaussian function
of the maximum magnitude of edges crossing the straight line connecting the two pixels
[9]. When there is a strong edge separating the two, the affinity is low. Darker intensities
mean larger values. d)Solution by graph partitioning. It is the second eigenvector from
normalized cuts [15] on the affinity matrix. It assigns a value to each pixel. Pixels of
similar values belong to the same group. e)Segmentation by thresholding the eigenvector
with o. This gives a bipartitioning of the image which corresponds to the best cuts that
have maximum within-region coherence and between-region distinction.
After an image is transcribed into a graph, image segmentation becomes a vertex
partitioning problem. Consider segmenting an image into foreground and background. This corresponds to vertex bipartitioning (VI, V2 ) on graph G, where
V = VI U V2 and VI n V2 = 0. A good segmentation seeks a partitioning such
that nodes within partitions are tightly connected and nodes across partitions are
loosely connected. A number of criteria have been proposed to achieve this goal.
For normalized cuts [15], the solution is given by some eigenvector of weight matrix
W (Fig. Id). Thresholding on it leads to a discrete segmentation (Fig. Ie). W.hile
we will focus on normalized cuts criteria [15], most of the following discussions apply
to other criteria as well.
2.1

Biased grouping as constrained optimization

Knowledge other than the image itself can greatly change the segmentation we might
obtain based on such low level cues. Rather than seeing boundaries between black
and white regions, we see objects. The sources of priors we consider in this paper
are: unitary generative models (Fig. 2a), which could arise from sensor models
in MRF [5], partial grouping (Fig. 2b), which could arise from human computer
interaction [8], and spatial attention (Fig. 2c). All of these provide additional, often
long-range, binding information for grouping.
We model such prior knowledge in the form of constraints on a valid grouping
configuration. In particular, we see that all such prior knowledge defines a partial

a)Bright foreground.

b)Partial grouping.

c)Spatial attention.

Figure 2: Examples of priors considered in this paper. a)Local constraints from unitary
generative models. In this case, pixels of light (dark) intensities are likely to be the foreground(background). This prior knowledge is helpful not only for identifying the tiger as
the foreground, but also for perceiving the river as one piece. How can we incorporate
these unitary constraints into a- graph that handles only pairwise relationships between
pixels? b )Global configuration constraints from partial grouping a priori. In this case,
we have manually selected two sets of pixels to be grouped together in foreground (+)
and background (JJ.) respectively. They are distributed across the image and often have
distinct local features. How can we force them to be in the same group and further bring
similar pixels along and push dissimilar pixels apart? c)Global constraints from spatial
attention. We move our eyes to the place of most interest and then devote our limited
visual processing to it. The complicated scene structures in the periphery can thus be
ignored while sparing the parts associated with the object at fovea. How can we use this
information to facilitate figural popout in segmentation?

grouping solution, indicating which set of pixels should belong to one partition.
Let Hz, 1 == 1"""" ,n, denote a partial grouping. H t have pixels known to be in
Vt , t == 1,2. These sets are derived as follows.
Unitary generative models: H l and H 2 contains a set of pixels that satisfy the
unitary generative models for foreground and background respectively. For example,
in Fig. 2a, H l (H2 ) contains pixels of brightest(darkest) intensities.
Partial grouping: Each Hz, 1 == 1, ... ,n, contains a set of pixels that users specify to
belong together. The relationships between Hz, 1 > 2 and Vt , t == 1,2 are indefinite.
Spatial attention: H l == 0 and H 2 contains pixels randomly selected outside the
visual fovea, since we want to maintain maximum discrimination at the -fovea but
merging pixels far away from the fovea to be one group.

To formulate these constraints induced on the graph partitioning, we introduce
binary group indicators X == [Xl, X 2 ]. Let N == IVI be the number of nodes in the
graph. For t == 1,2, X t is an N x 1 vector where Xt(k) == 1 if vertex k E Vt and 0
otherwise. The constrained grouping problem can be formally written as:
min
s.t.

?(Xl ,X2 )
Xt(i) == Xt(j), i, j E HE, 1 == 1"""" ,n, t == 1,2,
Xt(i) =1= Xt(j), i E H l , j E H 2 , t == 1,2,

where ?(X1 ,X2 ) is some graph partitioning cost function, such as minimum cuts
[6], average cuts [7], or normalized cuts [15]. The first set of constraints can be
re-written in matrix form: U T X == 0 , where, e.g. for some column k, Uik == 1,
Ujk == ~1. We search for the optimal solution only in the feasible set determined
by all the constraints.
2.2

Conditions on grouping constraints

The above formulation can be implemented by the maximum-flow algorithm for
minimum cuts criteria [6, 13, 3], where two special nodes called source and sink are

introduced,.with infinite weights set up between nodes in HI (H2 ) and source(sink).
In the context of learning from labeled and unlabeled data, the biased mincuts
are linked to minimizing leave-one-out cross validation [2]. In the normalize cuts
formulation, this leads to a constrained eigenvalue problem, as soon to be seen.
However, simply forcing a few nodes to be in the same group can produce some
undesirable graph partitioning results, illustrated in Fig. 3. Without bias, the data
points are naturally first organized into top and bottom groups, and then subdivided
into left and right halves (Fig. 3a). When we assign points from top and bottom
clusters to be together, we do not just want one of the groups to lose its labeled
point to the other group (Fig. 3b), but rather we desire the biased grouping process
to explore their neighbouring connections and change the organization to left and
right division accordingly.
Larger Cut
a
a
a
a
a
a

a
a
a
a
a
a

a
a
a
a
a
a

a
a
a
a
a
a

a
a
a
a
a
a

a
a
a
0

a
a

a
a
a
a
a
a

Desired Cut

a a a
a a a
0
a
0
a a a
a a a
a a a

a a
a a

a a a a a a
a a a a a a

0

0

0

a a
a a
a a

0

0

0

0

I

0

a a a a a a
a a a a a a

a
a
a
a
a
a

a
a
a
a
a
a

a
a
a
a
a
a

.A.

a
a
a
a
a

a
a
a
a
a

a
a
a
a
a

a
a
a
a
a

a
a
a
a
a

a
a
a
a
a
a

a
a
a
a
a
a

a
a
a
a
a

a
a
a
a
a

Min
Cut
a
a
a
a
a
a

a
a
a
a
a
a

a
a
a
a
a
a

a
a
a
a
a
a

a
a
a
a
a
a

a
a
a
a
a
0

a
a
a
a
a
a

a
a
a
a
a
a

a
a
a
a

a
a
a
a
0
a
a a

a
a
a
a
a
a

a
a
a
a
a
a

a
a
a
a
a
a

a II
a a
a
0
0
a
a a
a a

a
a
a
a
a
a

a
a
a
a
a
a

a
a
a
a
a
a

Perturbed
Min
Cut

b)With bias.

a)No bias.

Figure 3: Undesired grouping caused by simple grouping constraints. a)Data points
are distributed in four groups, with a larger spatial gap between top and bottom
groups than that between left and right groups. Defining weights based on proximity, we find the top-bottom grouping as the optimal bisection. b)Introduce two
pairs of filled nodes to be together. Each pair has one point from the top and the
other from the bottom group. The desired partitioning should now be the left-right
division. However, perturbation on the unconstrained optimal cut can lead to a
partitioning that satisfies the constraints while producing the smallest cut cost.
The desire of propagating partial grouping information on the constrained nodes
is, however, not reflected in the constrained partitioning criterion itself. Often,
a slightly perturbed version of the optimal unbiased cut becomes the legitimate
optimum. One reason for such a solution being undesirable is that some of the
""perturbed"" nodes-are isolated from their close neighbours.
To fix this problem, we introduce the notion of uniformity of a graph partitioning.
Intuitively, if two labeled nodes, i and j, have similar connections to their neighbours, we desire a cut to treat them fairly so that if i gets grouped with i's friends,
j also gets grouped with j's friends (Fig. 3b). This uniformity condition is one way
to propagate prior grouping information from labeled nodes to their neighbours.
For normalized cuts criteria, we define the normalized cuts of a single node to be

?.X)-

NC u t s ( ~,-

EXt(k)=I=Xt(i),YtWik

D..
n

.

This value is high for a node isolated from its close neighbours in partitioning X.

We may not know in advance what this value is for the optimal partitioning, but
we desire this value to be the same for any pair of nodes preassigned together:
NCuts(i;X) == NCuts(j;X), \li,j E Hz, l == 1,??? ,no
While this condition does not force NCuts(i; X) to be small for each labeled node,
it is unlikely for all of them to have a large value while producing the minimum
NCuts for the global bisection. Similar measures can be defined for other criteria.
In Fig. 4, we show that the uniformity condition on the bias helps preserving
the smoothness of solutions at every labeled point. Such smoothing is necessary
especially when partially labeled data are scarce.

0.5

0.5

: -.... .... -....
~

_o5A~,S/

-0.5

-1 [l...--V_-_l...--_-----'---_----'l
o

a)Point set data.

-1[?.....

300

b)Simple bias.

0

I

100

300

c) Conditioned bias.

0.5

0.5

o

o

0

-0.2

-0.4
-0.5

-1

-1
100

d)NCuts

wlo bias.

e)NCuts

100

wi bias b).

f)NCuts

wi bias c).

Figure 4: Condition constraints with uniformity. a)Data consist of three strips, with
100 points each, numbered from left toright. Two points from the side strips are randomly chosen to be pre-assigned together. b)Simple constraint U T X == 0 forces any
feasible solution to have equal valuation on the two marked points. c)Conditioned
constraint UTpX == o. Note that now we cannot tell which points are biased. We
compute W using Gaussian function of distance with u == 3. d) Segmentation without bias gives three separate groups. e)Segmentation with simple bias not only fails
to glue the two side strips into one, but also has two marked points isolated from
their neighbours. f)Segmentaion with conditioned bias brings two side strips into
one group. See the definition of P below.

2.3

COlllpntation: subspace projection

To develop a computational solution for the c9nstrained optimization problem, we
introduce some notations. Let the degree matrix D be a diagonal matrix, D ii ==
Ek Wik, \Ii. Let P == D-IW be the normalized weight matrix. It is a transition
probability matrix for nonnegative weight matrix W [10]. Let a == xI~~l be
the degree ratio of VI, where 1 is the vector of ones. We define a new variable
x == (1 - a)XI - aX2 ? We can show that for normalized cuts, the biased grouping
with the uniformity condition is translated into:
.
mIn E(X)

==

xT(D-W)x
T
TD
' s.t. U Px ==

x

x
Note, we have dropped the constraint Xt(i)

=1=

o.

Xt(j), i E HI, j E H 2 , t == 1,2.

Using Lagrange multipliers, we find that the optimal solution x* satisfies:

QPx* == AX*,

E(X*) == 1 - A,

where Q is a projector onto the feasible solution space:
Q == I - D-1V(VTD-1V)-lVT , V

==

pTU.

Here we assume that the conditioned constraint V is of full rank, thus V T D- 1V is
invertible. Since 1 is still the trivial 'Solution corresponding to the largest eigenvalue
of 1, the second leading right eigenvector of the matrix QP is the solution we seek.
To summarize, given weight matrix W, partial grouping in matrix form UT x
we do the following to find the optimal bipartitioning:

== 0,

Compute degree matrix D, D ii == E j Wij , Vi.
Compute normalized matrix P == D-1W.
Compute conditioned constraint V == pTU.
Compute projected weight matrix W == QP==p-n-1V(VTn-1V)-lVTp.
Compute the second largest eigenvector x*: Wx* == AX*.
Threshold x* to get a discrete segmentation.

Step
Step
Step
Step
Step
Step

1:
2:
3:
4:
5:
6:

3

Results and conclusions

We apply our method to the images in Fig. 2. For all the examples, we compute
pixel affinity W as in Fig. 1. All the segmentation results are obtained by thresholding the eigenvectors using their mean values. The results without bias, with
simple bias U T x == 0 and conditioned bias U T Px == 0 are compared in Fig. 5, 6, 7.

e)Simple bias.

b) Prior.

c)NCuts' on W.

f)Seg. on e)

g)Conditioned bias.

d)Seg.

wlo bias.

h)Seg. w/ bias.

Figure 5: Segmentation with bias from unitary generative models. a)Edge map of the
100 x 150 image. N = 15000. b)We randomly sample 17 brightest pixels for HI (+),48
darkest pixels for H2 (~), producing 63 constraints in total. c) and d) show the solution
without bias. It picks up the optimal bisection based on intensity distribution. e) and
f) show the solution with simple bias. The labeled nodes have an uneven influence on
grouping. g) and h) show the solution with conditioned bias. It successfully breaks the
image into tiger and river as our general impression of this image. The computation for
the three cases takes 11, 9 and 91ms respectively.

Prior knowledge is particularly useful in supplying long-range grouping information
which often lacks in data grouping based on low level cues. With our model, the
partial grouping prior can be integrated into the bottom-up grouping framework by
seeking the optimal solution in a restricted domain. We show that the uniformity
constraint is effective in eliminating spurious solutions resulting from simple perturbation on the optimal unbiased solutions. Segmentation from the discretization
of the continuous eigenvectors also becomes trivial.

e)Simple bias.

f)Seg. on e)

g)Conditioned bias.

h)Seg. w / bias.

Figure 6: Segmentation with bias from hand-labeled partial grouping. a)Edge map of
the 80 x 82 image. N = 6560. b)Hand-labeled partial grouping includes 21 pixels for HI
(+), 31 pixels for H 2 (A), producing 50 constraints in total. c) and d) show the solution
without bias. It favors a few largest nearby pieces of similar intensity. e) and f) show the
solution with simple bias. Labeled pixels in cluttered contexts are poor at binding their
segments together. g) and h) show the solution with conditioned bias. It successfully pops
out the pumpkin made of many small intensity patches. The computation for the three
cases takes 5, 5 and 71ms respectively.

f)4th eig. b)

g)6th eig. b)

h)4th eig. d)

i)6 th eig. d)

j)8 th eig. d)

Figure 7: Segmentation with bias from spatial attention. N = 25800. a)We randomly
choose 86 pixels far away from the fovea (Fig. 2c) for H 2 (A), producing 85 constraints.
b) and c) show the solution with simple bias. It is similar to the solution without bias
(Fig. 1). d) and e) show the solution with conditioned bias. It ignores the variation
in the background scene, which includes not only large pieces of constant intensity, but
also many small segments of various intensities. The foreground .successfully clips out the
human figure. f) and g) are two subsequent eigenvectors with simple bias. h), i) and
j.) are those with conditioned bias. There is still a lot of structural organization in the
former, but almost none in the latter. This greatly simplifies the task we face when seeking
a segmentation from the continuous eigensolution. The computation for the three cases
takes 16, 25 and 220ms respectively.

All these benefits come at a computational cost that is 10 times that for the original
unbiased grouping problem. We note that we can also impose both UT x == 0 and
U T Px == 0, or even U T pBX == 0, S > 1. Little improvement is observed in our
examples.' Since projected weight matrix W becomes denser, the computation slows
down. We hope that this problem can be alleviated by using multi-scale techniques.
It remains open for future research.
Acknowledgelllents

This research is supported by (DARPA HumanID) ONR
NSF IRI-9817496.

NOOOI4-00-1-091~ and

References
[1] A. Amir? and M. Lindenbaum. Quantitative analysis of grouping process. In
European Conference on Computer Vision, pages 371-84, 1996.
[2] A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph
mincuts, 2001.
[3] Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization
via graph cuts. In International Conference on Computer Vision, 1999.
[4] Y. Gdalyahu, D. Weinshall, and M. Werman. A randpmized algorithm for
pairwise clustering. ill Neural Information Processing Systems, pages 424-30,
1998.
[5] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6(6):721-41, 1984.
[6] H. Ishikawa and D. Geiger. Segmentation by grouping junctions. In IEEE
Conference on Computer Vision and Pattern Recognition, 1998.
[7] I. H. Jermyn and H. Ishikawa. Globally optimal regions and boundaries. In
International Conference on Computer Vision, 1999.
[8] M. Kass, A. Witkin, and D. Terzopoulos. Snakes: Active contour models.
International Journal of Computer Vision, pages 321-331, 1988.
[9] J. Malik, S. Belongie, T. Leung, and J. Shi. Contour and texture analysis for
image segmentation. International Journal of Computer Vision, 2001.
[10] M. Meila and J. Shi. Learning segmentation with random walk. ill Neural
Information Processing Systems, 2001.
[11] P. Perona and W. Freeman. A factorization approach to grouping. In European
Conference on Computer Vision, pages 655-70, 1998.
[12] J. Puzicha, T. Hofmann, and J. Buhmann. Unsupervised texture segmentation in a deterministic annealing framework. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 20(8):803-18, 1998.
[13] S. Roy and I. J. Cox. A maximum-flow formulation of then-camera stereo
correspondence problem. In International Conference on Computer Vision,
1998.
[14] E. Sharon, A. Brandt, and R. Basri. Fast multiscale image segmentation. In
IEEE Conference on Computer Vision and Pattern Recognition, pages 70-7,
2000.
[15] J. Shi and J. Malik. Normalized cuts and image segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 731-7, June 1997.

"
4581,"Convergence of Monte Carlo Tree Search in
Simultaneous Move Games
Viliam Lis?y1

Vojt?ech Kova?r??k1

Marc Lanctot2

Branislav Bo?sansk?y1

2

1

Department of Knowledge Engineering
Maastricht University, The Netherlands
marc.lanctot
@maastrichtuniversity.nl

Agent Technology Center
Dept. of Computer Science and Engineering
FEE, Czech Technical University in Prague
<name>.<surname>
@agents.fel.cvut.cz

Abstract
We study Monte Carlo tree search (MCTS) in zero-sum extensive-form games
with perfect information and simultaneous moves. We present a general template of MCTS algorithms for these games, which can be instantiated by various
selection methods. We formally prove that if a selection method is -Hannan consistent in a matrix game and satisfies additional requirements on exploration, then
the MCTS algorithm eventually converges to an approximate Nash equilibrium
(NE) of the extensive-form game. We empirically evaluate this claim using regret
matching and Exp3 as the selection methods on randomly generated games and
empirically selected worst case games. We confirm the formal result and show
that additional MCTS variants also converge to approximate NE on the evaluated
games.

1

Introduction

Non-cooperative game theory is a formal mathematical framework for describing behavior of interacting self-interested agents. Recent interest has brought significant advancements from the algorithmic perspective and new algorithms have led to many successful applications of game-theoretic
models in security domains [1] and to near-optimal play of very large games [2]. We focus on an
important class of two-player, zero-sum extensive-form games (EFGs) with perfect information and
simultaneous moves. Games in this class capture sequential interactions that can be visualized as a
game tree. The nodes correspond to the states of the game, in which both players act simultaneously.
We can represent these situations using the normal form (i.e., as matrix games), where the values
are computed from the successor sub-games. Many well-known games are instances of this class,
including card games such as Goofspiel [3, 4], variants of pursuit-evasion games [5], and several
games from general game-playing competition [6].
Simultaneous-move games can be solved exactly in polynomial time using the backward induction
algorithm [7, 4], recently improved with alpha-beta pruning [8, 9]. However, the depth-limited
search algorithms based on the backward induction require domain knowledge (an evaluation function) and computing the cutoff conditions requires linear programming [8] or using a double-oracle
method [9], both of which are computationally expensive. For practical applications and in situations
with limited domain knowledge, variants of simulation-based algorithms such as Monte Carlo Tree
Search (MCTS) are typically used in practice [10, 11, 12, 13]. In spite of the success of MCTS and
namely its variant UCT [14] in practice, there is a lack of theory analyzing MCTS outside two-player
perfect-information sequential games. To the best of our knowledge, no convergence guarantees are
known for MCTS in games with simultaneous moves or general EFGs.
1

Figure 1: A game tree of a game with perfect information and simultaneous moves. Only the leaves
contain the actual rewards; the remaining numbers are the expected reward for the optimal strategy.
In this paper, we present a general template of MCTS algorithms for zero-sum perfect-information
simultaneous move games. It can be instantiated using any regret minimizing procedure for matrix
games as a function for selecting the next actions to be sampled. We formally prove that if the algorithm uses an -Hannan consistent selection function, which assures attempting each action infinitely
many times, the MCTS algorithm eventually converges to a subgame perfect -Nash equilibrium of
the extensive form game. We empirically evaluate this claim using two different -Hannan consistent procedures: regret matching [15] and Exp3 [16]. In the experiments on randomly generated and
worst case games, we show that the empirical speed of convergence of the algorithms based on our
template is comparable to recently proposed MCTS algorithms for these games. We conjecture that
many of these algorithms also converge to -Nash equilibrium and that our formal analysis could be
extended to include them.

2

Definitions and background

A finite zero-sum game with perfect information and simultaneous moves can be described by a
tuple (N , H, Z, A, T , u1 , h0 ), where N = {1, 2} contains player labels, H is a set of inner states
and Z denotes the terminal states. A = A1 ? A2 is the set of joint actions of individual players and
we denote A1 (h) = {1 . . . mh } and A2 (h) = {1 . . . nh } the actions available to individual players
in state h ? H. The transition function T : H ? A1 ? A2 7? H ? Z defines the successor state given
a current state and actions for both players. For brevity, we sometimes denote T (h, i, j) ? hij .
The utility function u1 : Z 7? [vmin , vmax ] ? R gives the utility of player 1, with vmin and vmax
denoting the minimum and maximum possible utility respectively. Without loss of generality we
assume vmin = 0, vmax = 1, and ?z ? Z, u2 (z) = 1 ? u1 (z). The game starts in an initial state h0 .
A matrix game is a single-stage simultaneous move game with action sets A1 and A2 . Each entry
in the matrix M = (aij ) where (i, j) ? A1 ? A2 and aij ? [0, 1] corresponds to a payoff (to player
1) if row i is chosen by player 1 and column j by player 2. A strategy ?q ? ?(Aq ) is a distribution
over the actions in Aq . If ?1 is represented as a row vector and ?2 as a column vector, then the
expected value to player 1 when both players play with these strategies is u1 (?1 , ?2 ) = ?1 M ?2 .
Given a profile ? = (?1 , ?2 ), define the utilities against best response strategies to be u1 (br, ?2 ) =
max?10 ??(A1 ) ?10 M ?2 and u1 (?1 , br) = min?20 ??(A2 ) ?1 M ?20 . A strategy profile (?1 , ?2 ) is an
-Nash equilibrium of the matrix game M if and only if
u1 (br, ?2 ) ? u1 (?1 , ?2 ) ? 

u1 (?1 , ?2 ) ? u1 (?1 , br) ? 

and

(1)

Two-player perfect information games with simultaneous moves are sometimes appropriately called
stacked matrix games because at every state h each joint action from set A1 (h) ? A2 (h) either leads
to a terminal state or to a subgame which is itself another stacked matrix game (see Figure 1).
A behavioral strategy for player q is a mapping from states h ? H to a probability distribution over
the actions Aq (h), denoted ?q (h). Given a profile ? = (?1 , ?2 ), define the probability of reaching
a terminal state z under ? as ? ? (z) = ?1 (z)?2 (z), where each ?q (z) is a product of probabilities of
the actions taken by player q along the path to z. Define ?q to be the set of behavioral strategies for
player q. Then for any strategy profile ? = (?1 , ?2 ) ? ?1 ? ?2 we define the expected utility of the
strategy profile (for player 1) as
X
u(?) = u(?1 , ?2 ) =
? ? (z)u1 (z)
(2)
z?Z

2

An -Nash equilibrium profile (?1 , ?2 ) in this case is defined analogously to (1). In other words,
none of the players can improve their utility by more than  by deviating unilaterally. If the strategies
are an -NE in each subgame starting in an arbitrary game state, the equilibrium strategy is termed
subgame perfect. If ? = (?1 , ?2 ) is an exact Nash equilibrium (i.e., -NE with  = 0), then we
denote the unique value of the game v h0 = u(?1 , ?2 ). For any h ? H, we denote v h the value of
the subgame rooted in state h.

3

Simultaneous move Monte-Carlo Tree Search

Monte Carlo Tree Search (MCTS) is a simulation-based state space search algorithm often used
in game trees. The nodes in the tree represent game states. The main idea is to iteratively run
simulations to a terminal state, incrementally growing a tree rooted at the initial state of the game. In
its simplest form, the tree is initially empty and a single leaf is added each iteration. Each simulation
starts by visiting nodes in the tree, selecting which actions to take based on a selection function and
information maintained in the node. Consequently, it transitions to the successor states. When a
node is visited whose immediate children are not all in the tree, the node is expanded by adding a
new leaf to the tree. Then, a rollout policy (e.g., random action selection) is applied from the new
leaf to a terminal state. The outcome of the simulation is then returned as a reward to the new leaf
and the information stored in the tree is updated.
In Simultaneous Move MCTS (SM-MCTS), the main difference is that a joint action of both players
is selected. The algorithm has been previously applied, for example in the game of Tron [12], Urban
Rivals [11], and in general game-playing [10]. However, guarantees of convergence to NE remain
unknown. The convergence to a NE depends critically on the selection and update policies applied,
which are even more non-trivial than in purely sequential games. The most popular selection policy
in this context (UCB) performs very well in some games [12], but Shafiei et al. [17] show that it
does not converge to Nash equilibrium, even in a simple one-stage simultaneous move game. In this
paper, we focus on variants of MCTS, which provably converge to (approximate) NE; hence we do
not discuss UCB any further. Instead, we describe variants of two other selection algorithms after
explaining the abstract SM-MCTS algorithm.
Algorithm 1 describes a single simulation of SM-MCTS. T represents the MCTS tree in which
each state is represented by one node. Every node h maintains a cumulative reward sum over all
simulations through it, Xh , and a visit count nh , both initially set to 0. As depicted in Figure 1,
a matrix of references to the children is maintained at each inner node. The critical parts of the
algorithm are the updates on lines 8 and 14 and the selection on line 10. Each variant below will
describe a different way to select an action and update a node. The standard way of defining the
value to send back is RetVal(u1 , Xh , nh ) = u1 , but we discuss also RetVal(u1 , Xh , nh ) = Xh /nh ,
which is required for the formal analysis in Section 4. We denote this variant of the algorithms

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

SM-MCTS(node h)
if h ? Z then return u1 (h)
else if h ? T and ?(i, j) ? A1 (h) ? A2 (h) not previously selected then
Choose one of the previously unselected (i, j) and h0 ? T (h, i, j)
Add h0 to T
u1 ? Rollout(h0 )
Xh0 ? Xh0 + u1 ; nh0 ? nh0 + 1
Update(h, i, j, u1 )
return RetVal(u1 , Xh0 , nh0 )
(i, j) ? Select(h)
h0 ? T (h, i, j)
u1 ? SM-MCTS(h0 )
Xh ? Xh + u1 ; nh ? nh + 1
Update(h, i, j, u1 )
return RetVal(u1 , Xh , nh )
Algorithm 1: Simultaneous Move Monte Carlo Tree Search

3

with additional ?M? for mean. Algorithm 1 and the variants below are expressed from player 1?s
perspective. Player 2 does the same except using negated utilities.
3.1

Regret matching

This variant applies regret-matching [15] to the current estimated matrix game at each stage. Suppose iterations are numbered from s ? {1, 2, 3, ? ? ? } and at each iteration and each inner node h there
is a mixed strategy ? s (h) used by each player, initially set to uniform random: ? 0 (h, i) = 1/|A(h)|.
Each player maintains a cumulative regret rh [i] for having played ? s (h) instead of i ? A1 (h). The
values are initially set to 0.
On iteration s, the Select function (line 10 in Algorithm 1) first builds the player?s current strategies
from the cumulative regret. Define x+ = max(x, 0),
? s (h, a) =

X
rh+ [a]
1
+
+
rh+ [i].
if
R
>
0
oth.
,
where
R
=
sum
sum
+
|A1 (h)|
Rsum
i?A (h)

(3)

1

The strategy is computed by assigning higher weight proportionally to actions based on the regret of
having not taken them over the long-term. To ensure exploration, an ?-on-policy sampling procedure
is used choosing action i with probability ?/|A(h)| + (1 ? ?)? s (h, i), for some ? > 0.
The Updates on lines 8 and 14 add regret accumulated at the iteration to the regret tables rh . Suppose
joint action (i1 , j2 ) is sampled from the selection policy and utility u1 is returned from the recursive
call on line 12. Define x(h, i, j) = Xhij if (i, j) 6= (i1 , j2 ), or u1 otherwise. The updates to the
regret are:
?i0 ? A1 (h), rh [i0 ] ? rh [i0 ] + (x(h, i0 , j) ? u1 ).
3.2

Exp3

In Exp3 [16], a player maintains an estimate of the sum of rewards, denoted xh,i , and visit counts
nh,i for each of their actions i ? A1 . The joint action selected on line 10 is composed of an action
independently selected for each player. The probability of sampling action a in Select is
?
(1 ? ?) exp(?wh,a )
?
, where ? =
and wh,i = xh,i 1 .
? s (h, a) = P
+
|A
(h)|
|A
exp(?w
)
1
1 (h)|
h,i
i?A1 (h)

(4)

The Update after selecting actions (i, j) and obtaining a result (u1 , u2 ) updates the visits count
(nh,i ? nh,i + 1) and adds to the corresponding reward sum estimates the reward divided by the
probability that the action was played by the player (xh,i ? xh,i + u1 /? s (h, i)). Dividing the value
by the probability of selecting the corresponding action makes xh,i estimate the sum of rewards over
all iterations, not only the once where action i was selected.

4

Formal analysis

We focus on the eventual convergence to approximate NE, which allows us to make an important
simplification: We disregard the incremental building of the tree and assume we have built the
complete tree. We show that this will eventually happen with probability 1 and that the statistics
collected during the tree building phase cannot prevent the eventual convergence.
The main idea of the proof is to show that the algorithm will eventually converge close to the optimal
strategy in the leaf nodes and inductively prove that it will converge also in higher levels of the tree.
In order to do that, after introducing the necessary notation, we start by analyzing the situation in
simple matrix games, which corresponds mainly to the leaf nodes of the tree. In the inner nodes of
the tree, the observed payoffs are imprecise because of the stochastic nature of the selection functions
and bias caused by exploration, but the error can be bounded. Hence, we continue with analysis of
repeated matrix games with bounded error. Finally, we compose the matrices with bounded errors in
1
In practice, we set wh,i = xh,i ?maxi0 ?A1 (h) xh,i0 since exp(xh,i ) can easily cause numerical overflows.
This reformulation computes the same values as the original algorithm but is more numerically stable.

4

a multi-stage setting to prove convergence guarantees of SM-MCTS. Any proofs that are omitted in
the paper are included in the appendix available in the supplementary material and on http://arxiv.org
(arXiv:1310.8613).
4.1

Notation and definitions

Consider a repeatedly played matrix game where at time s players 1 and 2 choose actions is and js
respectively. We will use the convention (|A1 |, |A2 |) = (m, n). Define
G(t) =

t
X

ais js ,

s=1

g(t) =

1
G(t),
t

and

Gmax (t) = max
i?A1

t
X

aijs ,

s=1

where G(t) is the cumulative payoff, g(t) is the average payoff, and Gmax is the maximum cumulative payoff over all actions, each to player 1 and at time t. We also denote gmax (t) = Gmax (t)/t
and by R(t) = Gmax (t) ? G(t) and r(t) = gmax (t) ? g(t) the cumulative and average regrets.
For actions i of player 1 and j of player 2, we denote ti , tj the number of times these actions were
chosen up to the time t and tij the number of times both of these actions has been chosen at once.
By empirical frequencies we mean the strategy profile (?
?1 (t), ?
?2 (t)) ? h0, 1im ?h0, 1in given by
the formulas ?
?1 (t, i) = ti /t, ?
?2 (t, j) = tj /t. By average strategies, we mean the strategy profile
Pt
Pt
(?
?1 (t), ?
?2 (t)) given by the formulas ?
?1 (t, i) = s=1 ?1s (i)/t, ?
?2 (t, j) = s=1 ?2s (j)/t, where ?1s ,
?2s are the strategies used at time s.
Definition 4.1. We say that a player is -Hannan-consistent if, for any payoff sequences (e.g.,
against any opponent strategy), lim supt?? , r(t) ?  holds almost surely. An algorithm A is Hannan consistent, if a player who chooses his actions based on A is -Hannan consistent.
Hannan consistency (HC) is a commonly studied property in the context of online learning in repeated (single stage) decisions. In particular, RM and variants of Exp3 has been shown to be Hannan
consistent in matrix games [15, 16]. In order to ensure that the MCTS algorithm will eventually visit
each node infinitely many times, we need the selection function to satisfy the following property.
Definition 4.2. We say that A is an algorithm with guaranteed exploration, if for players 1 and 2
both using A for action selection limt?? tij = ? holds almost surely ?(i, j) ? A1 ? A2 .
Note that most of the HC algorithms, namely RM and Exp3, guarantee exploration without any
modification. If there is an algorithm without this property, it can be adjusted the following way.
Definition 4.3. Let A be an algorithm used for choosing action in a matrix game M . For fixed
exploration parameter ? ? (0, 1) we define a modified algorithm A? as follows: In each time,
with probability (1 ? ?) run one iteration of A and with probability ? choose the action randomly
uniformly over available actions, without updating any of the variables belonging to A.
4.2

Repeated matrix games

First we show that the -Hannan consistency is not lost due to the additional exploration.
Lemma 4.4. Let A be an -Hannan consistent algorithm. Then A? is an ( + ?)-Hannan consistent
algorithm with guaranteed exploration.
In previous works on MCTS in our class of games, RM variants generally suggested using the
average strategy and Exp3 variants the empirical frequencies to obtain the strategy to be played.
The following lemma says there eventually is no difference between the two.
Lemma 4.5. As t approaches infinity, the empirical frequencies and average strategies will almost
surely be equal. That is, lim supt?? maxi?A1 |?
?1 (t, i) ? ?
?1 (t, i)| = 0 holds with probability 1.
The proof is a consequence of the martingale version of Strong Law of Large Numbers.
It is well known that two Hannan consistent players will eventually converge to NE (see [18, p. 11]
and [19]). We prove a similar result for the approximate versions of the notions.
Lemma 4.6. Let  > 0 be a real number. If both players in a matrix game with value v are -Hannan
consistent, then the following inequalities hold for the empirical frequencies almost surely:
lim sup u (br, ?
?2 (t)) ? v + 2 and lim inf u (?
?1 (t), br) ? v ? 2.
(5)
t??

t??

5

The proof shows that if the value caused by the empirical frequencies was outside of the interval
infinitely many times with positive probability, it would be in contradiction with definition of -HC.
The following corollary is than a direct consequence of this lemma.
Corollary 4.7. If both players in a matrix game are -Hannan consistent, then there almost surely
exists t0 ? N, such that for every t ? t0 the empirical frequencies and average strategies form
(4 + ?)-equilibrium for arbitrarly small ? > 0.
The constant 4 is caused by going from a pair of strategies with best responses within 2 of the game
value guaranteed by Lemma 4.6 to the approximate NE, which multiplies the distance by two.
4.3

Repeated matrix games with bounded error

After defining the repeated games with error, we present a variant of Lemma 4.6 for these games.
Definition 4.8. We define M (t) = (aij (t)) to be a game, in which if players chose actions i and
j, they receive randomized payoffs aij (t, (i1 , ...it?1 ), (j1 , ...jt?1 )). We will denote these simply
as aij (t), but in fact they are random variables with values in [0, 1] and their distribution in time
t depends on the previous choices of actions. We say that M (t) = (aij (t)) is a repeated game
with error ?, if there is a matrix game M = (aij ) and almost surely exists t0 ? N, such that
|aij (t) ? aij | < ? holds for all t ? t0 .
P
In this context, we will denote G(t) = s?{1...t} ais js (s) etc. and use tilde for the corresponding
? = P ai j etc.). Symbols v and u (?, ?) will still be used with respect
variables without errors (G(t)
s s
to M without errors. The following lemma states that even with the errors, -HC algorithms still
converge to an approximate NE of the game.
Lemma 4.9. Let  > 0 and c ? 0. If M (t) is a repeated game with error c and both players are
-Hannan consistent then the following inequalities hold almost surely:
lim sup u (br, ?
?2 ) ? v + 2(c + 1), lim inf u (?
?1 , br) ? v ? 2(c + 1)
(6)
t??

t??

and v ? (c + 1) ? lim inf g(t) ? lim sup g(t) ? v + (c + 1).
t??

(7)

t??

The proof is similar to the proof of Lemma 4.6. It needs an additional claim that if the algorithm is
-HC with respect to the observed values with errors, it still has a bounded regret with respect to the
exact values. In the same way as in the previous subsection, a direct consequence of the lemma is
the convergence to an approximate Nash equilibrium.
Theorem 4.10. Let , c > 0 be real numbers. If M (t) is a repeated game with error c and both
players are -Hannan consistent, then for any ? > 0 there almost surely exists t0 ? N, such that for
all t ? t0 the empirical frequencies form (4(c + 1) + ?)-equilibrium of the game M .
4.4

Perfect-information extensive-form games with simultaneous moves

Now we have all the necessary components to prove the main theorem.

Theorem 4.11. Let M h h?H be a game with perfect information and simultaneous moves with
maximal depth D. Then for every -Hannan consistent algorithm A with guaranteed exploration
and arbitrary small ? > 0, there almost surely exists t0 , so that the average strategies (?
?1 (t), ?
?2 (t))
form a subgame perfect

2D2 + ? -Nash equilibrium for all t ? t0 .
Once we have established the convergence of the -HC algorithms in games with errors, we can
proceed by induction. The games in the leaf nodes are simple matrix game so they will eventually
converge and they will return the mean reward values in a bounded distance from the actual value
of the game (Lemma 4.9 with c = 0). As a result, in the level just above the leaf nodes, the HC algorithms are playing a matrix game with a bounded error and by Lemma 4.9, they will also
eventually return the mean values within a bounded interval. On level d from the leaf nodes, the
errors of returned values will be in the order of d and players can gain 2d by deviating. Summing
the possible gain of deviations on each level leads to the bound in the theorem. The subgame
perfection of the equilibrium results from the fact that for proving the bound on approximation in the
whole game (i.e., in the root of the game tree), a smaller bound on approximation of the equilibrium
is proven for all subgames in the induction. The formal proof is presented in the appendix.
6

BF = 2

BF = 3

BF = 5

0.01
0.10

0.1

Exploitability

0.05

0.10

0.01
Method
RM
RMM

10

1000

10

Depth = 2

0.400

1000

t

10

Depth = 3

1000

Depth = 4

0.200
0.100

0.1

Exploitability

0.01

0.2

0.10

0.050
0.025
100

10000

100

t

10000

100

10000

Figure 2: Exploitability of strategies given by the empirical frequencies of Regret matching with
propagating values (RM) and means (RMM) for various depths and branching factors.

5

Empirical analysis

In this section, we first evaluate the influence of propagating the mean values instead of the current
sample value in MCTS to the speed of convergence to Nash equilibrium. Afterwards, we try to
assess the convergence rate of the algorithms in the worst case. In most of the experiments, we
use as the bases of the SM-MCTS algorithm Regret matching as the selection strategy, because a
superior convergence rate bound is known for this algorithm and it has been reported to be very
successful also empirically in [20]. We always use the empirical frequencies to create the evaluated
strategy and measure the exploitability of the first player?s strategy (i.e., v h0 ? u(?
?1 , br)).
5.1

Influence of propagation of the mean

The formal analysis presented in the previous section requires the algorithms to return the mean of
all the previous samples instead of the value of the current sample. The latter is generally the case in
previous works on SM-MCTS [20, 11]. We run both variants with the Regret matching algorithm on
a set of randomly generated games parameterized by depth and branching factor. Branching factor
was always the same for both players. For the following experiments, the utility values are randomly
selected uniformly from interval h0, 1i. Each experiment uses 100 random games and 100 runs of
the algorithm.
Figure 2 presents how the exploitability of the strategies produced by Regret matching with propagation of the mean (RMM) and current sample value (RM) develops with increasing number of
iterations. Note that both axes are in logarithmic scale. The top graph is for depth of 2, different branching factors (BF) and ? ? {0.05, 0.1, 0.2}. The bottom one presents different depths for
BF = 2. The results show that both methods converge to the approximate Nash equilibrium of the
game. RMM converges slightly slower in all cases. The difference is very small in small games, but
becomes more apparent in games with larger depth.
5.2

Empirical convergence rate

Although the formal analysis guarantees the convergence to an -NE of the game, the rate of the convergence is not given. Therefore, we give an empirical analysis of the convergence and specifically
focus on the cases that reached the slowest convergence from a set of evaluated games.
7

Exploitability

0.8000
0.4000
0.2000
0.1000
0.0500
0.0250
0.0125

WC_RM

WC_RMM

Method
Exp3
Exp3M
RM
RMM

1e+02

1e+04

1e+06

1e+02

t

1e+04

1e+06

Figure 3: The games with maximal exploitability after 1000 iterations with RM (left) and RMM
(right) and the corresponding exploitabililty for all evaluated methods.
We have performed a brute force search through all games of depth 2 with branching factor 2 and
utilities form the set {0, 0.5, 1}. We made 100 runs of RM and RMM with exploration set to ? =
0.05 for 1000 iterations and computed the mean exploitability of the strategy. The games with the
highest exploitability for each method are presented in Figure 3. These games are not guaranteed to
be the exact worst case, because of possible error caused by only 100 runs of the algorithm, but they
are representatives of particularly difficult cases for the algorithms. In general, the games that are
most difficult for one method are difficult also for the other. Note that we systematically searched
also for games in which RMM performs better than RM, but this was never the case with sufficient
number of runs of the algorithms in the selected games.
Figure 3 shows the convergence of RM and Exp3 with propagating the current sample values and
the mean values (RMM and Exp3M) on the empirically worst games for the RM variants. The RM
variants converge to the minimal achievable values (0.0119 and 0.0367) after a million iterations.
This values corresponds exactly to the exploitability of the optimal strategy combined with the uniform exploration with probability 0.05. The Exp3 variants most likely converge to the same values,
however, they did not fully make it in the first million iterations in WC RM. The convergence rate of
all the variants is similar and the variants with propagating means always converge a little slower.

6

Conclusion

We present the first formal analysis of convergence of MCTS algorithms in zero-sum extensive-form
games with perfect information and simultaneous moves. We show that any -Hannan consistent
algorithm can be used to create a MCTS algorithm that provably converges to an approximate Nash
equilibrium of the game. This justifies the usage of the MCTS as an approximation algorithm for
this class of games from the perspective of algorithmic game theory. We complement the formal
analysis with experimental evaluation that shows that other MCTS variants for this class of games,
which are not covered by the proof, also converge to the approximate NE of the game. Hence, we
believe that the presented proofs can be generalized to include these cases as well. Besides this, we
will focus our future research on providing finite time convergence bounds for these algorithms and
generalizing the results to more general classes of extensive-form games with imperfect information.
Acknowledgments
This work is partially funded by the Czech Science Foundation (grant no. P202/12/2054), the Grant
Agency of the Czech Technical University in Prague (grant no. OHK3-060/12), and the Netherlands
Organisation for Scientific Research (NWO) in the framework of the project Go4Nature, grant number 612.000.938. The access to computing and storage facilities owned by parties and projects contributing to the National Grid Infrastructure MetaCentrum, provided under the programme ?Projects
of Large Infrastructure for Research, Development, and Innovations? (LM2010005) is appreciated.
8

References
[1] Manish Jain, Dmytro Korzhyk, Ondrej Vanek, Vincent Conitzer, Michal Pechoucek, and Milind Tambe. A
double oracle algorithm for zero-sum security games. In Tenth International Conference on Autonomous
Agents and Multiagent Systems (AAMAS 2011), pages 327?334, 2011.
[2] Michael Johanson, Nolan Bard, Neil Burch, and Michael Bowling. Finding optimal abstract strategies in
extensive-form games. In Proceedings of the Twenty-Sixth Conference on Artificial Intelligence (AAAI12), pages 1371?1379, 2012.
[3] S. M. Ross. Goofspiel ? the game of pure strategy. Journal of Applied Probability, 8(3):621?625, 1971.
[4] Glenn C. Rhoads and Laurent Bartholdi. Computer solution to the game of pure strategy. Games,
3(4):150?156, 2012.
[5] Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In In Proceedings of the Eleventh International Conference on Machine Learning (ICML-1994), pages 157?163.
Morgan Kaufmann, 1994.
[6] M. Genesereth and N. Love. General game-playing: Overview of the AAAI competition. AI Magazine,
26:62?72, 2005.
[7] Michael Buro. Solving the Oshi-Zumo game. In Proceedings of Advances in Computer Games 10, pages
361?366, 2003.
[8] Abdallah Saffidine, Hilmar Finnsson, and Michael Buro. Alpha-beta pruning for games with simultaneous
moves. In Proceedings of the Thirty-Second Conference on Artificial Intelligence (AAAI-12), pages 556?
562, 2012.
[9] Branislav Bosansky, Viliam Lisy, Jiri Cermak, Roman Vitek, and Michal Pechoucek. Using double-oracle
method and serialized alpha-beta search for pruning in simultaneous moves games. In Proceedings of the
Twenty-Third International Joint Conference on Artificial Intelligence (IJCAI), pages 48?54, 2013.
[10] H. Finnsson and Y. Bj?ornsson. Simulation-based approach to general game-playing. In The Twenty-Third
AAAI Conference on Artificial Intelligence, pages 259?264. AAAI Press, 2008.
[11] Olivier Teytaud and S?ebastien Flory. Upper confidence trees with short term partial information. In
Applications of Eolutionary Computation (EvoApplications 2011), Part I, volume 6624 of LNCS, pages
153?162, Berlin, Heidelberg, 2011. Springer-Verlag.
[12] Pierre Perick, David L. St-Pierre, Francis Maes, and Damien Ernst. Comparison of different selection
strategies in monte-carlo tree search for the game of Tron. In Proceedings of the IEEE Conference on
Computational Intelligence and Games (CIG), pages 242?249, 2012.
[13] Hilmar Finnsson. Simulation-Based General Game Playing. PhD thesis, Reykjavik University, 2012.
[14] L. Kocsis and C. Szepesv?ari. Bandit-based Monte Carlo planning. In 15th European Conference on
Machine Learning, volume 4212 of LNCS, pages 282?293, 2006.
[15] S. Hart and A. Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica,
68(5):1127?1150, 2000.
[16] Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed
bandit problem. SIAM Journal on Computing, 32(1):48?77, 2002.
[17] M. Shafiei, N. R. Sturtevant, and J. Schaeffer. Comparing UCT versus CFR in simultaneous games. In
Proceeding of the IJCAI Workshop on General Game-Playing (GIGA), pages 75?82, 2009.
[18] Kevin Waugh. Abstraction in large extensive games. Master?s thesis, University of Alberta, 2009.
[19] A. Blum and Y. Mansour. Learning, regret minimization, and equilibria. In Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay V. Vazirani, editors, Algorithmic Game Theory, chapter 4. Cambridge
University Press, 2007.
[20] Marc Lanctot, Viliam Lis?y, and Mark H.M. Winands. Monte Carlo tree search in simultaneous move
games with applications to Goofspiel. In Workshop on Computer Games at IJCAI, 2013.

9

"
5053,"Graphical Models for Recovering Probabilistic
and Causal Queries from Missing Data

Karthika Mohan and Judea Pearl
Cognitive Systems Laboratory
Computer Science Department
University of California, Los Angeles, CA 90024
{karthika,judea}@cs.ucla.edu

Abstract
We address the problem of deciding whether a causal or probabilistic query
is estimable from data corrupted by missing entries, given a model of missingness process. We extend the results of Mohan et al. [2013] by presenting more general conditions for recovering probabilistic queries of the form
P (y|x) and P (y, x) as well as causal queries of the form P (y|do(x)). We
show that causal queries may be recoverable even when the factors in their
identifying estimands are not recoverable. Specifically, we derive graphical
conditions for recovering causal effects of the form P (y|do(x)) when Y and
its missingness mechanism are not d-separable. Finally, we apply our results to problems of attrition and characterize the recovery of causal effects
from data corrupted by attrition.

1

Introduction

All branches of experimental science are plagued by missing data. Improper handling of
missing data can bias outcomes and potentially distort the conclusions drawn from a study.
Therefore, accurate diagnosis of the causes of missingness is crucial for the success of any research. We employ a formal representation called ?Missingness Graphs? (m-graphs, for short)
to explicitly portray the missingness process as well as the dependencies among variables in
the available dataset (Mohan et al. [2013]). Apart from determining whether recoverability is feasible namely, whether there exists any theoretical impediment to estimability of
queries of interest, m-graphs can also provide a means for communication and refinement
of assumptions about the missingness process. Furthermore, m-graphs permit us to detect
violations in modeling assumptions even when the dataset is contaminated with missing
entries (Mohan and Pearl [2014]).
In this paper, we extend the results of Mohan et al. [2013] by presenting general conditions
under which probabilistic queries such as joint and conditional distributions can be recovered. We show that causal queries of the type P (y|do(x)) can be recovered even when the
associated probabilistic relations such as P (y, x) and P (y|x) are not recoverable. In particular, causal effects may be recoverable even when Y is not separable from its missingness
mechanism. Finally, we apply our results to recover causal effects when the available dataset
is tainted by attrition.
This paper is organized as follows. Section 2 provides an overview of missingness graphs
and reviews the notion of recoverability i.e. obtaining consistent estimates of a query,
given a dataset and an m-graph. Section 3 refines the sequential factorization theorem
presented in Mohan et al. [2013] and extends its applicability to a wider range of problems
in which missingness mechanisms may influence each other. In section 4, we present general
1

Sex (S)

Qualifcation (Q)
RQ
Q*

U
Latent Variable

RI
Missingness Mechanism
of Income
Experience (X)

Income (I)

I*
Proxy variable for Income

Figure 1: Typical m-graph where Vo = {S, X}, Vm = {I, Q}, V ? = {I ? , Q? }, R = {Ri , Rq }
and U is the latent common cause. Members of Vo and Vm are represented by full and hollow
circles respectively. The associated missingness process and assumptions are elaborated in
appendix 10.1.
algorithms to recover joint distributions from the class of problems for which sequential
factorization theorem fails. In section 5, we introduce new graphical criteria that preclude
recoverability of joint and conditional distributions. In section 6, we discuss recoverability
of causal queries and show that unlike probabilistic queries, P (y|do(x)) may be recovered
even when Y and its missingness mechanism (Ry ) are not d-separable. In section 7, we
demonstrate how we can apply our results to problems of attrition in which missingness is a
severe obstacle to sound inferences. Related works are discussed in section 8 and conclusions
are drawn in section 9. Proofs of all theoretical results in this paper are provided in the
appendix.

2

Missingness Graph and Recoverability

Missingness graphs as discussed below was first defined in Mohan et al. [2013] and we adopt
the same notations. Let G(V, E) be the causal DAG where V = V ? U ? V ? ? R. V is the
set of observable nodes. Nodes in the graph correspond to variables in the data set. U is
the set of unobserved nodes (also called latent variables). E is the set of edges in the DAG.
We use bi-directed edges as a shorthand notation to denote the existence of a U variable
as common parent of two variables in V ? R. V is partitioned into Vo and Vm such that
Vo ? V is the set of variables that are observed in all records in the population and Vm ? V
is the set of variables that are missing in at least one record. Variable X is termed as fully
observed if X ? Vo , partially observed if X ? Vm and substantive if X ? Vo ? Vm . Associated
with every partially observed variable Vi ? Vm are two other variables Rvi and Vi? , where
Vi? is a proxy variable that is actually observed, and Rvi represents the status of the causal
mechanism responsible for the missingness of Vi? ; formally,

vi
if rvi = 0
(1)
vi? = f (rvi , vi ) =
m
if rvi = 1
V ? is the set of all proxy variables and R is the set of all causal mechanisms that are
responsible for missingness. R variables may not be parents of variables in V ? U . We
call this graphical representation Missingness Graph (or m-graph). An example of an
m-graph is given in Figure 1 (a).We use the following shorthand. For any variable X, let
X ? be a shorthand for X = 0. For any set W ? Vm ? Vo ? R, let Wr , Wo and Wm be the
shorthand for W ? R, W ? Vo and W ? Vm respectively. Let Rw be a shorthand for RVm ?W
i.e. Rw is the set containing missingness mechanisms of all partially observed variables in
W . Note that Rw and Wr are not the same. GX and GX represent graphs formed by
removing from G all edges leaving and entering X, respectively.
A manifest distribution P (Vo , V ? , R) is the distribution that governs the available dataset.
An underlying distribution P (Vo , Vm , R) is said to be compatible with a given manifest
distribution P (Vo , V ? , R) if the latter can be obtained from the former using equation 1.
Manifest distribution Pm is compatible with a given underlying distribution Pu if ?X, X ?
2

Y
X

Y
RY

R
Z

(a)

Z

R
RX

RW

Y

Z

X
RY

Z

W

X

RX

RW

Z
W

RX

(b)

RY

R
Z

(c)

Figure 2: (a) m-graph in which P (V ) is recoverable by the sequential factorization (b) &
(c): m-graphs for which no admissible sequence exists.
Vm and Y = Vm \ X, the following equality holds true.
Pm (Rx? , Ry , X ? , Y ? , Vo ) = Pu (Rx? , Ry , X, Vo )
where Rx? denotes Rx = 0 and Ry denotes Ry = 1. Refer Appendix 10.2 for an example.
2.1

Recoverability

Given a manifest distribution P (V ? , Vo , R) and an m-graph G that depicts the missingness
process, query Q is recoverable if we can compute a consistent estimate of Q as if no data
were missing. Formally,
Definition 1 (Recoverability (Mohan et al. [2013])). Given a m-graph G, and a target
relation Q defined on the variables in V , Q is said to be recoverable in G if there exists an
algorithm that produces a consistent estimate of Q for every dataset D such that P (D) is (1)
compatible with G and (2) strictly positive1 over complete cases i.e. P (Vo , Vm , R = 0) > 0.
For an introduction to the notion of recoverability see, Pearl and Mohan [2013] and Mohan
et al. [2013].

3

Recovering Probabilistic Queries by Sequential Factorization

Mohan et al. [2013] (theorem-4) presented a sufficient condition for recovering probabilistic
queries such as joint and conditional distributions by using ordered factorizations. However,
the theorem is not applicable to certain classes of problems such as those in longitudinal
studies in which edges exist between R variables. General ordered factorization defined
below broadens the concept of ordered factorization (Mohan et al. [2013]) to include the set of
R variables. Subsequently, the modified theorem (stated below as theorem 1) will permit us
to handle cases in which R variables are contained in separating sets that d-separate partially
observed variables from their respective missingness mechanisms (example: X??Rx |Ry in
figure 2 (a)).
Definition 2 (General Ordered factorization). Given a graph G and a set O of ordered V ?R
variables Y1 < Y2 < . . . < Yk , a general ordered factorization
relative to G, denoted by f (O),
Q
is a product of conditional probabilities f (O) = i P (Yi |Xi ) where Xi ? {Yi+1 , . . . , Yn } is
a minimal set such that Yi ??({Yi+1 , . . . , Yn } \ Xi )|Xi holds in G.
Theorem 1 (Sequential Factorization ). A sufficient condition for recoverability of a relation Q defined over substantive variables is that Q be decomposable into a general ordered
factorization, or a sum of such factorizations, such that every factor Qi = P (Yi |Xi ) satisfies, (1) Yi ??(Ryi , Rxi )|Xi \ {Ryi , Rxi }, if Yi ? (Vo ? Vm ) and (2) Z ?
/ Xi and Xr ? RXm = ?
and Rz ??RXi |Xi if Yi = Rz for any Z ? Vm .
An ordered factorization that satisfies the condition in Theorem 1 is called an admissible
sequence.
The following example illustrates the use of theorem 1 for recovering the joint distribution.
Additionally, it sheds light on the need for the notion of minimality in definition 2.
1
An extension to datasets that are not strictly positive over complete cases is sometimes feasible(Mohan et al. [2013]).

3

Example 1. We are interested in recovering P (X, Y, Z) given the m-graph in Figure 2
(a). We discern from the graph that definition 2 is satisfied because: (1) P (Y |X, Z, Ry ) =
P (Y |X, Z) and (X, Z) is a minimal set such that Y ??({X, Z, Ry } \ (X, Z))|(X, Z), (2)
P (X|Ry , Z) = P (X|Ry ) and Ry is the minimal set such that X??({Ry , Z} \ Ry )|Ry
and (3) P (Z|Ry ) = P (Z) and ? is the minimal set such that Z??Ry |?. Therefore,
the order Y < X < Z < Ry induces a general ordered factorization P (X, Y, Z, Ry ) =
P (Y |X, Z)P (X|Ry )P (Z)P (Ry ). We now rewrite P (X, Y, Z) as follows:
X
X
P (Y, X, Z, Ry ) = P (Y |X, Z)P (Z)
P (X|Ry )P (Ry )
P (X, Y, Z) =
Ry

Ry

Since Y ??Ry |X, Z, Z??Rz , X??Rx |Ry , by theorem 1 we have,
X
P (X|Rx? , Ry )P (Ry )
P (X, Y, Z) = P (Y |X, Z, Rx? , Ry? , Rz? )P (Z|Rz? )
Ry

Indeed, equation 1 permits us to rewrite it as:
P (X, Y, Z) = P (Y ? |X ? , Z ? , Rx? , Ry? , Rz? )P (Z ? |Rz? )

X

Ry

P (X ? |Rx? , Ry )P (Ry )

P (X, Y, Z) is recoverable because every term in the right hand side is consistently estimable
from the available dataset.
Had we ignored the minimality requirement in definition 2 and chosen to factorize
Y < X < Z < Ry using the chain rule, we would have obtained: P (X, Y, Z, Ry ) =
P (Y |X, Z, Ry )P (X|Z, Ry )P (Z|Ry )P (Ry ) which is not admissible since X??(Rz , Rx )|Z does
not hold in the graph. In other words, existence of one admissible sequence based on an order
O of variables does not guarantee that every factorization based on O is admissible; it is for
this reason that we need to impose the condition of minimality in definition 2.
The recovery procedure presented in example 1 requires that we introduce Ry into the order.
Indeed, there is no ordered factorization over the substantive variables {X, Y, Z} that will
permit recoverability of P (X, Y, Z) in figure 2 (a). This extension of Mohan et al. [2013]
thus permits the recovery of probabilistic queries from problems in which the missingness
mechanisms interact with one another.

4

Recoverability in the Absence of an Admissible Sequence

Mohan et al. [2013] presented a theorem (refer appendix 10.4) that stated the necessary and
sufficient condition for recovering the joint distribution for the class of problems in which the
parent set of every R variable is a subset of Vo ?Vm . In contrast to Theorem 1, their theorem
can handle problems for which no admissible sequence exists. The following theorem gives a
generalization and is applicable to any given semi-markovian model (for example, m-graphs
in figure 2 (b) & (c)). It relies on the notion of collider path and two new subsets, R(part) :
the partitions of R variables and M b(R(i) ): substantive variables related to R(i) , which we
will define after stating the theorem.
Theorem 2. Given an m-graph G in which no element in Vm is either a neighbor of its
missingness mechanism or connected to its missingness mechanism by a collider path, P (V )
is recoverable if no M b(R(i) ) contains a partially observed variable X such that Rx ? R(i)
i.e. ?i, R(i) ? RM b(R(i) ) = ?. Moreover, if recoverable, P (V ) is given by,
P (V, R = 0)
P (V ) = Q
(i) = 0|M b(R(i) ), R
P
(R
M b(R(i) ) = 0)
i
In theorem 2:
(i) collider path p between any two nodes X and Y is a path in which every intermediate
node is a collider. Example, X ? Z < ?? > Y .
(ii) Rpart = {R(1) , R(2) , ...R(N ) } are partitions of R variables such that for every element
Rx and Ry belonging to distinct partitions, the following conditions hold true: (i) Rx and
4

Ry are not neighbors and (ii) Rx and Ry are not connected by a collider path. In figure 2
(b): Rpart = {R(1) , R(2) } where R(1) = {Rw , Rz }, R(2) = {Rx , Ry }
(iii) M b(R(i) ) is the markov blanket of R(i) comprising of all substantive variables that are
either neighbors or connected to variables in R(i) by a collider path (Richardson [2003]). In
figure 2 (b): M b(R(1) ) = {X, Y } and M b(R(2) ) = {Z, W }.
Appendix 10.6 demonstrates how theorem 2 leads to the recoverability of P (V ) in figure 2,
to which theorems in Mohan et al. [2013] do not apply.
The following corollary yields a sufficient condition for recovering the joint distribution from
the class of problems in which no bi-directed edge exists between variables in sets R and
Vo ? Vm (for example, the m-graph described in Figure 2 (c)). These problems form a subset
of the class of problems covered in theorem 2. Subset P asub (R(i) ) used in the corollary is
the set of all substantive variables that are parents of variables in R(i) . In figure 2 (b):
P asub (R(1) ) = ? and P asub (R(2) ) = {Z, W }.
Corollary 1. Let G be an m-graph such that (i) ?X ? Vm ? Vo , no latent variable is a
common parent of X and any member of R, and (ii) ?Y ? Vm , Y is not a parent of Ry . If
?i, P asub (R(i) ) does not contain a partially observed variables whose missing mechanism is
in R(i) i.e. R(i) ? RP asub (R(i) ) = ?, then P (V ) is recoverable and is given by,
P (v) =

5

Q

i

P (R=0,V )
P (R(i) =0|P asub (R(i) ),RP asub (R(i) ) =0)

Non-recoverability Criteria for Joint and Conditional
Distributions

Up until now, we dealt with sufficient conditions for recoverability. It is important however
to supplement these results with criteria for non-recoverability in order to alert the user to
the fact that the available assumptions are insufficient to produce a consistent estimate of
the target query. Such criteria have not been treated formally in the literature thus far. In
the following theorem we introduce two graphical conditions that preclude recoverability.
Theorem 3 (Non-recoverability of P (V )). Given a semi-markovian model G, the following
conditions are necessary for recoverability of the joint distribution:
(i) ?X ? Vm , X and Rx are not neighbors and
(ii) ?X ? Vm , there does not exist a path from X to Rx in which every intermediate node
is both a collider and a substantive variable.
In the following corollary, we leverage theorem 3 to yield necessary conditions for recovering
conditional distributions.
Corollary 2. [Non-recoverability of P (Y |X)] Let X and Y be disjoint subsets of substantive
variables. P (Y |X) is non-recoverable in m-graph G if one of the following conditions is true:
(1) Y and Ry are neighbors
(2) G contains a collider path p connecting Y and Ry such that all intermediate nodes in p
are in X.

6

Recovering Causal Queries

Given a causal query and a causal bayesian network a complete algorithm exists for deciding
whether the query is identifiable or not (Shpitser and Pearl [2006]). Obviously, a query that
is not identifiable in the substantive model is not recoverable from missing data. Therefore,
a necessary condition for recoverability of a causal query is its identifiability which we will
assume in the rest of our discussion.
Definition 3 (Trivially Recoverable Query). A causal query Q is said to be trivially recoverable given an m-graph G if it has an estimand (in terms of substantive variables) in which
every factor is recoverable.
5

Ry

W

Z

Y

Figure 3: m-graph in which Y and Ry are not separable but still P (Y |do(Z)) is recoverable.
Classes of problems that fall into the MCAR (Missing Completely At Random) and MAR
(Missing At Random) category are much discussed in the literature ((Rubin [1976])) because in such categories probabilistic queries are recoverable by graph-blind algorithms. An
immediate but important implication of trivial recoverability is that if data are MAR or
MCAR and the query is identifiable, then it is also recoverable by model-blind algorithms.
Example 2. In the gender wage-gap study example in Figure 1 (a), the effect of sex on
income, P (I|do(S)), is identifiable and is given by P (I|S). By theorem 2, P (S, X, Q, I) is
recoverable. Hence P (I|do(S)) is recoverable.
6.1

Recovering P (y|do(z)) when Y and Ry are inseparable

The recoverability of P (V ) hinges on the separability of a partially observed variable from its
missingness mechanism (a condition established in theorem 3). Remarkably, causal queries
may circumvent this requirement. The following example demonstrates that P (y|do(z)) is
recoverable even when Y and Ry are not separable.
P
Example 3. Examine Figure 3. By backdoor criterion, P (y|do(z)) = w P (y|z, w)P (w).
One might be tempted to conclude that the causal relation is non-recoverable because
P (w, z, y) is non-recoverable (by theorem 2) and P (y|z, w) is not recoverable (by corollary
2). However, P (y|do(z)) is recoverable as demonstrated below:
X
P (y|do(z), w, Ry? )P (w|do(z), Ry? )
(2)
P (y|do(z)) = P (y|do(z), Ry? ) =
P (y|do(z), w, Ry? )
P (w|do(z), Ry? )

=
=

w
?
P (y|z, w, Ry ) (by Rule-2 of do-calculus
P (w|Ry? ) (by Rule-3 of do-calculus) )

(Pearl [2009]))

(3)
(4)

Substituting (3) and (4) in (2) we get:
X
X
P (y|z, w, Ry? )P (w|Ry? ) =
P (y ? |z, w, Ry? )P (w|Ry? )
P (y|do(z)) =
w

w

The recoverability of P (y|do(z)) in the previous example follows from the notion of d*separability and dormant independence [Shpitser and Pearl, 2008].
Definition 4 (d? -separation (Shpitser and Pearl [2008])). Let G be a causal diagram. Variable sets X, Y are d? -separated in G given Z, W (written X ?w Y |Z), if we can find sets
Z, W , such that X ? Y |Z in Gw , and P (y, x|z, do(w)) is identifiable.
Definition 5 (Inducing path (Verma and Pearl [1991])). An path p between X and Y is
called inducing path if every node on the path is a collider and an ancestor of either X or
Y.
Theorem 4. Given an m-graph in which |Vm | = 1 and Y and Ry are connected by an
inducing path, P (y|do(x)) is recoverable if there exists Z, W such that Y ?w Ry |Z and for
W = W \ X, the following conditions hold:
(1) Y ??W1 |X, Z in GX,W1 and
(2) P (W1 , Z|do(X)) and P (Y |do(W1 ), do(X), Z, R? y) are identifiable.
Moreover, if recoverable then,
P
P (y|do(x)) = W1 ,Z P (Y |do(W ), do(X), Z, Ry? )P (Z, W1 |do(X))

We can quickly conclude that P (y|do(z)) is recoverable in the m-graph in figure 3 by verifying
that the conditions in theorem 4 hold in the m-graph.
6

X

Y

Z

X

RY

Y

Z

RY

(b)

(a)

Figure 4: (a) m-graphs in which P (y|do(x)) is not recoverable (b) m-graphs in which
P (y|do(x)) is recoverable.

7

Attrition

Attrition (i.e. participants dropping out from a study/experiment), is a ubiquitous phenomenon, especially in longitudinal studies. In this section, we shall discuss a special case
of attrition called ?Simple Attrition? (Garcia [2013]). In this problem, a researcher conducts
a randomized trial, measures a set of variables (X,Y,Z) and obtains a dataset where outcome
(Y) is corrupted by missing values (due to attrition). Clearly, due to randomization, the
effect of treatment (X) on outcome (Y), P (y|do(x)), is identifiable and is given by P (Y |X).
We shall now demonstrate the usefulness of our previous discussion in recovering P (y|do(x)).
Typical attrition problems are depicted in figure 4.PIn Figure 4 (b) we can apply theorem 1
to recover P (y|do(x)) as given below: P (Y |X) = Z P (Y ? |X, Z, Ry? )P (Z|X). In Figure 4
(a), we observe that Y and Ry are connected by a collider path. Therefore by corollary 2,
P (Y |X) is not recoverable; hence P (y|do(x)) is also not recoverable.
7.1

Recovering Joint Distributions under simple attrition

The following theorem yields the necessary and sufficient condition for recovering joint distributions from semi-markovian models with a single partially observed variable i.e. |Vm | = 1
which includes models afflicted by simple attrition.
Theorem 5. Let Y ? Vm and |Vm | = 1. P (V ) is recoverable in m-graph G if and only
if Y and Ry are not neighbors and Y and Ry are not connected by a path in which all
intermediate nodes are colliders. If both conditions are satisfied, then P (V ) is given by,
P (V ) = P (Y |VO , Ry = 0)P (VO )
7.2

Recovering Causal Effects under Simple Attrition

Theorem 6. P (y|do(x)) is recoverable in the simple attrition case (with one partially observed variable) if and only if Y and Ry are neither neighbors nor connected by an inducing
path. Moreover, if recoverable,
X
P (Y |X) =
P (Y ? |X, Z, Ry? )P (Z|X)
(5)
z

where Z is the separating set that d-separates Y from Ry .
These results rectify prevailing opinion in the available literature. For example, according
to Garcia [2013] (Theorem-3), a necessary condition for non-recoverability of causal effect
under simple attrition is that X be an ancestor of Ry . In Figure 4 (a), X is not an ancestor
of Ry and still P (Y |X) is non-recoverable ( due to the collider path between Y and Ry ).

8

Related Work

Deletion based methods such as listwise deletion that are easy to understand as well as
implement, guarantee consistent estimates only for certain categories of missingness such as
MCAR (Rubin [1976]). Maximum Likelihood method is known to yield consistent estimates
under MAR assumption; expectation maximization algorithm and gradient based algorithms
are widely used for searching for ML estimates under incomplete data (Lauritzen [1995],
Dempster et al. [1977], Darwiche [2009], Koller and Friedman [2009]). Most work in machine
learning assumes MAR and proceeds with ML or Bayesian inference. However, there are
exceptions such as recent work on collaborative filtering and recommender systems which
7

develop probabilistic models that explicitly incorporate missing data mechanism (Marlin
et al. [2011], Marlin and Zemel [2009], Marlin et al. [2007]).
Other methods for handling missing data can be classified into two: (a) Inverse Probability
Weighted Methods and (b) Imputation based methods (Rothman et al. [2008]). Inverse
Probability Weighing methods analyze and assign weights to complete records based on
estimated probabilities of completeness (Van der Laan and Robins [2003], Robins et al.
[1994]). Imputation based methods substitute a reasonable guess in the place of a missing
value (Allison [2002]) and Multiple Imputation (Little and Rubin [2002]) is a widely used
imputation method.
Missing data is a special case of coarsened data and data are said to be coarsened at
random (CAR) if the coarsening mechanism is only a function of the observed data (Heitjan
and Rubin [1991]). Robins and Rotnitzky [1992] introduced a methodology for parameter
estimation from data structures for which full data has a non-zero probability of being fully
observed and their methodology was later extended to deal with censored data in which
complete data on subjects are never observed (Van Der Laan and Robins [1998]).
The use of graphical models for handling missing data is a relatively new development.
Daniel et al. [2012] used graphical models for analyzing missing information in the form of
missing cases (due to sample selection bias). Attrition is a common occurrence in longitudinal studies and arises when subjects drop out of the study (Twisk and de Vente [2002],
Shadish [2002]) and Garcia [2013] analysed the problem of attrition using causal graphs.
Thoemmes and Rose [2013] cautioned the practitioner that contrary to popular belief, not
all auxiliary variables reduce bias. Both Garcia [2013] and Thoemmes and Rose [2013]
associate missingness with a single variable and interactions among several missingness
mechanisms are unexplored.
Mohan et al. [2013] employed a formal representation called Missingness Graphs to depict
the missingness process, defined the notion of recoverability and derived conditions under
which queries would be recoverable when datasets are categorized as Missing Not At Random
(MNAR). Tests to detect misspecifications in the m-graph are discussed in Mohan and Pearl
[2014].

9

Conclusion

Graphical models play a critical role in portraying the missingness process, encoding and
communicating assumptions about missingness and deciding recoverability given a dataset
afflicted with missingness. We presented graphical conditions for recovering joint and conditional distributions and sufficient conditions for recovering causal queries. We exemplified
the recoverability of causal queries of the form P (y|do(x)) despite the existence of an inseparable path between Y and Ry , which is an insurmountable obstacle to the recovery of
P(Y). We applied our results to problems of attrition and presented necessary and sufficient
graphical conditions for recovering causal effects in such problems.

Acknowledgement
This paper has benefited from discussions with Ilya Shpitser. This research was supported
in parts by grants from NSF #IIS1249822 and #IIS1302448, and ONR #N00014-13-1-0153
and #N00014-10-1-0933.

References
P.D. Allison. Missing data series: Quantitative applications in the social sciences, 2002.
R.M. Daniel, M.G. Kenward, S.N. Cousens, and B.L. De Stavola. Using causal diagrams to guide
analysis in missing data problems. Statistical Methods in Medical Research, 21(3):243?256, 2012.
A Darwiche. Modeling and reasoning with Bayesian networks. Cambridge University Press, 2009.

8

A.P. Dempster, N.M. Laird, and D.B. Rubin. Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistical Society. Series B (Methodological), pages 1?38,
1977.
F. M. Garcia. Definition and diagnosis of problematic attrition in randomized controlled experiments. Working paper, April 2013. Available at SSRN: http://ssrn.com/abstract=2267120.
D.F. Heitjan and D.B. Rubin. Ignorability and coarse data. The Annals of Statistics, pages 2244?
2253, 1991.
D Koller and N Friedman. Probabilistic graphical models: principles and techniques. 2009.
S L Lauritzen. The em algorithm for graphical association models with missing data. Computational
Statistics & Data Analysis, 19(2):191?201, 1995.
R.J.A. Little and D.B. Rubin. Statistical analysis with missing data. Wiley, 2002.
B.M. Marlin and R.S. Zemel. Collaborative prediction and ranking with non-random missing data.
In Proceedings of the third ACM conference on Recommender systems, pages 5?12. ACM, 2009.
B.M. Marlin, R.S. Zemel, S. Roweis, and M. Slaney. Collaborative filtering and the missing at
random assumption. In UAI, 2007.
B.M. Marlin, R.S. Zemel, S.T. Roweis, and M. Slaney. Recommender systems: missing data and
statistical model estimation. In IJCAI, 2011.
K Mohan and J Pearl. On the testability of models with missing data. Proceedings of AISTAT,
2014.
K Mohan, J Pearl, and J Tian. Graphical models for inference with missing data. In Advances in
Neural Information Processing Systems 26, pages 1277?1285. 2013.
J. Pearl. Causality: models, reasoning and inference. Cambridge Univ Press, New York, 2009.
J Pearl and K Mohan.
Recoverability and testability of missing data:
Introduction and summary of results.
Technical Report R-417, UCLA, 2013.
Available at
http://ftp.cs.ucla.edu/pub/stat ser/r417.pdf.
Thomas Richardson. Markov properties for acyclic directed mixed graphs. Scandinavian Journal
of Statistics, 30(1):145?157, 2003.
J M Robins and A Rotnitzky. Recovery of information and adjustment for dependent censoring
using surrogate markers. In AIDS Epidemiology, pages 297?331. Springer, 1992.
J M Robins, A Rotnitzky, and L P Zhao. Estimation of regression coefficients when some regressors
are not always observed. Journal of the American Statistical Association, 89(427):846?866, 1994.
K J Rothman, S Greenland, and T L Lash. Modern epidemiology. Lippincott Williams & Wilkins,
2008.
D.B. Rubin. Inference and missing data. Biometrika, 63:581?592, 1976.
W R Shadish. Revisiting field experimentation: field notes for the future. Psychological methods, 7
(1):3, 2002.
I Shpitser and J Pearl. Identification of conditional interventional distributions. In Proceedings of
the Twenty-Second Conference on Uncertainty in Artificial Intelligence, pages 437?444. 2006.
I Shpitser and J Pearl. Dormant independence. In AAAI, pages 1081?1087, 2008.
F. Thoemmes and N. Rose. Selection of auxiliary variables in missing data problems: Not all
auxiliary variables are created equal. Technical Report R-002, Cornell University, 2013.
J Twisk and W de Vente. Attrition in longitudinal studies: how to deal with missing data. Journal
of clinical epidemiology, 55(4):329?337, 2002.
M J Van Der Laan and J M Robins. Locally efficient estimation with current status data and
time-dependent covariates. Journal of the American Statistical Association, 93(442):693?701,
1998.
M.J. Van der Laan and J.M. Robins. Unified methods for censored longitudinal data and causality.
Springer Verlag, 2003.
T.S Verma and J Pearl. Equivalence and synthesis of causal models. In Proceedings of the Sixth
Conference in Artificial Intelligence, pages 220?227. Association for Uncertainty in AI, 1991.

9

"
2606,"Sparse Feature Learning for Deep Belief Networks

Marc?Aurelio Ranzato1
Y-Lan Boureau2,1
Yann LeCun1
1
Courant Institute of Mathematical Sciences, New York University
2
INRIA Rocquencourt
{ranzato,ylan,yann@courant.nyu.edu}

Abstract
Unsupervised learning algorithms aim to discover the structure hidden in the data,
and to learn representations that are more suitable as input to a supervised machine
than the raw input. Many unsupervised methods are based on reconstructing the
input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on
approximating density by stochastically reconstructing the input from the representation. We describe a novel and efficient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machine
trained probabilistically, namely a Restricted Boltzmann Machine. We propose a
simple criterion to compare and select different unsupervised machines based on
the trade-off between the reconstruction error and the information content of the
representation. We demonstrate this method by extracting features from a dataset
of handwritten numerals, and from a dataset of natural image patches. We show
that by stacking multiple levels of such machines and by training sequentially,
high-order dependencies between the input observed variables can be captured.

1

Introduction

One of the main purposes of unsupervised learning is to produce good representations for data, that
can be used for detection, recognition, prediction, or visualization. Good representations eliminate
irrelevant variabilities of the input data, while preserving the information that is useful for the ultimate task. One cause for the recent resurgence of interest in unsupervised learning is the ability
to produce deep feature hierarchies by stacking unsupervised modules on top of each other, as proposed by Hinton et al. [1], Bengio et al. [2] and our group [3, 4]. The unsupervised module at one
level in the hierarchy is fed with the representation vectors produced by the level below. Higherlevel representations capture high-level dependencies between input variables, thereby improving
the ability of the system to capture underlying regularities in the data. The output of the last layer in
the hierarchy can be fed to a conventional supervised classifier.
A natural way to design stackable unsupervised learning systems is the encoder-decoder
paradigm [5]. An encoder transforms the input into the representation (also known as the code
or the feature vector), and a decoder reconstructs the input (perhaps stochastically) from the representation. PCA, Auto-encoder neural nets, Restricted Boltzmann Machines (RBMs), our previous
sparse energy-based model [3], and the model proposed in [6] for noisy overcomplete channels are
just examples of this kind of architecture. The encoder/decoder architecture is attractive for two reasons: 1. after training, computing the code is a very fast process that merely consists in running the
input through the encoder; 2. reconstructing the input with the decoder provides a way to check that
the code has captured the relevant information in the data. Some learning algorithms [7] do not have
a decoder and must resort to computationally expensive Markov Chain Monte Carlo (MCMC) sampling methods in order to provide reconstructions. Other learning algorithms [8, 9] lack an encoder,
which makes it necessary to run an expensive optimization algorithm to find the code associated
with each new input sample. In this paper we will focus only on encoder-decoder architectures.
1

In general terms, we can view an unsupervised model as defining a distribution over input vectors
Y through an energy function E(Y, Z, W ):
R ??E(Y,z,W )
Z
e
P (Y |W ) = P (Y, z|W ) = R z ??E(y,z,W )
(1)
e
z
y,z

where Z is the code vector, W the trainable parameters of encoder and decoder, and ? is an arbitrary
positive constant. The energy function includes the reconstruction error, and perhaps other terms
as well. For convenience, we will omit W from the notation in the following. Training the machine
to model the input distribution is performed by finding the encoder and decoder parameters that
minimize a loss function equal to the negative log likelihood of the training data under the model.
For a single training sample Y , the loss function is
Z
Z
1
1
??E(Y,z)
L(W, Y ) = ? log e
+ log
e??E(y,z)
(2)
?
?
z
y,z
The first term is the free energy F? (Y ). Assuming that the distribution over Z is rather peaked, it
can be simpler to approximate this distribution over Z by its mode, which turns the marginalization
over Z into a minimization:
1
L (W, Y ) = E(Y, Z (Y )) + log
?
?

?

Z

e??E(y,Z

?

(y))

(3)

y

where Z ? (Y ) is the maximum likelihood value Z ? (Y ) = argminz E(Y, z), also known as the
optimal code. We can then define an energy for each input point, that measures how well it is
reconstructed by the model:
Z
1
F? (Y ) = E(Y, Z ? (Y )) = lim ? log e??E(Y,z)
(4)
???
?
z
The second term in equation 2 and 3 is called the log partition function, and can be viewed as a
penalty term for low energies. It ensures that the system produces low energy only for input vectors
that have high probability in the (true) data distribution, and produces higher energies for all other
input vectors [5]. The overall loss is the average of the above over the training set.
Regardless of whether only Z ? or the whole distribution over Z is considered, the main difficulty
with this framework is that it can be very hard to compute the gradient of the log partition function
in equation 2 or 3 with respect to the parameters W . Efficient methods shortcut the computation by
drastically and cleverly reducing the integration domain. For instance, Restricted Boltzmann Machines (RBM) [10] approximate the gradient of the log partition function in equation 2 by sampling
values of Y whose energy will be pulled up using an MCMC technique. By running the MCMC for
a short time, those samples are chosen in the vicinity of the training samples, thereby ensuring that
the energy surface forms a ravine around the manifold of the training samples. This is the basis of
the Contrastive Divergence method [10].
The role of the log partition function is merely to ensure that the energy surface is lower around
training samples than anywhere else. The method proposed here eliminates the log partition function
from the loss, and replaces it by a term that limits the volume of the input space over which the energy
surface can take a low value. This is performed by adding a penalty term on the code rather than on
the input. While this class of methods does not directly maximize the likelihood of the data, it can be
seen as a crude approximation of it. To understand the method, we first note that if for each vector
Y , there exists a corresponding optimal code Z ? (Y ) that makes the reconstruction error (or energy)
F? (Y ) zero (or near zero), the model can perfectly reconstruct any input vector. This makes the
energy surface flat and indiscriminate. On the other hand, if Z can only take a small number of
different values (low entropy code), then the energy F? (Y ) can only be low in a limited number of
places (the Y ?s that are reconstructed from this small number of Z values), and the energy cannot
be flat.
More generally, a convenient method through which flat energy surfaces can be avoided is to limit
the maximum information content of the code. Hence, minimizing the energy F? (Y ) together with
the information content of the code is a good substitute for minimizing the log partition function.
2

A popular way to minimize the information content in the code is to make the code sparse or lowdimensional [5]. This technique is used in a number of unsupervised learning methods, including
PCA, auto-encoders neural network, and sparse coding methods [6, 3, 8, 9]. In sparse methods,
the code is forced to have only a few non-zero units while most code units are zero most of the
time. Sparse-overcomplete representations have a number of theoretical and practical advantages,
as demonstrated in a number of recent studies [6, 8, 3]. In particular, they have good robustness to
noise, and provide a good tiling of the joint space of location and frequency. In addition, they are
advantageous for classifiers because classification is more likely to be easier in higher dimensional
spaces. This may explain why biology seems to like sparse representations [11]. In our context, the
main advantage of sparsity constraints is to allow us to replace a marginalization by a minimization,
and to free ourselves from the need to minimize the log partition function explicitly.
In this paper we propose a new unsupervised learning algorithm called Sparse Encoding Symmetric
Machine (SESM), which is based on the encoder-decoder paradigm, and which is able to produce
sparse overcomplete representations efficiently without any need for filter normalization [8, 12] or
code saturation [3]. As described in more details in sec. 2 and 3, we consider a loss function which
is a weighted sum of the reconstruction error and a sparsity penalty, as in many other unsupervised
learning algorithms [13, 14, 8]. Encoder and decoder are constrained to be symmetric, and share
a set of linear filters. Although we only consider linear filters in this paper, the method allows
the use of any differentiable function for encoder and decoder. We propose an iterative on-line
learning algorithm which is closely related to those proposed by Olshausen and Field [8] and by us
previously [3]. The first step computes the optimal code by minimizing the energy for the given
input. The second step updates the parameters of the machine so as to minimize the energy.
In sec. 4, we compare SESM with RBM and PCA. Following [15], we evaluate these methods by
measuring the reconstruction error for a given entropy of the code. In another set of experiments,
we train a classifier on the features extracted by the various methods, and measure the classification
error on the MNIST dataset of handwritten numerals. Interestingly, the machine achieving the best
recognition performance is the one with the best trade-off between RMSE and entropy. In sec. 5, we
compare the filters learned by SESM and RBM for handwritten numerals and natural image patches.
In sec.5.1.1, we describe a simple way to produce a deep belief net by stacking multiple levels of
SESM modules. The representational power of this hierarchical non-linear feature extraction is
demonstrated through the unsupervised discovery of the numeral class labels in the high-level code.

2

Architecture

In this section we describe a Sparse Encoding Symmetric Machine (SESM) having a set of linear filters in both encoder and decoder. However, everything can be easily extended to any other choice of
parameterized functions as long as these are differentiable and maintain symmetry between encoder
and decoder. Let us denote with Y the input defined in RN , and with Z the code defined in RM ,
where M is in general greater than N (for overcomplete representations). Let the filters in encoder
and decoder be the columns of matrix W ? RN ?M , and let the biases in the encoder and decoder
be denoted by benc ? RM and bdec ? RN , respectively. Then, encoder and decoder compute:
fenc (Y ) = W T Y + benc ,

fdec (Z) = W l(Z) + bdec

(5)

where the function l is a point-wise logistic non-linearity of the form:
l(x) = 1/(1 + exp(?gx)),

(6)

with g fixed gain. The system is characterized by an energy measuring the compatibility between
pairs of input Y and latent code Z, E(Y, Z) [16]. The lower the energy, the more compatible (or
likely) is the pair. We define the energy as:
E(Y, Z) = ?e kZ ? fenc (Y )k22 + kY ? fdec (Z)k22

(7)

During training we minimize the following loss:
L(W, Y )

= E(Y, Z) + ?s h(Z) + ?r kW k1
= ?e kZ ? fenc (Y )k22 + kY ? fdec (Z)k22 + ?s h(Z) + ?r kW k1

(8)

The first term tries to make the output of the encoder as similar as possible to the code Z. The second
term is the mean-squared error between the input Y and the reconstruction provided by the decoder.
3

The third term ensures the sparsity of the code by penalizing non zero values of code units; this term
PM
acts independently on each code unit and it is defined as h(Z) = i=1 log(1+l2 (zi )), (corresponding to a factorized Student-t prior distribution on the non linearly transformed code units [8] through
the logistic of equation 6). The last term is an L1 regularization on the filters to suppress noise and
favor more localized filters. The loss formulated in equation 8 combines terms that characterize
also other methods. For instance, the first two terms appear in our previous model [3], but in that
work, the weights of encoder and decoder were not tied and the parameters in the logistic were updated using running averages. The second and third terms are present in the ?decoder-only? model
proposed in [8]. The third term was used in the ?encoder-only? model of [7]. Besides the alreadymentioned advantages of using an encoder-decoder architecture, we point out another good feature
of this algorithm due to its symmetry. A common idiosyncrasy for sparse-overcomplete methods
using both a reconstruction and a sparsity penalty in the objective function (second and third term in
equation 8), is the need to normalize the basis functions in the decoder during learning [8, 12] with
somewhat ad-hoc technique, otherwise some of the basis functions collapse to zero, and some blow
up to infinity. Because of the sparsity penalty and the linear reconstruction, code units become tiny
and are compensated by the filters in the decoder that grow without bound. Even though the overall
loss decreases, training is unsuccessful. Unfortunately, simply normalizing the filters makes less
clear which objective function is minimized. Some authors have proposed quite expensive methods to solve this issue: by making better approximations of the posterior distribution [15], or by
using sampling techniques [17]. In this work, we propose to enforce symmetry between encoder
and decoder (through weight sharing) so as to have automatic scaling of filters. Their norm cannot
possibly be large because code units, produced by the encoder weights, would have large values as
well, producing bad reconstructions and increasing the energy (the second term in equation 7 and
8).

3

Learning Algorithm

Learning consists of determining the parameters in W , benc , and bdec that minimize the loss in
equation 8. As indicated in the introduction, the energy augmented with the sparsity constraint is
minimized with respect to the code to find the optimal code. No marginalization over code distribution is performed. This is akin to using the loss function in equation 3. However, the log partition
function term is dropped. Instead, we rely on the code sparsity constraints to ensure that the energy
surface is not flat.
Since the second term in equation 8 couples both Z and W and bdec , it is not straightforward to
minimize this energy with respect to both. On the other hand, once Z is given, the minimization
with respect to W is a convex quadratic problem. Vice versa, if the parameters W are fixed, the
optimal code Z ? that minimizes L can be computed easily through gradient descent. This suggests
the following iterative on-line coordinate descent learning algorithm:
1. for a given sample Y and parameter setting, minimize the loss in equation 8 with respect to Z by
gradient descent to obtain the optimal code Z ?
2. clamping both the input Y and the optimal code Z ? found at the previous step, do one step of
gradient descent to update the parameters.
Unlike other methods [8, 12], no column normalization of W is required. Also, all the parameters
are updated by gradient descent unlike in our previous work [3] where some parameters are updated
using a moving average.
After training, the system converges to a state where the decoder produces good reconstructions
from a sparse code, and the optimal code is predicted by a simple feed-forward propagation through
the encoder.

4

Comparative Coding Analysis

In the following sections, we mainly compare SESM with RBM in order to better understand their
differences in terms of maximum likelihood approximation, and in terms of coding efficiency and
robustness.
RBM
As explained in the introduction, RBMs minimize an approximation of the negative log
likelihood of the data under the model. An RBM is a binary stochastic symmetric machine defined
4

by an energy function of the form: E(Y, Z) = ?Z T W T Y ? bTenc Z ? bTdec Y . Although this is not
obvious at first glance, this energy can be seen as a special case of the encoder-decoder architecture
that pertains to binary data vectors and code vectors [5]. Training an RBM minimizes an approximation of the negative log likelihood loss function 2, averaged over the training set, through a gradient
descent procedure. Instead of estimating the gradient of the log partition function, RBM training
uses contrastive divergence [10], which takes random samples drawn over a limited region ? around
the training samples. The loss becomes:
X
XX
1
1
L(W, Y ) = ? log
e??E(Y,z) + log
e??E(y,z)
(9)
?
?
z
z
y??

Because of the RBM architecture, given a Y , the components of Z are independent, hence the sum
over configurations of Z can be done independently for each component of Z. Sampling y in the
neighborhood ? is performed with one, or a few alternated MCMC steps over Y , and Z. This means
that only the energy of points around training samples is pulled up. Hence, the likelihood function
takes the right shape around the training samples, but not necessarily everywhere. However, the
code vector in an RBM is binary and noisy, and one may wonder whether this does not have the
effect of surreptitiously limiting the information content of the code, thereby further minimizing the
log partition function as a bonus.
SESM
RBM and SESM have almost the same architecture because they both have a symmetric
encoder and decoder, and a logistic non-linearity on the top of the encoder. However, RBM is trained
using (approximate) maximum likelihood, while SESM is trained by simply minimizing the average
energy F? (Y ) of equation 4 with an additional code sparsity term. SESM relies on the sparsity
term to prevent flat energy surfaces, while RBM relies on an explicit contrastive term in the loss, an
approximation of the log partition function. Also, the coding strategy is very different because code
units are ?noisy? and binary in RBM, while they are quasi-binary and sparse in SESM. Features
extracted by SESM look like object parts (see next section), while features produced by RBM lack
an intuitive interpretation because they aim at modeling the input distribution and they are used in a
distributed representation.
4.1

Experimental Comparison

In the first experiment we have trained SESM, RBM, and PCA on the first 20000 digits in the
MNIST training dataset [18] in order to produce codes with 200 components. Similarly to [15] we
have collected test image codes after the logistic non linearity (except for PCA which is linear), and
we have measured the root mean square error (RMSE) and the entropy. SESM was run for different
values of the sparsity coefficient ?s in equation 8 (while
q all other parameters are left unchanged, see
1
? 2 , where Z? is the uniformly
next section for details). The RMSE is defined as ? P1N kY ? fdec (Z)k
2
quantized code produced by the encoder, P is the number of test samples, and ? is the estimated
variance of units in the input Y . Assuming to encode the (quantized) code units independently and
with the same distribution, the lower bound on the number of bits required to encode each of them
PQ
i
i
log2 PcM
, where ci is the number of counts in the i-th bin, and Q
is given by: Hc.u. = ? i=1 PcM
is the number of quantization levels. The number of bits per pixel is then equal to: M
N Hc.u. . Unlike
in [15, 12], the reconstruction is done taking the quantized code in order to measure the robustness
of the code to the quantization noise. As shown in fig. 1-C, RBM is very robust to noise in the
code because it is trained by sampling. The opposite is true for PCA which achieves the lowest
RMSE when using high precision codes, but the highest RMSE when using a coarse quantization.
SESM seems to give the best trade-off between RMSE and entropy. Fig. 1-D/F compare the features
learned by SESM and RBM. Despite the similarities in the architecture, filters look quite different
in general, revealing two different coding strategies: distributed for RBM, and sparse for SESM.
In the second experiment, we have compared these methods by means of a supervised task in order to
assess which method produces the most discriminative representation. Since we have available also
the labels in the MNIST, we have used the codes (produced by these machines trained unsupervised)
as input to the same linear classifier. This is run for 100 epochs to minimize the squared error
between outputs and targets, and has a mild ridge regularizer. Fig. 1-A/B show the result of these
experiments in addition to what can be achieved by a linear classifier trained on the raw pixel data.
Note that: 1) training on features instead of raw data improves the recognition (except for PCA
5

10 samples

100 samples

45

18

40

16

1000 samples
RAW: train
RAW: test

9

35

14

30

12

10 samples

10

100 samples

45

18

40

16

35

14

30

12

1000 samples
10

9

PCA: train
PCA: test

15

6

10

4

5

2

0
0
1
2
ENTROPY (bits/pixel)

0
0
1
2
ENTROPY (bits/pixel)

SESM: train

7

SESM: test
6

25
20

ERROR RATE %

8

RBM: test

ERROR RATE %

20

10

8

RBM: train
ERROR RATE %

25

ERROR RATE %

ERROR RATE %

ERROR RATE %

8

10
8

15

6

10

4

5

2

5

6

5

4

(A)

7

4

3
0
1
2
ENTROPY (bits/pixel)

(B)

0
0

0.2
RMSE

0.4

0
0

0.2
RMSE

0.4

3
0

0.2
RMSE

0.4

Symmetric Sparse Coding ? RBM ? PCA
0.45
PCA: quantization in 5 bins
PCA: quantization in 256 bins
RBM: quantization in 5 bins
RBM: quantization in 256 bins
Sparse Coding: quantization in 5 bins
Sparse Coding: quantization in 256 bins

0.4
0.35

RMSE

0.3
0.25
0.2
0.15
0.1

(C)

0.05
0

0.5

1
Entropy (bits/pixel)

1.5

2

(D)

(E)

(F)

(G)

(H)

Figure 1: (A)-(B) Error rate on MNIST training (with 10, 100 and 1000 samples per class) and
test set produced by a linear classifier trained on the codes produced by SESM, RBM, and PCA.
The entropy and RMSE refers to a quantization into 256 bins. The comparison has been extended
also to the same classifier trained on raw pixel data (showing the advantage of extracting features).
The error bars refer to 1 std. dev. of the error rate for 10 random choices of training datasets
(same splits for all methods). The parameter ?s in eq. 8 takes values: 1, 0.5, 0.2, 0.1, 0.05. (C)
Comparison between SESM, RBM, and PCA when quantizing the code into 5 and 256 bins. (D)
Random selection from the 200 linear filters that were learned by SESM (?s = 0.2). (E) Some pairs
of original and reconstructed digit from the code produced by the encoder in SESM (feed-forward
propagation through encoder and decoder). (F) Random selection of filters learned by RBM. (G)
Back-projection in image space of the filters learned in the second stage of the hierarchical feature
extractor. The second stage was trained on the non linearly transformed codes produced by the first
stage machine. The back-projection has been performed by using a 1-of-10 code in the second stage
machine, and propagating this through the second stage decoder and first stage decoder. The filters
at the second stage discover the class-prototypes (manually ordered for visual convenience) even
though no class label was ever used during training. (H) Feature extraction from 8x8 natural image
patches: some filters that were learned.

6

when the number of training samples is small), 2) RBM performance is competitive overall when
few training samples are available, 3) the best performance is achieved by SESM for a sparsity level
which trades off RMSE for entropy (overall for large training sets), 4) the method with the best
RMSE is not the one with lowest error rate, 5) compared to a SESM having the same error rate
RBM is more costly in terms of entropy.

5

Experiments

This section describes some experiments we have done with SESM. The coefficient ?e in equation 8
has always been set equal to 1, and the gain in the logistic have been set equal to 7 in order to achieve
a quasi-binary coding. The parameter ?s has to be set by cross-validation to a value which depends
on the level of sparsity required by the specific application.
5.1

Handwritten Digits

Fig. 1-B/E shows the result of training a SESM with ?s is equal to 0.2. Training was performed on
20000 digits scaled between 0 and 1, by setting ?r to 0.0004 (in equation 8) with a learning rate
equal to 0.025 (decreased exponentially). Filters detect the strokes that can be combined to form a
digit. Even if the code unit activation has a very sparse distribution, reconstructions are very good
(no minimization in code space was performed).
5.1.1

Hierarchical Features

A hierarchical feature extractor can be trained layer-by-layer similarly to what has been proposed
in [19, 1] for training deep belief nets (DBNs). We have trained a second (higher) stage machine
on the non linearly transformed codes produced by the first (lower) stage machine described in the
previous example. We used just 20000 codes to produce a higher level representation with just 10
components. Since we aimed to find a 1-of-10 code we increased the sparsity level (in the second
stage machine) by setting ?s to 1. Despite the completely unsupervised training procedure, the
feature detectors in the second stage machine look like digit prototypes as can be seen in fig. 1-G.
The hierarchical unsupervised feature extractor is able to capture higher order correlations among
the input pixel intensities, and to discover the highly non-linear mapping from raw pixel data to the
class labels. Changing the random initialization can sometimes lead to the discover of two different
shapes of ?9? without a unit encoding the ?4?, for instance. Nevertheless, results are qualitatively
very similar to this one. For comparison, when training a DBN, prototypes are not recovered because
the learned code is distributed among units.
5.2

Natural Image Patches

A SESM with about the same set up was trained on a dataset of 30000 8x8 natural image patches
randomly extracted from the Berkeley segmentation dataset [20]. The input images were simply
scaled down to the range [0, 1.7], without even subtracting the mean. We have considered a 2
times overcomplete code with 128 units. The parameters ?s , ?r and the learning rate were set to
0.4, 0.025, and 0.001 respectively. Some filters are localized Gabor-like edge detectors in different
positions and orientations, other are more global, and some encode the mean value (see fig. 1-H).

6

Conclusions

There are two strategies to train unsupervised machines: 1) having a contrastive term in the loss
function minimized during training, 2) constraining the internal representation in such a way that
training samples can be better reconstructed than other points in input space. We have shown that
RBM, which falls in the first class of methods, is particularly robust to channel noise, it achieves very
low RMSE and good recognition rate. We have also proposed a novel symmetric sparse encoding
method following the second strategy which: is particularly efficient to train, has fast inference,
works without requiring any withening or even mean removal from the input, can provide the best
recognition performance and trade-off between entropy/RMSE, and can be easily extended to a
hierarchy discovering hidden structure in the data. We have proposed an evaluation protocol to
compare different machines which is based on RMSE, entropy and, eventually, error rate when also
7

labels are available. Interestingly, the machine achieving the best performance in classification is the
one with the best trade-off between reconstruction error and entropy. A future avenue of work is to
understand the reasons for this ?coincidence?, and deeper connections between these two strategies.
Acknowledgments
We wish to thank Jonathan Goodman, Geoffrey Hinton, and Yoshua Bengio for helpful discussions. This work
was supported in part by NSF grant IIS-0535166 ?toward category-level object recognition?, NSF ITR-0325463
?new directions in predictive learning?, and ONR grant N00014-07-1-0535 ?integration and representation of
high dimensional data?.

References
[1] G.E. Hinton and R. R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,
313(5786):504?507, 2006.
[2] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In
NIPS, 2006.
[3] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun. Efficient learning of sparse representations with an
energy-based model. In NIPS 2006. MIT Press, 2006.
[4] Y. Bengio and Y. LeCun. Scaling learning algorithms towars ai. In D. DeCoste L. Bottou, O. Chapelle
and J. Weston, editors, Large-Scale Kernel Machines. MIT Press, 2007.
[5] M. Ranzato, Y. Boureau, S. Chopra, and Y. LeCun. A unified energy-based framework for unsupervised
learning. In Proc. Conference on AI and Statistics (AI-Stats), 2007.
[6] E. Doi, D. C. Balcan, and M. S. Lewicki. A theoretical analysis of robust coding over noisy overcomplete
channels. In NIPS. MIT Press, 2006.
[7] Y. W. Teh, M. Welling, S. Osindero, and G. E. Hinton. Energy-based models for sparse overcomplete
representations. Journal of Machine Learning Research, 4:1235?1260, 2003.
[8] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: a strategy employed by
v1? Vision Research, 37:3311?3325, 1997.
[9] D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix factorization. Nature,
401:788?791, 1999.
[10] G.E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation,
14:1771?1800, 2002.
[11] P. Lennie. The cost of cortical computation. Current biology, 13:493?497, 2003.
[12] J.F. Murray and K. Kreutz-Delgado. Learning sparse overcomplete codes for images. The Journal of
VLSI Signal Processing, 45:97?110, 2008.
[13] G.E. Hinton and R.S. Zemel. Autoencoders, minimum description length, and helmholtz free energy. In
NIPS, 1994.
[14] G.E. Hinton, P. Dayan, and M. Revow. Modeling the manifolds of images of handwritten digits. IEEE
Transactions on Neural Networks, 8:65?74, 1997.
[15] M.S. Lewicki and T.J. Sejnowski. Learning overcomplete representations. Neural Computation, 12:337?
365, 2000.
[16] Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F.J. Huang. A tutorial on energy-based learning. In
G. Bakir and al.., editors, Predicting Structured Data. MIT Press, 2006.
[17] P. Sallee and B.A. Olshausen. Learning sparse multiscale image representations. In NIPS. MIT Press,
2002.
[18] http://yann.lecun.com/exdb/mnist/.
[19] G.E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18:1527?1554, 2006.
[20] http://www.cs.berkeley.edu/projects/vision/grouping/segbench/.

8

"
5292,"Deeply Learning the Messages in Message
Passing Inference

Guosheng Lin, Chunhua Shen, Ian Reid, Anton van den Hengel
The University of Adelaide, Australia; and Australian Centre for Robotic Vision
E-mail: {guosheng.lin,chunhua.shen,ian.reid,anton.vandenhengel}@adelaide.edu.au

Abstract
Deep structured output learning shows great promise in tasks like semantic image segmentation. We proffer a new, efficient deep structured model learning
scheme, in which we show how deep Convolutional Neural Networks (CNNs)
can be used to directly estimate the messages in message passing inference for
structured prediction with Conditional Random Fields (CRFs). With such CNN
message estimators, we obviate the need to learn or evaluate potential functions
for message calculation. This confers significant efficiency for learning, since otherwise when performing structured learning for a CRF with CNN potentials it is
necessary to undertake expensive inference for every stochastic gradient iteration.
The network output dimension of message estimators is the same as the number
of classes, rather than exponentially growing in the order of the potentials. Hence
it is more scalable for cases that involve a large number of classes. We apply
our method to semantic image segmentation and achieve impressive performance,
which demonstrates the effectiveness and usefulness of our CNN message learning method.

1

Introduction

Learning deep structured models has attracted considerable research attention recently. One popular approach to deep structured model is formulating conditional random fields (CRFs) using deep
Convolutional Neural Networks (CNNs) for the potential functions. This combines the power of
CNNs for feature representation learning and of the ability for CRFs to model complex relations.
The typical approach for the joint learning of CRFs and CNNs [1, 2, 3, 4, 5], is to learn the CNN
potential functions by optimizing the CRF objective, e.g., maximizing the log-likelihood. The CNN
and CRF joint learning has shown impressive performance for semantic image segmentation.
For the joint learning of CNNs and CRFs, stochastic gradient descent (SGD) is typically applied for
optimizing the conditional likelihood. This approach requires the marginal inference for calculating
the gradient. For loopy graphs, marginal inference is generally expensive even when using approximate solutions. Given that learning the CNN potential functions typically requires a large number of
gradient iterations, repeated marginal inference would make the training intractably slow. Applying
an approximate training objective is a solution to avoid repeat inference; pseudo-likelihood learning
[6] and piecewise learning [7, 3] are examples of this kind of approach. In this work, we advocate a
new direction for efficient deep structured model learning.
In conventional CRF approaches, the final prediction is the result of inference based on the learned
potentials. However, our ultimate goal is the final prediction (not the potentials themselves), so we
propose to directly optimize the inference procedure for the final prediction. Our focus here is on
the extensively studied message passing based inference algorithms. As discussed in [8], we can
directly learn message estimators to output the required messages in the inference procedure, rather

than learning the potential functions as in conventional CRF learning approaches. With the learned
message estimators, we then obtain the final prediction by performing message passing inference.
Our main contributions are as follows:
1) We explore a new direction for efficient deep structured learning. We propose to directly learn the
messages in message passing inference as training deep CNNs in an end-to-end learning fashion.
Message learning does not require any inference step for the gradient calculation, which allows
efficient training. Furthermore, when cast as a tradiational classification task, the network output
dimension for message estimation is the same as the number of classes (K), while the network
output for general CNN potential functions in CRFs is K a , which is exponential in the order (a)
of the potentials (for example, a = 2 for pairwise potentials, a = 3 for triple-cliques, etc). Hence
CNN based message learning has significantly fewer network parameters and thus is more scalable,
especially in cases which involve a large number of classes.
2) The number of iterations in message passing inference can be explicitly taken into consideration
in the message learning procedure. In this paper, we are particularly interested in learning messages
that are able to offer high-quality CRF prediction results with only one message passing iteration,
making the message passing inference very fast.
3) We apply our method to semantic image segmentation on the PASCAL VOC 2012 dataset and
achieve impressive performance.
Related work Combining the strengths of CNNs and CRFs for segmentation has been explored in
several recent methods. Some methods resort to a simple combination of CNN classifiers and CRFs
without joint learning. DeepLab-CRF in [9] first train fully CNN for pixel classification and applies
a dense CRF [10] method as a post-processing step. Later the method in [2] extends DeepLab
by jointly learning the dense CRFs and CNNs. RNN-CRF in [1] also performs joint learning of
CNNs and the dense CRFs. They implement the mean-field inference as Recurrent Neural Networks
which facilitates the end-to-end learning. These methods usually use CNNs for modelling the unary
potentials only. The work in [3] trains CNNs to model both the unary and pairwise potentials in
order to capture contextual information. Jointly learning CNNs and CRFs has also been explored
for other applications like depth estimation [4, 11]. The work in [5] explores joint training of Markov
random fields and deep networks for predicting words from noisy images and image classification.
All these above-mentioned methods that combine CNNs and CRFs are based upon conventional
CRF approaches. They aim to jointly learn or incorporate pre-trained CNN potential functions, and
then perform inference/prediction using the potentials. In contrast, our method here directly learns
CNN message estimators for the message passing inference, rather than learning the potentials.
The inference machine proposed in [8] is relevant to our work in that it has discussed the idea of
directly learning message estimators instead of learning potential functions for structured prediction. They train traditional logistic regressors with hand-crafted features as message estimators.
Motivated by the tremendous success of CNNs, we propose to train deep CNNs based message estimators in an end-to-end learning style without using hand-crafted features. Unlike the approach in
[8] which aims to learn variable-to-factor message estimators, our proposed method aims to learn
the factor-to-variable message estimators. Thus we are able to naturally formulate the variable
marginals ? which is the ultimate goal for CRF inference ? as the training objective (see Sec. 3.3).
The approach in [12] jointly learns CNNs and CRFs for pose estimation, in which they learn the
marginal likelihood of body parts but ignore the partition function in the likelihood. Message learning is not discussed in that work, and the exact relationship between this pose estimation approach
and message learning remains unclear.

2

Learning CRF with CNN potentials

Before describing our message learning method, we review the CRF-CNN joint learning approach
and discuss limitations. An input image is denoted by x ? X and the corresponding labeling mask
is denoted by y ? Y. The energy function is denoted by E(y, x), which measures the score of the
prediction y given the input image x. We consider the following form of conditional likelihood:
P (y|x) =

1
exp [?E(y, x)]
exp [?E(y, x)] = P
.
0
Z(x)
y 0 exp [?E(y , x)]

(1)

Here Z is the partition function. The CRF model is decomposed by a factor graph over a set of
factors F. Generally, the energy function is written as a sum of potential functions (factor functions):
P
E(y, x) = F ?F EF (y F , xF ).
(2)
Here F indexes one factor in the factor graph; y F denotes the variable nodes which are connected
to the factor F ; EF is the (log-) potential function (factor function). The potential function can be
a unary, pairwise, or high-order potential function. The recent method in [3] describes examples of
constructing general CNN based unary and pairwise potentials.
Take semantic image segmentation as an example. To predict the pixel labels of a test image, we can
find the mode of the joint label distribution by solving the maximum a posteriori (MAP) inference
problem: y ? = argmax y P (y|x). We can also obtain the final prediction by calculating the label
marginal distribution of each variable, which requires to solve a marginal inference problem:
P
?p ? N : P (yp |x) = y\yp P (y|x).
(3)
Here y\yp indicates the output variables y excluding yp . For a general CRF graph with cycles,
the above inference problems is known to be NP-hard, thus approximate inference algorithms are
applied. Message passing is a type of widely applied algorithms for approximate inference: loopy
belief propagation (BP) [13], tree-reweighted message passing [14] and mean-field approximation
[13] are examples of the message passing methods.
CRF-CNN joint learning aims to learn CNN potential functions by optimizing the CRF objective,
typically, the negative conditional log-likelihood, which is:
? log P (y|x; ?) = E(y, x; ?) + log Z(x; ?).

(4)

The energy function E(y, x) is constructed by CNNs, for which all the network parameters are
denoted by ?. Adding regularization, minimizing negative log-likelihood for CRF learning is:
PN
2
min? ?2 k?k2 + i=1 [E(y (i) , x(i) ; ?) + log Z(x(i) ; ?)].
(5)
Here x(i) , y (i) denote the i-th training image and its segmentation mask; N is the number of training
images; ? is the weight decay parameter. We can apply stochastic gradient descent (SGD) to optimize the above problem for learning ?. The energy function E(y, x; ?) is constructed from CNNs,
and its gradient ?? E(y, x; ?) can be easily computed by applying the chain rule as in conventional
CNNs. However, the partition function Z brings difficulties for optimization. Its gradient is:
?? log Z(x; ?) =

X

exp [?E(y, x; ?)]
?? [?E(y, x; ?)]
0
y 0 exp [?E(y , x; ?)]

P
y

= ? Ey?P (y|x;?) ?? E(y, x; ?).

(6)

Direct calculation of the above gradient is computationally infeasible for general CRF graphs. Usually it is necessary to perform approximate marginal inference to calculate the gradients at each SGD
iteration [13]. However, repeated marginal inference can be extremely expensive, as discussed in
[3]. CNN training usually requires a huge number of SGD iterations (hundreds of thousands, or even
millions), hence this inference based learning approach is in general not scalable or even infeasible.

3

Learning CNN message estimators

In conventional CRF approaches, the potential functions are first learned, and then inference is
performed based on the learned potential functions to generate the final prediction. In contrast, our
approach directly optimizes the inference procedure for final prediction. We propose to learn CNN
estimators to directly output the required intermediate values in an inference algorithm.
Here we focus on the message passing based inference algorithm which has been extensively studied
and widely applied. In the CRF prediction procedure, the ?message? vectors are recursively calculated based on the learned potentials. We propose to construct and learn CNNs to directly estimate
these messages in the message passing procedure, rather than learning the potential functions. In
particular, we directly learn factor-to-variable message estimators. Our message learning framework

is general and can accommodate all message passing based algorithms such as loopy belief propagation (BP) [13], mean-field approximation [13] and their variants. Here we discuss using loopy BP
for calculating variable marginals. As shown by Yedidia et al. [15], loopy BP has a close relation
with Bethe free energy approximation.
Typically, the message is a K-dimensional vector (K is the number of classes) which encodes the
information of the label distribution. For each variable-factor connection, we need to recursively
compute the variable-to-factor message: ? p?F ? RK , and the factor-to-variable message: ? F ?p ?
RK . The unnormalized variable-to-factor message is computed as:
P
?
?
(yp ) =
? 0 (yp ).
(7)
0
p?F

F ?Fp \F

F ?p

Here Fp is a set of factors connected to the variable p; Fp \F is the set of factors Fp excluding the
factor F . For loopy graphs, the variable-to-factor message is normalized at each iteration:
?
exp ?
p?F (yp )
? p?F (yp ) = log P
.
0
?
y 0 exp ? p?F (yp )

(8)

p

The factor-to-variable message is computed as:


X
X
0
0
? F ?p (yp ) = log
exp ? EF (y F ) +
? q?F (yq ) .
y 0F \yp0 ,yp0 =yp

(9)

q?NF \p

Here NF is a set of variables connected to the factor F ; NF \p is the set of variables NF excluding
the variable p. Once we get all the factor-to-variable messages of one variable node, we are able to
calculate the marginal distribution (beliefs) of that variable:
 X

X
1
P (yp |x) =
P (y|x) =
exp
? F ?p (yp ) ,
(10)
Zp
F ?Fp

y\yp

in which Zp is a normalizer: Zp =
3.1

P

P
yp exp [
F ?Fp ? F ?p (yp )].

CNN message estimators

The calculation of factor-to-variable message ? F ?p depends on the variable-to-factor messages
? p?F . Substituting the definition of ? p?F in (8), ? F ?p can be re-written as:


0
?
X 
X
exp ?
q?F (yq )
exp ? EF (y 0F ) +
log P
? F ?p (yp ) = log
00
?
yq00 exp ? q?F (yq )
q?NF \p
y 0F \yp0 ,yp0 =yp
P


X
X 
exp F 0 ?Fq \F ? F 0 ?q (yq0 )
0
P
= log
exp ? EF (y F ) +
log P
00
yq00 exp
F 0 ?Fq \F ? F 0 ?q (yq )
0
0
0
y F \yq ,yp =yp

q?NF \p

(11)
Here q denotes the variable node which is connected to the node p by the factor F in the factor
graph. We refer to the variable node q as a neighboring node of q. NF \p is a set of variables
connected to the factor F excluding the node p. Clearly, for a pairwise factor which only connects
to two variables, the set NF \p only contains one variable node. The above equations show that
the factor-to-variable message ? F ?p depends on the potential EF and ? F 0 ?q . Here ? F 0 ?q is the
factor-to-variable message which is calculated from a neighboring node q and a factor F 0 6= F .
Conventional CRF learning approaches learn the potential function then follow the above equations
to compute the messages for calculating marginals. As discussed in [8], given that the goal is to
estimate the marginals, it is not necessary to exactly follow the above equations, which involve
learning potential functions, to calculate messages. We can directly learn message estimators, rather
than indirectly learning the potential functions as in conventional methods.
Consider the calculation in (11). The message ? F ?p depends on the observation xpF and the
messages ? F 0 ?q . Here xpF denotes the observations that correspond to the node p and the factor
F . We are able to formulate a factor-to-variable message estimator which takes xpF and ? F 0 ?q as

inputs and outputs the message vector, and we directly learn such estimators. Since one message
? F ?p depends on a number of previous messages ? F 0 ?q , we can formulate a sequence of message
estimators to model the dependence. Thus the output from a previous message estimator will be the
input of the following message estimator.
There are two message passing strategies for loopy BP: synchronous and asynchronous passing.
We here focus on the synchronous message passing, for which all messages are computed before
passing them to the neighbors. The synchronous passing strategy results in much simpler message
dependences than the asynchronous strategy, which simplifies the training procedure. We define one
inference iteration as one pass of the graph with the synchronous passing strategy.
We propose to learn CNN based factor-to-variable message estimator. The message estimator models the interaction between neighboring variable nodes. We denote by M a message estimator. The
factor-to-variable message is calculated as:
? F ?p (yp ) = MF (xpF , dpF , yp ).

(12)

We refer to dpF as the dependent message feature vector which encodes all dependent messages
from the neighboring nodes that are connected to the node p by F . Note that the dependent messages
are the output of message estimators at the previous inference iteration. In the case of running only
one message passing iteration, there are no dependent messages for MF , and thus we do not need
to incorporate dpF . To have a general exposition, we here describe the case of running arbitrarily
many inference iterations.
We can choose any effective strategy to generate the feature vector dpF from the dependent messages. Here we discuss a simple example. According to (11), we define the feature vector dpF as a
K-dimensional vector which aggregates all dependent messages. In this case, dpF is computed as:
P

X 
exp F 0 ?Fq \F MF 0 (xqF 0 , dqF 0 , y)
P
dpF (y) =
log P
.
(13)
0
0
0
0
y 0 exp
F 0 ?Fq \F MF (xqF , dqF , y )
q?NF \p

With the definition of dpF in (13) and ? F ?p in (12), it clearly shows that the message estimation requires evaluating a sequence of message estimators. Another example is to concatenate all
dependent messages to construct the feature vector dpF .
There are different strategies to formulate the message estimators in different iterations. One strategy
is using the same message estimator across all inference iterations. In this case the message estimator
becomes a recursive function, and thus the CNN based estimator becomes a recurrent neural network
(RNN). Another strategy is to formulate different estimator for each inference iteration.
3.2

Details for message estimator networks

We formulate the estimator MF as a CNN, thus the estimation is the network outputs:
PK
? F ?p (yp ) = MF (xpF , dpF , yp ; ? F ) = k=1 ?(k = yp )zpF,k (x, dpF ; ? F ).

(14)

Here ? F denotes the network parameter which we need to learn. ?(?) is the indicator function, which
equals 1 if the input is true and 0 otherwise. We denote by z pF ? RK as the K-dimensional output
vector (K is the number of classes) of the message estimator network for the node p and the factor
F ; zpF,k is the k-th value in the network output z pF corresponding to the k-th class.
We can consider any possible strategies for implementing z pF with CNNs. For example, we here
describe a strategy which is analogous to the network design in [3]. We denote by C (1) as a fully
convolutional network (FCNN) [16] for convolutional feature generation, and C (2) as a traditional
fully connected network for message estimation.
Given an input image x, the network output C (1) (x) ? RN1 ?N2 ?r is a convolutional feature map,
in which N1 ? N2 = N is the feature map size and r is the dimension of one feature vector. Each
spatial position (each feature vector) in the feature map C (1) (x) corresponds to one variable node
in the CRF graph. We denote by C (1) (x, p) ? Rr , the feature vector corresponding to the variable
node p. Likewise, C (1) (x, NF \p) ? Rr is the averaged vector of the feature vectors that correspond
to the set of nodes NF \p. Recall that NF \p is a set of nodes connected by the factor F excluding
the node p. For pairwise factors, NF \p contains only one node.

(1)

We construct the feature vector z C
pF

? R2r for the node-factor pair (p, F ) by concatenating
(1)

C (1) (x, p) and C (1) (x, NF \p). Finally, we concatenate the node-factor feature vector z C
pF and
(2)
the dependent message feature vector dpF as the input for the second network C . Thus the input
(1)
dimension for C (2) is (2r + K). For running only one inference iteration, the input for C (2) is z C
pF
alone. The final output from the second network C (2) is the K-dimensional message vector z pF .
To sum up, we generate the final message vector z pF as:
z pF = C (2) { [ C (1) (x, p)> ; C (1) (x, NF \p )> ; d>pF ]> }.

(15)

For a general CNN based potential function in conventional CRFs, the potential network is usually
required to have a large number of output units (exponential in the order of the potentials). For
example, it requires K 2 (K is the number of classes) outputs for the pairwise potentials [3]. A large
number of output units would significantly increase the number of network parameters. It leads to
expensive computations and tends to over-fit the training data. In contrast, for learning our CNN
message estimator, we only need to formulate K output units for the network. Clearly it is more
scalable in the cases of a large number of classes.
3.3

Training CNN message estimators

Our goal is to estimate the variable marginals in (3), which can be re-written with the estimators:
 X

X
X
1
1
P (yp |x) =
P (y|x) =
exp
? F ?p (yp ) =
exp
MF (xpF , dpF , yp ; ? F ).
Zp
Zp
F ?Fp

y\yp

F ?Fp

Here Zp is the normalizer. The ideal variable marginal, for example, has the probability of 1 for the
ground truth class and 0 for the remaining classes. Here we consider the cross entropy loss between
the ideal marginal and the estimated marginal.
? ; ?) = ?
J(x, y

K
X X

?(yp = y?p ) log P (yp |x; ?)

p?N yp =1

=?

K
X X
p?N yp

P
exp F ?Fp MF (xpF , dpF , yp ; ? F )
P
?(yp = y?p ) log P
,
0
yp0 exp
F ?Fp MF (xpF , dpF , yp ; ? F )
=1

(16)

in which y?p is the ground truth label for the variable node p. Given a set of N training images and
label masks, the optimization problem for learning the message estimator network is:
PN
2
? (i) ; ?).
min? ?2 k?k2 + i=1 J(x(i) , y
(17)
The work in [8] proposed to learn the variable-to-factor message (? p?F ). Unlike their approach, we
aim to learn the factor-to-variable message (? F ?p ), for which we are able to naturally formulate the
variable marginals, which is the ultimate goal for prediction, as the training objective. Moreover, for
learning ? p?F in their approach, the message estimator will depend on all neighboring nodes (connected by any factors). Given that variable nodes will have different numbers of neighboring nodes,
they only consider a fixed number of neighboring nodes (e.g., 20) and concatenate their features to
generate a fixed-length feature vector for classification. In our case for learning ? F ?p , the message
estimator only depends on a fixed number of neighboring nodes (connected by one factor), thus we
do not have this problem. Most importantly, they learn message estimators by training traditional
probabilistic classifiers (e.g., simple logistic regressors) with hand-craft features, and in contrast, we
train deep CNNs in an end-to-end learning style without using hand-craft features.
3.4

Message learning with inference-time budgets

One advantage of message learning is that we are able to explicitly incorporate the expected number
of inference iterations into the learning procedure. The number of inference iterations defines the
learning sequence of message estimators. This is particularly useful if we aim to learn the estimators
which are capable of high-quality predictions within only a few inference iterations. In contrast,

Table 1: Segmentation results on the PASCAL VOC 2012 ?val? set. We compare with several recent CNN
based methods with available results on the ?val? set. Our method performs the best.
method
ContextDCRF [3]
Zoom-out [17]
Deep-struct [2]
DeepLab-CRF [9]
DeepLap-MCL [9]
BoxSup [18]
BoxSup [18]
ours
ours+

training set
VOC extra
VOC extra
VOC extra
VOC extra
VOC extra
VOC extra
VOC extra + COCO
VOC extra
VOC extra

# train (approx.)
10k
10k
10k
10k
10k
10k
133k
10k
10k

IoU val set
70.3
63.5
64.1
63.7
68.7
63.8
68.1
71.1
73.3

conventional potential function learning in CRFs is not able to directly incorporate the expected
number of inference iterations.
We are particularly interested in learning message estimators for use with only one message passing
iteration, because of the speed of such inference. In this case it might be preferable to have largerange neighborhood connections, so that large range interaction can be captured within one inference
pass.

4

Experiments

We evaluate the proposed CNN message learning method for semantic image segmentation. We
use the publicly available PASCAL VOC 2012 dataset [19]. There are 20 object categories and one
background category in the dataset. It contains 1464 images in the training set, 1449 images in the
?val? set and 1456 images in the test set. Following the common practice in [20, 9], the training
set is augmented to 10582 images by including the extra annotations provided in [21] for the VOC
images. We use intersection-over-union (IoU) score [19] to evaluate the segmentation performance.
For the learning and prediction of our method, we only use one message passing iteration.
The recent work in [3] (referred to as ContextDCRF) learns multi-scale fully convolutional CNNs
(FCNNs) for unary and pairwise potential functions to capture contextual information. We follow
this CRF learning method and replace the potential functions by the proposed message estimators.
We consider 2 types of spatial relations for constructing the pairwise connections of variable nodes.
One is the ?surrounding? spatial relation, for which one node is connected to its surround nodes. The
other one is the ?above/below? spatial relation, for which one node is connected to the nodes that lie
above. For the pairwise connections, the neighborhood size is defined by a range box. We learn one
type of unary message estimator and 3 types of pairwise message estimators in total. One type of
pairwise message estimator is for the ?surrounding? spatial relations, and the other two are for the
?above/below? spatial relations. We formulate one network for one type of message estimator.
We formulate our message estimators as multi-scale FCNNs, for which we apply a similar network
configuration as in [3]. The network C (1) (see Sec. 3.2 for details) has 6 convolution blocks and C (2)
has 2 fully connected layers (with K output units). Our networks are initialized using the VGG-16
model [22]. We train all layers using back-propagation. Our system is built on MatConvNet [23].
We first evaluate our method on the VOC 2012 ?val? set. We compare with several recent CNN
based methods with available results on the ?val? set. Results are shown in Table 1. Our method
achieves the best performance. The comparing method ContextDCRF follows a conventional CRF
learning and prediction scheme: they first learn potentials and then perform inference based on
the learned potentials to output final predictions. The result shows that learning the CNN message
estimators is able to achieve similar performance compared to learning CNN potential functions in
CRFs. Note that since here we only use one message passing iteration for the training and prediction,
the inference is particularly efficient.
To further improve the performance, we perform simple data augmentation in training. We generate
extra 4 scales ([0.8, 0.9, 1.1, 1.2]) of the training images and their flipped images for training. This
result is denoted by ?ours+? in the result table.

bird

boat

bottle

bus

car

cat

chair

cow

table

dog

horse

mbike

person

potted

sheep

sofa

train

tv

mean
66.4
71.6
62.2
72.0
73.4

bike

method
DeepLab-CRF [9]
DeepLab-MCL [9]
FCN-8s [16]
CRF-RNN [1]
ours

aero

Table 2: Category results on the PASCAL VOC 2012 test set. Our method performs the best.

78.4
84.4
76.8
87.5
90.1

33.1
54.5
34.2
39.0
38.6

78.2
81.5
68.9
79.7
77.8

55.6
63.6
49.4
64.2
61.3

65.3
65.9
60.3
68.3
74.3

81.3
85.1
75.3
87.6
89.0

75.5
79.1
74.7
80.8
83.4

78.6
83.4
77.6
84.4
83.3

25.3
30.7
21.4
30.4
36.2

69.2
74.1
62.5
78.2
80.2

52.7
59.8
46.8
60.4
56.4

75.2
79.0
71.8
80.5
81.2

69.0
76.1
63.9
77.8
81.4

79.1
83.2
76.5
83.1
83.1

77.6
80.8
73.9
80.6
82.9

54.7
59.7
45.2
59.5
59.2

78.3
82.2
72.4
82.8
83.4

45.1
50.4
37.4
47.8
54.3

73.3
73.1
70.9
78.3
80.6

56.2
63.7
55.1
67.1
70.8

Table 3: Segmentation results on the PASCAL VOC 2012 test set. Compared to methods that use the same
augmented VOC dataset, our method has the best performance.
method
ContextDCRF [3]
Zoom-out [17]
FCN-8s [16]
SDS [20]
DeconvNet-CRF [24]
DeepLab-CRF [9]
DeepLab-MCL [9]
CRF-RNN [1]
DeepLab-CRF [25]
DeepLab-MCL [25]
BoxSup (semi) [18]
CRF-RNN [1]
ours

training set
VOC extra
VOC extra
VOC extra
VOC extra
VOC extra
VOC extra
VOC extra
VOC extra
VOC extra + COCO
VOC extra + COCO
VOC extra + COCO
VOC extra + COCO
VOC extra

# train (approx.)
10k
10k
10k
10k
10k
10k
10k
10k
133k
133k
133k
133k
10k

IoU test set
70.7
64.4
62.2
51.6
72.5
66.4
71.6
72.0
70.4
72.7
71.0
74.7
73.4

We further evaluate our method on the VOC 2012 test set. We compare with recent state-of-the-art
CNN methods with competitive performance. The results are described in Table 3. Since the ground
truth labels are not available for the test set, we evaluate our method through the VOC evaluation
server. We achieve very competitive performance on the test set: 73.4 IoU score1 , which is to date
the best performance amongst methods that use the same augmented VOC training dataset [21]
(marked as ?VOC extra? in the table). These results validate the effectiveness of direct message
learning with CNNs. We also include a comparison with methods which are trained on the much
larger COCO dataset (around 133K training images). Our performance is comparable with these
methods, even though we make use of many fewer training images.
The results for each category is shown in Table 2. We compare with several recent methods which
transfer layers from the same VGG-16 model and use the same training data. Our method performs
the best for 13 out of 20 categories.

5

Conclusion

We have proposed a new deep message learning framework for structured CRF prediction. Learning
deep message estimators for the message passing inference reveals a new direction for learning deep
structured model. Learning CNN message estimators is efficient, which does not involve expensive
inference steps for gradient calculation. The network output dimension for message estimation is
the same as the number of classes, which does not increase with the order of the potentials, and thus
CNN message learning has less network parameters and is more scalable in the number of classes
compared to conventional potential function learning. Our impressive performance for semantic
segmentation demonstrates the effectiveness and usefulness of the proposed deep message learning.
Our framework is general and can be readily applied to other structured prediction applications.
Acknowledgements This research was supported by the Data to Decisions Cooperative Research
Centre and by the Australian Research Council through the ARC Centre for Robotic Vision
CE140100016 and through a Laureate Fellowship FL130100102 to I. Reid. Correspondence should
be addressed to C. Shen.
1

The result link provided by VOC evaluation server: http://host.robots.ox.ac.uk:8080/anonymous/DBD0SI.html

References
[1] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, and
P. Torr, ?Conditional random fields as recurrent neural networks,? 2015. [Online]. Available:
http://arxiv.org/abs/1502.03240
[2] A. Schwing and R. Urtasun, ?Fully connected deep structured networks,? 2015. [Online]. Available:
http://arxiv.org/abs/1503.02351
[3] G. Lin, C. Shen, I. Reid, and A. van den Hengel, ?Efficient piecewise training of deep structured models
for semantic segmentation,? 2015. [Online]. Available: http://arxiv.org/abs/1504.01013
[4] F. Liu, C. Shen, and G. Lin, ?Deep convolutional neural fields for depth estimation from a single image,?
in Proc. IEEE Conf. Comp. Vis. Pattern Recogn., 2015.
[5] L. Chen, A. Schwing, A. Yuille, and R. Urtasun, ?Learning deep structured models,? 2014. [Online].
Available: http://arxiv.org/abs/1407.2538
[6] J. Besag, ?Efficiency of pseudolikelihood estimation for simple Gaussian fields,? Biometrika, 1977.
[7] C. Sutton and A. McCallum, ?Piecewise training for undirected models,? in Proc. Conf. Uncertainty
Artificial Intelli, 2005.
[8] S. Ross, D. Munoz, M. Hebert, and J. Bagnell, ?Learning message-passing inference machines for structured prediction,? in Proc. IEEE Conf. Comp. Vis. Pattern Recogn., 2011.
[9] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. Yuille, ?Semantic image segmentation with deep
convolutional nets and fully connected CRFs,? 2014. [Online]. Available: http://arxiv.org/abs/1412.7062
[10] P. Kr?ahenb?uhl and V. Koltun, ?Efficient inference in fully connected CRFs with Gaussian edge potentials,?
in Proc. Adv. Neural Info. Process. Syst., 2012.
[11] F. Liu, C. Shen, G. Lin, and I. Reid, ?Learning depth from single monocular images using deep
convolutional neural fields,? 2015. [Online]. Available: http://arxiv.org/abs/1502.07411
[12] J. Tompson, A. Jain, Y. LeCun, and C. Bregler, ?Joint training of a convolutional network and a graphical
model for human pose estimation,? in Proc. Adv. Neural Info. Process. Syst., 2014.
[13] S. Nowozin and C. Lampert, ?Structured learning and prediction in computer vision,? Found. Trends.
Comput. Graph. Vis., 2011.
[14] V. Kolmogorov, ?Convergent tree-reweighted message passing for energy minimization,? IEEE T. Pattern
Analysis & Machine Intelligence, 2006.
[15] J. S. Yedidia, W. T. Freeman, Y. Weiss et al., ?Generalized belief propagation,? in Proc. Adv. Neural Info.
Process. Syst., 2000.
[16] J. Long, E. Shelhamer, and T. Darrell, ?Fully convolutional networks for semantic segmentation,? in Proc.
IEEE Conf. Comp. Vis. Pattern Recogn., 2015.
[17] M. Mostajabi, P. Yadollahpour, and G. Shakhnarovich, ?Feedforward semantic segmentation with
zoom-out features,? 2014. [Online]. Available: http://arxiv.org/abs/1412.0774
[18] J. Dai, K. He, and J. Sun, ?BoxSup: exploiting bounding boxes to supervise convolutional networks for
semantic segmentation,? 2015. [Online]. Available: http://arxiv.org/abs/1503.01640
[19] M. Everingham, L. V. Gool, C. Williams, J. Winn, and A. Zisserman, ?The pascal visual object classes
(VOC) challenge,? Int. J. Comp. Vis., 2010.
[20] B. Hariharan, P. Arbel?aez, R. Girshick, and J. Malik, ?Simultaneous detection and segmentation,? in Proc.
European Conf. Computer Vision, 2014.
[21] B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, and J. Malik, ?Semantic contours from inverse detectors,?
in Proc. Int. Conf. Comp. Vis., 2011.
[22] K. Simonyan and A. Zisserman, ?Very deep convolutional networks for large-scale image recognition,?
2014. [Online]. Available: http://arxiv.org/abs/1409.1556
[23] A. Vedaldi and K. Lenc, ?Matconvnet ? convolutional neural networks for matlab,? in Proceeding of the
ACM Int. Conf. on Multimedia, 2015.
[24] H. Noh, S. Hong, and B. Han, ?Learning deconvolution network for semantic segmentation,? in Proc.
IEEE Conf. Comp. Vis. Pattern Recogn., 2015.
[25] G. Papandreou, L. Chen, K. Murphy, and A. Yuille, ?Weakly-and semi-supervised learning of a DCNN
for semantic image segmentation,? 2015. [Online]. Available: http://arxiv.org/abs/1502.02734

"
105,"Plasticity of Center-Surround Opponent
Receptive Fields in Real and Artificial
Neural Systems of Vision

S. Yasui
Kyushu Institute of Technology
lizuka 820, Japan

T. Furukawa
Kyushu Institute of Technology
lizuka 820, Japan

M. Yamada
Electrotechnical Laboratory
Tsukuba 305, Japan

T. Saito
Tsukuba University
Tsukuba 305, Japan

Abstract
Despite the phylogenic and structural differences, the visual systems of different species, whether vertebrate or invertebrate, share
certain functional properties. The center-surround opponent receptive field (CSRF) mechanism represents one such example. Here,
analogous CSRFs are shown to be formed in an artificial neural
network which learns to localize contours (edges) of the luminance
difference. Furthermore, when the input pattern is corrupted by
a background noise, the CSRFs of the hidden units becomes shallower and broader with decrease of the signal-to-noise ratio (SNR).
The same kind of SNR-dependent plasticity is present in the CSRF
of real visual neurons; in bipolar cells of the carp retina as is shown
here experimentally, as well as in large monopolar cells of the fly
compound eye as was described by others. Also, analogous SNRdependent plasticity is shown to be present in the biphasic flash
responses (BPFR) of these artificial and biological visual systems .
Thus, the spatial (CSRF) and temporal (BPFR) filtering properties with which a wide variety of creatures see the world appear to
be optimized for detectability of changes in space and time.

1

INTRODUCTION

A number of learning algorithms have been developed to make synthetic neural
machines be trainable to function in certain optimal ways. If the brain and nervous
systems that we see in nature are best answers of the evolutionary process, then
one might be able to find some common 'softwares' in real and artificial neural
systems. This possibility is examined in this paper, with respect to a basic visual

S. YASUI, T. FURUKAWA, M. YAMADA, T. SAITO

160

mechanism relevant to detection of brightness contours (edges). In most visual
systems of vertebrate and invertebrate, one finds interneurons which possess centersurround opponent receptive fields (CSRFs). CSRFs underlie the mechanism of
lateral inhibition which produces edge enhancement effects such as Mach band. It
has also been shown in the fly compound eye that the CSRF of large monopolar cells
(LMCs) changes its shape in accordance with SNR; the CSRF becomes wider with
increase of the noise level in the sensory environment. Furthermore, whereas CSRFs
describe a filtering function in space, an analogous observation has been made
in LMCs as regards the filtering property in the time domain; the biphasic flash
response (BPFR) lasts longer as the noise level increases (Dubs, 1982; Laughlin,
1982).
A question that arises is whether similar SNR-dependent spatia-temporal filtering
properties might be present in vertebrate visual cells. To investigate this, we made
an intracellular recording experiment to measure the CSRF and BPFR profiles of
bipolar cells in the carp retina under appropriate conditions, and the results are
described in the first part of this paper. In the second part, we ask the same
question in a 3-layer feedforward artificial neural network (ANN) trained to detect
and localize spatial and temporal changes in simulated visual inputs corrupted by
noise. In this case, the ANN wiring structure evolves from an initial random state so
as to minimize the detection error, and we look into the internal ANN organization
that emerges as a result of training. The findings made in the real and artificial
neural systems are compared and discussed in the final section.
In this study, the backpropagation learning algorithm was applied to update the
synaptic parameters of the ANN. This algorithm was used as a means for the computational optimization. Accordingly, the present choice is not necessarily relevant
to the question of whether the error backpropagation pathway actually might exist
in real neural systems( d. Stork & Hall, 1989).

2

THE CASE OF A REAL NEURAL SYSTEM:
RETINAL BIPOLAR CELL

Bipolar cells occur as a second order neuron in the vertebrate retina, and they have
a good example of CSRF Here we are interested in the possibility that the CSRF
and BPFR of bipolar cells might change their size and shape as a function of the
visual environment, particularly as regards the dark- versus light-adapted retinal
states which correspond to low versus high SNR conditions as explained later. Thus,
the following intracellular recording experiment was carried out .

2.1

MATERIAL AND METHOD

The retina was isolated from the carp which had been kept in complete darkness
for 2 hrs before being pithed for sacrifice. The specimen was then mounted on
a chamber with the receptor side up, and it was continuously superfused with a
Ringer solution composed of (in mM) 102 NaCI, 28 NaHC0 3 , 2.6 KCI, 1 CaCh, 1
MgCh and 5 glucose, maintained at pH=7 .6 and aerated with a gas mixture of 95%
O 2 and 5% CO 2 ? Glass micropipettes filled with 3M KCI and having tip resistances
of about 150 Mn were used to record the membrane potential. Identification of
bipolar cell units was made on the basis of presence or absence of CSRF . For this
preliminary test, the center and peripheral responses were examined by using flashes
of a small centered spot and a narrow annular ring. To map their receptive field
profile, the stimulus was given as flashes of a narrow slit presented at discrete
positions 60 pm apart on the retina. The slit of white light was 4 mm long and 0.17
mm wide, and its flash had intensity of 7.24 pW /cm 2 and duration of 250 msec.
The CSRF measurement was made under dark- and light- adapted conditions. A

161

Plasticity of Center-Surround Opponent Receptive Fields
(b) 1.0
aUght
? Dark

(a)

o .

t CCnler

Lighl

""'/I~~I~~1~!.yvr""""'~
_0_

_

. _ . _._ . _ ._

I

n.,k

0_0_0_0_._

""_

-1.0

""- "".

""."".
i

i

-1.0

0

i

1.0

(c)

~

~~H?\J)rl. !rt~~\..+,~
I

I

i

SIIlV _ ._

._

_ _ _0_0_

GO/1m

""_""_

0_

._0_""_._

""

5mV
~

I Osee

lsec
Figure 1: (a) Intracellular recordings from an ON-center bipolar cell of the carp
retina with moving slit stimuli under light and dark adapted condition. (b) The
receptive field profiles plotted from the recordings. (c) The response recorded when
the slit was positioned at the receptive field center.
steady background light of 0.29 JJW /cm2 was provided for light adaptation.

2.2

RESULTS

Fig.la shows a typical set of records obtained from a bipolar cell. The response
to each flash of slit was biphasic (i.e., BPFR), consisting of a depolarization (ON)
followed by a hyperpolarization(OFF) . The ON response was the major component
when the slit was positioned centrally on the receptive field, whereas the OFF
response was dominant at peripheral locations and somewhat sluggish. The CSRF
pattern was portrayed by plotting the response membrane potential measured at
the time just prior to the cessation of each test flash. The result compiled from
the data of Figola is presented in Fig.lb, showing that the CSRF of the darkadapted state was shallow and broad as opposed to the sharp profile produced during
light adaptation. The records with the slit positioned at the receptive field center
are enlarged in Fig.lc, indicating that the OFF part of the BPFR waveform was
shallower and broader when the retina was dark adapted than when light adaptedo

3

THE CASE OF ARTIFICIAL NEURAL NETWORKS

Visual pattern recognition and imagery data processing have been a traditional
application area of ANNs. There are also ANNs that deal with time series signals.
These both types of ANNs are considered here, and they are trained to detect and
localize spatial or temporal changes of the input signal corrupted by noise.

162

3.1

S. YASUI, T. FURUKAWA, M. YAMADA, T. SAITO

PARADIGMS AND METHODS

The ANN models we used are illustrated in Figs.2. The model of Fig.2a deals
with one-dimensional spatial signals. It consists of three layers (input, hidden,
output), each having the same number of 12 or 20 neuronal units. The pattern
given to the input layer represents the brightness distribution of light. The network
was trained by means of the standard backpropagation algorithm, to detect and
localize step-wise changes (edges) which were distributed on each training pattern
in a random fashion with respect to the number, position and height. The mean
level of the whole pattern was varied randomly as well. In addition, there was
a background noise (not illustrated in Figs.2); independent noise signals of the
same statistics were given to the all input units, and the maximum noise amplitude
(NL: noise level) remained constant throughout each training session. The teacher
signal was the ""true"" edge positions which were subject to obscuration due to the
background noise; the learning was supervised such that each output unit would
respond with 1 when a step-wise change not due to the background noise occurred
at the corresponding position, and respond with -1 otherwise. The value of each
synaptic weight parameter was given randomly at the outset and updated by using
the backpropagation algorithm after presentation of each training pattern. The
training session was terminated when the mean square error stopped decreasing.
To process time series inputs, the ANN model of Fig.2b was constructed with the
backpropagation learning algorithm. This temporal model also has three layers,
but the meaning of this is quite different from the spatial network model of Fig.2a.
That is, whereas each unit of each layer in the spatial model is an anatomical
entity, this is not the case with respect to the temporal model. Thus, each layer
represents a single neuron so that there are actually only three neuronal elements,
i.e., a receptor, an interneuron, and an output cell. And, the units in the same
layer represent activity states of one neuron at different time slices; the rightmost
unit for the present time, the next one for one time unit ago, and so on. As is
apparent from Fig.2b, therefore, there is no convergence from the future (right) to
the past (left). Each cell has memory of T-units time. Accordingly, the network
requires 2T - 1 units in the input layer, T units in the hidden layer and 1 units in
the output layer to calculate the output at present time. The input was a discrete
time series in which step-wise changes took place randomly in a manner analogous
to the spatial input of Fig.2a. As in the spatial case, there was a background noise
(b)

Input

Correetiom

Correctiom

Figure 2: The neural network architectures. Spatial (a) and temporal model (b).

163

Plasticity of Center-Surround Opponent Receptive Fields
(8)

11I1JJ11JJ111
iFJiiii? ?
2000

Oulput ut,i'""
4000
10000

30000

0.2

0.1

01.O-=~~~~4~.0~!!iI!!:!II""'''-'l8.0xl04

0.0

Iterations
Figure 3: Development of receptive fields. Synaptic weights (a) and mean square
error (b), both as a function of the number of iterations.
added to the input. The network was trained to respond with +1/ -1 when the
original input signal increased/decreased, and to respond with 0 otherwise.

3.2

RESULTS

Spatial case: Emergence of CSRFs with SNR-dependent plasticity
As regards the edge detection learning by the
ANN model of Fig.2a, the results without
the background noise are described first (Furukawa & Yasui, 1990; Joshi & Lee, 1993).
Fig.3a illustrates how the synaptic connecHIddm Layer
tions developed from the initial random state.
If the final distribution of synaptic weight parameters is examined from input units to any
hidden unit and also from hidden units to
Output Layer
any output unit, then it can be seen in either case that the central and peripheral connections are opposite in the polarity of their
weight parameters; the central group had eiFigure 4: A Sample of activity
ther positive (ON-center) or negative (OFFpattern of each layer
center) values, but the reversed profiles are
shown in the drawing of Fig.3a for the OFF -center case. In any event, CSRFs were
formed inside the network as a result of the edge detection learning. Fig.3b shows
the performance improvement during a learning session. FigA shows the activation
pattern of each layer in response to a sample input, and edge enhancement like
the Mach band effect can be observed in the hidden layer. Fig.5a presents sample
input patterns corrupted by the background noise of various NL values, and Fig.5b
shows how a hidden unit was connected to the input layer at the end of training.
CSRFs were still formed when the environment suffered from the noise. However,
the structure of the center-surround antagonism changed as a function of NL; the
CSRFs became shallow and broad as NL increased, i.e., as the SNR decreased.

\'---

I

I

Temporal case: Emergence of BPFRs with SNR-dependent plasticity
With reference to the learning paradigm of Fig.2b, Fig.5c reveals how a representative hidden unit made synaptic connections with the input units as a function of
NL; the weight parameters are plotted against the elapsed time. Each trace would
correspond to the response of the hidden unit to a flash of light, and it consists of

164

S. Y ASUI, T. FURUKAWA, M. YAMADA, T . SAITO

two phases of ON and OFF, i.e., BPFRs (biphasic flash responses) emerged in this
ANN as a result of learning, and the biphasic time course changed depending on
NL; the negative-going phase became shallower and longer with decrease of SNR.

4

DISCUSSION: Common Receptive Field Properties in
Vertebrate, Invertebrate and Artificial Systems

A CSRF profile emerges after differentiating twice in space a small patch of light,
and CSRF is a kind of point spreading function. Accordingly, the response to any
input distribution can be obtained by convolving the input pattern with CSRF.
The double differentiation of this spatial filtering acts to locate edge positions. On
the other hand, the waveform of BPFR appears by differentiating once in time a
short flash of light. Thus, the BPFR is an impulse response function with which
to convolve the given input time series to obtain the response waveform. This is
a derivative filtering, which subserves detection of temporal changes in the input
visual signal. While both CSRF and BPFR occur in visual neurons of a wide variety
of vertebrates and invertebrates, the first part of the present study shows that these
spatial and temporal filtering functions can develop autonomously in our ANNs.
The neural system of visual signal processing encounters various kinds of noise.
There are non-biological ones such as a background noise in the visual input itself
and the photon noise which cannot be ignored when the light intensity is low. Endogenous sources of noise include spontaneous photoisomerization in photoreceptor
cells, quantal transmitter release at synaptic sites, open/close activities of ion channels and so on. Generally speaking, therefore, since the surroundings are dim when
the retina is dark adapted, SNR in the neuronal environment tends to be low during
dark adaptation. According to the present experiment on the carp retina, the CSRF
of bipolar cells widens in space and the BPFR is prolonged in time when the retina
is dark adapted, that is, when SNR is presumably low. Interestingly, the same
SNR-dependent properties have also been described in connection with the CSRF
and BPFR of large monopolar cells in the fly compound eye. These spatial and
temporal observations are both in accord with a notion that a method to remove
noise is smoothing which requires averaging for a sufficiently long interval. In other
words, when SNR is low , the signal averaging takes place over a large portion of
the spatio-temporal domain comprised of CSRF and BPFR. Smoothing and differentiation are entirely opposite in the signal processing role. The SNR dependency
of the CSRF and BPFR profiles can be viewed as a compromise between these two
operations, for the need to detect signal changes in the presence of noise. These

(a)

(b)

~

0

10

20

-io

(c)

0

10

10

20

Figure 5: (a) A sample set of training patterns with different background noise
levels (NLs). The NLs are 0.0, 0.4, 1.0 from bottom to top. The receptive field
profiles (b) and flash responses (c) after training with each NL. The ordinate scale
is linear but in arbitrary unit, with the zero level indicated by dotted lines.

Plasticity of Center-Surround Opponent Receptive Fields

165

points parallel the results of information-theoretic analysis by Atick and Redlich
(1992) and by Laughlin (1982).

5

CONCLUDING REMARKS

We have learnt from this study that the same software is at work for the SNRdependent control of the spati~temporal visual receptive field in entirely different
hardwares; namely, vertebrate, invertebrate and artificial neural systems. In other
words, the plasticity scheme represents nature's optimum answer to the visual functional demand, not a result of compromise with other factors such as metabolism
or morphology. Some mention needs to be made of the standard regularization
theory. If the theory is applied to the edge detection problem, then one obtains
the Laplacian-Gaussian filter which is a well-known CSRF example(Torre & Poggio, 1980). And, the shape of this spatial filter can be made wide or narrow by
manipulating the value of a constant usually referred to as the regularization parameter . This parameter choice corresponds to the compromise that our ANN finds
autonomously between smoothing and differentiation. The present type of research
aided by trainable artificial neural networks seems to be a useful top-down approach
to gain insight into the brain and neural mechanisms. Earlier , Lehky and Sejnowski
(1988) were able to create neuron-like units similar to the complex cells of the visual
cortex by using the backpropagation algorithm, however, the CSRF mechanism was
given a priori to an early stage in their ANN processor. It should also be noted that
Linsker (1986) succeeded in self-organization of CSRFs in an ANN model that operates under the learning law of Hebb. Perhaps, it remains to be examined whether
the CSRFs formed in such an unsupervised learning paradigm might also possess
an SNR-dependent plasticity similar to that described in this paper.

References
Atick, J .J. & Redlich, A.N . (1992) What does the retina know about natural scenes?
Neural Computation, 4, 196-210.
Dubs, A. (1982) The spatial integration of signals in the retina and lamina of the fly
compound eye under different conditions of luminance. 1. Compo Physiol A, 146,
321-334.
Furukawa, T. & Yasui, S. (1990) Development of center-surround opponent receptive
fields in a neural network through backpropagation training. Proc. Int. Con/. Fuzzy
Logic & Neural Networks (Iizuka, Japan) 473-490.
Joshi, A. & Lee, C.H . (1993) Backpropagation learns Marr's operator Bioi. Cybern.,
10, 65-73.
Laughlin, S. B. (1982) Matching coding to scenes to enhance efficiency. In Braddick
OJ, Sleigh AC(eds) The physical and biological processing of images (pp.42-52).
Springer, Berlin, Heidelberg New York.
Lehky, S. R. & Sejnowski, T. J. (1988) Network model of shape-from shading: neural
function arises from both receptive and projective fields . Nature, 333, 452-454.
Linsker, R. (1986) From basic network principles to neural architecture: Emergence
of spatial-opponent cells. Proc. Natl. Acad. Sci. USA, 83, 7508-7512.
Stork, D. G. & Hall, J. (1989) Is backpropagation biologically plausible? International Join Con/. Neural Networks, II (Washington DC), 241-246.
Torre, V . & Poggio, T. A. (1986) On edge detection. IEEE Trans. Pattern Anal.
Machine Intel. , PAMI-8, 147-163.

PART III
THEORY

"
