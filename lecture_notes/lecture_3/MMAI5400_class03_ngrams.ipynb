{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5et293vU3SX"
   },
   "source": [
    "## Toy example\n",
    "\n",
    "Let's look at the miniature corpus from the class:\n",
    "\n",
    "    I am Sam. \n",
    "    Sam I am. \n",
    "    I do not like green eggs and ham.\n",
    "    \n",
    "### Pre-processing\n",
    "\n",
    "#### Tokenize the text into sentences\n",
    "\n",
    "Split the text into a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "id": "_1jIxlYeCrUM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/christine/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "id": "ApvviFoDCrUN"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"I am Sam. Sam I am. I do not like green eggs and ham.\"\n",
    "\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN6CPi8PCrUO"
   },
   "source": [
    "#### Tokenize the sentences into lists of words\n",
    "\n",
    "The text will become a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "id": "MSVyv09OCrUQ"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize \n",
    "tokenized_text = []\n",
    "\n",
    "# replace('.', '') removes the period.\n",
    "for sentence in sentences:\n",
    "    tokenized_text.append(word_tokenize(sentence.replace('.', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'am', 'Sam'],\n",
       " ['Sam', 'I', 'am'],\n",
       " ['I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham']]"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial question 1**: How many lists are in `tokenized_text`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_text)\n",
    "\n",
    "#trick question - include the outer nested list, so it's 3 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyhJTAnXCrUR"
   },
   "source": [
    "### Get the N-grams\n",
    "\n",
    "We'll get unigrams, bigrams and trigrams.\n",
    "\n",
    "Note that this is not the N-gram **language model**, only the two and three word sequences.\n",
    "\n",
    "Let's start with **bigrams**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "id": "JwhFktfPCrUS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "bigram_list = []\n",
    "\n",
    "for sentence in tokenized_text:\n",
    "    bigram_list.extend(list(bigrams(sentence, pad_right=True, pad_left=True, right_pad_symbol='</s>', left_pad_symbol='<s>')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial question 2:** What is it the last bigram in `bigram_list`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ham', '</s>')"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_length = len(bigram_list)\n",
    "\n",
    "bigram_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('am', 'Sam')"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_list[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZMPnGe3CrUT"
   },
   "source": [
    "**Trigrams**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "id": "C2UdQmL3CrUU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import trigrams\n",
    "trigram_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate `trigram_list` with trigrams using `trigrams` from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in tokenized_text:\n",
    "    trigram_list.extend(list(trigrams(sentence, pad_right=True, pad_left=True, right_pad_symbol='</s>', left_pad_symbol='<s>')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', '<s>', 'I'),\n",
       " ('<s>', 'I', 'am'),\n",
       " ('I', 'am', 'Sam'),\n",
       " ('am', 'Sam', '</s>'),\n",
       " ('Sam', '</s>', '</s>'),\n",
       " ('<s>', '<s>', 'Sam'),\n",
       " ('<s>', 'Sam', 'I'),\n",
       " ('Sam', 'I', 'am'),\n",
       " ('I', 'am', '</s>'),\n",
       " ('am', '</s>', '</s>'),\n",
       " ('<s>', '<s>', 'I'),\n",
       " ('<s>', 'I', 'do'),\n",
       " ('I', 'do', 'not'),\n",
       " ('do', 'not', 'like'),\n",
       " ('not', 'like', 'green'),\n",
       " ('like', 'green', 'eggs'),\n",
       " ('green', 'eggs', 'and'),\n",
       " ('eggs', 'and', 'ham'),\n",
       " ('and', 'ham', '</s>'),\n",
       " ('ham', '</s>', '</s>')]"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial question 3**: What is the thrid trigram in `trigram_list`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I', 'am', 'Sam')"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_list[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkjoQXHVCrUW"
   },
   "source": [
    "### N-gram language model\n",
    "\n",
    "For the bigrams we need first to get the counts of individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "id": "PXSnzLzOCrUY",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'Sam', '<s>', '</s>', 'Sam', 'I', 'am', '<s>', '</s>', 'I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '<s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "unigram_list = []\n",
    "\n",
    "for sentence in tokenized_text:\n",
    "    unigram_list.extend(sentence)\n",
    "    unigram_list.append('<s>')\n",
    "    unigram_list.append('</s>')\n",
    "\n",
    "print(unigram_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajyHfBkxCrUa"
   },
   "source": [
    "#### Maximum Likelyhood Estimate (MLE) bigram model\n",
    "\n",
    "Populate the dictionary `bigram_lm` with the bigrams as keys and their corresponding probablities as values.\n",
    "So, for each bigram compute it's probability and store as a value in `bigram_lm` under the key given by the bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "id": "8H2zs_8xCrUa"
   },
   "outputs": [],
   "source": [
    "bigram_lm = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bigram in bigram_list:\n",
    "    \n",
    "    bigram_lm[bigram] = bigram_list.count(bigram) / unigram_list.count(bigram[0])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<s>', 'I'): 0.6666666666666666,\n",
       " ('I', 'am'): 0.6666666666666666,\n",
       " ('am', 'Sam'): 0.5,\n",
       " ('Sam', '</s>'): 0.5,\n",
       " ('<s>', 'Sam'): 0.3333333333333333,\n",
       " ('Sam', 'I'): 0.5,\n",
       " ('am', '</s>'): 0.5,\n",
       " ('I', 'do'): 0.3333333333333333,\n",
       " ('do', 'not'): 1.0,\n",
       " ('not', 'like'): 1.0,\n",
       " ('like', 'green'): 1.0,\n",
       " ('green', 'eggs'): 1.0,\n",
       " ('eggs', 'and'): 1.0,\n",
       " ('and', 'ham'): 1.0,\n",
       " ('ham', '</s>'): 1.0}"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial question 4**: What is the probability of the bigram `('I', 'do')`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability is 0.33\n"
     ]
    }
   ],
   "source": [
    "bigram_lm[(\"I\", \"do\")]\n",
    "\n",
    "print(f\"Probability is {bigram_lm[('I', 'do')]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ5EAAWlCrUc"
   },
   "source": [
    "#### MLE trigram model\n",
    "\n",
    "First we need to add ```<s><s>``` and ```</s></s>``` to the bigram_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "id": "20YR7fgfCrUc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "\t [('<s>', 'I'), ('I', 'am'), ('am', 'Sam'), ('Sam', '</s>'), ('<s>', 'Sam'), ('Sam', 'I'), ('I', 'am'), ('am', '</s>'), ('<s>', 'I'), ('I', 'do'), ('do', 'not'), ('not', 'like'), ('like', 'green'), ('green', 'eggs'), ('eggs', 'and'), ('and', 'ham'), ('ham', '</s>')]\n",
      "After:\n",
      "\t [('<s>', 'I'), ('I', 'am'), ('am', 'Sam'), ('Sam', '</s>'), ('<s>', 'Sam'), ('Sam', 'I'), ('I', 'am'), ('am', '</s>'), ('<s>', 'I'), ('I', 'do'), ('do', 'not'), ('not', 'like'), ('like', 'green'), ('green', 'eggs'), ('eggs', 'and'), ('and', 'ham'), ('ham', '</s>'), ('<s>', '<s>'), ('<s>', '<s>'), ('<s>', '<s>'), ('</s>', '</s>'), ('</s>', '</s>'), ('</s>', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "left_pads = []\n",
    "right_pads = []\n",
    "\n",
    "print('Before:\\n\\t', bigram_list)\n",
    "\n",
    "for bigram in bigram_list:\n",
    "        if '<s>' in bigram:\n",
    "            left_pads.append(('<s>', '<s>'))\n",
    "        if '</s>' in bigram:\n",
    "            right_pads.append(('</s>', '</s>'))\n",
    "            \n",
    "bigram_list.extend(left_pads)\n",
    "bigram_list.extend(right_pads)\n",
    "\n",
    "print('After:\\n\\t', bigram_list)\n",
    "\n",
    "# ASK HJALMAR - WHY ARE WE ADDING THE DOUBLE STARTER AND ENDING TOKENS TO THE BIGRAM LIST?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the dictionary `trigram_lm` with the trigrams as keys and their corresponding probablities as values. So, for each trigram compute it's probability and store as a value in `trigram_lm` under the key given by the trigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "id": "3uiW4KxVCrUd"
   },
   "outputs": [],
   "source": [
    "trigram_lm = {}\n",
    "for trigram in trigram_list:\n",
    "    trigram_lm[trigram] = trigram_list.count(trigram) / bigram_list.count(trigram[0:2])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<s>', '<s>', 'I'): 0.6666666666666666,\n",
       " ('<s>', 'I', 'am'): 0.5,\n",
       " ('I', 'am', 'Sam'): 0.5,\n",
       " ('am', 'Sam', '</s>'): 1.0,\n",
       " ('Sam', '</s>', '</s>'): 1.0,\n",
       " ('<s>', '<s>', 'Sam'): 0.3333333333333333,\n",
       " ('<s>', 'Sam', 'I'): 1.0,\n",
       " ('Sam', 'I', 'am'): 1.0,\n",
       " ('I', 'am', '</s>'): 0.5,\n",
       " ('am', '</s>', '</s>'): 1.0,\n",
       " ('<s>', 'I', 'do'): 0.5,\n",
       " ('I', 'do', 'not'): 1.0,\n",
       " ('do', 'not', 'like'): 1.0,\n",
       " ('not', 'like', 'green'): 1.0,\n",
       " ('like', 'green', 'eggs'): 1.0,\n",
       " ('green', 'eggs', 'and'): 1.0,\n",
       " ('eggs', 'and', 'ham'): 1.0,\n",
       " ('and', 'ham', '</s>'): 1.0,\n",
       " ('ham', '</s>', '</s>'): 1.0}"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trigram in trigram_list:\n",
    "#     print(\"---\")\n",
    "#     print(trigram)\n",
    "#     print(trigram[0:2])\n",
    "#     print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# left_pads = []\n",
    "# right_pads = []\n",
    "\n",
    "# print('Before:\\n\\t', bigram_list)\n",
    "\n",
    "# for bigram in bigram_list:\n",
    "#         if '<s>' in bigram:\n",
    "#             left_pads.append(('<s>', '<s>'))\n",
    "#         if '</s>' in bigram:\n",
    "#             right_pads.append(('</s>', '</s>'))\n",
    "            \n",
    "# bigram_list.extend(left_pads)\n",
    "# bigram_list.extend(right_pads)\n",
    "\n",
    "# print('After:\\n\\t', bigram_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial question 5**: What is the probability of the bigram `('I', 'am', '</s>')`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_lm[('I', 'am', '</s>')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqshVfJNVWqu"
   },
   "source": [
    "## Real N-gram example\n",
    "\n",
    "### Data\n",
    "\n",
    "We'll use the Reuters corpus. Reuters is a news agency, and the consists of 1.3M words in 10k news documents.\n",
    "\n",
    "#### Download with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "id": "LcZWjcR2CrUf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/christine/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FISntBKkCrUg"
   },
   "source": [
    "#### Load and look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "id": "c928Fi3LCrUg",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.']\n",
      "['Now', 'it', \"'\", 's', 'largely', 'out', 'of', 'their', 'hands', ',\"', 'said', 'Kleinwort', 'Benson', 'Ltd', 'financial', 'analyst', 'Simon', 'Smithson', '.']\n",
      "['\"', 'The', 'government', ',', 'however', ',', 'does', 'not', 'want', 'to', 'accelerate', 'reducing', 'the', 'debt', 'by', 'making', 'an', 'excessive', 'trade', 'surplus', ',\"', 'he', 'said', '.']\n",
      "['This', 'was', 'after', 'taking', 'into', 'account', 'inflation', 'in', 'consumer', 'prices', 'of', '1', '.', '5', 'pct', 'in', '1985', ',', 'slowing', 'to', '1', '.', '0', 'pct', 'in', '1986', '.']\n",
      "['De', 'los', 'Angeles', \"'\", 'earlier', 'complaint', 'related', 'to', 'SMC', 'assuming', 'last', 'December', 'a', '26', '.', '5', 'mln', 'dlr', 'loan', 'contracted', 'by', 'SMC', \"'\", 's', 'Hong', 'Kong', 'subsidiary', '&', 'lt', ';', 'Neptunia', 'Corp', '>', 'for', 'a', 'down', 'payment', 'on', 'the', 'shares', '.']\n",
      "['RUBBERMAID', 'INC', '1ST', 'QTR', 'SHR', '28', 'CTS', 'VS', '22', 'CTS', 'RUBBERMAID', 'INC', '1ST', 'QTR', 'SHR', '28', 'CTS', 'VS', '22', 'CTS']\n",
      "['Mead', 'said', 'sales', 'of', 'its', 'unbleached', 'coated', 'paperboard', 'was', 'particularly', 'strong', ',', 'up', '13', 'pct', 'versus', 'the', 'first', 'quarter', '1986', '.']\n",
      "['A', 'year', 'earlier', 'the', 'figure', 'was', '28', ',', '763', '.']\n",
      "['Revenues', 'in', 'the', 'quarter', 'rose', '1', '.', '4', 'pct', 'to', '1', '.', '75', 'billion', 'dlrs', ',', 'it', 'said', '.']\n",
      "['SOUND', 'WAREHOUSE', 'INC', '&', 'lt', ';', 'SWHI', '>', '3RD', 'QTR', 'FEB', '28', 'NET', 'Shr', '26', 'cts', 'vs', '52', 'cts', 'Net', '1', ',', '386', ',', '000', 'vs', '2', ',', '773', ',', '000', 'Revs', '47', '.', '7', 'mln', 'vs', '38', '.', '5', 'mln', 'Nine', 'mths', 'Shr', '52', 'cts', 'vs', '97', 'cts', 'Net', '2', ',', '765', ',', '000', 'vs', '5', ',', '419', ',', '000', 'Revs', '116', '.', '9', 'mln', 'vs', '97', 'mln']\n",
      "['JUDY', \"'\", 'S', 'INC', '&', 'lt', ';', 'JUDY', '>', '4TH', 'QTR', 'JAN', '31', 'LOSS', 'Shr', 'loss', 'two', 'cts', 'vs', 'profit', 'nine', 'cts', 'Net', 'loss', '74', ',', '000', 'vs', 'profit', '418', ',', '000', 'Sales', '18', '.', '2', 'mln', 'vs', '17', '.', '5', 'mln', 'Year', 'Shr', 'profit', 'nine', 'cts', 'vs', 'profit', '26', 'cts', 'Net', 'profit', '426', ',', '000', 'vs', 'profit', '1', ',', '170', ',', '000', 'Sales', '58', '.', '7', 'mln', 'vs', '56', '.', '7', 'mln']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "tokenized_text = reuters.sents()\n",
    "\n",
    "for i, sentence in enumerate(tokenized_text):\n",
    "    if (i % 100) == 0:\n",
    "        print(sentence)\n",
    "    if i > 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePYji9ktCrUg"
   },
   "source": [
    "### N-grams\n",
    "\n",
    "To pad and get all N-grams up to N, we can use a preprocessing pipeline from NLTK.\n",
    "Here we will get tri-grams so we set ```n = 3```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "id": "G9GR_m2eCrUh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: <generator object padded_everygram_pipeline.<locals>.<genexpr> at 0x17cb8ece0>\n",
      "padded_sents: <itertools.chain object at 0x17e4825c0>\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
    "\n",
    "print(\"train_data:\", train_data)\n",
    "print(\"padded_sents:\", padded_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial question 6**: What is a generator object in Python?\n",
    "\n",
    "A generator is a function which returns an object on which you can call next, such that for every call it returns some value, until it raises a StopIteration exception, signalling that all values have been generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WN0qd5ZTCrUi"
   },
   "source": [
    "### Train language model with MLE\n",
    "\n",
    "#### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "id": "HdNPZVYrCrUi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.lm.models.MLE object at 0x30c0adb90>\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import MLE\n",
    "model = MLE(n) # n = 3 (above)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsUdVgg0CrUj"
   },
   "source": [
    "#### Check\n",
    "The model should be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "id": "z-uYLNDmCrUj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4m55f5SCrUk"
   },
   "source": [
    "#### Fit and check again\n",
    "\n",
    "**Tutorial question 7**: What does fit do, and how does it do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "id": "AFO7ACtGCrUk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does it fit?\n",
    "# Counts number of occurences in the N-grams, i.e. trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a vocabulary with plenty of words.\n",
    "\n",
    "**Tutorial question 8**: How many words are there in the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41602"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05gExZaCCrUl"
   },
   "source": [
    "#### Out-of-vocabulary words\n",
    "\n",
    "What will the model do with unknown words?\n",
    "\n",
    "Let's try a sentence from class.\n",
    "\n",
    "Try the sentence:\n",
    "\n",
    "    The suicidal Norway lemming is small, angry & adorable.\n",
    "\n",
    "You can test whether words are present in the vocabulary, and what happens if they are not present, with ```model.vocab.lookup()```.\n",
    "\n",
    "To tokenize the you can simply use ```\"The suicidal Norway lemming is small, angry & adorable.\".split()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "id": "PZjE6DTCCrUl"
   },
   "outputs": [],
   "source": [
    "# tokens = \"The suicidal Norway lemming is small, angry & adorable.\".strip(\".\").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = \"The suicidal Norway lemming is small, angry & adorable.\".strip(\".\").replace(\",\", \"\").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "suicidal\n",
      "Norway\n",
      "lemming\n",
      "is\n",
      "small\n",
      "angry\n",
      "&\n",
      "adorable\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'suicidal',\n",
       " 'Norway',\n",
       " 'lemming',\n",
       " 'is',\n",
       " 'small',\n",
       " 'angry',\n",
       " '&',\n",
       " 'adorable']"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0xSU3X_CrUm"
   },
   "source": [
    "**Tutorial question 9**: What words were missing from the vocabulary, and happened with those words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "id": "cuNHVKXxCrUm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The', 'suicidal', 'Norway', '<UNK>', 'is', 'small', 'angry', '&', '<UNK>')"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look Up\n",
    "model.vocab.lookup(tokens)\n",
    "# lemming and adorable are missing from the vocabulary\n",
    "\n",
    "# for token in tokens:\n",
    "#     print(token)\n",
    "\n",
    "\n",
    "# What happened here??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zKY-cvCCrUm"
   },
   "source": [
    "### Use the N-gram LM\n",
    "\n",
    "#### Check the number of N-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "id": "BxCW_g8JCrUm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 3 ngram orders and 5655195 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(model.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxU92ob3CrUn"
   },
   "source": [
    "Count the number of individual N-grams\n",
    "\n",
    "Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "id": "W7eAaWhbCrUo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(model.counts['angry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGtD8gYOCrUo"
   },
   "source": [
    "Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "id": "8YAgBKiqCrUo",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# C(into | taking)\n",
    "print(model.counts[['taking']]['into'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYG79ePDCrUp"
   },
   "source": [
    "Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "id": "xyGDLYhwCrUp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# C(was | the figure)\n",
    "print(model.counts[['the', 'figure']]['was'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdSN9AkYCrUq"
   },
   "source": [
    "#### Check the N-gram probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "id": "lMQpz0tTCrUq",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(angry): 3.6086547914429515e-06\n",
      "P(into | taking): 0.05851063829787234\n",
      "P(was | the figure): 0.17857142857142858\n"
     ]
    }
   ],
   "source": [
    "# P(angry)\n",
    "print('P(angry):', model.score('angry'))\n",
    "\n",
    "# P(into | taking)\n",
    "print('P(into | taking):', model.score('into', ['taking']))\n",
    "\n",
    "# P(was | the figure)\n",
    "print('P(was | the figure):', model.score('was', ['the', 'figure']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdjiazJ0CrUr"
   },
   "source": [
    "How are unknown words dealt with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "id": "ZIgcjSOzCrUs",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(\"<UNK>\") == model.score(\"lemming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_bewmJCCrUs"
   },
   "source": [
    "#### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "id": "7FSDh7ZwCrUt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'relatively', 'new', 'problem', 'banks', 'in', 'Georgia', 'will', 'be', 'offered', 'intervention', 'grain', 'is', 'not', 'attached', 'to', 'a', 'select', 'committee', 'that']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(20, random_seed=12))\n",
    "\n",
    "# Sentence does not look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVkz9jtKCrUt"
   },
   "source": [
    "**Tutorial question 10**: Does the generated sequence look natural?\n",
    "\n",
    "**Tutorial question 11**: If not, suggest two improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "id": "kE2DzjxNCrUt"
   },
   "outputs": [],
   "source": [
    "# How to Improve:\n",
    "# Use Trigrams\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
