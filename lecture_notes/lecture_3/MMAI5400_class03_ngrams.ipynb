{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5et293vU3SX"
   },
   "source": [
    "## Toy example\n",
    "\n",
    "Let's look at the miniature corpus from the class:\n",
    "\n",
    "    I am Sam. \n",
    "    Sam I am. \n",
    "    I do not like green eggs and ham.\n",
    "    \n",
    "### Pre-processing\n",
    "\n",
    "#### Tokenize the text into sentences\n",
    "\n",
    "Split the text into a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1jIxlYeCrUM"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApvviFoDCrUN"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"I am Sam. Sam I am. I do not like green eggs and ham.\"\n",
    "\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN6CPi8PCrUO"
   },
   "source": [
    "#### Tokenize the sentences into lists of words\n",
    "\n",
    "The text will become a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSVyv09OCrUQ"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize \n",
    "tokenized_text = []\n",
    "\n",
    "# replace('.', '') removes the period.\n",
    "for sentence in sentences:\n",
    "    tokenized_text.append(word_tokenize(sentence.replace('.', '')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial question 1**: How many lists are in `tokenized_text`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyhJTAnXCrUR"
   },
   "source": [
    "### Get the N-grams\n",
    "\n",
    "We'll get unigrams, bigrams and trigrams.\n",
    "\n",
    "Note that this is not the N-gram **language model**, only the two and three word sequences.\n",
    "\n",
    "Let's start with **bigrams**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwhFktfPCrUS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "bigram_list = []\n",
    "\n",
    "for sentence in tokenized_text:\n",
    "    bigram_list.extend(list(bigrams(sentence, pad_right=True, pad_left=True, right_pad_symbol='</s>', left_pad_symbol='<s>')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial question 2:** What is it the last bigram in `bigram_list`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZMPnGe3CrUT"
   },
   "source": [
    "**Trigrams**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2UdQmL3CrUU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import trigrams\n",
    "\n",
    "trigram_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate `trigram_list` with trigrams using `trigrams` from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial question 3**: What is the thrid trigram in `trigram_list`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkjoQXHVCrUW"
   },
   "source": [
    "### N-gram language model\n",
    "\n",
    "For the bigrams we need first to get the counts of individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXSnzLzOCrUY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unigram_list = []\n",
    "\n",
    "for sentence in tokenized_text:\n",
    "    unigram_list.extend(sentence)\n",
    "    unigram_list.append('<s>')\n",
    "    unigram_list.append('</s>')\n",
    "\n",
    "print(unigram_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajyHfBkxCrUa"
   },
   "source": [
    "#### Maximum Likelyhood Estimate (MLE) bigram model\n",
    "\n",
    "Populate the dictionary `bigram_lm` with the bigrams as keys and their corresponding probablities as values.\n",
    "So, for each bigram compute it's probability and store as a value in `bigram_lm` under the key given by the bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8H2zs_8xCrUa"
   },
   "outputs": [],
   "source": [
    "bigram_lm = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bigram in bigram_list:\n",
    "    \n",
    "    bigram_lm[bigram] = bigram_list.count(bigram) / unigram_list.count(bigram[0])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial question 4**: What is the probability of the bigram `('I', 'do')`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ5EAAWlCrUc"
   },
   "source": [
    "#### MLE trigram model\n",
    "\n",
    "First we need to add ```<s><s>``` and ```</s></s>``` to the bigram_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20YR7fgfCrUc"
   },
   "outputs": [],
   "source": [
    "left_pads = []\n",
    "right_pads = []\n",
    "\n",
    "print('Before:\\n\\t', bigram_list)\n",
    "\n",
    "for bigram in bigram_list:\n",
    "        if '<s>' in bigram:\n",
    "            left_pads.append(('<s>', '<s>'))\n",
    "        if '</s>' in bigram:\n",
    "            right_pads.append(('</s>', '</s>'))\n",
    "            \n",
    "bigram_list.extend(left_pads)\n",
    "bigram_list.extend(right_pads)\n",
    "\n",
    "print('After:\\n\\t', bigram_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the dictionary `trigram_lm` with the trigrams as keys and their corresponding probablities as values. So, for each trigram compute it's probability and store as a value in `trigram_lm` under the key given by the trigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3uiW4KxVCrUd"
   },
   "outputs": [],
   "source": [
    "trigram_lm = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial question 5**: What is the probability of the bigram `('I', 'am', '</s>')`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqshVfJNVWqu"
   },
   "source": [
    "## Real N-gram example\n",
    "\n",
    "### Data\n",
    "\n",
    "We'll use the Reuters corpus. Reuters is a news agency, and the consists of 1.3M words in 10k news documents.\n",
    "\n",
    "#### Download with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcZWjcR2CrUf"
   },
   "outputs": [],
   "source": [
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FISntBKkCrUg"
   },
   "source": [
    "#### Load and look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c928Fi3LCrUg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "tokenized_text = reuters.sents()\n",
    "\n",
    "for i, sentence in enumerate(tokenized_text):\n",
    "    if (i % 100) == 0:\n",
    "        print(sentence)\n",
    "    if i > 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePYji9ktCrUg"
   },
   "source": [
    "### N-grams\n",
    "\n",
    "To pad and get all N-grams up to N, we can use a preprocessing pipeline from NLTK.\n",
    "Here we will get tri-grams so we set ```n = 3```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9GR_m2eCrUh"
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
    "\n",
    "print(\"train_data:\", train_data)\n",
    "print(\"padded_sents:\", padded_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial question 6**: What is a generator object in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WN0qd5ZTCrUi"
   },
   "source": [
    "### Train language model with MLE\n",
    "\n",
    "#### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdNPZVYrCrUi"
   },
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "model = MLE(n) # n = 3 (above)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsUdVgg0CrUj"
   },
   "source": [
    "#### Check\n",
    "The model should be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-uYLNDmCrUj"
   },
   "outputs": [],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4m55f5SCrUk"
   },
   "source": [
    "#### Fit and check again\n",
    "\n",
    "**Tutorial question 7**: What does fit do, and how does it do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFO7ACtGCrUk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a vocabulary with plenty of words.\n",
    "\n",
    "**Tutorial question 8**: How many words are there in the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05gExZaCCrUl"
   },
   "source": [
    "#### Out-of-vocabulary words\n",
    "\n",
    "What will the model do with unknown words?\n",
    "\n",
    "Let's try a sentence from class.\n",
    "\n",
    "Try the sentence:\n",
    "\n",
    "    The suicidal Norway lemming is small, angry & adorable.\n",
    "\n",
    "You can test whether words are present in the vocabulary, and what happens if they are not present, with ```model.vocab.lookup()```.\n",
    "\n",
    "To tokenize the you can simply use ```\"The suicidal Norway lemming is small, angry & adorable.\".split()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZjE6DTCCrUl"
   },
   "outputs": [],
   "source": [
    "tokens = \"The suicidal Norway lemming is small, angry & adorable.\".strip('.').split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0xSU3X_CrUm"
   },
   "source": [
    "**Tutorial question 9**: What words were missing from the vocabulary, and happened with those words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuNHVKXxCrUm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zKY-cvCCrUm"
   },
   "source": [
    "### Use the N-gram LM\n",
    "\n",
    "#### Check the number of N-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxCW_g8JCrUm"
   },
   "outputs": [],
   "source": [
    "print(model.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxU92ob3CrUn"
   },
   "source": [
    "Count the number of individual N-grams\n",
    "\n",
    "Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7eAaWhbCrUo"
   },
   "outputs": [],
   "source": [
    "print(model.counts['angry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGtD8gYOCrUo"
   },
   "source": [
    "Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YAgBKiqCrUo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# C(into | taking)\n",
    "print(model.counts[['taking']]['into'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYG79ePDCrUp"
   },
   "source": [
    "Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyGDLYhwCrUp"
   },
   "outputs": [],
   "source": [
    "# C(was | the figure)\n",
    "print(model.counts[['the', 'figure']]['was'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdSN9AkYCrUq"
   },
   "source": [
    "#### Check the N-gram probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMQpz0tTCrUq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# P(angry)\n",
    "print('P(angry):', model.score('angry'))\n",
    "\n",
    "# P(into | taking)\n",
    "print('P(into | taking):', model.score('into', ['taking']))\n",
    "\n",
    "# P(was | the figure)\n",
    "print('P(was | the figure):', model.score('was', ['the', 'figure']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdjiazJ0CrUr"
   },
   "source": [
    "How are unknown words dealt with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIgcjSOzCrUs",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.score(\"<UNK>\") == model.score(\"lemming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_bewmJCCrUs"
   },
   "source": [
    "#### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FSDh7ZwCrUt"
   },
   "outputs": [],
   "source": [
    "print(model.generate(20, random_seed=12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVkz9jtKCrUt"
   },
   "source": [
    "**Tutorial question 10**: Does the generated sequence look natural?\n",
    "\n",
    "**Tutorial question 11**: If not, suggest two improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kE2DzjxNCrUt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
