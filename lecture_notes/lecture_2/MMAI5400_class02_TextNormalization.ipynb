{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihWLs0TNqcSn"
   },
   "source": [
    "# Text Normalization\n",
    "\n",
    "## Install NLTK\n",
    "\n",
    "```\n",
    "!pip install -U nltk\n",
    "```\n",
    "if you're not in a virtual env\n",
    "```\n",
    "sudo pip install -U nltk\n",
    "```\n",
    "\n",
    "## Simple text normalization with Python\n",
    "\n",
    "### Load data\n",
    "Upload `nlp_history_wikipedia.txt`; it's an excerpt from the Wikipedia page about NLP.\n",
    "\n",
    "**Tutorial question 1**:\n",
    "What Python two-liner can you use to load the file into a string called `text`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "with open(\"nlp_history_wikipedia.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial question 2**: How many characters are there in the string `text`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's 3474 characters\n"
     ]
    }
   ],
   "source": [
    "# for c in text[:10]:\n",
    "#     print(c)\n",
    "len(text)\n",
    "\n",
    "print(\"There's 3474 characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case normalization\n",
    "**Tutorial question 3**: Make all characters lower case using only a string method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history\\nfurther information: history of natural language processing\\n\\nnatural language processing has its roots in the 1950s.[1] already in 1950, alan turing published an article titled \"computing machinery and intelligence\" which proposed what is now called the turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. the proposed test includes a task that involves the automated interpretation and generation of natural language.\\n\\nsymbolic nlp (1950s – early 1990s)\\n\\nthe premise of symbolic nlp is well-summarized by john searle\\'s chinese room experiment: given a collection of rules (e.g., a chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other nlp tasks) by applying those rules to the data it confronts.\\n\\n * 1950s: the georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english. the authors claimed that within three or five years, machine translation would be a solved problem.[2] however, real progress was much slower, and after the alpac report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. little further research in machine translation was conducted in america (though some research continued elsewhere, such as japan and europe[3]) until the late 1980s when the first statistical machine translation systems were developed.\\n * 1960s: some notably successful natural language processing systems developed in the 1960s were shrdlu, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and eliza, a simulation of a rogerian psychotherapist, written by joseph weizenbaum between 1964 and 1966. using almost no information about human thought or emotion, eliza sometimes provided a startlingly human-like interaction. when the \"patient\" exceeded the very small knowledge base, eliza might provide a generic response, for example, responding to \"my head hurts\" with \"why do you say your head hurts?\". ross quillian\\'s successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time.[4]\\n\\n * 1970s: during the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. examples are margie (schank, 1975), sam (cullingford, 1978), pam (wilensky, 1978), talespin (meehan, 1976), qualm (lehnert, 1977), politics (carbonell, 1979), and plot units (lehnert 1981). during this time, the first chatterbots were written (e.g., parry).\\n * 1980s: the 1980s and early 1990s mark the heyday of symbolic methods in nlp. focus areas of the time included research on rule-based parsing (e.g., the development of hpsg as a computational operationalization of generative grammar), morphology (e.g., two-level morphology[5]), semantics (e.g., lesk algorithm), reference (e.g., within centering theory[6]) and other areas of natural language understanding (e.g., in the rhetorical structure theory). other lines of research were continued, e.g., the development of chatterbots with racter and jabberwacky. an important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.[7]\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lower case entire string\n",
    "new_text = text.lower()\n",
    "\n",
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history further information: history of natural language processing  natural language processing has its roots in the 1950s.[1] already in 1950, alan turing published an article titled \"computing machinery and intelligence\" which proposed what is now called the turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. the proposed test includes a task that involves the automated interpretation and generation of natural language.  symbolic nlp (1950s – early 1990s)  the premise of symbolic nlp is well-summarized by john searle\\'s chinese room experiment: given a collection of rules (e.g., a chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other nlp tasks) by applying those rules to the data it confronts.   * 1950s: the georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english. the authors claimed that within three or five years, machine translation would be a solved problem.[2] however, real progress was much slower, and after the alpac report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. little further research in machine translation was conducted in america (though some research continued elsewhere, such as japan and europe[3]) until the late 1980s when the first statistical machine translation systems were developed.  * 1960s: some notably successful natural language processing systems developed in the 1960s were shrdlu, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and eliza, a simulation of a rogerian psychotherapist, written by joseph weizenbaum between 1964 and 1966. using almost no information about human thought or emotion, eliza sometimes provided a startlingly human-like interaction. when the \"patient\" exceeded the very small knowledge base, eliza might provide a generic response, for example, responding to \"my head hurts\" with \"why do you say your head hurts?\". ross quillian\\'s successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time.[4]   * 1970s: during the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. examples are margie (schank, 1975), sam (cullingford, 1978), pam (wilensky, 1978), talespin (meehan, 1976), qualm (lehnert, 1977), politics (carbonell, 1979), and plot units (lehnert 1981). during this time, the first chatterbots were written (e.g., parry).  * 1980s: the 1980s and early 1990s mark the heyday of symbolic methods in nlp. focus areas of the time included research on rule-based parsing (e.g., the development of hpsg as a computational operationalization of generative grammar), morphology (e.g., two-level morphology[5]), semantics (e.g., lesk algorithm), reference (e.g., within centering theory[6]) and other areas of natural language understanding (e.g., in the rhetorical structure theory). other lines of research were continued, e.g., the development of chatterbots with racter and jabberwacky. an important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.[7] '"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacement\n",
    "new_text = new_text.replace(\"\\n\", \" \")\n",
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all non-alphanumeric characters\n",
    "# non_alpha_list = [\".\", \"[\", \"]\", \"?\", \",\", \"$\", \"%\", \"^\", \"@\", \"&\", \":\", \";\"]\n",
    "\n",
    "# Remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ' '\n",
    "a.isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove special non-alpha numeric characters\n",
    "# text_2 = re.sub(r'[^a-zA-Z0-9 ]', '', new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = re.sub(r'\\W+', \" \", new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history further information history of natural language processing natural language processing has its roots in the 1950s 1 already in 1950 alan turing published an article titled computing machinery and intelligence which proposed what is now called the turing test as a criterion of intelligence though at the time that was not articulated as a problem separate from artificial intelligence the proposed test includes a task that involves the automated interpretation and generation of natural language symbolic nlp 1950s early 1990s the premise of symbolic nlp is well summarized by john searle s chinese room experiment given a collection of rules e g a chinese phrasebook with questions and matching answers the computer emulates natural language understanding or other nlp tasks by applying those rules to the data it confronts 1950s the georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english the authors claimed that within three or five years machine translation would be a solved problem 2 however real progress was much slower and after the alpac report in 1966 which found that ten years of research had failed to fulfill the expectations funding for machine translation was dramatically reduced little further research in machine translation was conducted in america though some research continued elsewhere such as japan and europe 3 until the late 1980s when the first statistical machine translation systems were developed 1960s some notably successful natural language processing systems developed in the 1960s were shrdlu a natural language system working in restricted blocks worlds with restricted vocabularies and eliza a simulation of a rogerian psychotherapist written by joseph weizenbaum between 1964 and 1966 using almost no information about human thought or emotion eliza sometimes provided a startlingly human like interaction when the patient exceeded the very small knowledge base eliza might provide a generic response for example responding to my head hurts with why do you say your head hurts ross quillian s successful work on natural language was demonstrated with a vocabulary of only twenty words because that was all that would fit in a computer memory at the time 4 1970s during the 1970s many programmers began to write conceptual ontologies which structured real world information into computer understandable data examples are margie schank 1975 sam cullingford 1978 pam wilensky 1978 talespin meehan 1976 qualm lehnert 1977 politics carbonell 1979 and plot units lehnert 1981 during this time the first chatterbots were written e g parry 1980s the 1980s and early 1990s mark the heyday of symbolic methods in nlp focus areas of the time included research on rule based parsing e g the development of hpsg as a computational operationalization of generative grammar morphology e g two level morphology 5 semantics e g lesk algorithm reference e g within centering theory 6 and other areas of natural language understanding e g in the rhetorical structure theory other lines of research were continued e g the development of chatterbots with racter and jabberwacky an important development that eventually led to the statistical turn in the 1990s was the rising importance of quantitative evaluation in this period 7 '"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_2\n",
    "\n",
    "# # 3320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(text_2[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Replace all new line characters with space.\n",
    "2. Use regular expressions to remove all non-alphanumeric characters (`.`, `[`, `]`, `,`, `?`, etc).\n",
    "\n",
    "**Tutorial question 4**: How long is `text` after the two steps above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3320"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_2)\n",
    "\n",
    "# It's 3474 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Use only Python string method(s) to split `text` by whitespace and call the output `tokens`.\n",
    "\n",
    "**Tutorial question 5**: How many tokens are in `tokens`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3320"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "521"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_2.split(\" \"))\n",
    "\n",
    "# 521"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text normalization with NLTK\n",
    "\n",
    "[NLTK](https://www.nltk.org/) - the natural language toolkit - has built-in loaders for some datasets.\n",
    "### Interactive with a GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "nyGlKYuPqcS1"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    " # interactive download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbztoBgBqcS2"
   },
   "source": [
    "### Direct download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "-LdR0qICqcS4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/christine/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mweES_fhqcS5"
   },
   "source": [
    "#### The Brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "QvQcpyKUqcS5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/christine/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgqLTXFhqcS6"
   },
   "source": [
    "### Word tokenization – basic\n",
    "\n",
    "Tokenize the sentence `\"Your computer is getting hacked!\"` using NLTK's `word_tokenize`.\n",
    "\n",
    "**Tutorial question 6**: How many tokens were returned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "5-9vUuScqcS8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(nltk.word_tokenize(text_2))\n",
    "\n",
    "a = nltk.word_tokenize(\"Your computer is getting hacked!\")\n",
    "len(a) #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Your', 'computer', 'is', 'getting', 'hacked', '!']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qy2iLBXwqcS9"
   },
   "source": [
    "## Stemming using the Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "tzogLOHhqcS-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming results:\n",
      "-----------------\n",
      "It it\n",
      "is is\n",
      "important import\n",
      "to to\n",
      "be be\n",
      "very veri\n",
      "pythonly pythonli\n",
      "while while\n",
      "you you\n",
      "are are\n",
      "pythoning python\n",
      "with with\n",
      "python python\n",
      ". .\n",
      "All all\n",
      "pythoners python\n",
      "have have\n",
      "pythoned python\n",
      "poorly poorli\n",
      "at at\n",
      "least least\n",
      "once onc\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "doc = \"It is important to be very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
    "words = word_tokenize(doc)\n",
    "print('Stemming results:\\n-----------------')\n",
    "for w in words:\n",
    "    print(w, ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6kXu6JKqcTB"
   },
   "source": [
    "## Lemmatization using the WordNetLemmatizer\n",
    "\n",
    "Download `wordnet` and `averaged_perceptron_tagger`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "bJwsXfclqcTB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/christine/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/christine/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "dz61dJXHqcTC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatizing results:\n",
      "--------------------\n",
      "It it\n",
      "is be\n",
      "important important\n",
      "to to\n",
      "be be\n",
      "very very\n",
      "pythonly pythonly\n",
      "while while\n",
      "you you\n",
      "are be\n",
      "pythoning pythoning\n",
      "with with\n",
      "python python\n",
      ". .\n",
      "All all\n",
      "pythoners pythoners\n",
      "have have\n",
      "pythoned pythoned\n",
      "poorly poorly\n",
      "at at\n",
      "least least\n",
      "once once\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "tag_map['N'] = wn.NOUN\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, text):\n",
    "        lemmatized = []\n",
    "        for token, tag in pos_tag(word_tokenize(text)):\n",
    "            lemmatized.append(self.wnl.lemmatize(token.lower(), tag_map[tag[0]]))\n",
    "\n",
    "        return lemmatized\n",
    "\n",
    "text = \"It is important to be very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
    "lt = LemmaTokenizer()\n",
    "lemmatized = lt(text)\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(\"\\nLemmatizing results:\\n--------------------\")\n",
    "for word, lemma in zip(words, lemmatized):\n",
    "    print(word, lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIDuCXhvqcTD"
   },
   "source": [
    "## Remove stop words\n",
    "\n",
    "Download `stopwords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "nu9dENVuqcTD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/christine/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Tutorial question 7**: How many English stopwords are there in `stopwords` from `nltk.corpus`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "eQZob1BmqcTE"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stoplist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMUMLLVgqcTE"
   },
   "source": [
    "Remove the stopwords from the text below, and print the new text.\n",
    "\n",
    "**Tutorial question 8**: How many tokens remain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "KQpRSglbqcTF"
   },
   "outputs": [],
   "source": [
    "text = \"It is important to be very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "\n",
    "for word in text:\n",
    "    if word not in stoplist:\n",
    "        new_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " ' ',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'n',\n",
       " ' ',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " ' ',\n",
       " 'v',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 'p',\n",
       " 'h',\n",
       " 'n',\n",
       " 'l',\n",
       " ' ',\n",
       " 'w',\n",
       " 'h',\n",
       " 'l',\n",
       " 'e',\n",
       " ' ',\n",
       " 'u',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'p',\n",
       " 'h',\n",
       " 'n',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'w',\n",
       " 'h',\n",
       " ' ',\n",
       " 'p',\n",
       " 'h',\n",
       " 'n',\n",
       " '.',\n",
       " ' ',\n",
       " 'A',\n",
       " 'l',\n",
       " 'l',\n",
       " ' ',\n",
       " 'p',\n",
       " 'h',\n",
       " 'n',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 'h',\n",
       " 'v',\n",
       " 'e',\n",
       " ' ',\n",
       " 'p',\n",
       " 'h',\n",
       " 'n',\n",
       " 'e',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'l',\n",
       " ' ',\n",
       " ' ',\n",
       " 'l',\n",
       " 'e',\n",
       " ' ',\n",
       " 'n',\n",
       " 'c',\n",
       " 'e',\n",
       " '.']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jB8_j3d0qcTG"
   },
   "source": [
    "## Extra: visualize a corpus using WordCloud\n",
    "\n",
    "### install\n",
    "```\n",
    "pip install -U wordcloud\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "nNgROEsMqcTG"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\", stopwords=['nn', 'pp', 'cc'])\n",
    "wordcloud.generate(nltk.corpus.brown.raw())\n",
    "fig = plt.figure(figsize=[10, 16])\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "MMAI5400_class01_TextNormalization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
