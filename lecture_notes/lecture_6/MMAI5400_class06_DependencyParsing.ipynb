{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0PGjQoqQHwk_",
   "metadata": {
    "id": "0PGjQoqQHwk_"
   },
   "source": [
    "# Intro to spaCy and Dependency Parsing\n",
    "\n",
    "Modified from [Natural Language Processing With spaCy in Python](https://realpython.com/natural-language-processing-spacy-python/) by Taranjeet Singh.\n",
    "\n",
    "\n",
    "## Getting started with spaCy \n",
    "\n",
    "Load the language model instance in spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yc8MsHqBHS0T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yc8MsHqBHS0T",
    "outputId": "42ed5fe9-a9fd-4dae-94e5-7387d3437905"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MNG4ATBKRsLc",
   "metadata": {
    "id": "MNG4ATBKRsLc"
   },
   "source": [
    "In case you get the following error:\n",
    "```Python\n",
    "OSError: [E053] Could not read config file from /usr/local/lib/python3.7/dist-packages/en_core_web_sm/en_core_web_sm-2.2.5/config.cfg\n",
    "```\n",
    "Try running\n",
    "```\n",
    "!pip install spacytextblob\n",
    "!python -m textblob.download_corpora\n",
    "!python -m spacy download en_core_web_sm\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rhyEKBc7Rjih",
   "metadata": {
    "id": "rhyEKBc7Rjih"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "KdMeFrINJ6NU",
   "metadata": {
    "id": "KdMeFrINJ6NU"
   },
   "source": [
    "### Check the attributes of the object called `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bwo-WtCgJ3AJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bwo-WtCgJ3AJ",
    "outputId": "f7f171d5-00f1-43f3-d1e0-697b236342df"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VhmNFtX2h5zE",
   "metadata": {
    "id": "VhmNFtX2h5zE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "nqXzrh6LHjOT",
   "metadata": {
    "id": "nqXzrh6LHjOT"
   },
   "source": [
    "### Read and tokenize a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_BcsF8ZZHWlz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BcsF8ZZHWlz",
    "outputId": "4e23882a-ebb4-4499-bb09-4a93ceb51292"
   },
   "outputs": [],
   "source": [
    "text = 'This tutorial is about Natural Language Processing using spaCy.'\n",
    "doc = nlp(text)\n",
    "# Extract tokens in the given doc\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5lcDbAZYIZhk",
   "metadata": {
    "id": "5lcDbAZYIZhk"
   },
   "source": [
    "### Read and tokenize a text file\n",
    "\n",
    "#### Write the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3x_o280sINF0",
   "metadata": {
    "id": "3x_o280sINF0"
   },
   "outputs": [],
   "source": [
    "text = (\"This tutorial is about Natural Language Processing using spaCy.\"\n",
    "        \"But, you know that already.\"\n",
    "        \"Notice how the file is written differently than it is read.\")\n",
    "\n",
    "fname = 'text.txt'\n",
    "with open(fname, 'w') as f:\n",
    "  f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GNx0HFmXI0nf",
   "metadata": {
    "id": "GNx0HFmXI0nf"
   },
   "source": [
    "#### Read the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R3n0sq5KIy0A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3n0sq5KIy0A",
    "outputId": "04aa9235-9d74-44c9-d9c7-626048cee8ca"
   },
   "outputs": [],
   "source": [
    "text_from_file = open(fname).read()\n",
    "doc_from_file = nlp(text_from_file)\n",
    "# Extract tokens from the text file.\n",
    "print([token.text for token in doc_from_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sIAIh3WWMqTD",
   "metadata": {
    "id": "sIAIh3WWMqTD"
   },
   "source": [
    "## Sentence detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L-mV2-IqJTUU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-mV2-IqJTUU",
    "outputId": "fac5b641-c381-4dac-92ea-e452c11e689c"
   },
   "outputs": [],
   "source": [
    "text = (\"This tutorial is about Natural Language Processing using spaCy. \"\n",
    "        \"But, you know that already. 654. \"\n",
    "        \"However, I'm just going to keep adding text here because I have \"\n",
    "        \"nothing better to do with my life. It is getting a bit predictable \"\n",
    "        \"but I'm still adding this final sentence.\")\n",
    "doc = nlp(text)\n",
    "sentences = list(doc.sents)\n",
    "\n",
    "print('There are', len(sentences), 'sentences in the doc.\\n')\n",
    "\n",
    "print('Sentences:')\n",
    "for sentence in sentences:\n",
    "    print('\\t', sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NeGtsENtNtqj",
   "metadata": {
    "id": "NeGtsENtNtqj"
   },
   "source": [
    "### Custom sentence delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uGBBwB76NSw-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGBBwB76NSw-",
    "outputId": "ff8fadb9-41ba-4161-fff7-6057d22364ad"
   },
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component('custom_boundaries')\n",
    "def set_custom_boundaries(doc):\n",
    "    # Adds support to use `...` as the delimiter for sentence detection\n",
    "\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == '...':\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "\n",
    "ellipsis_text = ('Hi, can you, ... never mind, I forgot'\n",
    "                 ' what I was saying. So, do you think'\n",
    "                 ' we should ...')\n",
    "\n",
    "# Load a new model instance\n",
    "custom_nlp = spacy.load('en_core_web_sm')\n",
    "# Add the custom method to the langauge model pipeline.\n",
    "custom_nlp.add_pipe('custom_boundaries', before='parser')\n",
    "# Process the doc\n",
    "custom_ellipsis_doc = custom_nlp(ellipsis_text)\n",
    "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\n",
    "\n",
    "print('Custom sentence tokenization:')\n",
    "for sentence in custom_ellipsis_sentences:\n",
    "    print('\\t', sentence)\n",
    "\n",
    "# Sentence tokenization with no customization\n",
    "ellipsis_doc = nlp(ellipsis_text)\n",
    "ellipsis_sentences = list(ellipsis_doc.sents)\n",
    "print('\\nDefault sentence tokenization:')\n",
    "for sentence in ellipsis_sentences:\n",
    "    print('\\t', sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JwYoxJN7Q3zQ",
   "metadata": {
    "id": "JwYoxJN7Q3zQ"
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "\n",
    "In spaCy, we can print tokens by iterating on the `Doc` object. The token index is accessible in `token.idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VDlilxaiOEeP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDlilxaiOEeP",
    "outputId": "884a7017-393a-470d-e641-6d2ecb543ee9"
   },
   "outputs": [],
   "source": [
    "print('index\\ttoken\\n-------------')\n",
    "for token in doc:\n",
    "    print(f'{token.idx}\\t{token}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-Ay20JD7oKFI",
   "metadata": {
    "id": "-Ay20JD7oKFI"
   },
   "source": [
    "The token index is the starting position of the word in the orignal string. This is useful for example when replacing words.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fePR_j3RBl5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fePR_j3RBl5",
    "outputId": "b44056a1-c000-4560-c872-5f29a3540fff"
   },
   "outputs": [],
   "source": [
    "print(f'token index: {doc[8].idx}, token: {doc[8]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HouW2apXkspF",
   "metadata": {
    "id": "HouW2apXkspF"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2iJGfEW1oI9E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "2iJGfEW1oI9E",
    "outputId": "dc95bc21-d378-4e6f-8605-b42f82f18815"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cVHmKMhqpPPb",
   "metadata": {
    "id": "cVHmKMhqpPPb"
   },
   "source": [
    "The token class has more attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oWvg4Hhlonlz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWvg4Hhlonlz",
    "outputId": "e0bb86f7-623b-4c08-94d2-e93ec1e880dd"
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, token.idx, token.text_with_ws,\n",
    "          token.is_alpha, token.is_punct, token.is_space,\n",
    "          token.shape_, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KruEHTIrpbk2",
   "metadata": {
    "id": "KruEHTIrpbk2"
   },
   "source": [
    "Some of the commonly used attributes:\n",
    "\n",
    " * **`text_with_ws`**: token text with trailing space (if present).\n",
    " * **`is_alpha`**: detects whether the token consists of alphabetic characters or not.\n",
    " * **`is_punct`**: whether the token is a punctuation symbol or not.\n",
    " * **`is_space`**: whether the token is a space or not.\n",
    " * **`shape_`**: the shape of the word.\n",
    " * **`is_stop`**: whether the token is a stop word or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Tke3QryWqfy_",
   "metadata": {
    "id": "Tke3QryWqfy_"
   },
   "source": [
    "<!-- ### Customized tokenization -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Dz6upG64pXAo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dz6upG64pXAo",
    "outputId": "6cffb5fd-2106-47d4-f476-f467fdbd491a"
   },
   "outputs": [],
   "source": [
    "custom_tokenization_text = (\"Toronto-based and short-term are added to \"\n",
    "                            \"illustrate custom tokenization.\")\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "custom_nlp = spacy.load('en_core_web_sm')\n",
    "prefix_re = spacy.util.compile_prefix_regex(custom_nlp.Defaults.prefixes)\n",
    "suffix_re = spacy.util.compile_suffix_regex(custom_nlp.Defaults.suffixes)\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    # Adds support to use `-` as the delimiter for tokenization\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                     suffix_search=suffix_re.search,\n",
    "                     infix_finditer=infix_re.finditer,\n",
    "                     token_match=None\n",
    "                     )\n",
    "\n",
    "\n",
    "custom_nlp.tokenizer = custom_tokenizer(custom_nlp)\n",
    "\n",
    "print('With custom tokenization:')\n",
    "custom_tokenizer_doc = custom_nlp(custom_tokenization_text)\n",
    "print([token.text for token in custom_tokenizer_doc])\n",
    "\n",
    "print('\\nWithout custom tokenization:')\n",
    "doc = nlp(custom_tokenization_text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xf7RGvZVvhAs",
   "metadata": {
    "id": "xf7RGvZVvhAs"
   },
   "source": [
    "## Stop words\n",
    "\n",
    "spaCy has a list of stop words for the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uPZ16v37r47Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uPZ16v37r47Y",
    "outputId": "39abfc8a-b1d6-4a30-e60a-4978b157ac57"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords)\n",
    "\n",
    "for stop_word in list(spacy_stopwords)[:10]:\n",
    "    print(stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-i1GWSe7v8iN",
   "metadata": {
    "id": "-i1GWSe7v8iN"
   },
   "source": [
    "You can remove stop words from the input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qkqsZ5isv9W4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qkqsZ5isv9W4",
    "outputId": "e1d72b50-14cb-4f89-caa3-44cc0a9218bf"
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if not token.is_stop:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DZQGcxDbwNzM",
   "metadata": {
    "id": "DZQGcxDbwNzM"
   },
   "source": [
    "You can also create a list of tokens not containing stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J6gzX-MwwBgu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6gzX-MwwBgu",
    "outputId": "e2f53bef-399d-4dc4-ab77-bca43809d0e4"
   },
   "outputs": [],
   "source": [
    "no_stopword_doc = [token for token in doc if not token.is_stop]\n",
    "print(no_stopword_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CYojOTW2wrWn",
   "metadata": {
    "id": "CYojOTW2wrWn"
   },
   "source": [
    "## Lemmatization\n",
    "\n",
    "spaCy has the attribute `lemma_` on the Token class. This attribute has the lemmatized form of a token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BV-F53_jw9za",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BV-F53_jw9za",
    "outputId": "9ce0026e-ea40-46e6-a58a-2a4c3a45f018"
   },
   "outputs": [],
   "source": [
    "conference_text = ('Rose is helping organize a developer '\n",
    "                   'conference on Legal Applications of Natural Language'\n",
    "                   ' Processing. She keeps organizing local Python meetups'\n",
    "                   ' and several internal talks at her workplace.')\n",
    "conference_doc = nlp(conference_text)\n",
    "print('token\\tlemma\\n-------------------')\n",
    "for token in conference_doc:\n",
    "    print(f'{token}\\t{token.lemma_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "byZBDN-1yVk4",
   "metadata": {
    "id": "byZBDN-1yVk4"
   },
   "source": [
    "## Word Frequency\n",
    "\n",
    "You can now convert a given text into tokens and perform statistical analysis over it. This analysis can give you various insights about word patterns, such as common words or unique words in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KeUQ3EB9xdEA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KeUQ3EB9xdEA",
    "outputId": "ea0e694c-5725-4782-8cbe-1a31cc4057df"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "complete_text = ('Eira is a Python developer currently'\n",
    "                 ' working for a Toronto-based Legaltech company.'\n",
    "                 ' She is interested in learning Natural Language Processing.'\n",
    "                 ' There is a developer conference happening on 21 July'\n",
    "                 ' 2022 in Toronto. It is titled \"Applications of Natural'\n",
    "                 ' Language Processing in Law\". There is a helpline number '\n",
    "                 ' available at +1-1234567891. Eira is helping organize it.'\n",
    "                 ' She keeps organizing local Python meetups and several'\n",
    "                 ' internal talks at her workplace. Eira is also presenting'\n",
    "                 ' a talk. The talk will introduce the reader about \"Use'\n",
    "                 ' cases of Natural Language Processing in Legaltech\".'\n",
    "                 ' Apart from her work, she is very passionate about music.'\n",
    "                 ' Eira is learning to play the Clave. She has enrolled '\n",
    "                 ' herself in the weekend batch of Great Clave Academy.'\n",
    "                 ' Great Clave Academy is situated in Kingston and Toronto'\n",
    "                 ' and has world-class clave instructors.')\n",
    "\n",
    "complete_doc = nlp(complete_text)\n",
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in complete_doc\n",
    "         if not token.is_stop and not token.is_punct]\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# 5 commonly occurring words with their frequencies\n",
    "print('Top-5 most common words:')\n",
    "common_words = word_freq.most_common(5)\n",
    "print('\\t', common_words)\n",
    "\n",
    "# Unique words\n",
    "print('All words that only occurs once in the text:')\n",
    "print('\\t', [word for (word, freq) in word_freq.items() if freq == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WtLkxOJ11XtQ",
   "metadata": {
    "id": "WtLkxOJ11XtQ"
   },
   "source": [
    "## Part of Speech Tagging\n",
    "\n",
    "Part of speech or POS is a grammatical role that explains how a particular word is used in a sentence. Eight common parts of speech:\n",
    "\n",
    " * Noun\n",
    " * Pronoun\n",
    " * Adjective\n",
    " * Verb\n",
    " * Adverb\n",
    " * Preposition\n",
    " * Conjunction\n",
    " * Interjection\n",
    "\n",
    "Part of speech tagging is the process of assigning a POS tag to each token depending on its usage in the sentence. POS tags are useful for assigning a syntactic category like noun or verb to each word.\n",
    "\n",
    "In spaCy, POS tags are available as an attribute on the Token object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V0ef38iAz8nW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0ef38iAz8nW",
    "outputId": "bcd5aa8b-9823-46bd-82bf-89ea6e5c9def"
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(f'{token}\\t{token.tag}\\t{token.pos}\\t{spacy.explain(token.tag_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZvgoSYge2ea_",
   "metadata": {
    "id": "ZvgoSYge2ea_"
   },
   "source": [
    "Here, two attributes of the Token class are accessed:\n",
    "\n",
    " * **`tag_`**: fine-grained part of speech.\n",
    " * **`pos_`**: coarse-grained part of speech.\n",
    "\n",
    "`spacy.explain` gives descriptive details about a particular POS tag. spaCy provides a complete tag list along with an explanation for each tag.\n",
    "\n",
    "Using POS tags, you can extract a particular category of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3Ff25UcR0eGo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Ff25UcR0eGo",
    "outputId": "58fc8df2-8ee3-4fd4-e4dd-4902c038e048"
   },
   "outputs": [],
   "source": [
    "nouns = []\n",
    "adjectives = []\n",
    "for token in complete_doc:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "\n",
    "print('Nouns:')\n",
    "print('\\t', nouns)\n",
    "\n",
    "print('Adjectives:')\n",
    "print('\\t', adjectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G-s-C1zp3WVZ",
   "metadata": {
    "id": "G-s-C1zp3WVZ"
   },
   "source": [
    "This can be used to derive insights, for example, remove the most common nouns, or see which adjectives are used for a particular noun.\n",
    "\n",
    "## Visualization: Using displaCy\n",
    "\n",
    "spaCy comes with a built-in visualizer called displaCy. You can use it to visualize a dependency parse or named entities in a browser or a Jupyter notebook.\n",
    "\n",
    "You can use displaCy to find POS tags for tokens. By default, displayCy will spin a simple web server so that you can see the visualization by opening http://127.0.0.1:5000 in your browser. However, this only works if you run the code on your local machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hVDcyD4D25z1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hVDcyD4D25z1",
    "outputId": "e398e44b-8144-4cca-ffb5-a80224287ecd"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "interest_text = ('We are interested in learning Natural Language Processing.')\n",
    "interest_doc = nlp(interest_text)\n",
    "displacy.serve(interest_doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B5QfABVJBctU",
   "metadata": {
    "id": "B5QfABVJBctU"
   },
   "source": [
    " If are running the code on Google Colab (or some other cloud-based platform) then you need to render the figure in Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aIFzhTfCAQbi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "aIFzhTfCAQbi",
    "outputId": "b7afa9b2-13f0-44c4-e5d3-4644f598b0c3"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "interest_text = ('We are interested in learning Natural Language Processing.')\n",
    "interest_doc = nlp(interest_text)\n",
    "displacy.render(interest_doc, style='dep', jupyter=True, options={'distance': 150})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bW2iqe1RCL06",
   "metadata": {
    "id": "bW2iqe1RCL06"
   },
   "source": [
    "In the image above, each token is assigned a POS tag written just below the token.\n",
    "\n",
    "## Preprocessing Functions\n",
    "\n",
    "A preprocessing function converts text to an analyzable format.\n",
    "You can create a preprocessing function that takes text as input and applies the following operations:\n",
    "\n",
    " * Lowercases the text\n",
    " * Lemmatizes each token\n",
    " * Removes punctuation symbols\n",
    " * Removes stop words\n",
    "\n",
    " Here’s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y7NkbV5PBVRG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7NkbV5PBVRG",
    "outputId": "96a067c3-a2fe-4183-91dc-3319ebc91484"
   },
   "outputs": [],
   "source": [
    "def is_token_allowed(token):\n",
    "    '''\n",
    "    Only allow valid tokens which are not stop words\n",
    "    and punctuation symbols.\n",
    "    '''\n",
    "    if (not token or not token.text.strip() or\n",
    "        token.is_stop or token.is_punct):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def preprocess_token(token):\n",
    "    # Reduce token to its lowercase lemma form\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "complete_filtered_tokens = [preprocess_token(token)\n",
    "    for token in complete_doc if is_token_allowed(token)]\n",
    "    \n",
    "complete_filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y3Ln_ii9CFaO",
   "metadata": {
    "id": "y3Ln_ii9CFaO"
   },
   "source": [
    "Note that the complete_filtered_tokens does not contain any stop word or punctuation symbols and consists of lemmatized lowercase tokens.\n",
    "\n",
    "\n",
    "## Rule-Based Matching Using spaCy\n",
    "\n",
    "Rule-based matching is one of the steps in extracting information from unstructured text. It is used to identify and extract tokens and phrases according to patterns (such as lowercase) and grammatical features (such as part of speech).\n",
    "\n",
    "Rule-based matching can use regular expressions to extract entities (such as phone numbers) from an unstructured text. It is different from extracting text using regular expressions only in the sense that regular expressions don’t consider the lexical and grammatical attributes of the text.\n",
    "\n",
    "With rule-based matching, you can extract a first name and a last name, which are always proper nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xYaXqOE2A7zP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xYaXqOE2A7zP",
    "outputId": "4835d1f5-a582-4c6b-c3f5-17ccf153e6ec"
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "text = ('Hjalmar Turesson has never helped to organize anything, '\n",
    "        ' especially not a developer conference on Legal Applications of '\n",
    "        'Natural Language Processing.')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def extract_full_name(doc):\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    matcher.add('FULL_NAME', [pattern])\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        return span.text\n",
    "\n",
    "doc = nlp(text)\n",
    "extract_full_name(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kcrrPXZO_1hb",
   "metadata": {
    "id": "kcrrPXZO_1hb"
   },
   "source": [
    "In this example, pattern is a list of objects that defines the combination of tokens to be matched. Both POS tags in it are `PROPN` (proper noun). So, the pattern consists of two objects in which the POS tags for both tokens should be `PROPN`. This pattern is then added to `Matcher` using `FULL_NAME` and the the `match_id`. Finally, matches are obtained with their starting and end indexes.\n",
    "\n",
    "You can also use rule-based matching to extract phone numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ESP1DMZt_GzO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ESP1DMZt_GzO",
    "outputId": "7ead586b-8045-4374-bd6a-cbbbafb3773e"
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "conference_org_text = ('There is a developer conference'\n",
    "                       'happening on 21 July 2022 in London, Ontario. '\n",
    "                       'It is titled \"Applications of Natural Language '\n",
    "                       'Processing\". There is a helpline number available '\n",
    "                       'at (519) 123-456')\n",
    "\n",
    "def extract_phone_number(nlp_doc):\n",
    "    pattern = [{'ORTH': '('}, {'SHAPE': 'ddd'},\n",
    "               {'ORTH': ')'}, {'SHAPE': 'ddd'},\n",
    "               {'ORTH': '-', 'OP': '?'},\n",
    "               {'SHAPE': 'ddd'}]\n",
    "    matcher.add('PHONE_NUMBER', [pattern])\n",
    "    matches = matcher(nlp_doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        return span.text\n",
    "\n",
    "conference_org_doc = nlp(conference_org_text)\n",
    "extract_phone_number(conference_org_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8aja68AqaP",
   "metadata": {
    "id": "1e8aja68AqaP"
   },
   "source": [
    "In this example, only the pattern is updated in order to match phone numbers from the previous example. Here, some attributes of the token are also used:\n",
    "\n",
    " * **`ORTH`** gives the exact text of the token.\n",
    " * **`SHAPE`** transforms the token string to show orthographic features.\n",
    " * **`OP`** defines operators. Using `?` as a value means that the pattern is optional, meaning it can match `0` or `1` times.\n",
    "\n",
    "## Dependency Parsing Using spaCy\n",
    "\n",
    "Dependency parsing is the process of extracting the dependency parse of a sentence to represent its grammatical structure. It defines the dependency relationship between *headwords* and their *dependents*. The head of a sentence has no dependency and is called the *root* of the sentence. The verb is usually the head of the sentence. All other words are directly or indirectly linked to the headword.\n",
    "\n",
    "The dependencies can be mapped in a directed graph (DAG) representation:\n",
    "\n",
    " * Words are the nodes.\n",
    " * The grammatical relationships are the edges.\n",
    "\n",
    "Dependency parsing helps you know what role a word plays in the text and how different words relate to each other. It is also used in shallow parsing and *named entity recognition* (NER).\n",
    "\n",
    "Here is how you can use dependency parsing to see the relationships between words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MKzHolNtA9Jl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKzHolNtA9Jl",
    "outputId": "fe2e93ea-b1a6-41ea-91c0-64b1e750d5ef"
   },
   "outputs": [],
   "source": [
    "text = 'Liv is not learning to dance salsa'\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text, token.tag_, token.head.text, token.dep_, '||' , spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iLcJMtxdGDfA",
   "metadata": {
    "id": "iLcJMtxdGDfA"
   },
   "source": [
    "In this example, the sentence contains five relationships:\n",
    "\n",
    " * **`nsubj`**: nominal subject, the subject of the sentence. Its headword is a verb.\n",
    " * **`aux`**: auxiliary word. A function word whose headword is a verb. It expresses categories such as tense, mood, aspect, voice or evidentiality. It is often a verb (which may have non-auxiliary uses as well).\n",
    " * **`dobj`** is the direct object of the verb. Its headword is a verb.\n",
    " * **`neg`**\n",
    " * **`xcomp`**\n",
    " * **`npadvmod`**\n",
    "\n",
    "\n",
    "The [Stanford typed dependencies manual](https://downloads.cs.stanford.edu/nlp/software/dependencies_manual.pdf) is a detailed list of relationships with descriptions. You can use displaCy to visualize the dependency tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DDJvghmgF9a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "DDJvghmgF9a0",
    "outputId": "6c323168-a3f6-4ed2-a208-7eb3887ea0c6"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "khQEBMmWJCC8",
   "metadata": {
    "id": "khQEBMmWJCC8"
   },
   "source": [
    "This image shows you that the subject of the sentence is the proper noun `Liv` and that it has a `learning` relationship with `dance`.\n",
    "\n",
    "### Navigating the Tree and Subtree\n",
    "\n",
    "The dependency parse tree has all the properties of a [tree](https://en.wikipedia.org/wiki/Tree_(data_structure)). This tree contains information about sentence structure and grammar and can be traversed in different ways to extract relationships.\n",
    "\n",
    "spaCy provides attributes like children, lefts, rights, and subtree to navigate the parse tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SMQzgycQI71E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SMQzgycQI71E",
    "outputId": "0efe6269-7407-4279-fb78-5378c43fd0ac"
   },
   "outputs": [],
   "source": [
    "salsa_text = ('Liv Ernerdahl is a professional salsa dancer '\n",
    "              'currenly performing in the kitchen')\n",
    "salsa_doc = nlp(salsa_text)\n",
    "# Extract children of `dancing`\n",
    "print(f'Children of {salsa_doc[8]}:', [token.text for token in salsa_doc[8].children])\n",
    "\n",
    "# Extract previous neighboring node of `dancing`\n",
    "print(f'One neighbour of {salsa_doc[8]}:',salsa_doc[8].nbor(-1))\n",
    "\n",
    "# Extract next neighboring node of `dancing`\n",
    "print(salsa_doc[8].nbor())\n",
    "\n",
    "# Extract all tokens on the left of `dancing`\n",
    "print([token.text for token in salsa_doc[8].lefts])\n",
    "\n",
    "# Extract tokens on the right of `dancer`\n",
    "print([token.text for token in salsa_doc[8].rights])\n",
    "\n",
    "# Print subtree of `dancing`\n",
    "print(list(salsa_doc[8].subtree))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86rFXio9LHQO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "86rFXio9LHQO",
    "outputId": "0855fdaf-c661-41b7-f7c2-e46616cc8e24"
   },
   "outputs": [],
   "source": [
    "displacy.render(salsa_doc, style='dep', jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xlMuKjzzNUUV",
   "metadata": {
    "id": "xlMuKjzzNUUV"
   },
   "source": [
    "You can construct a function that takes a subtree as an argument and returns a string by merging words in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8FoYsWBYLNYo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FoYsWBYLNYo",
    "outputId": "5aabab48-1d1b-4233-a5d2-4c377259c124"
   },
   "outputs": [],
   "source": [
    "def flatten_tree(tree):\n",
    "    return ''.join([token.text_with_ws for token in list(tree)]).strip()\n",
    "\n",
    "# Print flattened subtree of `developer`\n",
    "print(flatten_tree(salsa_doc[8].subtree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jczZDDTTNj2V",
   "metadata": {
    "id": "jczZDDTTNj2V"
   },
   "source": [
    "\n",
    "You can use this function to print all the tokens in a subtree.\n",
    "\n",
    "### Shallow Parsing\n",
    "\n",
    "Shallow parsing, or chunking, is the process of extracting phrases from unstructured text. Chunking groups of adjacent tokens into phrases on the basis of their POS tags. There are some standard well-known chunks such as noun phrases, verb phrases, and prepositional phrases.\n",
    "\n",
    "#### Noun Phrase Detection\n",
    "\n",
    "A *noun phrase* is a phrase that has a noun as its head. It could also include other kinds of words, such as adjectives, ordinals, determiners. Noun phrases are useful for explaining the context of the sentence. They help you infer what is being talked about in the sentence.\n",
    "\n",
    "spaCy has the property `noun_chunks` in the `Doc` object. You can use it to extract noun phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MQYauaE6Lhgy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQYauaE6Lhgy",
    "outputId": "a91445f4-4a59-45c7-e876-4f749638f49c"
   },
   "outputs": [],
   "source": [
    "conference_text = ('There is a developer conference '\n",
    "                   'happening on 21 July 2022 in London, Ontario. ')\n",
    "conference_doc = nlp(conference_text)\n",
    "# Extract Noun Phrases\n",
    "for chunk in conference_doc.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dV81lCGPa6K",
   "metadata": {
    "id": "1dV81lCGPa6K"
   },
   "source": [
    "By looking at noun phrases, you can get information about your text. For example, `a developer conference` indicates that the text mentions a conference, while the date `21 July` lets you know that conference is scheduled for 21 July. You can figure out whether the conference is in the past or the future. `London` tells you that the conference is in London.\n",
    "\n",
    "#### Verb Phrase Detection\n",
    "\n",
    "A *verb phrase* is a syntactic unit composed of at least one verb. This verb can be followed by other chunks, such as noun phrases. Verb phrases are useful for understanding the actions that nouns are involved in.\n",
    "\n",
    "spaCy has no built-in functionality to extract verb phrases, so you’ll need a library called [textacy](https://textacy.readthedocs.io/en/latest/):\n",
    "\n",
    "```\n",
    "pip install textacy thinc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZsO_TQ_eMFUr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsO_TQ_eMFUr",
    "outputId": "20ad644b-e855-4020-ae65-07ff7e964943"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0SEjpA0nQNIz",
   "metadata": {
    "id": "0SEjpA0nQNIz"
   },
   "source": [
    "With `textacy` installed, you can use it to extract verb phrases based on grammar rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VVTknynMMKse",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVTknynMMKse",
    "outputId": "ec537bcc-7877-4731-8ac0-ff45f764039a"
   },
   "outputs": [],
   "source": [
    "import textacy\n",
    "# text = \"All living things are made of cells. Cells have organelles.\"\n",
    "talk_text = ('The talk will introduce the reader to use '\n",
    "             'cases of Natural Language Processing in Legaltech')\n",
    "verb_patterns = [[{\"POS\":\"AUX\"}, {\"POS\":\"VERB\"}, {\"POS\":\"ADP\"}], \n",
    "                 [{\"POS\": \"VERB\"}, {\"POS\":\"NOUN\"}],\n",
    "                 [{\"POS\":\"VERB\"}]]\n",
    "\n",
    "talk_doc = textacy.make_spacy_doc(talk_text, lang='en_core_web_sm')\n",
    "verb_phrases = textacy.extract.token_matches(talk_doc, verb_patterns)\n",
    "\n",
    "# Print all Verb Phrase\n",
    "print('Verb phrases:')\n",
    "for chunk in verb_phrases:\n",
    "    print(f'\\t{chunk.text}')\n",
    "\n",
    "# Extract Noun Phrase to explain what nouns are involved\n",
    "print('Noun phrases:')\n",
    "for chunk in talk_doc.noun_chunks:\n",
    "    print(f'\\t{chunk}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rJdJY0raS7-2",
   "metadata": {
    "id": "rJdJY0raS7-2"
   },
   "source": [
    "In this example, the verb phrase `introduce` indicates that something will be introduced. By looking at noun phrases, you can see that there is `The talk` (NP) that will `introduce` (VP) `the reader` (NP) to `use cases` (VP) of `Natural Language Processing` (NP) or `Legaltech` (NP).\n",
    "\n",
    "The above code extracts all the verb phrases using a regular expression pattern of POS tags. You can tweak the pattern for verb phrases depending upon your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BP3CyiU6OZNX",
   "metadata": {
    "id": "BP3CyiU6OZNX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
