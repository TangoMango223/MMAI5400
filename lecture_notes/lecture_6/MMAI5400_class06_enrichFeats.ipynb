{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enriched feature engineering for NLP\n",
    "\n",
    "By HK Turesson\n",
    "\n",
    "This tutorial explores how to enrich BOW representations with non-standard features such as part-of-speech (POS) tags, dependencies, word shapes, etc. \n",
    "\n",
    "We will use [spaCy](https://spacy.io/) - an advanced NLP library - to enrich the documents.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1hNukC5o2br"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spaCy's English pipeline\n",
    "[`en_core_web_sm`](https://spacy.io/models/en#en_core_web_sm) is an English spaCy pipeline optimized for CPU ([see here](https://spacy.io/models/en#en_core_web_sm) for details). It's components are: `tok2vec`, `tagger`, `parser`, `senter`, `ner`, `attribute_ruler`, `lemmatizer`.\n",
    "`en_core_web_sm` is already installed on Google Colab, however if get an error when loading it try downloading with `python -m spacy download en_core_web_sm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFUDuE4QpZrN"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZODRcbVo352"
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxprlGEapKrD",
    "outputId": "72bd1eb6-5ad9-45dc-83a8-57e640560b50",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Text\\t\\tLemma\\tPOS\\tTag\\tDep\\tShape\\talpha\\tstop')\n",
    "print('-'*80)\n",
    "for token in doc:\n",
    "    print(f'{token.text}\\t\\t{token.lemma_}\\t{token.pos_}\\t{token.tag_}\\t{token.dep_}\\t{token.shape_}\\t{token.is_alpha}\\t{token.is_stop}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See spaCy's [linguistic features documentation](https://spacy.io/usage/linguistic-features) for full explaination.\n",
    "\n",
    "## Data\n",
    "\n",
    "We will use the dataset [BANKING77](https://huggingface.co/datasets/PolyAI/banking77).\n",
    "BANKING77 is composed of online banking queries annotated with their corresponding intents. It provides a very fine-grained set of intents in the banking domain. It comprises 13,083 customer service queries labelled with 77 intents. It focuses on fine-grained single-domain intent detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bs1y0xoSrmff",
    "outputId": "ba5233d8-bc68-4b48-92b4-f140dc8146a1"
   },
   "outputs": [],
   "source": [
    "!unzip banking_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Read `train.csv` and `test.csv,` storing the data with the names `train_data` and `test_data,` respectively.\n",
    "\n",
    "**Tutorial question 1**: What is the last text in `train_data`?\n",
    "\n",
    "**Tutorial question 2**: How many unique classes are in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9B45yldPo81Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "75DNmS2asi8X",
    "outputId": "7ffe0b3b-7780-4c2c-e2ff-e46b1eb3c82b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Applying spaCy's `nlp()` pipeline to a document takes a bit of time. If possible, it is best to only do it once. Thus, we'll do it once, store the output in `train_docs` and `test_docs` and then use these pre-computed lists repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrLoIap-sk3U"
   },
   "outputs": [],
   "source": [
    "train_docs, test_docs = [], []\n",
    "\n",
    "for i, row in train_data.iterrows():\n",
    "  train_docs.append(nlp(row['text']))\n",
    "\n",
    "for i, row in test_data.iterrows():\n",
    "  test_docs.append(nlp(row['text']))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uALL3CcY3RZD"
   },
   "source": [
    "### Helper function to enrich features\n",
    "\n",
    "Concatenating the linguistic features into a new long string (i.e. un-tokenized document) and then tokenizing it again using sklearn's `TfidfVectorizer` is a bit hacky. However, here we do it for educational puproses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1vAypVztzhZ"
   },
   "outputs": [],
   "source": [
    "def enrich_features(docs, features):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "        docs     : A list of outputs from spaCy's nlp()\n",
    "        features : A dictionary with the following keys\n",
    "                    'keep_noalpha', \n",
    "                    'rm_stop',\n",
    "                    'text',\n",
    "                    'lemma',\n",
    "                    'pos',\n",
    "                    'tag',\n",
    "                    'dep',\n",
    "                    'shape'\n",
    "                   and boolean values.\n",
    "    \n",
    "                   E.g.:\n",
    "                       features = {\n",
    "                        'keep_noalpha': False,\n",
    "                        'rm_stop': True,\n",
    "                        'text': False,\n",
    "                        'lemma': True,\n",
    "                        'pos': False,\n",
    "                        'tag': True,\n",
    "                        'dep': False,\n",
    "                        'shape': False}\n",
    "    Return\n",
    "    ------\n",
    "    enriched : A list of enriched docs.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    enriched = []\n",
    "    \n",
    "    for doc in docs:\n",
    "      \n",
    "        enriched_doc = ''\n",
    "          \n",
    "        for token in doc:\n",
    "            \n",
    "            enriched_token = ''\n",
    "            \n",
    "            if features['keep_noalpha'] or token.is_alpha:\n",
    "              \n",
    "                if not (features['rm_stop'] and token.is_stop):        \n",
    "                  \n",
    "                    if features['text']:\n",
    "                        enriched_token = f'{enriched_token}{token.text}'\n",
    "                    if features['lemma']:\n",
    "                        enriched_token = f'{enriched_token}{token.lemma_}'\n",
    "                    if features['pos']:\n",
    "                        enriched_token = f'{enriched_token}{token.pos_}'\n",
    "                    if features['tag']:\n",
    "                        enriched_token = f'{enriched_token}{token.tag_}'                  \n",
    "                    if features['dep']:\n",
    "                        enriched_token = f'{enriched_token}{token.dep_}'\n",
    "                    if features['shape']:\n",
    "                        enriched_token = f'{enriched_token}{token.shape_}'                  \n",
    "                \n",
    "                    enriched_doc = f'{enriched_doc} {enriched_token}'\n",
    "                    \n",
    "        enriched.append(enriched_doc)\n",
    "    \n",
    "    return enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYkqZ04O3xxe"
   },
   "outputs": [],
   "source": [
    "features = {\n",
    "    'keep_noalpha': False,\n",
    "    'rm_stop': True,\n",
    "    'text': False,\n",
    "    'lemma': True,\n",
    "    'pos': False,\n",
    "    'tag': True,\n",
    "    'dep': False,\n",
    "    'shape': False}\n",
    "train = enrich_features(train_docs, features)\n",
    "test = enrich_features(test_docs, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VKrHOeRxurqM",
    "outputId": "1e628449-5227-4231-cde3-ef7a2737ada4"
   },
   "outputs": [],
   "source": [
    "train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "leNV95eQ0eZ5",
    "outputId": "48393a55-e582-43b4-d245-57b6abdabce5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize again\n",
    "\n",
    "**Task**: Use sklearn's [`TfidfVectorizer`](https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#) to vectorize `train` and `test`, storing the outputs in `X_train` and `X_test`, respectively.\n",
    "\n",
    "Set `lowercase` to `False` `stop_words` to `None` and `use_idf` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1F8G9RF1LHv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBcCNMhv1450"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial question 3**: How many features are there in `X_train` (i.e. what is $|V|$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mUHXRzy_2lYg",
    "outputId": "72877f8d-9945-4a55-d40b-a711b09a7dcd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial question 4**: What is the 23rd token in $V$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial question 5**: What is the POS associated with that token?\n",
    "\n",
    "## Text classification\n",
    "\n",
    "Here, we focus on feature enrichment and not the learner. Thus, we'll stick with one learner (Multinomial Naive Bayes) and default hyperparameters.\n",
    "\n",
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s11AfDLE2vMq"
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train, train_data['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLJvzhKD28Kt"
   },
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test)\n",
    "\n",
    "print('Test set accuracy:', (preds == test_data['category']).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kroz104T5RhF",
    "outputId": "2b051da4-a1a7-422f-dac8-5c54b931571a"
   },
   "source": [
    "**Tutorial question 6**: What is the test set accuracy?\n",
    "\n",
    "**Task**: Combine the above steps (`enrich_feathers`, `TfidfVectorizer`, training and evaluation) into a pipline called `pipeline`.\n",
    "`pipeline()` should take `train_docs`, `test_docs`, and `features` as arguments and return the accuracy. Make sure that it can handle empty docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(train_docs, test_docs, features):\n",
    "    \n",
    "    train = enrich_features(train_docs, features)\n",
    "    test = enrich_features(test_docs, features)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(lowercase=False, stop_words=None, use_idf=True)\n",
    "\n",
    "    try:\n",
    "        X_train = vectorizer.fit_transform(train)\n",
    "        X_test = vectorizer.transform(test)\n",
    "\n",
    "        clf = MultinomialNB().fit(X_train, train_data['category'])\n",
    "    \n",
    "        preds = clf.predict(X_test)\n",
    "    \n",
    "        acc = (preds == test_data['category']).mean()    \n",
    "        \n",
    "    except:\n",
    "\n",
    "        acc = 0\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Find the best feature combination by training and evaluating models on all possible combinations. Store the feature configurations and accuracies in a list called `configs`. Don't forget to use `features.copy()` when storing the feature configurations in `configs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZJVKauc40eL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial question 7**: What is the best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "No7rM59q6hkK",
    "outputId": "6aa6ea53-b18e-4d5e-c84c-fcf7bc8e8aab"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMGQdm1j48xQ",
    "outputId": "ecc7a015-adb3-44f7-f7b5-f1a97c982abf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nu2f2zdn5x3-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome To Colab",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
